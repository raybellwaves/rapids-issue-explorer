[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1923269108",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1923269108",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366",
        "id": 1923269108,
        "node_id": "IC_kwDOD7z77c5yosH0",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-02T07:55:23Z",
        "updated_at": "2024-02-02T08:06:51Z",
        "author_association": "COLLABORATOR",
        "body": "Spark supports [two algorithms generating the bucket id](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala#L123). We'd better support both of them.\r\n```\r\n      if (options.getOrElse(BucketingUtils.optionForHiveCompatibleBucketWrite, \"false\") ==\r\n        \"true\") {\r\n        // Hive bucketed table: use `HiveHash` and bitwise-and as bucket id expression.\r\n        // Without the extra bitwise-and operation, we can get wrong bucket id when hash value of\r\n        // columns is negative. See Hive implementation in\r\n        // `org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#getBucketNumber()`.\r\n        val hashId = BitwiseAnd(HiveHash(bucketColumns), Literal(Int.MaxValue))\r\n        val bucketIdExpression = Pmod(hashId, Literal(spec.numBuckets))\r\n\r\n        // The bucket file name prefix is following Hive, Presto and Trino conversion, so this\r\n        // makes sure Hive bucketed table written by Spark, can be read by other SQL engines.\r\n        //\r\n        // Hive: `org.apache.hadoop.hive.ql.exec.Utilities#getBucketIdFromFile()`.\r\n        // Trino: `io.trino.plugin.hive.BackgroundHiveSplitLoader#BUCKET_PATTERNS`.\r\n        val fileNamePrefix = (bucketId: Int) => f\"$bucketId%05d_0_\"\r\n        WriterBucketSpec(bucketIdExpression, fileNamePrefix)\r\n      } else {\r\n        // Spark bucketed table: use `HashPartitioning.partitionIdExpression` as bucket id\r\n        // expression, so that we can guarantee the data distribution is same between shuffle and\r\n        // bucketed data source, which enables us to only shuffle one side when join a bucketed\r\n        // table and a normal one.\r\n        val bucketIdExpression = HashPartitioning(bucketColumns, spec.numBuckets)\r\n          .partitionIdExpression\r\n        WriterBucketSpec(bucketIdExpression, (_: Int) => \"\")\r\n      }\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1923269108/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924063095",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1924063095",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366",
        "id": 1924063095,
        "node_id": "IC_kwDOD7z77c5yrt93",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-02T15:05:20Z",
        "updated_at": "2024-02-02T15:05:20Z",
        "author_association": "COLLABORATOR",
        "body": "Hash partitioning is simple because we already support it. To support the other one we would need to implement the HiveHash expression. That would be a new kernel.\r\n\r\nhttps://github.com/apache/spark/blob/25d96f7bacb43a7d5a835454ecc075e40d4f3c93/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/hash.scala#L686-L1035\r\n\r\nIs the Spark code for this. The core hash function itself is very simple.\r\n\r\nints are unchanged\r\nlongs are computed as `(int) ((input >>> 32) ^ input);`\r\na byte array uses\r\n```\r\n    int result = 0;\r\n    for (int i = 0; i < lengthInBytes; i++) {\r\n      result = (result * 31) + (int) Platform.getByte(base, offset + i);\r\n    }\r\n    return result;\r\n```\r\nfor timestamps it is\r\n```\r\n    val timestampInSeconds = MICROSECONDS.toSeconds(timestamp)\r\n    val nanoSecondsPortion = (timestamp % MICROS_PER_SECOND) * NANOS_PER_MICROS\r\n\r\n    var result = timestampInSeconds\r\n    result <<= 30 // the nanosecond part fits in 30 bits\r\n    result |= nanoSecondsPortion\r\n    ((result >>> 32) ^ result).toInt\r\n```\r\n\r\nBut for decimal numbers they convert it to a BigDecimal, normalize the value, and then call `hashCode` on it. I think in the short term we will not be able to support decimal values as the BigDecimal implementation is GPL licensed so we cannot copy it. \r\n\r\nThe hard parts are with how some data is normalized before it is hashed.\r\n\r\nWe are also going to have problems with Maps. The way maps are hashed it depends on the order that they are stored. Maps are not ordered so we are not likely to get the same result and it might be best to just not say that we support them.\r\n\r\nWe also don't really support CalendarInterval very well, so we probably want to say at the beginning that don't support it.\r\n\r\nOr we could just fall back to the CPU if it is a hive compatible bucketing and file a follow on for the rest.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924063095/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924068613",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1924068613",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366",
        "id": 1924068613,
        "node_id": "IC_kwDOD7z77c5yrvUF",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-02-02T15:08:30Z",
        "updated_at": "2024-02-02T15:08:30Z",
        "author_association": "MEMBER",
        "body": "Note that the normal spark hash bucketing feature request is already tracked by #22.  We should either morph this into \"I want Hive-style bucketing\" and leave #22 to track normal bucketing, or make this a bucket write epic with the Hive and Spark hashing implementations as subtasks.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924068613/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]