[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697573233",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697573233",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697573233,
        "node_id": "IC_kwDOD7z77c5lLulx",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T14:36:35Z",
        "updated_at": "2023-08-29T14:36:35Z",
        "author_association": "COLLABORATOR",
        "body": "I got an nsys trace of it happening, and it looks like a join gather can do a lot of work without having the GPU semaphore. I saw all 12 threads doing things, many of them in join gathers without the semaphore held.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697573233/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697576750",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697576750",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697576750,
        "node_id": "IC_kwDOD7z77c5lLvcu",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T14:38:28Z",
        "updated_at": "2023-08-29T14:38:28Z",
        "author_association": "COLLABORATOR",
        "body": "I should correct this a bit. It was not all 12 threads. Some of the threads were \"unscheduled\" but I did see some running without the GPU semaphore.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697576750/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697695334",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697695334",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697695334,
        "node_id": "IC_kwDOD7z77c5lMMZm",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T15:38:36Z",
        "updated_at": "2023-08-29T15:38:36Z",
        "author_association": "COLLABORATOR",
        "body": "A little more information. The join itself was not the problem, although it showed up in the metrics as a problem because of the semaphore not being grabbed. Most of the time the join didn't even start to do anything. The problem appears to be the parquet writes. When 4 of them would go at once, we have enough memory to do the write, but only if we keep switching it between different threads. But the device synchronize is a very expensive way of doing this. I'm not 100% sure we can fix the problem without some help from the team doing the ASYNC allocator.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697695334/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697722228",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697722228",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697722228,
        "node_id": "IC_kwDOD7z77c5lMS90",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T15:52:34Z",
        "updated_at": "2023-08-29T15:52:34Z",
        "author_association": "COLLABORATOR",
        "body": "> But the device synchronize is a very expensive way of doing this. I'm not 100% sure we can fix the problem without some help from the team doing the ASYNC allocator.\r\n\r\nIs the time to synchronize not accounted for in the `spillTime` metric? Or do you see that being high as well.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697722228/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697732489",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697732489",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697732489,
        "node_id": "IC_kwDOD7z77c5lMVeJ",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T15:58:12Z",
        "updated_at": "2023-08-29T15:58:12Z",
        "author_association": "COLLABORATOR",
        "body": "If I switch the target batch size from 2g to 1g it fixes the problem because it drops the memory pressure (took 100.279 seconds instead of 5+ mins)\r\nIf I switch the parallelism to 3 instead of 4 it also mostly fixes the problem (took 131.437 seconds)\r\n\r\nBoth together is even faster still 96.760 seconds.\r\n\r\nOddly if I drop the total memory allowed to 16 GiB instead of the full 48 available it is still faster to spill to host memory in this case than to continually do the device synchronize (247.655 seconds).\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697732489/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697742087",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697742087",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697742087,
        "node_id": "IC_kwDOD7z77c5lMX0H",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T16:03:57Z",
        "updated_at": "2023-08-29T16:03:57Z",
        "author_association": "COLLABORATOR",
        "body": "> Is the time to synchronize not accounted for in the `spillTime` metric? Or do you see that being high as well.\r\n\r\nWhy would it be a part of spillTime?  We don't grab the semaphore except when we are reading back in spilled data. I \"fixed\" the semaphore problem by just grabbing it in hasNext for the join iterator.\r\n\r\n```\r\ndiff --git a/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala b/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala\r\nindex 76f6c1ed9..92003ac46 100644\r\n--- a/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala\r\n+++ b/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala\r\n@@ -85,6 +85,7 @@ abstract class AbstractGpuJoinIterator(\r\n     if (closed) {\r\n       return false\r\n     }\r\n+    GpuSemaphore.acquireIfNecessary(TaskContext.get())\r\n     var mayContinue = true\r\n     while (nextCb.isEmpty && mayContinue) {\r\n       if (gathererStore.exists(!_.isDone)) {\r\n```\r\n\r\nThis is arguably not the right place to do this because we might grab it in some situations where we don't need to, but it is better than not grabbing it at the right time. I think there are likely a number of other places where we need to grab it too. I'll try to come up with a patch for it generally.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697742087/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697746619",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697746619",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697746619,
        "node_id": "IC_kwDOD7z77c5lMY67",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T16:06:33Z",
        "updated_at": "2023-08-29T16:06:33Z",
        "author_association": "COLLABORATOR",
        "body": "I was thinking about the synchronize within this `spillTime` counter:\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/branch-23.10/sql-plugin/src/main/scala/com/nvidia/spark/rapids/DeviceMemoryEventHandler.scala#L120\r\n\r\nWe start the `spillTime` on line 120, and the synchronize happens on line 138, and we stop the timer on line 170. You were wondering why this time wasn't in the Spark UI metrics, and so I am curious if this is the same synchronize or something different.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697746619/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697998078",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697998078",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1697998078,
        "node_id": "IC_kwDOD7z77c5lNWT-",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T19:17:20Z",
        "updated_at": "2023-08-29T19:17:20Z",
        "author_association": "COLLABORATOR",
        "body": "I don't think that it is the deviceSynchronize that is causing problems. I think that this is the perfect storm in terms of memory allocation. If I disable the device synchronize the performance is much worse. If I try to use arena the performance is much worse. If I use no pooling the performance is not great, but at least it is consistent.\r\n\r\nI got an nsys trace of a bad run and a good run (where I tuned the parallelism and batch size).  The CUDA API summary report is quite interesting.\r\n\r\nOn the bad run `cudaDeviceSynchronize` took 5.3 seconds, and the good run never called it.  But 5 seconds does not explain much about the time taken. I am going to ignore `cudaStramSynchronize_ptsz`, and instead look at the next highest time for the slow run`cudaMallocFromPoolAsync_ptsz_v11020`. On the slow run this took 702.952 seconds vs 28.798 seconds for the good run. What is more the good run actually called it more often, because the batch size is smaller. That is enough to even offset the number or retried calls. So the average time per call changed from 0.0057 ms to 0.1541 ms. That is 27x slower. The other memory related APIs are also similarly slower. I think that as the GPU fills up the allocator has to do more work to find space for an allocation, which slows the allocations down by a lot.\r\n\r\nBut I am not 100% sure on that. Because it is not just the cuda*Aync memory APIs that are slower. cudaMallocHost is 10x slower. cudaFreeHost is 3x slower.  The kernels are actually a little faster, but not massively so. I think the speedup is because less of them are running, fewer batches. \r\n\r\nIt is almost as if interacting with the page table on the GPU from the CPU is the bottleneck. Like we are locking it when we try to do an allocation, and for large GPUs it takes forever to find a free page, and in turn slows all of the other allocations down.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697998078/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175203",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1699175203",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130",
        "id": 1699175203,
        "node_id": "IC_kwDOD7z77c5lR1sj",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T13:26:37Z",
        "updated_at": "2023-08-30T13:26:37Z",
        "author_association": "COLLABORATOR",
        "body": "I filed https://github.com/NVIDIA/spark-rapids/issues/9139 to look at and fix the semaphore problem properly.  I honestly don't know how to fix the performance problems here without a major change to how we deal with low memory setups. I don't know if the is something the cuda team can do, or if we need a way to detect that we are in a situation like this and try to throttle down the parallelism like we do with split and retry.\r\n\r\nIt is also looks like retry might be too aggressive with trying to have threads start to run again, because if I disable the device synchronize and just rely on retry to throttle the threads it is not doing it very well. The performance is worse than the device synchronize, and far worse than running with 3 in parallel to begin with.\r\n\r\nPerhaps also there are metrics we could produce to detect this type of situation so the auto-tuner could also help avoid this type of situation. ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175203/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]