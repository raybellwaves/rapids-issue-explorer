[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538470394",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1538470394",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1538470394,
        "node_id": "IC_kwDOD7z77c5bszH6",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-08T14:37:33Z",
        "updated_at": "2023-05-08T14:37:33Z",
        "author_association": "COLLABORATOR",
        "body": "Spilling to disk is done to keep the host store at a specific target size. So when the host store is full, this pauses host spills as well.\r\n\r\nThe `RapidsBufferCatalog` triggers the spill to disk here https://github.com/NVIDIA/spark-rapids/blob/branch-23.06/sql-plugin/src/main/scala/com/nvidia/spark/rapids/RapidsBufferCatalog.scala#L527, for the host store (the only store that has a maximum size as of today).\r\n\r\nSo I think instead of spilling (and after detecting that we are with `RmmRapidsRetryIterator`, so we need to set some sort of thread local state here), we would need to throw a special exception instead of spilling to disk, such that the JNI adapter could handle it and keep state for this thread that has \"wanted to spill to disk N times\". The `onAllocFailure` callback in `DeviceMemoryEventHandler` would need to get back some state from the state machine saying that it already paused this thread due to a prior disk spill attempt, and this time it means it so it should let it through.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538470394/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538653093",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1538653093",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1538653093,
        "node_id": "IC_kwDOD7z77c5btful",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-08T16:03:10Z",
        "updated_at": "2023-05-08T16:03:10Z",
        "author_association": "COLLABORATOR",
        "body": "Just a few notes about this. This is not supposed to produce production level code.  We have a lot of tech debt to pay off first in the spill framework before we get to putting this into production. I would love to have you do some experiments to see if this even makes since to go this route. If it does, then we can go back and figure out a long term plan to make it happen cleanly. When putting together changes for the experiments you don't have to worry about the UCX shuffle for the state machine, and you can hack up the spill code just to make something work so we can get an idea of what the performance might look like. You also don't have to worry about GPU Direct Storage for the spill code.\r\n\r\nThe current code will do the following to avoid an OOM\r\n\r\n1. Spill to host memory\r\n2. Spill to disk\r\n3. pause the current thread\r\n4. throw retry exception on lowest priority thread\r\n5. throw split and retry exception\r\n\r\nWe want to change the order of these so that the priority is\r\n\r\n1. Spill to host memory\r\n2. pause the current thread\r\n3. throw retry exception on lowest priority thread\r\n4. spill to disk\r\n5. throw split and retry exception\r\n\r\nTo do this I think the big change would have to be that the spill [callback](https://github.com/rapidsai/cudf/blob/0a5065fe03c40977016febb4b9f324f4902fa0dd/java/src/main/native/src/RmmJni.cpp#L163) code would have to move to the [state machine](https://github.com/NVIDIA/spark-rapids-jni/blob/branch-23.06/src/main/cpp/src/SparkResourceAdaptorJni.cpp) in Spark Rapids JNI. We would also have to provide an API to the callback that lets the spill code know if it is allowed to spill all the way to disk, or just spill to host memory. Then the state machine when an allocation fails it would first call the spill code saying spill only to host memory. If that succeeds and some memory is spilled/freed, then we retry the allocation, if nothing was freed then we got through the pause code like we do today. Exception if all of the threads are paused instead of throwing an `SplitAndRetryOOM` we try to spill again, this time we allow it to spill to disk. If that works, then we can go on (retry the allocation). If it does not work, then we throw the `SplitAndRetryOOM`.\r\n\r\nFor testing I really would like to see the performance of running a query that would normally spill to disk before and after the patch. I think we can do a lot of this with NDS queries and adjusting the maximum memory that the GPU has access to. The config `spark.rapids.memory.gpu.allocSize` is intended to only be used with testing, and it should give you a good approximation of running on a GPU with much lower memory than it actually has. The idea is that it is probably cheaper to pause a thread, and drop parallelism than it is to spill to disk.  But the way our operators work, and how they use memory does not make it super clear if that is true all the time. Especially with really fast disks. So because of that I would love to see the impact of testing this on an on prem cluster with fast disks, but I would also love to see it run on some other cloud where we know that the disk speeds are relatively slow (or at least are slow by default without adding NVMe store to them).",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538653093/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1606398999",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1606398999",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1606398999,
        "node_id": "IC_kwDOD7z77c5fv7QX",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-26T01:27:58Z",
        "updated_at": "2023-06-27T01:40:52Z",
        "author_association": "COLLABORATOR",
        "body": "Figured out a way to do the POC things. Here are the two relevant changes in my personal repos.\r\n\r\nhttps://github.com/firestarman/spark-rapids-jni/pull/1/files\r\nhttps://github.com/firestarman/spark-rapids/pull/6/files\r\n\r\nNOTE: We are NOT going to merge these and they are only used for POC.\r\n\r\nWill run the perf tests next.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1606398999/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612537817",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1612537817",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1612537817,
        "node_id": "IC_kwDOD7z77c5gHV_Z",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-29T07:16:08Z",
        "updated_at": "2023-07-03T06:05:03Z",
        "author_association": "COLLABORATOR",
        "body": "So far I have tried a customer case where the spilling indeed happened on spark2a, but did not get any perf improvement. \r\nWhat's more, I got some OOM errors when running with the two PRs. Will investgate more.\r\n\r\nAnd I ran the NDS on Dataproc (Spilling happened with setting `allocSize` to 8G on Tesla 4) for this and dit not get any perf improvement either.\r\n \r\n[numbers](https://docs.google.com/spreadsheets/d/1-hmZTTslpWlEONSaKcjCXHqcTfGQRs_SJ0BiaRNEWpE/edit#gid=1228411614)",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612537817/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612657472",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1612657472",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1612657472,
        "node_id": "IC_kwDOD7z77c5gHzNA",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-29T08:53:38Z",
        "updated_at": "2023-07-03T06:04:49Z",
        "author_association": "COLLABORATOR",
        "body": "Pls correct me if wrong.\r\nIn theory, if the new pipeline wants to get better performance, we may need to have a good runtime data size that can be all held by the host memory store, which can avoid the disk writing during the spilling process to run faster. Otherwise, disk writing is still necessary in the new pipeline.\r\n\r\nFor most cases, seems it is impossible to have such good data size during runtime, then we may not get better performance.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612657472/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1613178748",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1613178748",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1613178748,
        "node_id": "IC_kwDOD7z77c5gJyd8",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-29T13:24:17Z",
        "updated_at": "2023-06-29T13:24:17Z",
        "author_association": "COLLABORATOR",
        "body": "@firestarman The experiment where there is lots of spilling is informative.  I was hoping that there would be a reduction in the total data spilled to disk, and hence a reduction in the runtime of the query.\r\n\r\nI think we want to start out with some kind of a synthetic load.  We know that it should have no impact on performance if there is no spill at all, but I think we can come up with a few interesting situations to see what happens.\r\n\r\nI would like to see times for running NDS (just because it is a good synthetic load).  Then we can start to adjust `spark.rapids.memory.gpu.allocSize` to artificially reduce the GPUs memory to induce a spill and also adjust `spark.rapids.memory.pinnedPool.size` and `spark.rapids.memory.host.spillStorageSize` to see what the impact would be. We should keep the concurrency at a set amount, ideally 4 so there are multiple threads to pause, but 2 works too.\r\n\r\nI would love to see what the total `gpuSpillBlockTime`, `gpuSpillReadTime`, `gpuSemaphoreWait`, `gpuRetryBlockTime`, `gpuRetryCount`, and `gpuSplitAndRetryCount` in addition to the total run time, memory spill and disk spill for different settings. The first set of tests would be a baseline to measure the impact of spilling to memory on the runtime of the NDS. We start with setting where there is no spilling because everything fits in GPU memory.  Then we reduce the GPU memory and increase the host spill storage size at varying amounts keeping the total memory available for storage the same. The amount of memory for the GPU should not get below 4 to 6 GiB, just because I to be sure there is enough memory for a single task to execute on the GPU without any issues. We should try this with both your patch and the baseline jar. Because there would be no spill to disk there should be no retries happen, no OOM errors or anything like that. Just a clean curve that should ideally be the same for both jars.\r\n\r\nNext we can take one of the situations where there is a decent amount of spilling to host memory, and start to reduce the amount of host memory available. My hope is that there is a rather wide range in which we can avoid spilling to disk by pausing some threads and in those cases the overall runtime will be faster. After that we can start to look at the data and decide on next steps.\r\n\r\nIt really comes down to a few different things. If pausing and retrying computation is more expensive than spilling to disk, then we have the order of operations correct. If spilling to disk is more expensive that pausing/retrying computation, then we should look at shifting the order of operations around. Or there is inherently something else that is happening that we are blocked on when running out of memory that we don't totally understand yet.  I hope that the metrics collected will help us understand which outcome it is.\r\n\r\nAlso these may be different for different environments.  a PCIe gen3 card in a setup with a really complicated PCIe switch structure (like many cloud service providers on a T4) and possibly slow disks will likely behave very differently compared to a V100 on PCIe gen4 with dedicated lanes directly to the CPU and very fast NVMe storage also directly connected to the CPU.  OR even a Grace Hopper setup where we have something close to a TiB/s of bandwidth between the CPU and GPU for spilling.\r\n\r\nSo we might want to explore a few different setups.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1613178748/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724896384",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1724896384",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1724896384,
        "node_id": "IC_kwDOD7z77c5mz9SA",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-19T06:23:39Z",
        "updated_at": "2023-09-19T06:23:39Z",
        "author_association": "COLLABORATOR",
        "body": "Hi @revans2, shall we move it to 23.12 ?\r\nSeems there is a lot of testing work to do, so I probably can not make it for 23.08.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724896384/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725903774",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1725903774",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068",
        "id": 1725903774,
        "node_id": "IC_kwDOD7z77c5m3zOe",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-19T15:34:44Z",
        "updated_at": "2023-09-19T15:34:44Z",
        "author_association": "COLLABORATOR",
        "body": "yes we can push this back. I think the work to add in retry support in more places is more important.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725903774/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    }
]