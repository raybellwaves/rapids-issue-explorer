[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108707761",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108707761",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1108707761,
        "node_id": "IC_kwDOD7z77c5CFYmx",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T15:14:55Z",
        "updated_at": "2022-04-25T15:16:28Z",
        "author_association": "COLLABORATOR",
        "body": "From what I see in the logs @wjxiz1992 the issue in the description is not the actual problem. The description is pointing at a brand new executor that is trying to start, but it can't, because the GPU is busy with a prior executor that is having issues. \r\n\r\nThe earlier failure is an OOM with the async allocator that according to the logs looks just like fragmentation (which seems odd). \r\n\r\n@wjxiz1992 could you confirm that prior runs (without the mono jar) were actually using ASYNC allocator? Or where they using ARENA? My guess is that this is what actually changed.\r\n\r\n1. We have 4 tasks with the semaphore (task 88, 89, 92, 94). Tasks 89, 94, and 92 are allocating ~50MB, and task 88 is trying to allocate 560MB. \r\n2. RMM was at 3.3 GB and we spill 300MB trying to let these allocations through. \r\n3. With RMM at 3.0 GB we still can't handle the allocations, note this is a 40GB GPU and our pool was sized correctly from the beginning (`dispatcher-Executor 22/04/24 05:24:25:995 INFO GpuDeviceManager: Initializing RMM ASYNC pool size = 39164.1875 MB on gpuId 0`)\r\n\r\nSo this seems like exactly the type of case where the async allocator should be helping. Alternatively we could have our stats wrong on what the RMM pool actually has allocated. It needs to be investigated further.\r\n\r\nPertinent logs:\r\n\r\n```\r\nExecutor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:533 INFO DeviceMemoryEventHandler: Device allocation of 45350928 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\nExecutor task launch worker for task 94.0 in stage 1128.0 (TID 139368) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 47549192 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\nExecutor task launch worker for task 92.0 in stage 1128.0 (TID 139366) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 45360496 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\nExecutor task launch worker for task 88.0 in stage 1128.0 (TID 139362) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 560718784 bytes failed, device store has 0 bytes. Total RMM allocated is 3043957504 bytes.\r\nExecutor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:548 ERROR Executor: Exception in task 89.0 in stage 1128.0 (TID 139363)\r\njava.lang.OutOfMemoryError: Could not allocate native memory: std::bad_alloc: out_of_memory: CUDA error at: /home/jenkins/agent/workspace/jenkins-cudf_nightly-dev-github-683-cuda11/java/target/cmake-build/_deps/rmm-src/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:120: cudaErrorMemoryAllocation out of memory\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108707761/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108710445",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108710445",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1108710445,
        "node_id": "IC_kwDOD7z77c5CFZQt",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T15:17:10Z",
        "updated_at": "2022-04-25T15:17:10Z",
        "author_association": "COLLABORATOR",
        "body": "I believe this is a P1 because the logs are telling me this is a fragmentation issue and I was surprised by this behavior with the ASYNC allocator.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108710445/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108858018",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108858018",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1108858018,
        "node_id": "IC_kwDOD7z77c5CF9Si",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-25T17:42:55Z",
        "updated_at": "2022-04-25T17:42:55Z",
        "author_association": "COLLABORATOR",
        "body": "I can reproduce this easily. I also computed the amount of memory allocated on the GPU manually using the RMM log and I ran OOM with 9GB allocated for example in a 40GB GPU while trying to allocate 500MB, so this seems like fragmentation still.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108858018/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1109210097",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1109210097",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1109210097,
        "node_id": "IC_kwDOD7z77c5CHTPx",
        "user": {
            "login": "wjxiz1992",
            "id": 20476954,
            "node_id": "MDQ6VXNlcjIwNDc2OTU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/20476954?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wjxiz1992",
            "html_url": "https://github.com/wjxiz1992",
            "followers_url": "https://api.github.com/users/wjxiz1992/followers",
            "following_url": "https://api.github.com/users/wjxiz1992/following{/other_user}",
            "gists_url": "https://api.github.com/users/wjxiz1992/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wjxiz1992/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wjxiz1992/subscriptions",
            "organizations_url": "https://api.github.com/users/wjxiz1992/orgs",
            "repos_url": "https://api.github.com/users/wjxiz1992/repos",
            "events_url": "https://api.github.com/users/wjxiz1992/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wjxiz1992/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-26T01:43:02Z",
        "updated_at": "2022-04-26T01:43:02Z",
        "author_association": "COLLABORATOR",
        "body": "> From what I see in the logs @wjxiz1992 the issue in the description is not the actual problem. The description is pointing at a brand new executor that is trying to start, but it can't, because the GPU is busy with a prior executor that is having issues.\r\n> \r\n> The earlier failure is an OOM with the async allocator that according to the logs looks just like fragmentation (which seems odd).\r\n> \r\n> @wjxiz1992 could you confirm that prior runs (without the mono jar) were actually using ASYNC allocator? Or where they using ARENA? My guess is that this is what actually changed.\r\n> \r\n> 1. We have 4 tasks with the semaphore (task 88, 89, 92, 94). Tasks 89, 94, and 92 are allocating ~50MB, and task 88 is trying to allocate 560MB.\r\n> 2. RMM was at 3.3 GB and we spill 300MB trying to let these allocations through.\r\n> 3. With RMM at 3.0 GB we still can't handle the allocations, note this is a 40GB GPU and our pool was sized correctly from the beginning (`dispatcher-Executor 22/04/24 05:24:25:995 INFO GpuDeviceManager: Initializing RMM ASYNC pool size = 39164.1875 MB on gpuId 0`)\r\n> \r\n> So this seems like exactly the type of case where the async allocator should be helping. Alternatively we could have our stats wrong on what the RMM pool actually has allocated. It needs to be investigated further.\r\n> \r\n> Pertinent logs:\r\n> \r\n> ```\r\n> Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:533 INFO DeviceMemoryEventHandler: Device allocation of 45350928 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\n> Executor task launch worker for task 94.0 in stage 1128.0 (TID 139368) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 47549192 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\n> Executor task launch worker for task 92.0 in stage 1128.0 (TID 139366) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 45360496 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.\r\n> Executor task launch worker for task 88.0 in stage 1128.0 (TID 139362) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 560718784 bytes failed, device store has 0 bytes. Total RMM allocated is 3043957504 bytes.\r\n> Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:548 ERROR Executor: Exception in task 89.0 in stage 1128.0 (TID 139363)\r\n> java.lang.OutOfMemoryError: Could not allocate native memory: std::bad_alloc: out_of_memory: CUDA error at: /home/jenkins/agent/workspace/jenkins-cudf_nightly-dev-github-683-cuda11/java/target/cmake-build/_deps/rmm-src/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:120: cudaErrorMemoryAllocation out of memory\r\n> ```\r\n\r\nI didn't set this parameter in our template file, so it should always be the default one. I can switch to ARENA for another run.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1109210097/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113503107",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1113503107",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1113503107,
        "node_id": "IC_kwDOD7z77c5CXrWD",
        "user": {
            "login": "sameerz",
            "id": 7036315,
            "node_id": "MDQ6VXNlcjcwMzYzMTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7036315?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sameerz",
            "html_url": "https://github.com/sameerz",
            "followers_url": "https://api.github.com/users/sameerz/followers",
            "following_url": "https://api.github.com/users/sameerz/following{/other_user}",
            "gists_url": "https://api.github.com/users/sameerz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sameerz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sameerz/subscriptions",
            "organizations_url": "https://api.github.com/users/sameerz/orgs",
            "repos_url": "https://api.github.com/users/sameerz/repos",
            "events_url": "https://api.github.com/users/sameerz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sameerz/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-29T16:27:05Z",
        "updated_at": "2022-04-29T16:27:05Z",
        "author_association": "COLLABORATOR",
        "body": "> I didn't set this parameter in our template file, so it should always be the default one. I can switch to ARENA for another run.\r\n\r\n@wjxiz1992 can you please try with ARENA and let us know whether the same problem occurs? ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113503107/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125560411",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1125560411",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303",
        "id": 1125560411,
        "node_id": "IC_kwDOD7z77c5DFrBb",
        "user": {
            "login": "GaryShen2008",
            "id": 22128535,
            "node_id": "MDQ6VXNlcjIyMTI4NTM1",
            "avatar_url": "https://avatars.githubusercontent.com/u/22128535?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GaryShen2008",
            "html_url": "https://github.com/GaryShen2008",
            "followers_url": "https://api.github.com/users/GaryShen2008/followers",
            "following_url": "https://api.github.com/users/GaryShen2008/following{/other_user}",
            "gists_url": "https://api.github.com/users/GaryShen2008/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GaryShen2008/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GaryShen2008/subscriptions",
            "organizations_url": "https://api.github.com/users/GaryShen2008/orgs",
            "repos_url": "https://api.github.com/users/GaryShen2008/repos",
            "events_url": "https://api.github.com/users/GaryShen2008/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GaryShen2008/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-13T01:07:36Z",
        "updated_at": "2022-05-13T13:36:01Z",
        "author_association": "COLLABORATOR",
        "body": "We tested Decimal with UCX on and ARENA pool, but the job ran for a long time (more than 1 hour), then we aborted it.\r\nFrom the driver log, it said some executor has no response as below.\r\nWe ran it with spark 3.2.1. The application id is app-20220511061958-0000.\r\n\r\n```\r\n[2022-05-11T06:27:19.946Z] block-manager-ask-thread-pool-202 22/05/11 06:27:05:372 WARN BlockManagerMaster: Failed to remove shuffle 122 - Cannot receive any reply from /****:49074 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\r\n[2022-05-11T06:27:19.946Z] org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /****:49074 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\r\n[2022-05-11T06:27:19.946Z] \tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n[2022-05-11T06:27:19.946Z] \tat scala.util.Failure.recover(Try.scala:234)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Promise.complete(Promise.scala:53)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Promise.complete$(Promise.scala:52)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\r\n[2022-05-11T06:27:19.946Z] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\r\n[2022-05-11T06:27:19.946Z] \tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[2022-05-11T06:27:19.946Z] \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[2022-05-11T06:27:19.946Z] \tat java.lang.Thread.run(Thread.java:748)\r\n[2022-05-11T06:27:19.946Z] Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /****:49074 in 120 seconds\r\n[2022-05-11T06:27:19.946Z] \tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\r\n[2022-05-11T06:27:19.946Z] \t... 7 more\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125560411/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]