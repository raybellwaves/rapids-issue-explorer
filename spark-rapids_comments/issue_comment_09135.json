[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698196089",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698196089",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1698196089,
        "node_id": "IC_kwDOD7z77c5lOGp5",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T21:54:03Z",
        "updated_at": "2023-08-29T21:54:03Z",
        "author_association": "COLLABORATOR",
        "body": "I should point out that this is a pre-existing test (to stress-test coalesced reads in Parquet). Literally 3 years ago: 7ac919b4eea9a40c39bbf583a10bd8f6b19648bb.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698196089/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698203774",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698203774",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1698203774,
        "node_id": "IC_kwDOD7z77c5lOIh-",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T22:00:59Z",
        "updated_at": "2023-08-29T22:00:59Z",
        "author_association": "COLLABORATOR",
        "body": "I'm wondering if this message has anything to do with the problem:\r\n```\r\n23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 2.0 in stage 5.0 (TID 2015)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20\r\n23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 3.0 in stage 5.0 (TID 2016)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20\r\n23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 0.0 in stage 5.0 (TID 2013)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20\r\n23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 1.0 in stage 5.0 (TID 2014)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698203774/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698253520",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698253520",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1698253520,
        "node_id": "IC_kwDOD7z77c5lOUrQ",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-29T23:02:19Z",
        "updated_at": "2023-08-29T23:13:27Z",
        "author_association": "COLLABORATOR",
        "body": "I've done some more digging. I think this might have to do with resetting the thread pool size or something.\r\n\r\nHere is an easy repro:\r\n```\r\nspark-shell --jars /home/cloudera/mithunr/spark-rapids/dist/target/rapids-4-spark_2.12-23.10.0-SNAPSHOT-cuda11.jar --conf sparns=com.nvidia.spark.SQLPlugin --conf spark.rapids.sql.explain=ALL --conf spark.kryo.registrator=com.nvidia.spark.rapids.GpuKryoRegistrator --conf fs.defaultFS=\"file:///\" --conf spark.rapids.sql.format.parquet.reader.type=COALESCING --conf spark.sql.sources.useV1SourceList=\"\" --conf spark.sql.files.maxPartitionBytes=1g --master local[1]\r\n```\r\n```scala\r\n// Write corpus.\r\nspark.conf.set(\"spark.rapids.sql.enabled\", false)\r\n(0 to 2048).toDF.repartition(2000).write.mode(\"overwrite\").parquet(\"file:///tmp/myth_parq_ints\")\r\n\r\n// Read for boom.\r\nspark.conf.set(\"spark.rapids.sql.enabled\", true)\r\nspark.read.parquet(\"file:///tmp/myth_parq_ints\").show\r\n```\r\n1. It does not matter if the data resides on HDFS or local disk.\r\n2. `PERFILE` and `MULTITHREADED` readers are fine.  Only `COALESCING` reader is conking.\r\n\r\nI'm not yet convinced that this behaviour is specific to CDH.  I wonder if this could be an artifact of having a large number of threads, maybe?\r\n```\r\n$ lscpu | fgrep CPU\\(s\\) | head -1\r\nCPU(s):                128\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698253520/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698575865",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698575865",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1698575865,
        "node_id": "IC_kwDOD7z77c5lPjX5",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T06:32:12Z",
        "updated_at": "2023-08-30T06:35:44Z",
        "author_association": "COLLABORATOR",
        "body": "More experiments:  \r\n1. Reproduced same crash with Apache Spark 3.3.0, on the same node.  Looks to be independent of CDH. \r\n2. The read succeeds if Spark is started up with `PERFILE` reader, and then changed to `COALESCING`.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698575865/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699450468",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699450468",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699450468,
        "node_id": "IC_kwDOD7z77c5lS45k",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T16:01:57Z",
        "updated_at": "2023-08-30T16:01:57Z",
        "author_association": "MEMBER",
        "body": "What is the executor core count set to on the cluster?  Is the test running in local mode or in cluster mode?",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699450468/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699463279",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699463279",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699463279,
        "node_id": "IC_kwDOD7z77c5lS8Bv",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T16:09:47Z",
        "updated_at": "2023-08-30T16:09:47Z",
        "author_association": "MEMBER",
        "body": "I think this is related to #9051.  I suspect we were using the incorrect driver core count instead of the (I suspect larger) executor core count when setting up the multithreaded reader pool.  Easiest fix is to give the executors more heap space on the cloudera cluster setup if that's possible.  Long-term fix is budgeting host memory usage.  This initiative is underway for off-heap memory, but this is a heap OOM.  Would be interesting to get a heap dump on OOM to see what's taking up all the heap space.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699463279/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699866015",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699866015",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699866015,
        "node_id": "IC_kwDOD7z77c5lUeWf",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T21:27:16Z",
        "updated_at": "2023-08-30T21:27:16Z",
        "author_association": "COLLABORATOR",
        "body": "> What is the executor core count set to on the cluster? Is the test running in local mode or in cluster mode?\r\n\r\nI've run it in local mode, for both CDH and Apache Spark. I have set `--master local[1]`.  That should imply a single core, no? \r\n\r\nI've captured a heap-dump on OOM. Analyzing it now.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699866015/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699921748",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699921748",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699921748,
        "node_id": "IC_kwDOD7z77c5lUr9U",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T22:13:18Z",
        "updated_at": "2023-08-30T22:15:28Z",
        "author_association": "COLLABORATOR",
        "body": "Heap dump indicates that there are 128 `java.lang.Thread`s, each holding about 8MB Java local `byte[]`.  That exhausts the default 1GB heap in my runs.  IMO this implies that the large core count might be part of the problem.\r\n\r\nI see that the `COALESCING` multi-threaded Parquet reader decides to use `128` threads in its memory pool, thereby exhausting the heap.\r\n```\r\n23/08/30 22:09:33 WARN MultiFileReaderThreadPool: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20\r\n```\r\nThis is with `spark.executor.cores` not set to anything, probably causing the pool to consume all available CPUs.\r\n\r\nThere might be a couple of ways around the OOM:\r\n1. As @jlowe suggested already, we could bump the heap for the test.  (In my run, `--driver-memory 8g` worked, even with 128 threads.)\r\n2. We could set `spark.executor.cores` to a specific, smaller value.\r\n\r\nI'm inclined to try the latter.  I'll update here with results.\r\n\r\nEdit: There might be a longer term solution here, to size the `GpuMultiFileReader`'s thread-pool as a function of both number of cores *and* available memory.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699921748/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699931149",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699931149",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699931149,
        "node_id": "IC_kwDOD7z77c5lUuQN",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T22:21:46Z",
        "updated_at": "2023-08-30T22:21:46Z",
        "author_association": "MEMBER",
        "body": "We may want to revisit the 8MB heap buffer that is being used.  This was copied from some existing Parquet reading code and could make sense to produce large, chunky reads, especially for cloud applications, but asking for 8MB when there are hundreds of threads is...problematic.  We may want to automatically scale that buffer size based on the number of threads or simply use a smaller value for the temporary buffer.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699931149/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699954181",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699954181",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1699954181,
        "node_id": "IC_kwDOD7z77c5lUz4F",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-30T22:48:56Z",
        "updated_at": "2023-08-30T22:49:26Z",
        "author_association": "COLLABORATOR",
        "body": "Ah, it turns out that one can't simply set `spark.executor.cores` for a specific test (`test_small_file_memory`). The Spark application is already up by that point.\r\n\r\nOne option is to set `spark.executor.cores` in `run_pyspark_from_build.sh`. I don't know if that would be acceptable. I did verify that that allowed this test to run.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699954181/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703309347",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1703309347",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1703309347,
        "node_id": "IC_kwDOD7z77c5lhnAj",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-01T20:54:11Z",
        "updated_at": "2023-09-01T20:54:11Z",
        "author_association": "COLLABORATOR",
        "body": "There is a tentative workaround to get this test to run on the large CDH nodes here:\r\nhttps://github.com/NVIDIA/spark-rapids/pull/9177.\r\n\r\nThe more robust fix (to size the Parquet reader's buffers \"appropriately\") may be tackled later on.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703309347/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726637088",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1726637088",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1726637088,
        "node_id": "IC_kwDOD7z77c5m6mQg",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-19T22:44:37Z",
        "updated_at": "2023-09-19T22:44:37Z",
        "author_association": "COLLABORATOR",
        "body": "I have raised #9269 and #9271 to mitigate this problem.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726637088/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736849150",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1736849150",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1736849150,
        "node_id": "IC_kwDOD7z77c5nhjb-",
        "user": {
            "login": "pxLi",
            "id": 8086184,
            "node_id": "MDQ6VXNlcjgwODYxODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8086184?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pxLi",
            "html_url": "https://github.com/pxLi",
            "followers_url": "https://api.github.com/users/pxLi/followers",
            "following_url": "https://api.github.com/users/pxLi/following{/other_user}",
            "gists_url": "https://api.github.com/users/pxLi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pxLi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pxLi/subscriptions",
            "organizations_url": "https://api.github.com/users/pxLi/orgs",
            "repos_url": "https://api.github.com/users/pxLi/repos",
            "events_url": "https://api.github.com/users/pxLi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pxLi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-27T07:25:58Z",
        "updated_at": "2023-09-27T07:48:24Z",
        "author_association": "COLLABORATOR",
        "body": "We also saw this case failed OOM constantly while testing in arm environment \r\n(CI HW specs: GPU: A30, CPU 128 cores which is similar to CDH node)\r\n\r\n```\r\n00:28:11      def test_small_file_memory(spark_tmp_path, v1_enabled_list):\r\n00:28:11          # stress the memory usage by creating a lot of small files.\r\n00:28:11          # The more files we combine the more the offsets will be different which will cause\r\n00:28:11          # footer size to change.\r\n00:28:11          # Without the addition of extraMemory in GpuParquetScan this would cause reallocations\r\n00:28:11          # of the host memory buffers.\r\n00:28:11          cols = [string_gen] * 4\r\n00:28:11          gen_list = [('_c' + str(i), gen ) for i, gen in enumerate(cols)]\r\n00:28:11          first_data_path = spark_tmp_path + '/PARQUET_DATA'\r\n00:28:11          with_cpu_session(\r\n00:28:11                  lambda spark : gen_df(spark, gen_list).repartition(2000).write.parquet(first_data_path),\r\n00:28:11                  conf=rebase_write_corrected_conf)\r\n00:28:11          data_path = spark_tmp_path + '/PARQUET_DATA'\r\n00:28:11  >       assert_gpu_and_cpu_are_equal_collect(\r\n00:28:11                  lambda spark : spark.read.parquet(data_path),\r\n00:28:11                  conf={'spark.rapids.sql.format.parquet.reader.type': 'COALESCING',\r\n00:28:11                        'spark.sql.sources.useV1SourceList': v1_enabled_list,\r\n00:28:11                        'spark.sql.files.maxPartitionBytes': \"1g\"})\r\n```\r\n\r\n```\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 19511.0 failed 1 times, most recent failure: Lost task 3.0 in stage 19511.0 (TID 77495) (verify-pxli-mvn-verify-github-jtzwk-6nhg1 executor driver): java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\r\n\r\n00:28:11  E                   Driver stacktrace:\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n00:28:11  E                   \tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n00:28:11  E                   \tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n00:28:11  E                   \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n00:28:11  E                   \tat scala.Option.foreach(Option.scala:407)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n00:28:11  E                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n00:28:11  E                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n00:28:11  E                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n00:28:11  E                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n00:28:11  E                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n00:28:11  E                   \tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n00:28:11  E                   \tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n00:28:11  E                   \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n00:28:11  E                   \tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n00:28:11  E                   \tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n00:28:11  E                   \tat sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\r\n00:28:11  E                   \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n00:28:11  E                   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n00:28:11  E                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n00:28:11  E                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n00:28:11  E                   \tat py4j.Gateway.invoke(Gateway.java:282)\r\n00:28:11  E                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n00:28:11  E                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n00:28:11  E                   \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n00:28:11  E                   \tat java.lang.Thread.run(Thread.java:750)\r\n00:28:11  E                   Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\r\n00:28:11  E                   \tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n00:28:11  E                   \tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7(GpuMultiFileReader.scala:1216)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7$adapted(GpuMultiFileReader.scala:1215)\r\n00:28:11  E                   \tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n00:28:11  E                   \tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n00:28:11  E                   \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n00:28:11  E                   \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n00:28:11  E                   \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n00:28:11  E                   \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$4(GpuMultiFileReader.scala:1215)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:88)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$1(GpuMultiFileReader.scala:1198)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.readPartFiles(GpuMultiFileReader.scala:1185)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readBatch$1(GpuMultiFileReader.scala:1146)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.readBatch(GpuMultiFileReader.scala:1125)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.next(GpuMultiFileReader.scala:1098)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.PartitionIterator.hasNext(dataSourceUtil.scala:29)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.MetricsBatchIterator.hasNext(dataSourceUtil.scala:46)\r\n00:28:11  E                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n00:28:11  E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$3(GpuColumnarToRowExec.scala:285)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:284)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:257)\r\n00:28:11  E                   \tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:301)\r\n00:28:11  E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n00:28:11  E                   \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n00:28:11  E                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n00:28:11  E                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n00:28:11  E                   \tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n00:28:11  E                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n00:28:11  E                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n00:28:11  E                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n00:28:11  E                   \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n00:28:11  E                   \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n00:28:11  E                   \t... 1 more\r\n00:28:11  E                   Caused by: java.lang.OutOfMemoryError: Java heap space\r\n```\r\n\r\nI will re-verify after #9269 #9271 are resolved",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736849150/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1738399296",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1738399296",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
        "id": 1738399296,
        "node_id": "IC_kwDOD7z77c5nnd5A",
        "user": {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-28T04:00:55Z",
        "updated_at": "2023-09-28T04:00:55Z",
        "author_association": "COLLABORATOR",
        "body": "It stands to reason that it fails.  :/ High core count, with low heap allocation. ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1738399296/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]