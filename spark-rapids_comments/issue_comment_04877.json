[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1054356117",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1054356117",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877",
        "id": 1054356117,
        "node_id": "IC_kwDOD7z77c4-2DKV",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-28T15:12:57Z",
        "updated_at": "2022-02-28T15:12:57Z",
        "author_association": "COLLABORATOR",
        "body": "I don't know how much background you have in this so please excuse me if I explain some things that you already know. \r\n\r\nTLDR; It looks like your GPU is fully utilized and that is the bottleneck. Would need to look at GPU stats to know that for sure.\r\n\r\nYou have set your GPU concurrent task parallelism to 4.  This is the number of tasks that can share a GPU at any one point in time.  We limit the number of tasks that can be on the GPU so we don't use up all of the GPU's memory, but we have also seen some issues with compute on some GPUs with lots of memory and I should update the docs to reflect this. We keep it separate from the parallelism of the CPU, because it allows a user to configure the query to run with more CPU cores. This is especially helpful if there are portions of the query that require a lot of CPU. Adding more CPU cores beyond that parallelism number only impacts the parallelism of what can be run on the CPU. Looking at your screen shots it is clear that all of the query is running on the GPU. This means that once you get above 4 tasks the extra tasks may be able to do more I/O in parallel, but will not improve the computation speed.  In many cases we have seen this be a big win for performance, especially if the I/O is relatively slow. The reason why the average task time goes up for 5 tasks over 4 tasks is because Spark is measuring that from the perspective of the CPU, so if one of the tasks is blocked on a semaphore waiting to get on the GPU, then it looks like that task took longer to complete.\r\n\r\nIn your case there could be a number of different things happening and I probably would need more metrics to be 100% sure what it is. One of the first things that comes to mind is HDFS limitations. I don't think this is the case here, but you might want to check your network utilization and also the disk I/O on your HDFS nodes. You could also look at the buffer time metrics for reading the data. This should really only impact the first stage, and it looks like it is scaling much better than the second stage. So, like I said, it is not likely the problem.\r\n\r\nThe second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks. The GPU is great and is able to do sub-linear scaling so long as it has enough compute/memory bandwidth to be able to keep up. After that point it has to try to schedule what to run next. In our testing we have seen it struggle when there are a lot more things to do than resources to fulfill those requests. \r\n\r\nWhen running with just one task we tend to see a number of gaps in GPU utilization. This is because of I/O or just the time it takes for JNI to call down to the GPU to issue the next set of commands to run per task.  With 2 tasks we often see these gaps really fill in. One task will be on the GPU while the other is getting something else queued up to run. This is why the biggest gains often come with setting a parallelism of 2 (a 37% improvement in your case). After that there are diminishing returns, and it really depends on how good your hardware is, and the algorithms that you are going to run on it.\r\n\r\nFor your setup I would recommend trying to set your gpuParallelism to 3, and the number of tasks that you have to 6.  Ideally that would allow half of the tasks to be reading data while the other half are doing computation and you can overlap as much of that as possible.  In your setup that is not as likely to make as much of a difference because most of your I/O is local, and because of the amount of RAM you have on your device it is likely all coming from the page cache.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1054356117/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1055071783",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1055071783",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877",
        "id": 1055071783,
        "node_id": "IC_kwDOD7z77c4-4x4n",
        "user": {
            "login": "YeahNew",
            "id": 33194373,
            "node_id": "MDQ6VXNlcjMzMTk0Mzcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33194373?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YeahNew",
            "html_url": "https://github.com/YeahNew",
            "followers_url": "https://api.github.com/users/YeahNew/followers",
            "following_url": "https://api.github.com/users/YeahNew/following{/other_user}",
            "gists_url": "https://api.github.com/users/YeahNew/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YeahNew/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YeahNew/subscriptions",
            "organizations_url": "https://api.github.com/users/YeahNew/orgs",
            "repos_url": "https://api.github.com/users/YeahNew/repos",
            "events_url": "https://api.github.com/users/YeahNew/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YeahNew/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-01T06:28:04Z",
        "updated_at": "2022-03-01T06:36:31Z",
        "author_association": "NONE",
        "body": "> I don't know how much background you have in this so please excuse me if I explain some things that you already know.\r\n> \r\n> TLDR; It looks like your GPU is fully utilized and that is the bottleneck. Would need to look at GPU stats to know that for sure.\r\n> \r\n> You have set your GPU concurrent task parallelism to 4. This is the number of tasks that can share a GPU at any one point in time. We limit the number of tasks that can be on the GPU so we don't use up all of the GPU's memory, but we have also seen some issues with compute on some GPUs with lots of memory and I should update the docs to reflect this. We keep it separate from the parallelism of the CPU, because it allows a user to configure the query to run with more CPU cores. This is especially helpful if there are portions of the query that require a lot of CPU. Adding more CPU cores beyond that parallelism number only impacts the parallelism of what can be run on the CPU. Looking at your screen shots it is clear that all of the query is running on the GPU. This means that once you get above 4 tasks the extra tasks may be able to do more I/O in parallel, but will not improve the computation speed. In many cases we have seen this be a big win for performance, especially if the I/O is relatively slow. The reason why the average task time goes up for 5 tasks over 4 tasks is because Spark is measuring that from the perspective of the CPU, so if one of the tasks is blocked on a semaphore waiting to get on the GPU, then it looks like that task took longer to complete.\r\n> \r\n> In your case there could be a number of different things happening and I probably would need more metrics to be 100% sure what it is. One of the first things that comes to mind is HDFS limitations. I don't think this is the case here, but you might want to check your network utilization and also the disk I/O on your HDFS nodes. You could also look at the buffer time metrics for reading the data. This should really only impact the first stage, and it looks like it is scaling much better than the second stage. So, like I said, it is not likely the problem.\r\n> \r\n> The second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks. The GPU is great and is able to do sub-linear scaling so long as it has enough compute/memory bandwidth to be able to keep up. After that point it has to try to schedule what to run next. In our testing we have seen it struggle when there are a lot more things to do than resources to fulfill those requests.\r\n> \r\n> When running with just one task we tend to see a number of gaps in GPU utilization. This is because of I/O or just the time it takes for JNI to call down to the GPU to issue the next set of commands to run per task. With 2 tasks we often see these gaps really fill in. One task will be on the GPU while the other is getting something else queued up to run. This is why the biggest gains often come with setting a parallelism of 2 (a 37% improvement in your case). After that there are diminishing returns, and it really depends on how good your hardware is, and the algorithms that you are going to run on it.\r\n> \r\n> For your setup I would recommend trying to set your gpuParallelism to 3, and the number of tasks that you have to 6. Ideally that would allow half of the tasks to be reading data while the other half are doing computation and you can overlap as much of that as possible. In your setup that is not as likely to make as much of a difference because most of your I/O is local, and because of the amount of RAM you have on your device it is likely all coming from the page cache.\r\n\r\n@revans2 \r\nThank you for your reply\uff0c\r\nYou mentioned above that you may need to get more metrics to to be 100% sure what it is, can you give some examples? I would like to show you. \r\n\r\n\"The second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks\" you mentioned above. Sorry, here I don't fully understand the explanation. Is the diminishing returns because of the GPU itself or the algorithm? During the execution process, does the data need to be frequently transferred from the host memory to the GPU memory, and will this also have a great impact on the execution performance? \r\n\r\nI have tried to set  gpuParallelism to 3, and the number of tasks to 6, The execution efficiency will drop further, which doesn't seem to be a good setup for my case. ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1055071783/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056831964",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1056831964",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877",
        "id": 1056831964,
        "node_id": "IC_kwDOD7z77c4-_fnc",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-02T11:37:51Z",
        "updated_at": "2022-03-02T11:37:51Z",
        "author_association": "COLLABORATOR",
        "body": "My assumptions were apparently wrong. Could I get a little more information about your setup so I can try to reproduce it locally, or at least as close as I can come?  What is the file format that the data is stored in? Is it the original \"|\" separated values, ORC, parquet, or something else? What version of the RAPIDS Accelerator are you using? \r\n\r\nThere are a number of SQL metrics that we have that can really help to debug some kinds of problems like this. If you could set the config `spark.rapids.sql.metrics.level` to DEBUG, it should enable all of the metrics. Then if you rerun the query and look at the sql tab it should show a lot of information about how long each part of the query is taking. This, ideally could help you understand how the scaling is happening for the join separate form reading parquet etc.  All of that information would be stored in the history file for each application. If you feel comfortable sending that to me I could take a look.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056831964/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056859296",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1056859296",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877",
        "id": 1056859296,
        "node_id": "IC_kwDOD7z77c4-_mSg",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-02T12:10:52Z",
        "updated_at": "2022-03-02T12:10:52Z",
        "author_association": "COLLABORATOR",
        "body": "Also a few more questions. From the size it looks like you are reading from the original data format, \"|\" separated. I assume you are reading the data as floats and not decimal values?\r\n\r\nI should also add that I would be interested in seeing the number of tasks run, simply because you are not setting spark.sql.files.maxPartitionBytes, which will default to 128MB, which is rather small for the GPU. But I first want to match your execution before I start making recommendations to change, because my first assumption was way off.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056859296/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1057060233",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1057060233",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877",
        "id": 1057060233,
        "node_id": "IC_kwDOD7z77c4_AXWJ",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-02T15:29:56Z",
        "updated_at": "2022-03-02T15:29:56Z",
        "author_association": "COLLABORATOR",
        "body": "I was not able to reproduce the issue you saw locally. I have a lot more evidence now that you are using CSV because if I try to do the same query with parquet, and the same settings you have except with 12 tasks on a single a6000 GPU I can complete the query in under 20 seconds.  If I increase `spark.sql.files.maxPartitionByte` to `1g` I am able to complete the query in 15 seconds. This appears to indicate that the issue is with I/O or CSV parsing. But that is just speculation because I was not able to reproduce the problem.\r\n\r\nLooking at GPU utilization, for me, the GPU is not fully utilized. I see my NVMe only doing around 2.2 GiB/sec in the best case when reading the CSV data. I manually verified that I can `cat` the files to `/dev/null` in parallel at 3 GiB/sec. I also saw 18,000 read operations per second, which feels rather high. I filed #6 a while ago to try and  improve it for CSV. But like I said before this is a performance problem with my setup, not with yours.\r\n\r\nI don't see the time taken jump back up when using more cores per executor.\r\n![time vs number of cores](https://user-images.githubusercontent.com/3441321/156392840-11f5551b-d27d-418b-b3b4-f71df1d3eb64.png)\r\n\r\nIt even goes down a little when going from 8 to 12 instead of up.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1057060233/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]