[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572726710",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1572726710",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1572726710,
        "node_id": "IC_kwDOD7z77c5dvee2",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-01T20:22:33Z",
        "updated_at": "2023-06-01T20:22:33Z",
        "author_association": "COLLABORATOR",
        "body": "I have not run this, but I am just guessing. The GPU does not support UserDefinedTypes right now, and VectorUDT is a user defined type, so we are going to fall back to the CPU to serialize and deserialize them.  This might not be ideal because the CPU is really bad at writing parquet in many cases compared to the GPU.  600x better (that is hard to believe so I need to do some testing).",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572726710/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572844170",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1572844170",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1572844170,
        "node_id": "IC_kwDOD7z77c5dv7KK",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-01T21:59:16Z",
        "updated_at": "2023-06-01T21:59:16Z",
        "author_association": "COLLABORATOR",
        "body": "@eordentlich do you have any instructions on how to get an environment setup to do this?  I tried to use conda to setup an environment following the instructions at https://xgboost.readthedocs.io/en/stable/tutorials/spark_estimator.html\r\n\r\n```\r\nconda create -y -n xgboost_env -c conda-forge conda-pack python=3.9\r\nconda activate xgboost_env\r\n# use conda when the supported version of xgboost (1.7) is released on conda-forge\r\npip install xgboost\r\nconda install cudf pyarrow pandas -c rapids -c nvidia -c conda-forge\r\n```\r\n\r\nBut it didn't work and I had to change the last command to\r\n\r\n```\r\nconda install cudf pyarrow pandas -c rapidsai -c nvidia -c conda-forge\r\n```\r\n\r\nThen when I tried to import xgboost in the notebook\r\n\r\n```\r\nfrom xgboost.spark import SparkXGBClassifier, SparkXGBClassifierModel\r\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n```\r\n\r\nI got an error about no `sklearn` so I installed it\r\n\r\n```\r\nconda install scikit-learn\r\n```\r\n\r\nAnd now I am getting what appears to be CUDA mismatch of some kind.\r\n\r\n```\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\r\n    process()\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\r\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\r\n    for batch in iterator:\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\r\n    for series in iterator:\r\n  File \"spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py\", line 519, in func\r\n    for result_batch, result_type in result_iter:\r\n  File \"xgboost_env/lib/python3.9/site-packages/xgboost/spark/core.py\", line 795, in _train_booster\r\n    use_qdm = use_hist and is_cudf_available()\r\n  File \"xgboost_env/lib/python3.9/site-packages/xgboost/compat.py\", line 83, in is_cudf_available\r\n    import cudf\r\n  File \"xgboost_env/lib/python3.9/site-packages/cudf/__init__.py\", line 5, in <module>\r\n    validate_setup()\r\n  File \"xgboost_env/lib/python3.9/site-packages/cudf/utils/gpu_utils.py\", line 20, in validate_setup\r\n    from rmm._cuda.gpu import (\r\n  File \"xgboost_env/lib/python3.9/site-packages/rmm/__init__.py\", line 16, in <module>\r\n    from rmm import mr\r\n  File \"xgboost_env/lib/python3.9/site-packages/rmm/mr.py\", line 14, in <module>\r\n    from rmm._lib.memory_resource import (\r\n  File \"xgboost_env/lib/python3.9/site-packages/rmm/_lib/__init__.py\", line 15, in <module>\r\n    from .device_buffer import DeviceBuffer\r\n  File \"device_buffer.pyx\", line 1, in init rmm._lib.device_buffer\r\nTypeError: C function cuda.ccudart.cudaStreamSynchronize has wrong signature (expected __pyx_t_4cuda_7ccudart_cudaError_t (__pyx_t_4cuda_7ccudart_cudaStream_t), got cudaError_t (cudaStream_t))\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572844170/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1573254442",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1573254442",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1573254442,
        "node_id": "IC_kwDOD7z77c5dxfUq",
        "user": {
            "login": "eordentlich",
            "id": 36281329,
            "node_id": "MDQ6VXNlcjM2MjgxMzI5",
            "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eordentlich",
            "html_url": "https://github.com/eordentlich",
            "followers_url": "https://api.github.com/users/eordentlich/followers",
            "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
            "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
            "organizations_url": "https://api.github.com/users/eordentlich/orgs",
            "repos_url": "https://api.github.com/users/eordentlich/repos",
            "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eordentlich/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-02T07:03:49Z",
        "updated_at": "2023-06-02T07:03:49Z",
        "author_association": "CONTRIBUTOR",
        "body": "Indeed, looks like those instructions can use some work.    I think a conda cudatoolkit package needs to be added to your conda environment create command: e.g cudatoolkit=11.5 with version ( >= 11.2, <= 11.8) that matches the one installed on your host.    If you are running on a single node, you can activate the conda environment and run in either local mode or standalone, with master and worker started in the environment.\r\n\r\nThat said, I think you can replicate the key issue via the following running in a pyspark shell started as\r\n```\r\npyspark --master local[4] --conf spark.driver.memory=20g --jars rapids-4-spark_2.12-23.04.0.jar --conf spark.plugins=com.nvidia.spark.SQLPlugin --conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer\r\n```\r\nAnd then in the shell, paste\r\n```\r\nfrom pyspark.sql.functions import rand, element_at, sum\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.functions import vector_to_array\r\nimport timeit\r\n\r\ndf = spark.range(10000000)\r\ndf = df.select(rand().alias(\"r0\"),rand().alias(\"r1\"))\r\n\r\ndf_vec = VectorAssembler(inputCols=[\"r0\",\"r1\"],outputCol=\"vec\").transform(df).drop(\"r0\",\"r1\")\r\ntimeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\ndf_vec.cache()\r\ntimeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\n```\r\nThe first `timeit` call finishes reasonably fast, while the second, after the `cache()`, takes \"forever\".    You can try dialing down the range size to compare running times.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1573254442/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574164659",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1574164659",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1574164659,
        "node_id": "IC_kwDOD7z77c5d09iz",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-02T18:44:55Z",
        "updated_at": "2023-06-02T18:44:55Z",
        "author_association": "COLLABORATOR",
        "body": "Thanks for the simplified setup. I was able to reproduce the caching issue. At least I was able to get the Spark to crash with a timeout when using the parquet cached batch serializer for your initial request (I made the data bigger because I was using more cores, but I guess I made it too big!!!).",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574164659/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574224567",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1574224567",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1574224567,
        "node_id": "IC_kwDOD7z77c5d1MK3",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-02T19:39:43Z",
        "updated_at": "2023-06-02T19:39:43Z",
        "author_association": "COLLABORATOR",
        "body": "I found at least one really bad problem where we were doing code generation for each row in a specific code path.  I need to do some more profiling to see what else might be bad about it.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574224567/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577262107",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577262107",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1577262107,
        "node_id": "IC_kwDOD7z77c5eAxwb",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-05T18:21:11Z",
        "updated_at": "2023-06-05T18:21:11Z",
        "author_association": "COLLABORATOR",
        "body": "@eordentlich do you have the ability to try out #8495?  It is not going to solve all of your problems but it would be good to know if it is good enough for now or if we have to start looking at some of the other optimizations too.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577262107/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577359605",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577359605",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1577359605,
        "node_id": "IC_kwDOD7z77c5eBJj1",
        "user": {
            "login": "eordentlich",
            "id": 36281329,
            "node_id": "MDQ6VXNlcjM2MjgxMzI5",
            "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eordentlich",
            "html_url": "https://github.com/eordentlich",
            "followers_url": "https://api.github.com/users/eordentlich/followers",
            "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
            "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
            "organizations_url": "https://api.github.com/users/eordentlich/orgs",
            "repos_url": "https://api.github.com/users/eordentlich/repos",
            "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eordentlich/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-05T19:30:36Z",
        "updated_at": "2023-06-05T19:30:36Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks.  I'll have to build the jar (unless it is already in cicd somewhere) and give it a try on that notebook.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577359605/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577794331",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577794331",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1577794331,
        "node_id": "IC_kwDOD7z77c5eCzsb",
        "user": {
            "login": "eordentlich",
            "id": 36281329,
            "node_id": "MDQ6VXNlcjM2MjgxMzI5",
            "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eordentlich",
            "html_url": "https://github.com/eordentlich",
            "followers_url": "https://api.github.com/users/eordentlich/followers",
            "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
            "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
            "organizations_url": "https://api.github.com/users/eordentlich/orgs",
            "repos_url": "https://api.github.com/users/eordentlich/repos",
            "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eordentlich/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T02:20:54Z",
        "updated_at": "2023-06-06T02:20:54Z",
        "author_association": "CONTRIBUTOR",
        "body": "@revans2 I tested the PR and it is a huge improvement.   Still 4x slower than mapping vector to array type and back, with PCBS, and about 6x slower than regular non-PCBS caching for the notebook example eval stage.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577794331/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579004740",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1579004740",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
        "id": 1579004740,
        "node_id": "IC_kwDOD7z77c5eHbNE",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T15:39:00Z",
        "updated_at": "2023-06-06T15:39:00Z",
        "author_association": "COLLABORATOR",
        "body": "> @revans2 I tested the PR and it is a huge improvement. Still 4x slower than mapping vector to array type and back, with PCBS, and about 6x slower than regular non-PCBS caching for the notebook example eval stage.\r\n\r\n\r\n@eordentlich glad to hear that it is helping. I'll see if we can get some help in improving the performance even more.\r\n\r\n@sameerz looks like we should spend some time on the other issues I filed especially https://github.com/NVIDIA/spark-rapids/issues/8496 I think we can make it work without too much difficulty.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579004740/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]