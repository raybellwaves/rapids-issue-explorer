[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1223645987",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5944#issuecomment-1223645987",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5944",
        "id": 1223645987,
        "node_id": "IC_kwDOD7z77c5I71sj",
        "user": {
            "login": "wjxiz1992",
            "id": 20476954,
            "node_id": "MDQ6VXNlcjIwNDc2OTU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/20476954?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wjxiz1992",
            "html_url": "https://github.com/wjxiz1992",
            "followers_url": "https://api.github.com/users/wjxiz1992/followers",
            "following_url": "https://api.github.com/users/wjxiz1992/following{/other_user}",
            "gists_url": "https://api.github.com/users/wjxiz1992/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wjxiz1992/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wjxiz1992/subscriptions",
            "organizations_url": "https://api.github.com/users/wjxiz1992/orgs",
            "repos_url": "https://api.github.com/users/wjxiz1992/repos",
            "events_url": "https://api.github.com/users/wjxiz1992/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wjxiz1992/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-23T07:10:12Z",
        "updated_at": "2022-08-23T07:10:12Z",
        "author_association": "COLLABORATOR",
        "body": "@jlowe is this issue talking about the error when I try to run a  \"DELETE\" query in Iceberg:\r\n```\r\n: java.lang.IllegalStateException: Row-based execution should not occur for this class\r\n        at com.nvidia.spark.rapids.GpuHashAggregateExec.doExecute(aggregate.scala:1532)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n        at com.nvidia.spark.rapids.GpuBringBackToHost.doExecute(GpuBringBackToHost.scala:38)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n        at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\r\n        at org.apache.spark.sql.execution.datasources.v2.DynamicFileFilterExec.doPrepare(DynamicFileFilterExec.scala:103)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:267)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)\r\n        at scala.collection.immutable.List.foreach(List.scala:431)\r\n        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:216)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n        at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n        at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\r\n        at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\r\n        at org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.writeWithV2(ReplaceDataExec.scala:26)\r\n        at org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(ReplaceDataExec.scala:34)\r\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\r\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\r\n        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\r\n        at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\r\n        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r\n        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n        at sun.reflect.GeneratedMethodAccessor120.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n        at py4j.Gateway.invoke(Gateway.java:282)\r\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1223645987/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224079820",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5944#issuecomment-1224079820",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5944",
        "id": 1224079820,
        "node_id": "IC_kwDOD7z77c5I9fnM",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-23T13:29:36Z",
        "updated_at": "2022-08-23T14:15:16Z",
        "author_association": "MEMBER",
        "body": "@wjxiz1992 that exception is a completely different issue.  This is about handling `merge-on-read` mode for Iceberg, where it can encode row deletes in metadata rather than rewriting data partitions to remove the rows.  That does not mean we cannot support DELETE operations, as v1 style deletes where it rewrites the data partitions is supported.\r\n\r\nThe exception you're getting above has to do with writes crashing and does not seem to have anything to do with reads. Please file a separate issue to track this.\r\n\r\nFrom the stacktrace, @revans2 noticed there's an issue with `GpuBringBackToHost` not handling `doExecute` properly, so I filed #6397 to track.  Fixing that may resolve your issue, but we should track your issue separately in case it does not.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224079820/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]