[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1652319762",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1652319762",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724",
        "id": 1652319762,
        "node_id": "IC_kwDOD7z77c5ifGYS",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-26T18:46:39Z",
        "updated_at": "2023-07-26T18:46:39Z",
        "author_association": "COLLABORATOR",
        "body": "It should be noted that something similar to this already exists in the tools repo.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids-tools/tree/dev/data_validation\r\n\r\nI'm not 100% sure if we want to have two tools or just one. But the code is not that complicated. The data_validation tool is not great. It requires a primary key, which should not be a requirement. It does not handle maps, nulls, duplicate rows, or floating point approximate matches correctly.\r\n\r\nThe above example can be updated to handle nulls by using the null equals operator '<=>' as the join condition for the anti-join columns.  Maps can be replaced with a `map_entries` that are array_sorted. And for now we are just going to punt on floats/doubles in both cases and not include them in our scale testing suite until this can support doing a fuzzy check for them.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1652319762/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1713483581",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1713483581",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724",
        "id": 1713483581,
        "node_id": "IC_kwDOD7z77c5mIa89",
        "user": {
            "login": "wjxiz1992",
            "id": 20476954,
            "node_id": "MDQ6VXNlcjIwNDc2OTU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/20476954?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wjxiz1992",
            "html_url": "https://github.com/wjxiz1992",
            "followers_url": "https://api.github.com/users/wjxiz1992/followers",
            "following_url": "https://api.github.com/users/wjxiz1992/following{/other_user}",
            "gists_url": "https://api.github.com/users/wjxiz1992/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wjxiz1992/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wjxiz1992/subscriptions",
            "organizations_url": "https://api.github.com/users/wjxiz1992/orgs",
            "repos_url": "https://api.github.com/users/wjxiz1992/repos",
            "events_url": "https://api.github.com/users/wjxiz1992/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wjxiz1992/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-11T09:10:17Z",
        "updated_at": "2023-09-11T09:10:17Z",
        "author_association": "COLLABORATOR",
        "body": "@revans2 I'm building pipelines to run preliminary scale test on our internal cluster and I got some numbers for this issue.\r\n\r\nwith `scale_factor=100, complexity=300`, I got the data at size\r\n```\r\n134.1 G  268.1 G  /data/scale-test/SCALE_100_300_parquet_1.0.0_41\r\n```\r\nGPU output size (0 means failed queries)\r\n```\r\n1.2 G     2.4 G    /data/scale-test/output/q10_1\r\n46.2 G    92.4 G   /data/scale-test/output/q11_1\r\n0         0        /data/scale-test/output/q12_1\r\n46.2 G    92.4 G   /data/scale-test/output/q13_1\r\n17.0 G    34.1 G   /data/scale-test/output/q14_1\r\n12.9 G    25.7 G   /data/scale-test/output/q15_1\r\n39.3 K    78.7 K   /data/scale-test/output/q16_1\r\n9.8 G     19.7 G   /data/scale-test/output/q17_1\r\n5.8 G     11.6 G   /data/scale-test/output/q18_1\r\n38.5 K    77.1 K   /data/scale-test/output/q19_1\r\n645.0 G   1.3 T    /data/scale-test/output/q1_1\r\n38.5 K    77.1 K   /data/scale-test/output/q20_1\r\n0         0        /data/scale-test/output/q21_1\r\n11.1 G    22.2 G   /data/scale-test/output/q22_1\r\n19.5 K    39.1 K   /data/scale-test/output/q23_1\r\n16.5 M    33.1 M   /data/scale-test/output/q24_1\r\n290.7 K   581.3 K  /data/scale-test/output/q25_1\r\n775.5 M   1.5 G    /data/scale-test/output/q26_1\r\n1.7 G     3.5 G    /data/scale-test/output/q27_1\r\n1004.1 M  2.0 G    /data/scale-test/output/q28_1\r\n1.2 M     2.4 M    /data/scale-test/output/q29_1\r\n644.9 G   1.3 T    /data/scale-test/output/q2_1\r\n1.4 G     2.9 G    /data/scale-test/output/q30_1\r\n603.6 M   1.2 G    /data/scale-test/output/q31_1\r\n841.9 K   1.6 M    /data/scale-test/output/q32_1\r\n174.3 M   348.6 M  /data/scale-test/output/q33_1\r\n191.7 M   383.3 M  /data/scale-test/output/q34_1\r\n70.6 M    141.3 M  /data/scale-test/output/q35_1\r\n26.8 M    53.5 M   /data/scale-test/output/q36_1\r\n27.1 G    54.1 G   /data/scale-test/output/q37_1\r\n26.5 G    53.1 G   /data/scale-test/output/q38_1\r\n4.0 G     7.9 G    /data/scale-test/output/q39_1\r\n644.9 G   1.3 T    /data/scale-test/output/q3_1\r\n0         0        /data/scale-test/output/q40_1\r\n15.7 M    31.4 M   /data/scale-test/output/q41_1\r\n12.1 G    24.2 G   /data/scale-test/output/q4_1\r\n12.1 G    24.2 G   /data/scale-test/output/q5_1\r\n1.2 G     2.4 G    /data/scale-test/output/q6_1\r\n1.2 G     2.4 G    /data/scale-test/output/q7_1\r\n1.2 G     2.4 G    /data/scale-test/output/q8_1\r\n1.2 G     2.4 G    /data/scale-test/output/q9_1\r\n```\r\n\r\nLots of the output are in large size which I doubt the validation will take a very long time. Should we perform like what NDS queries do? they use `LIMIT` for all the queries that the output are quite small.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1713483581/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716236248",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1716236248",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724",
        "id": 1716236248,
        "node_id": "IC_kwDOD7z77c5mS6_Y",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-12T18:39:25Z",
        "updated_at": "2023-09-12T18:39:25Z",
        "author_association": "COLLABORATOR",
        "body": "Limit has problems, even if there is an order by before it there can be ambiguity in the order of the results.  I would much rather do something like the following.\r\n\r\n```\r\nimport org.apache.spark.sql.DataFrame\r\n\r\ndef compare(left: DataFrame, right: DataFrame): DataFrame = {\r\n  val leftCount = left.groupBy(left.columns.map(col(_)): _*).count\r\n  val rightCount = right.groupBy(right.columns.map(col(_)): _*).count\r\n  val joinOn = leftCount.columns.map(c => leftCount(c) <=> rightCount(c)).reduceLeft(_ and _)\r\n  val onlyRight = rightCount.join(leftCount, joinOn, joinType=\"left_anti\").withColumn(\"_in_column\", lit(\"right\"))\r\n  val onlyLeft = leftCount.join(rightCount, joinOn, joinType=\"left_anti\").withColumn(\"_in_column\", lit(\"left\"))\r\n  onlyRight.union(onlyLeft)\r\n}\r\n```\r\n\r\nThis should run a query that compares the two dataframes. There is a lot we can do to improve it. It does not support Maps, and floating-point is not going to be compared very well, but that should be good enough for the current scale tests and should be a good starting point.\r\n\r\nIt would also be nice to explicitly verify that the schemas match before running the query and if we see a map to throw an exception, or if you see floating point to output a warning.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716236248/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]