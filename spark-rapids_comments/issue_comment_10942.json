[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137412549",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2137412549",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942",
        "id": 2137412549,
        "node_id": "IC_kwDOD7z77c5_ZlPF",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-05-29T13:25:45Z",
        "updated_at": "2024-05-29T13:25:45Z",
        "author_association": "COLLABORATOR",
        "body": "Do you have any plans on how to support this? Project is fairly simple because it is a one to one relationship with input and output batches. Technically it could be a one to many relationship if we run out of memory and split an input batch to make it work.  How would this work for hash aggregate that can have a many to many relationship?  Even trying to trigger it on how long it took to run feels problematic because we might not see any slowness until after the first batch, which then would require us to save around all input batches or save them out on the chance that they might be needed to reproduce the problem.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137412549/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148761507",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2148761507",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942",
        "id": 2148761507,
        "node_id": "IC_kwDOD7z77c6AE3-j",
        "user": {
            "login": "winningsix",
            "id": 2278268,
            "node_id": "MDQ6VXNlcjIyNzgyNjg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2278268?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/winningsix",
            "html_url": "https://github.com/winningsix",
            "followers_url": "https://api.github.com/users/winningsix/followers",
            "following_url": "https://api.github.com/users/winningsix/following{/other_user}",
            "gists_url": "https://api.github.com/users/winningsix/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/winningsix/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/winningsix/subscriptions",
            "organizations_url": "https://api.github.com/users/winningsix/orgs",
            "repos_url": "https://api.github.com/users/winningsix/repos",
            "events_url": "https://api.github.com/users/winningsix/events{/privacy}",
            "received_events_url": "https://api.github.com/users/winningsix/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-06-05T02:54:56Z",
        "updated_at": "2024-06-05T02:54:56Z",
        "author_association": "COLLABORATOR",
        "body": "@revans2 Sorry for my late reply. Internally we had a discussion around this with @binmahone @res-life @liurenjie1024 @GaryShen2008. Just as what we offline discussed, we will change the granularity from batch to task level. Thus, it should work for stateful operator like aggregation. Also, regards to the dump timing, we're considering introducing other two modes: i. exact id matching via task id or split id; ii. dumping first few tasks. Later one can help non-tailing case. For this part, let's explore option whether we can be consistent with @jlowe 's profiler tool.   @liurenjie1024 will help on that later and @binmahone is helping explore options regards id matching approach.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148761507/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151541838",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2151541838",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942",
        "id": 2151541838,
        "node_id": "IC_kwDOD7z77c6APexO",
        "user": {
            "login": "binmahone",
            "id": 6416599,
            "node_id": "MDQ6VXNlcjY0MTY1OTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6416599?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/binmahone",
            "html_url": "https://github.com/binmahone",
            "followers_url": "https://api.github.com/users/binmahone/followers",
            "following_url": "https://api.github.com/users/binmahone/following{/other_user}",
            "gists_url": "https://api.github.com/users/binmahone/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/binmahone/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/binmahone/subscriptions",
            "organizations_url": "https://api.github.com/users/binmahone/orgs",
            "repos_url": "https://api.github.com/users/binmahone/repos",
            "events_url": "https://api.github.com/users/binmahone/events{/privacy}",
            "received_events_url": "https://api.github.com/users/binmahone/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-06-06T06:56:31Z",
        "updated_at": "2024-06-06T07:02:27Z",
        "author_association": "COLLABORATOR",
        "body": "hi @revans2 \r\n\r\nIn the new LORE implementation we'll use two IDs to uniquely identify the lifespan of a specific operator in a specific task:\r\n\r\n1. LoreID, to identify an operator in the SQL. We can't use SparkPlan.id as it is not stable. We will use sth like a DFS traversal to number each operator as LoreID,  and print the LoreID in spark UI for each operator. AQE case will also be covered.\r\n2. ParittionID, since there will be N concurrent task for a specific operator, we need to identify which one. We can't use taskID as it is unstable. We'll instead use RDD's parititon index.\r\n\r\nConsider a case where we have skew in e.g. JoinExec, the skew task will exhibit consistent LoreID+ParittionID among different runs of the same SQL (Even in the same spark session). With this design, users can dump data only related to the problematic operator in a specific task, and we can replay the specific operator at local in a single thread.\r\n\r\nThe LoreID+PartitionID design can also be extend to enable self-contained profiling (https://github.com/NVIDIA/spark-rapids/pull/10870). Currently, #10870 can be enabled based on time range/job range/stage range. However job range and stage range should be considered unstable and may result in unexpected traces dumped. With LoreID+PartitionID, we are more specific and acurate to express what traces we need. (LoreID+PartitionID can uniquely identify which task on which executor)\r\n\r\nThis is how we see it, what you think  @revans2 @jlowe @GaryShen2008 ?  @winningsix @liurenjie1024 please feel free to add your inputs.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151541838/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]