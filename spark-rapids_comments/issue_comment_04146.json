[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049771167",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1049771167",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146",
        "id": 1049771167,
        "node_id": "IC_kwDOD7z77c4-kjyf",
        "user": {
            "login": "res-life",
            "id": 8166419,
            "node_id": "MDQ6VXNlcjgxNjY0MTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8166419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/res-life",
            "html_url": "https://github.com/res-life",
            "followers_url": "https://api.github.com/users/res-life/followers",
            "following_url": "https://api.github.com/users/res-life/following{/other_user}",
            "gists_url": "https://api.github.com/users/res-life/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/res-life/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/res-life/subscriptions",
            "organizations_url": "https://api.github.com/users/res-life/orgs",
            "repos_url": "https://api.github.com/users/res-life/repos",
            "events_url": "https://api.github.com/users/res-life/events{/privacy}",
            "received_events_url": "https://api.github.com/users/res-life/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-24T11:37:38Z",
        "updated_at": "2022-02-24T11:37:38Z",
        "author_association": "COLLABORATOR",
        "body": "@revans2  Please help review the solution:\r\n\r\n\r\nInterval types can be found in https://spark.apache.org/docs/latest/sql-ref-datatypes.html\r\n\r\nCurrently plugin do not support write for csv, so let's talk about reading interval type from CSV.\r\n\r\nSpark read interval code is in:\r\n[IntervalUtils.fromDayTimeString](https://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala#L430)\r\n\r\nThere are legacy form and normal form switched by SQLConf.LEGACY_FROM_DAYTIME_STRING.\r\n\r\n**legacy form**\r\nBy default, legacy from daytime string is disable, so we may not support this.\r\n\r\nSQLConf.LEGACY_FROM_DAYTIME_STRING See: https://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L3042\r\n\r\nparseDayTimeLegacy see:\r\nhttps://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala#L463toL475\r\n\r\n**normal form**\r\nBy default, Spark use this form,  some examples see below table, we will support this form only.\r\n\r\n|type|example|valid|comment|\r\n|:-|:-|:-|:-|\r\n|INTERVAL DAY | INTERVAL '100' DAY  |true    |    |\r\n|INTERVAL DAY | 100 DAY  |true    |    |\r\n|INTERVAL DAY TO HOUR | INTERVAL '100 10' DAY TO HOUR  |true    |    |\r\n|INTERVAL DAY TO HOUR | 100 10   |true    |    |\r\n|INTERVAL DAY TO SECOND | INTERVAL '100 10:30:40.999999' DAY TO SECOND  |true    |    |\r\n|INTERVAL DAY TO SECOND | 100 10:30:40.999999  |true    |    |\r\n|INTERVAL DAY TO SECOND | INTERVAL '100 10:30:40. &nbsp;  &nbsp;999999' DAY TO SECOND  |false    | has extra spaces   |\r\n|INTERVAL DAY TO SECOND | INTERVAL '100  &nbsp;  &nbsp; 10:30:40.999999' DAY TO SECOND  |false    |  has extra spaces     |\r\n|INTERVAL DAY TO SECOND | 100  &nbsp;  &nbsp;  10:30:40.999999  |false    | has extra spaces      |\r\n|INTERVAL DAY TO SECOND | 100 10:30: &nbsp;  &nbsp; 40.999999  |false    | has extra spaces      |\r\n|INTERVAL DAY TO SECOND | INTERVAL '-100 10:30:40.999999' DAY TO SECOND  |true    |       |\r\n|INTERVAL DAY TO SECOND | INTERVAL -'-100 10:30:40.999999' DAY TO SECOND  |true    |    two negative signs is positive  |\r\n|INTERVAL DAY TO SECOND | -100 10:30:40.999999  |true    |       |\r\n|INTERVAL DAY TO SECOND | INTERVAL '100 26:30:40.999999' DAY TO SECOND  |false    |    hour is 26 > 23  |\r\n\r\nThe invalid value will be null when reading csv.\r\n\r\n**proposed solution for the normal form**\r\n\r\nUse Cudf ColumnView.extractRe to extract the day, hour, ... , second fields by specifing the groups in regexp, and then calculate the micros.\r\n\r\nGpu code is like:\r\n```\r\n    val intervalCV = builder.buildAndPutOnDevice()\r\n    val start = System.nanoTime()\r\n    val p = \"^INTERVAL\\\\s+([+|-])?'([+|-])?(\\\\d{1,9}) (\\\\d{1,2}):(\\\\d{1,2}):\" +\r\n        \"(\\\\d{1,2})(\\\\.\\\\d{1,9})?'\\\\s+DAY\\\\s+TO\\\\s+SECOND$\"\r\n\r\n    // e.g.: INTERVAL -'-100 10:30:40.999999' DAY TO SECOND\r\n    // group 0: sign is -\r\n    // group 1: sign is -\r\n    // group 2: day is 100\r\n    // group 3: hour 10\r\n    // group 4: minute 30\r\n    // group 5: second 40\r\n    // group 6: micro seconds 999999\r\n\r\n    withResource(intervalCV.extractRe(p)) {\r\n      table => {\r\n        println(\"row count: \" + table.getRowCount)\r\n\r\n        val micros = table.getColumn(2).castTo(DType.INT64).mul(Scalar.fromLong(86400L * 1000000L))   // day to micros\r\n            .add(table.getColumn(3).castTo(DType.INT64).mul(Scalar.fromLong(3600 * 1000000L))) // hour to micros\r\n            .add(table.getColumn(4).castTo(DType.INT64).mul(Scalar.fromLong(60 * 1000000L))) // minute\r\n            .add(table.getColumn(5).castTo(DType.INT64).mul(Scalar.fromLong(1000000L))) \r\n            .add(table.getColumn(6).castTo(DType.INT64))\r\n      }\r\n    }\r\n    val timeS =(System.nanoTime() - start).toDouble / TimeUnit.SECONDS.toNanos(1)\r\n    println(s\"GPU used time $timeS S\")\r\n```\r\n\r\nCpu code is like\r\n```\r\n    while (i < rowCount) {\r\n      IntervalUtils.fromDayTimeString(intervals(i))\r\n      i += 1\r\n    }\r\n```\r\n\r\nrow count: 10,000,000\r\nGPU used time 0.86659265 S\r\nCPU used time 16.818680952 S\r\nGPU speedups about 19x\r\nIs this acceptable or have more effient approach?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049771167/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049986672",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1049986672",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146",
        "id": 1049986672,
        "node_id": "IC_kwDOD7z77c4-lYZw",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-24T15:39:58Z",
        "updated_at": "2022-02-24T15:39:58Z",
        "author_association": "COLLABORATOR",
        "body": "I really dislike regular expression use in casts, but it is a good first step.  It would be nice to file a follow on issue to write a custom kernel to do this for us.\r\n\r\nAlso I assume you know that your code to do the conversion is leaking a lot of column views. I assume you did that just for readability of the code.\r\n\r\nSecond have you tested this with CSV?  The patch that added in support for writing/reading CSV https://issues.apache.org/jira/browse/SPARK-36831 did not add in anything that calls `fromDayTimeString` or similar. The Parquet code just stored these as an int or a long.  Please check to see if CSV is doing the same, because I suspect that they are, and then we don't need to do as much special processing.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049986672/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1050499527",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1050499527",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146",
        "id": 1050499527,
        "node_id": "IC_kwDOD7z77c4-nVnH",
        "user": {
            "login": "res-life",
            "id": 8166419,
            "node_id": "MDQ6VXNlcjgxNjY0MTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8166419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/res-life",
            "html_url": "https://github.com/res-life",
            "followers_url": "https://api.github.com/users/res-life/followers",
            "following_url": "https://api.github.com/users/res-life/following{/other_user}",
            "gists_url": "https://api.github.com/users/res-life/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/res-life/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/res-life/subscriptions",
            "organizations_url": "https://api.github.com/users/res-life/orgs",
            "repos_url": "https://api.github.com/users/res-life/repos",
            "events_url": "https://api.github.com/users/res-life/events{/privacy}",
            "received_events_url": "https://api.github.com/users/res-life/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-25T04:01:48Z",
        "updated_at": "2022-02-25T05:06:38Z",
        "author_association": "COLLABORATOR",
        "body": "Filed an issue: https://github.com/rapidsai/cudf/issues/10356\r\n\r\nCSV is text file, the day-time interval is stored in string form, e.g: \r\n```\r\nINTERVAL '100 10:30:40.999999' DAY TO SECOND \r\n```\r\nSpark used a similar method to parse interval string to day-time interval:  IntervalUtils.castStringToDTInterval\r\nThe IntervalUtils.fromDayTimeString in the example code also invoked IntervalUtils.castStringToDTInterval.\r\n\r\nI know the leaking in the example code, thanks the kindly reminder.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1050499527/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1092382787",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1092382787",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146",
        "id": 1092382787,
        "node_id": "IC_kwDOD7z77c5BHHBD",
        "user": {
            "login": "res-life",
            "id": 8166419,
            "node_id": "MDQ6VXNlcjgxNjY0MTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8166419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/res-life",
            "html_url": "https://github.com/res-life",
            "followers_url": "https://api.github.com/users/res-life/followers",
            "following_url": "https://api.github.com/users/res-life/following{/other_user}",
            "gists_url": "https://api.github.com/users/res-life/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/res-life/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/res-life/subscriptions",
            "organizations_url": "https://api.github.com/users/res-life/orgs",
            "repos_url": "https://api.github.com/users/res-life/repos",
            "events_url": "https://api.github.com/users/res-life/events{/privacy}",
            "received_events_url": "https://api.github.com/users/res-life/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-08T02:30:19Z",
        "updated_at": "2022-04-08T02:30:19Z",
        "author_association": "COLLABORATOR",
        "body": "Spark Accelerator already supported  reading `day-time` interval from CSV temporarily.\r\nBut is still waiting cuDF kernel to support.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1092382787/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280243559",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1280243559",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146",
        "id": 1280243559,
        "node_id": "IC_kwDOD7z77c5MTvdn",
        "user": {
            "login": "res-life",
            "id": 8166419,
            "node_id": "MDQ6VXNlcjgxNjY0MTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8166419?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/res-life",
            "html_url": "https://github.com/res-life",
            "followers_url": "https://api.github.com/users/res-life/followers",
            "following_url": "https://api.github.com/users/res-life/following{/other_user}",
            "gists_url": "https://api.github.com/users/res-life/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/res-life/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/res-life/subscriptions",
            "organizations_url": "https://api.github.com/users/res-life/orgs",
            "repos_url": "https://api.github.com/users/res-life/repos",
            "events_url": "https://api.github.com/users/res-life/events{/privacy}",
            "received_events_url": "https://api.github.com/users/res-life/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-17T03:54:23Z",
        "updated_at": "2022-10-17T03:54:23Z",
        "author_association": "COLLABORATOR",
        "body": "The cuDF issue is closed but without a fix.\r\nFor details see https://github.com/rapidsai/cudf/issues/10356#issuecomment-1168208942.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280243559/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]