[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123765617",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5458#issuecomment-1123765617",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5458",
        "id": 1123765617,
        "node_id": "IC_kwDOD7z77c5C-01x",
        "user": {
            "login": "tgravescs",
            "id": 4563792,
            "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tgravescs",
            "html_url": "https://github.com/tgravescs",
            "followers_url": "https://api.github.com/users/tgravescs/followers",
            "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
            "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
            "organizations_url": "https://api.github.com/users/tgravescs/orgs",
            "repos_url": "https://api.github.com/users/tgravescs/repos",
            "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tgravescs/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-11T13:30:07Z",
        "updated_at": "2022-05-11T13:30:07Z",
        "author_association": "COLLABORATOR",
        "body": "I'm a bit confused as to how it would be worse.  The perfile one is doing the same thing and reading essentially reading the file twice correct?  once to scan through to get all block metadata and once to actually read it.  In the multi-file version isn't it supposed to change to only scan the block necessary to get the metadata by skipping to the location and thus read a lot less of the file?   I guess you can see in the cloud version that perfile got much much better at least.  Maybe if local and all in page cache its quicker but I would be curious to see what is slower.  Is the seek in the newer code slower for instance.\r\n\r\nIn your tests is it reading chunks of a file or does it end up reading whole files?  How big are the files?",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123765617/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124489037",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5458#issuecomment-1124489037",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5458",
        "id": 1124489037,
        "node_id": "IC_kwDOD7z77c5DBldN",
        "user": {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-12T03:18:57Z",
        "updated_at": "2022-05-12T08:12:35Z",
        "author_association": "COLLABORATOR",
        "body": ">The perfile one is doing the same thing and reading essentially reading the file twice correct? ... In the multi-file version isn't it supposed to change to only scan the block necessary to get the metadata by skipping to the location and thus read a lot less of the file?\r\n\r\nYes, PERFILE reads only the necessary part of a file twice. But the newer reader used by multi-threaded reading changes more than essentially reading a file. It reads the data directly as stream, similar to how CPU does, without collecting the block metadata ahead. It is almost another thing comparing to the original collecting-meta-then-copying-data way. \r\n\r\nThe key point that the newer code can improve perf a lot for cloud is, I think, it calls `seek` only once at the begining for each partition. But `seek` is called as many times as the number of the necessary blocks (more than 1000 in my tests) by collecting block metadata, so it took a lot of time to read the metadata in our tests, even only for the necessary part. I guess `seek` is probably slow for cloud cases. And the newer code reduces the calls to `seek` quite a lot.\r\n\r\nThe reasons why the orignal code is faster for local files in my tests are, I think,\r\n\r\n1. `seek` is fast enough for local files, so collecting metadata should not take much time.\r\n2. With the given block's metadata, most blocks were merged into one copy range, it reduced the data copying count a lot.\r\n3. It copied the raw data chunk by chunk according to the merged copy ranges directly into the batch buffer, without parsing.\r\n\r\nWhile the newer code needs to parse the meta and sync, rewirte the meta and sync, and copy the data to batch buffer per block, without merging. \r\n\r\n>In your tests is it reading chunks of a file or does it end up reading whole files? How big are the files?\r\n\r\nIn my tests, each file is about 5 MB, with about 1000 blocks inside (used for coalescing benchmarks before). So it should read the whole file.\r\nBesides, the original code copied the data only once for each file, but the newer code ran data copying about 1000 times.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124489037/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]