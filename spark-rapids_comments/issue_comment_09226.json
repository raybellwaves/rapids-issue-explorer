[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724771107",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9226#issuecomment-1724771107",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9226",
        "id": 1724771107,
        "node_id": "IC_kwDOD7z77c5mzesj",
        "user": {
            "login": "GaryShen2008",
            "id": 22128535,
            "node_id": "MDQ6VXNlcjIyMTI4NTM1",
            "avatar_url": "https://avatars.githubusercontent.com/u/22128535?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GaryShen2008",
            "html_url": "https://github.com/GaryShen2008",
            "followers_url": "https://api.github.com/users/GaryShen2008/followers",
            "following_url": "https://api.github.com/users/GaryShen2008/following{/other_user}",
            "gists_url": "https://api.github.com/users/GaryShen2008/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GaryShen2008/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GaryShen2008/subscriptions",
            "organizations_url": "https://api.github.com/users/GaryShen2008/orgs",
            "repos_url": "https://api.github.com/users/GaryShen2008/repos",
            "events_url": "https://api.github.com/users/GaryShen2008/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GaryShen2008/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-19T03:15:43Z",
        "updated_at": "2023-09-19T03:15:43Z",
        "author_association": "COLLABORATOR",
        "body": "Discussed a little about this issue with Peixin.\r\nThe original #9213 issue was caused because the default jar of the DB's aggregate jar was changed to empty content, but the deploy script was to deploy the default jar. \r\nI think we don't want to do all the same things in nightly pipeline to the premerge pipeline.\r\nThe premerge pipeline is required to be fast enough to run. So currently we run the parrallel builds on DB and the other shims. If we change as nightly pipeline, it'll require some sequential builds, which spend much time.\r\nSo, the problem seems the premerge doesn't verify what deploy.sh does and what the last packaging in dist needs.\r\nI wonder if we can create a validation script to check the aggregate jar(which is used to deploy in deploy.sh) about what the minimum requirement in the dist packaging step, for example, the properties file should exist. And integrate this script into the profile for each aggregate jar or added into the premerge build script. So that we don't do the real deployment, but still check the requirement from deployment point of view.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724771107/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736089171",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9226#issuecomment-1736089171",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9226",
        "id": 1736089171,
        "node_id": "IC_kwDOD7z77c5nep5T",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-09-26T18:43:12Z",
        "updated_at": "2023-09-26T18:43:12Z",
        "author_association": "COLLABORATOR",
        "body": "> The original #9213 issue was caused because the default jar of the DB's aggregate jar was changed to empty content, but the deploy script was to deploy the default jar.\r\n\r\nThis already points at the deficiency that there is no single source of truth for what modules require deployment.\r\n\r\n> I think we don't want to do all the same things in nightly pipeline to the premerge pipeline.\r\n> The premerge pipeline is required to be fast enough to run. So currently we run the parrallel builds on DB and the other shims. If we change as nightly pipeline, it'll require some sequential builds, which spend much time.\r\n\r\nThink of this as Spark or `buildall`. We should have a single way of coding the build flow logic. The speed should be determined  just by a scalability tunables. This includes the list of shims required for the given pipeline instance, and the degree of parallelism available for that pipeline instance.   \r\n\r\n> So, the problem seems the premerge doesn't verify what deploy.sh does and what the last packaging in dist needs.\r\n> I wonder if we can create a validation script to check the aggregate jar(which is used to deploy in deploy.sh) about what the minimum requirement in the dist packaging step, for example, the properties file should exist. And integrate this script into the profile for each aggregate jar or added into the premerge build script. So that we don't do the real deployment, but still check the requirement from deployment point of view.\r\n\r\nAddressing specific regressions does not take us to a significantly higher level of confidence offered by the request in this issue if the build logic in the nightly and pre-merge continue to diverge.\r\n\r\nOnce the pre-merge passed we should only worry about test failures from the set of tests \"nightly minus pre-merge\", but not about whether the nightly jar build is working at all. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736089171/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]