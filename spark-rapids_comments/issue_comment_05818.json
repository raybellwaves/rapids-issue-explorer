[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155701479",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155701479",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1155701479,
        "node_id": "IC_kwDOD7z77c5E4prn",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-14T20:52:28Z",
        "updated_at": "2022-06-14T20:52:28Z",
        "author_association": "COLLABORATOR",
        "body": "@gerashegalov, I am trying to follow these instructions and I can't replicate it so far. I tried local-cluster mode and standalone mode. Could you provide a bit more information:\r\n\r\n1. What version of ucx do you have installed (`ucx_info -v`). I am running 1.12.0 locally.\r\n2. I see 5.5 average constantly. Do you ever see that? Or is it 1 or 10 alternating. I am wondering if the issue that you are seeing is on the write, or on the read (or both). It would be nice to get it to happen, and try to inspect the parquet file on the CPU or with the shuffle manger \"off\" (e.g. a read + a show to get its contents).\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155701479/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155736996",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155736996",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1155736996,
        "node_id": "IC_kwDOD7z77c5E4yWk",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-14T21:39:00Z",
        "updated_at": "2022-06-14T21:56:33Z",
        "author_association": "COLLABORATOR",
        "body": "@abellina I downloaded the latest full revision https://github.com/openucx/ucx/tags yesterday \r\n```\r\n$ ucx_info -v\r\n# UCT version=1.12.1 revision dc92435\r\n# configured with: --disable-logging --disable-debug --disable-assertions --disable-params-check --prefix=/usr --enable-examples --with-java=no\r\n```\r\nI'll try 1.12.0 too.\r\n\r\nhttps://github.com/openucx/ucx/tags\r\n\r\nUPDATE downgrading 1.12.0 yields the same buggy behavior\r\n```\r\n$ ucx_info -v\r\n# UCT version=1.12.0 revision d367332\r\n# configured with: --disable-logging --disable-debug --disable-assertions --disable-params-check --prefix=/usr --enable-examples --with-java=no\r\n```\r\n\r\n> I see 5.5 average constantly. Do you ever see that? Or is it 1 or 10 alternating. I am wondering if the issue that you are seeing is on the write, or on the read (or both). It would be nice to get it to happen, and try to inspect the parquet file on the CPU or with the shuffle manger \"off\" (e.g. a read + a show to get its contents).\r\n\r\nI see 5.5 too occasionally (updated description)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155736996/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155770339",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155770339",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1155770339,
        "node_id": "IC_kwDOD7z77c5E46fj",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-14T22:26:53Z",
        "updated_at": "2022-06-14T22:26:53Z",
        "author_association": "COLLABORATOR",
        "body": "@abellina Interestingly this behavior has not reproduced for me yet on a bare-metal Ubuntu20.04 although it reproduces very easily on WSL2 VM Ubuntu20.04",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155770339/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974078",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1156974078",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1156974078,
        "node_id": "IC_kwDOD7z77c5E9gX-",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-15T21:40:55Z",
        "updated_at": "2022-06-15T21:40:55Z",
        "author_association": "COLLABORATOR",
        "body": "Debugging with h/t @abellina it looks like the workaround is to set ` --conf spark.executorEnv.UCX_TLS=cuda_copy,tcp`\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974078/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974866",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1156974866",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1156974866,
        "node_id": "IC_kwDOD7z77c5E9gkS",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-15T21:42:07Z",
        "updated_at": "2022-06-15T21:42:07Z",
        "author_association": "COLLABORATOR",
        "body": "```\r\nWed Jun 15 14:41:09 2022\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 515.48.03    Driver Version: 516.25       CUDA Version: 11.7     |\r\n|-------------------------------+----------------------+----------------------+\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974866/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159615192",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1159615192",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1159615192,
        "node_id": "IC_kwDOD7z77c5FHlLY",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-19T04:34:24Z",
        "updated_at": "2022-06-19T04:36:01Z",
        "author_association": "COLLABORATOR",
        "body": "<details>\r\n<summary>ucx debug log files for the repro run</summary>\r\n<pre>\r\n$ $SPARK_HOME/bin/pyspark   --driver-class-path $PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar    --conf spark.executor.extraClassPath=$PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar    --conf spark.plugins=com.nvidia.spark.SQLPlugin    --conf spark.rapids.sql.explain=ALL   --conf spark.shuffle.manager=com.nvidia.spark.rapids.spark321.RapidsShuffleManager   --conf spark.shuffle.service.enabled=false    --conf spark.dynamicAllocation.enabled=false  --conf spark.executorEnv.UCX_ERROR_SIGNALS= --conf spark.executorEnv.UCX_LOG_LEVEL=data --conf spark.executorEnv.UCX_LOG_FILE=/tmp/ucx_log_%p --conf spark.executorEnv.LD_LIBRARY_PATH=$HOME/dist/ucx_debug/lib --conf spark.executorEnv.UCX_MEMTYPE_CACHE=n   --conf spark.rapids.memory.gpu.minAllocFraction=0  --conf spark.rapids.memory.gpu.allocFraction=0.2 --master local-cluster[2,1,1200]\r\nPython 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \r\n[GCC 10.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n22/06/18 01:19:08 WARN Utils: Your hostname, NV-3L4YVG3 resolves to a loopback address: 127.0.1.1; using 172.22.19.221 instead (on interface eth0)\r\n22/06/18 01:19:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n22/06/18 01:19:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n22/06/18 01:19:10 WARN RapidsPluginUtils: RAPIDS Accelerator 22.08.0-SNAPSHOT using cudf 22.08.0-SNAPSHOT.\r\n22/06/18 01:19:10 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\r\n22/06/18 01:19:10 WARN RapidsShuffleInternalManager: Rapids Shuffle Plugin enabled. Transport enabled (remote fetches will use com.nvidia.spark.rapids.shuffle.ucx.UCXShuffleTransport. To disable the RAPIDS Shuffle Manager set `spark.rapids.shuffle.enabled` to false\r\nWelcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.1\r\n      /_/\r\n\r\nUsing Python version 3.8.13 (default, Mar 25 2022 06:04:10)\r\nSpark context Web UI available at http://172.22.19.221:4040/\r\nSpark context available as 'sc' (master = local-cluster[2,1,1200], app id = app-20220618011911-0000).\r\nSparkSession available as 'spark'.\r\n>>> spark.read.format('parquet').load('/tmp/df.parquet').selectExpr('avg(a)').collect()\r\n22/06/18 01:19:23 WARN GpuOverrides:                                            \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:23 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:23 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:23 WARN GpuOverrides: \r\n*Exec <ShuffleExchangeExec> will run on GPU\r\n  *Partitioning <SinglePartition$> will run on GPU\r\n  *Exec <HashAggregateExec> will run on GPU\r\n    *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n      *Expression <Average> avg(a#0) will run on GPU\r\n    *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:25 WARN GpuOverrides: =>                             (1 + 1) / 2]\r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n\r\n22/06/18 01:19:25 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n\r\n[Row(avg(a)=1.0)]                                                               \r\n>>> spark.read.format('parquet').load('/tmp/df.parquet').selectExpr('avg(a)').collect()\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#31) will run on GPU\r\n    *Expression <Average> avg(a#31) will run on GPU\r\n  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU\r\n        *Expression <Average> avg(a#31) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#31) will run on GPU\r\n    *Expression <Average> avg(a#31) will run on GPU\r\n  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU\r\n        *Expression <Average> avg(a#31) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#31) will run on GPU\r\n    *Expression <Average> avg(a#31) will run on GPU\r\n  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU\r\n        *Expression <Average> avg(a#31) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <ShuffleExchangeExec> will run on GPU\r\n  *Partitioning <SinglePartition$> will run on GPU\r\n  *Exec <HashAggregateExec> will run on GPU\r\n    *Expression <AggregateExpression> partial_avg(a#31) will run on GPU\r\n      *Expression <Average> avg(a#31) will run on GPU\r\n    *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#31) will run on GPU\r\n    *Expression <Average> avg(a#31) will run on GPU\r\n  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU\r\n\r\n22/06/18 01:19:27 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#31) will run on GPU\r\n    *Expression <Average> avg(a#31) will run on GPU\r\n  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU\r\n\r\n[Row(avg(a)=10.0)]\r\n</pre>\r\n</details>\r\n\r\n[ucx_log_sanitized.zip](https://github.com/NVIDIA/spark-rapids/files/8934760/ucx_log_sanitized.zip)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159615192/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162099203",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1162099203",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1162099203,
        "node_id": "IC_kwDOD7z77c5FRDoD",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-21T17:52:34Z",
        "updated_at": "2022-06-21T17:52:34Z",
        "author_association": "COLLABORATOR",
        "body": "@gerashegalov the UCX debug logs are useful, but I would like to see corresponding executor logs as well (especially in standalone). If you can turn the log level to DEBUG in spark that would be really great.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162099203/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162513861",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1162513861",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1162513861,
        "node_id": "IC_kwDOD7z77c5FSo3F",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-22T01:11:41Z",
        "updated_at": "2022-06-22T01:11:41Z",
        "author_association": "COLLABORATOR",
        "body": "Repro logs with the DEBUG level Spark logs @abellina  [ucx_log_sanitized.zip](https://github.com/NVIDIA/spark-rapids/files/8953661/ucx_log_sanitized.zip)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162513861/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163113472",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1163113472",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1163113472,
        "node_id": "IC_kwDOD7z77c5FU7QA",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-22T13:41:23Z",
        "updated_at": "2022-06-22T13:41:23Z",
        "author_association": "COLLABORATOR",
        "body": "OK, thanks for the logs! With @gerashegalov's help I was able to setup WSL2 and repro this in a windows machine.\r\n\r\nThe issue is specific to cuda_ipc with wakeup, which is using events to trigger when a send/recv is finished. What looks to be happening is that we are getting a callback from UCX even though the copy is not complete, which explains why it sometimes works. This is our default mode, rather than busy waiting, since we've found it to be better for performance and we know it works in other environments. Setting: `--conf spark.rapids.shuffle.ucx.useWakeup=false` (turning off wakeup and relying on busy waiting), is a potential workaround for WSL2, but it's not great. This looks to be a bug in UCX or CUDA (callback functions).\r\n\r\nI added some logging to capture the batch being received and the corresponding sent batch. This is the batch at the receiving end:\r\n\r\n```\r\nDEBUG fetched cb Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 27 7f77914ebbb0)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 28 7f77914fffd0)}], cudfTable=140151513387040, rows=1}\r\nGPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000200, length=8, id=-1} VAL: null\r\nCOLUMN 0 - FLOAT64\r\n0 0.0\r\nGPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000240, length=8, id=-1} VAL: null\r\nCOLUMN 1 - INT64\r\n0 0\r\n```\r\n\r\nBut the sender sent this (before UCX send):\r\n\r\n```\r\nDEBUG to send Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 25 7f160808ea00)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 26 7f160808eb70)}], cudfTable=139732600243792, rows=1}\r\nGPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000000, length=8, id=-1} VAL: null\r\nCOLUMN 0 - FLOAT64\r\n0 10.0\r\nGPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000040, length=8, id=-1} VAL: null\r\nCOLUMN 1 - INT64\r\n0 1\r\n```\r\n\r\nI believe the next step is to see if the above explanation makes sense to @Akshay-Venkatesh, then we may need to help repro at a lower level or help test a fix.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163113472/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178400825",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1178400825",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1178400825,
        "node_id": "IC_kwDOD7z77c5GPPg5",
        "user": {
            "login": "sameerz",
            "id": 7036315,
            "node_id": "MDQ6VXNlcjcwMzYzMTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7036315?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sameerz",
            "html_url": "https://github.com/sameerz",
            "followers_url": "https://api.github.com/users/sameerz/followers",
            "following_url": "https://api.github.com/users/sameerz/following{/other_user}",
            "gists_url": "https://api.github.com/users/sameerz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sameerz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sameerz/subscriptions",
            "organizations_url": "https://api.github.com/users/sameerz/orgs",
            "repos_url": "https://api.github.com/users/sameerz/repos",
            "events_url": "https://api.github.com/users/sameerz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sameerz/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-08T00:28:03Z",
        "updated_at": "2022-07-08T00:28:03Z",
        "author_association": "COLLABORATOR",
        "body": "Removing P1 and removing from 22.08 since the issue only occurs in WSL2 (which we do not support).  ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178400825/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180674173",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1180674173",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1180674173,
        "node_id": "IC_kwDOD7z77c5GX6h9",
        "user": {
            "login": "Akshay-Venkatesh",
            "id": 3201794,
            "node_id": "MDQ6VXNlcjMyMDE3OTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3201794?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Akshay-Venkatesh",
            "html_url": "https://github.com/Akshay-Venkatesh",
            "followers_url": "https://api.github.com/users/Akshay-Venkatesh/followers",
            "following_url": "https://api.github.com/users/Akshay-Venkatesh/following{/other_user}",
            "gists_url": "https://api.github.com/users/Akshay-Venkatesh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Akshay-Venkatesh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Akshay-Venkatesh/subscriptions",
            "organizations_url": "https://api.github.com/users/Akshay-Venkatesh/orgs",
            "repos_url": "https://api.github.com/users/Akshay-Venkatesh/repos",
            "events_url": "https://api.github.com/users/Akshay-Venkatesh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Akshay-Venkatesh/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-11T17:26:23Z",
        "updated_at": "2022-07-11T17:26:35Z",
        "author_association": "NONE",
        "body": "> OK, thanks for the logs! With @gerashegalov's help I was able to setup WSL2 and repro this in a windows machine.\r\n> \r\n> The issue is specific to cuda_ipc with wakeup, which is using events to trigger when a send/recv is finished. What looks to be happening is that we are getting a callback from UCX even though the copy is not complete, which explains why it sometimes works. This is our default mode, rather than busy waiting, since we've found it to be better for performance and we know it works in other environments. Setting: `--conf spark.rapids.shuffle.ucx.useWakeup=false` (turning off wakeup and relying on busy waiting), is a potential workaround for WSL2, but it's not great. This looks to be a bug in UCX or CUDA (callback functions).\r\n> \r\n> I added some logging to capture the batch being received and the corresponding sent batch. This is the batch at the receiving end:\r\n> \r\n> ```\r\n> DEBUG fetched cb Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 27 7f77914ebbb0)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 28 7f77914fffd0)}], cudfTable=140151513387040, rows=1}\r\n> GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000200, length=8, id=-1} VAL: null\r\n> COLUMN 0 - FLOAT64\r\n> 0 0.0\r\n> GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000240, length=8, id=-1} VAL: null\r\n> COLUMN 1 - INT64\r\n> 0 0\r\n> ```\r\n> \r\n> But the sender sent this (before UCX send):\r\n> \r\n> ```\r\n> DEBUG to send Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 25 7f160808ea00)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 26 7f160808eb70)}], cudfTable=139732600243792, rows=1}\r\n> GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000000, length=8, id=-1} VAL: null\r\n> COLUMN 0 - FLOAT64\r\n> 0 10.0\r\n> GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000040, length=8, id=-1} VAL: null\r\n> COLUMN 1 - INT64\r\n> 0 1\r\n> ```\r\n> \r\n> I believe the next step is to see if the above explanation makes sense to @Akshay-Venkatesh, then we may need to help repro at a lower level or help test a fix.\r\n\r\n@abellina Sorry for the late response but do you happen to know if this issue is specific to WSL2? Want to know if there is an identical linux setup where the issue doesn't show up.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180674173/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180774586",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1180774586",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818",
        "id": 1180774586,
        "node_id": "IC_kwDOD7z77c5GYTC6",
        "user": {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-11T19:19:58Z",
        "updated_at": "2022-07-11T19:19:58Z",
        "author_association": "COLLABORATOR",
        "body": "@Akshay-Venkatesh we haven't been able to reproduce the issue on available bare-metal Ubuntu environments. So it may be related to either virtualization aspect of WSL2 or to the Runtime/Driver combination aspect of WSL2. We statically link 11.5 CUDA Runtime.  ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180774586/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]