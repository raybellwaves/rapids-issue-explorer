[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1070336174",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1070336174",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1070336174,
        "node_id": "IC_kwDOD7z77c4_zAiu",
        "user": {
            "login": "sperlingxx",
            "id": 6276118,
            "node_id": "MDQ6VXNlcjYyNzYxMTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6276118?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sperlingxx",
            "html_url": "https://github.com/sperlingxx",
            "followers_url": "https://api.github.com/users/sperlingxx/followers",
            "following_url": "https://api.github.com/users/sperlingxx/following{/other_user}",
            "gists_url": "https://api.github.com/users/sperlingxx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sperlingxx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sperlingxx/subscriptions",
            "organizations_url": "https://api.github.com/users/sperlingxx/orgs",
            "repos_url": "https://api.github.com/users/sperlingxx/repos",
            "events_url": "https://api.github.com/users/sperlingxx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sperlingxx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-17T05:18:00Z",
        "updated_at": "2022-03-17T05:18:00Z",
        "author_association": "COLLABORATOR",
        "body": "This issue sounds interesting to me. Could I pick it up?\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1070336174/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1071216706",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1071216706",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1071216706,
        "node_id": "IC_kwDOD7z77c4_2XhC",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-17T18:56:45Z",
        "updated_at": "2022-03-17T18:56:45Z",
        "author_association": "COLLABORATOR",
        "body": "If you want to give it a try I am fine with it. I would start with a quick a dirty prototype and then measure performance so we can fail fast if it is not helping. I am a little skeptical that we are going to see too much performance improvement on properly configured queries. It is mainly going to be for queries with skewed data or if someone didn't configure the query to have enough shuffle partitions.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1071216706/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077552857",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077552857",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1077552857,
        "node_id": "IC_kwDOD7z77c5AOibZ",
        "user": {
            "login": "sperlingxx",
            "id": 6276118,
            "node_id": "MDQ6VXNlcjYyNzYxMTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6276118?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sperlingxx",
            "html_url": "https://github.com/sperlingxx",
            "followers_url": "https://api.github.com/users/sperlingxx/followers",
            "following_url": "https://api.github.com/users/sperlingxx/following{/other_user}",
            "gists_url": "https://api.github.com/users/sperlingxx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sperlingxx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sperlingxx/subscriptions",
            "organizations_url": "https://api.github.com/users/sperlingxx/orgs",
            "repos_url": "https://api.github.com/users/sperlingxx/repos",
            "events_url": "https://api.github.com/users/sperlingxx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sperlingxx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-24T12:01:12Z",
        "updated_at": "2022-03-24T12:45:49Z",
        "author_association": "COLLABORATOR",
        "body": "Hi @revans2, I built a demo with `OneSizeBuffer` which acts as a blocking queue holding single element.  `OneSizeBuffer` is supposed to minimize the memory cost on caching host buffer while running the CPU workloads asychronizely. Based on the assumption that shuffle read and concat usually spends less time than subquent GPU processing, caching single host buffer on the deck is enough for the most of time.\r\n \r\nThe first host batch is computed in main thread. If there exists subsequent host batches, they are processed in a separate thread.\r\n\r\n```scala\r\nclass GpuAsyncShuffleCoalesceIterator(child: Iterator[HostConcatResult],\r\n                                      dataTypes: Array[DataType],\r\n                                      metricsMap: Map[String, GpuMetric])\r\n  extends Iterator[ColumnarBatch] with Arm {\r\n\r\n  private[this] val semWaitTime = metricsMap(GpuMetric.SEMAPHORE_WAIT_TIME)\r\n  private[this] val opTimeMetric = metricsMap(GpuMetric.OP_TIME)\r\n  private[this] val outputBatchesMetric = metricsMap(GpuMetric.NUM_OUTPUT_BATCHES)\r\n  private[this] val outputRowsMetric = metricsMap(GpuMetric.NUM_OUTPUT_ROWS)\r\n\r\n  @transient private lazy val buffer = new OneSizeBuffer()\r\n\r\n  private var hostConcatThread: Thread = _\r\n\r\n  private class OneSizeBuffer {\r\n    private val lock = new util.concurrent.locks.ReentrantLock()\r\n    private val notFull = lock.newCondition()\r\n    private val notEmpty = lock.newCondition()\r\n    @volatile private var deck: HostConcatResult = _\r\n    @volatile private var childIsOpen = true\r\n\r\n    def nonEmpty: Boolean = deck != null || (if (childIsOpen) {\r\n        if (!hostConcatThread.isAlive) {\r\n          val stackTraceMsg = hostConcatThread.getStackTrace.mkString(\"\\n\")\r\n          throw new IllegalStateException(\r\n            \"The host concat thread crashed because: \" + stackTraceMsg)\r\n        }\r\n        true\r\n      } else {\r\n        false\r\n      })\r\n\r\n    def offer(): Unit = {\r\n      println(\"===== offer start =====\")\r\n      lock.lock()\r\n      try {\r\n        while (deck != null) notFull.await()\r\n        deck = child.next()\r\n        notEmpty.signal()\r\n        println(\"===== offer end =====\")\r\n      } finally {\r\n        lock.unlock()\r\n      }\r\n    }\r\n\r\n    def take(): HostConcatResult = {\r\n      println(\"===== take start =====\")\r\n      lock.lock()\r\n      try {\r\n        while (deck == null) notEmpty.await()\r\n        val ret = deck\r\n        deck = null\r\n        notFull.signal()\r\n        println(\"===== take end =====\")\r\n        ret\r\n      } finally {\r\n        lock.unlock()\r\n      }\r\n    }\r\n\r\n    def closeHostIterator(): Unit = {\r\n      childIsOpen = false\r\n    }\r\n  }\r\n\r\n  private var isFirstBatch: Boolean = true\r\n  private var childIsEmpty: Boolean = _\r\n\r\n  private def convertHostBatchToDevice(hostConcatResult: HostConcatResult) = {\r\n    GpuSemaphore.acquireIfNecessary(TaskContext.get(), semWaitTime)\r\n    withResource(new MetricRange(opTimeMetric)) { _ =>\r\n      withResource(hostConcatResult) { hostConcatBatch =>\r\n        val batch = HostConcatResultUtil.getColumnarBatch(hostConcatBatch, dataTypes)\r\n        outputBatchesMetric += 1\r\n        outputRowsMetric += batch.numRows()\r\n        batch\r\n      }\r\n    }\r\n  }\r\n\r\n  override def hasNext: Boolean = {\r\n    if (isFirstBatch) {\r\n      isFirstBatch = false\r\n      if (child.hasNext) {\r\n        buffer.offer()\r\n        hostConcatThread = new Thread(() => {\r\n          while (child.hasNext) buffer.offer()\r\n          buffer.closeHostIterator()\r\n        })\r\n        hostConcatThread.start()\r\n        childIsEmpty = false\r\n      } else {\r\n        childIsEmpty = true\r\n      }\r\n      !childIsEmpty\r\n    } else {\r\n      !childIsEmpty && buffer.nonEmpty\r\n    }\r\n  }\r\n\r\n  override def next(): ColumnarBatch = {\r\n    if (!hasNext) {\r\n      throw new NoSuchElementException(\"No more columnar batches\")\r\n    }\r\n    convertHostBatchToDevice(buffer.take())\r\n  }\r\n}\r\n```\r\n\r\nSadly,  I compared this approach with original `GpuShuffleCoalesceIterator`. I didn't find significant performance difference bewteen two approaches. \r\nI created the reduce partition of huge size with `collect_set` on a hot key:\r\n```python\r\nspark = SparkSession.builder.appName(\"Async shuffle coalese\").enableHiveSupport().getOrCreate()\r\n\r\n# generate a single column table which holds 1200000000 random ints ranging from 0 to 30000000\r\ndef gen_data(spark, data_path, n_part=10, rows_per_part=100000):\r\n    rdd = spark.sparkContext.parallelize(list(range(n_part)), numSlices=n_part)\r\n    def rand_gen(seed_iter):\r\n        from random import Random\r\n        from pyspark.sql import Row\r\n        rd = Random(next(seed_iter))\r\n        for _ in range(rows_per_part):\r\n            rdVal = rd.randint(0, 1000 * 1000 * 30)\r\n            yield Row(a=rdVal)\r\n\r\n    rows = rdd.mapPartitions(rand_gen)\r\n    schema = StructType([StructField('a', IntegerType(), False)])\r\n    df = spark.createDataFrame(rows, schema)\r\n    df.write.parquet(data_path)\r\n\r\ngen_data(spark, path, n_part=400, rows_per_part=3000 * 1000) \r\n\r\n# set a moderate batch size\r\nspark.conf.set(\"spark.rapids.sql.batchSizeBytes\", str(1 << 20))\r\n\r\n# run collect_set on a constant key to create a large shuffle read partition\r\ndf = spark.read.parquet(path) \\\r\n                            .groupby(lit(1)) \\\r\n                            .agg(collect_set(col(\"a\")).alias(\"arr\")) \\\r\n                            .selectExpr(\"arr[0]\")\r\n```\r\n\r\nI conducted above tests with various batch size settings both on local machine and yarn. In no condition, there existed a significant performance boost by running shuffle read on a background thread.\r\n\r\nTheoretically, the maximium performance boost which we can gain from the current improvement is the cost of shuffle read + host concat. However, I failed to create a scenairo which has a large I/O cost of shuffle read with a relatively small cost of subsequent GPU processing.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077552857/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077818251",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077818251",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1077818251,
        "node_id": "IC_kwDOD7z77c5APjOL",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-24T16:36:01Z",
        "updated_at": "2022-03-24T16:36:01Z",
        "author_association": "COLLABORATOR",
        "body": "I think this might be because we have good I/O.  Either there is enough memory that reads always hit the page cache or the disks + network are fast enough that it is not a problem.\r\n\r\n@jlowe and @abellina do you think it is worth spending more time trying to find a use case for this, or should we let it go until we are playing around with releasing the GPU Semaphore #4970? Just because I can see it helping there even if the I/O is not slow.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077818251/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077855359",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077855359",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1077855359,
        "node_id": "IC_kwDOD7z77c5APsR_",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-24T17:17:05Z",
        "updated_at": "2022-03-24T17:17:05Z",
        "author_association": "COLLABORATOR",
        "body": "We may need to trigger a lot more than 1 batch concurrently to make them fit under the concat time. I do not think that host concat by itself is the bottleneck, but instead it's the high processing needed upstream closer to the actual network fetch, and by the time this runs we've already waited for the fetch and block decompress that seems to be the highest CPU consumer.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077855359/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077878893",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077878893",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1077878893,
        "node_id": "IC_kwDOD7z77c5APyBt",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-24T17:43:04Z",
        "updated_at": "2022-03-24T17:43:04Z",
        "author_association": "COLLABORATOR",
        "body": "@abellina am I reading the code wrong? We are fetching and concating in a background thread. That would make it so that if we take longer to process the current batch than we do to fetch and concatenate on the CPU the next batch, then the fetch/concat is effectively free.  But if we take longer to fetch/concat, then the GPU processing time is effectively free.\r\n\r\nIf we concatenated the data on the main thread, and not the background thread, then you would be correct.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077878893/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077882589",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077882589",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1077882589,
        "node_id": "IC_kwDOD7z77c5APy7d",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-24T17:47:00Z",
        "updated_at": "2022-03-24T17:47:00Z",
        "author_association": "COLLABORATOR",
        "body": "Oh I think I misunderstood @revans2. Then yeah, agree. The next gives you whatever it could fetch/concat first, then keeps pulling on the shuffle to fetch more.\r\n\r\nThen I agree, if the batch you returned on next() takes a while to process, the next time you call next() it should be free. ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077882589/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096540050",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1096540050",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1096540050,
        "node_id": "IC_kwDOD7z77c5BW9-S",
        "user": {
            "login": "sperlingxx",
            "id": 6276118,
            "node_id": "MDQ6VXNlcjYyNzYxMTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6276118?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sperlingxx",
            "html_url": "https://github.com/sperlingxx",
            "followers_url": "https://api.github.com/users/sperlingxx/followers",
            "following_url": "https://api.github.com/users/sperlingxx/following{/other_user}",
            "gists_url": "https://api.github.com/users/sperlingxx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sperlingxx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sperlingxx/subscriptions",
            "organizations_url": "https://api.github.com/users/sperlingxx/orgs",
            "repos_url": "https://api.github.com/users/sperlingxx/repos",
            "events_url": "https://api.github.com/users/sperlingxx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sperlingxx/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-12T10:30:07Z",
        "updated_at": "2022-04-12T10:30:07Z",
        "author_association": "COLLABORATOR",
        "body": "Hi @abellina @revans2, shall we move further on this issue ? If so, what should we do next ? ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096540050/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096721201",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1096721201",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964",
        "id": 1096721201,
        "node_id": "IC_kwDOD7z77c5BXqMx",
        "user": {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-12T13:21:27Z",
        "updated_at": "2022-04-12T13:21:54Z",
        "author_association": "COLLABORATOR",
        "body": "I am taking a look at this issue: https://github.com/NVIDIA/spark-rapids/issues/5039, and I should have something  to post in the next couple of days, especially for writes. I think perhaps the threading strategy proposed there could be applied here as well, but that shouldn't stop us from getting a speed-of-light of this issue or adding NVTX ranges that would help us call out how often it would trigger as defined. \r\n\r\nThe main thing that could override this is if we can parallelize the reads at a lower level than whole batch. I have not looked at the reads yet, as I've focused on writes. On the writes, there are two stages, one of which is easily parallelizable: each shuffle block per reduce partition is independently written to disk. The second stage is to take all these blocks and combine them in one file per map_id, so this second stage needs to read all the blocks and append to a single file (doable in parallel, just not as easily as the first stage). The first stage is where shuffle compression happens, so we can leverage more cores to compress. So far, this seems to yield some benefits (I estimate around 5% of execution of NDS at 3TB), to the point that now the second stage of the write to a single map_id file, is becoming the next bottleneck, as well as reads.\r\n\r\nSo I think this is the main part: could we decompress also in parallel at a lower level? If so, I don't think doing the whole batch in the background would make as much sense. If we can't decompress in parallel then yes, I think this is the only choice. I could be missing something here so would like to hear other's comments as well.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096721201/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]