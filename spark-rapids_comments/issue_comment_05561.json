[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132978706",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1132978706",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1132978706,
        "node_id": "IC_kwDOD7z77c5Dh-IS",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T14:38:53Z",
        "updated_at": "2022-05-20T14:38:53Z",
        "author_association": "COLLABORATOR",
        "body": "Yes, technically it is possible, but there are a lot of challenges with it. The biggest challenge is not moving the data, but in how do we partition the GPU properly so performance is not horrible. We support having python processes use the same GPU that the main Spark worker process is also using.\r\n\r\nhttps://nvidia.github.io/spark-rapids/docs/additional-functionality/rapids-udfs.html#gpu-support-for-pandas-udf\r\n\r\nBut if all you want to do is a simple MapInPandas operation you have to statically partition the GPUs memory between the python processes and the Spark process. On a 16 GiB card like a v100 it can be the difference between running 2 tasks in parallel and only being able to run a single task at a time on the GPU. This has a big impact for overall performance. It can also impact how much we end up spilling to host memory or disk for out of core operations like sorting. This has a large impact on all GPUs, if they hit those cases. But if what you are primarily doing is ML/DL using python APIs it can make these use cases possible without needing a separate GPU for them in python.\r\n\r\n> a full serialization using arrow is required\r\n\r\nThat is true, but happily CUDF operates on data that is in an arrow like format already so serializing to/from arrow is much cheaper than doing it on the CPU where it goes from Spark's UnsafeRow to arrow before being serialized with the java arrow API. We have seen some really great performance improvements by going from CUDF to Arrow on the CPU to the serialized arrow format using the native arrow library.\r\n\r\nIf you have a specific use case we are happy to work with you on it. ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132978706/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133165691",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133165691",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1133165691,
        "node_id": "IC_kwDOD7z77c5Dirx7",
        "user": {
            "login": "trivialfis",
            "id": 16746409,
            "node_id": "MDQ6VXNlcjE2NzQ2NDA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/16746409?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/trivialfis",
            "html_url": "https://github.com/trivialfis",
            "followers_url": "https://api.github.com/users/trivialfis/followers",
            "following_url": "https://api.github.com/users/trivialfis/following{/other_user}",
            "gists_url": "https://api.github.com/users/trivialfis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/trivialfis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/trivialfis/subscriptions",
            "organizations_url": "https://api.github.com/users/trivialfis/orgs",
            "repos_url": "https://api.github.com/users/trivialfis/repos",
            "events_url": "https://api.github.com/users/trivialfis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/trivialfis/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T17:49:41Z",
        "updated_at": "2022-05-20T18:07:50Z",
        "author_association": "NONE",
        "body": "Thank you for the detailed information. I'm looking into the possibility of using Python interface to implement XGBoost pyspark interface, one of the concerns is that there might be overhead during data serialization, which is not just performance concern but also CPU and GPU memory usage. XGBoost has to load the data in full instead of batches, being memory hungry is one of its most significant blockers for users. As a result we hope that we can the reduce data copies as much as possible.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133165691/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133179698",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133179698",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1133179698,
        "node_id": "IC_kwDOD7z77c5DivMy",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T18:08:54Z",
        "updated_at": "2022-05-20T18:08:54Z",
        "author_association": "COLLABORATOR",
        "body": "There is a java/scala XGBoost interface that bypasses those issues. We have been testing it in integration with the RAPIDs Accelerator and there is no data movement. It stays on the GPU and plays nicely with RAPIDs.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids-examples/tree/branch-22.06/examples/XGBoost-Examples\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133179698/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133186002",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133186002",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1133186002,
        "node_id": "IC_kwDOD7z77c5DiwvS",
        "user": {
            "login": "trivialfis",
            "id": 16746409,
            "node_id": "MDQ6VXNlcjE2NzQ2NDA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/16746409?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/trivialfis",
            "html_url": "https://github.com/trivialfis",
            "followers_url": "https://api.github.com/users/trivialfis/followers",
            "following_url": "https://api.github.com/users/trivialfis/following{/other_user}",
            "gists_url": "https://api.github.com/users/trivialfis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/trivialfis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/trivialfis/subscriptions",
            "organizations_url": "https://api.github.com/users/trivialfis/orgs",
            "repos_url": "https://api.github.com/users/trivialfis/repos",
            "events_url": "https://api.github.com/users/trivialfis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/trivialfis/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T18:16:56Z",
        "updated_at": "2022-05-20T20:04:53Z",
        "author_association": "NONE",
        "body": "Yes. It has been a heated debate between using the Python interface and the Java interface.  There are many concerns around the JAVA interface as well, like dependencies management and user defined objective/metric/callback that has to be run on Python process, synchronization between Python XGB package and JVM package, etc.\r\n\r\nI'm thinking that the cost of serialization might be worth, especially during early development. It's out of scope for XGBoost to work with these non-ML issues (rather it's a data management issue that's best solved in spark). But we also want to understand better if there's a way to avoid the overhead in the future as this is not a XGB specific feature request and might benefit many other workflows as well.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133186002/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133253204",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133253204",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1133253204,
        "node_id": "IC_kwDOD7z77c5DjBJU",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T19:32:00Z",
        "updated_at": "2022-05-20T19:32:00Z",
        "author_association": "COLLABORATOR",
        "body": "You have a number of options. Sorry that this is so long and rambling, I don't have a ton of time to clean it up before sending it.\r\n\r\n1. Disable RMM pooling and use python for XGBoost. The RAPIDs Accelerator will use a little bit of memory that XGBoost will not be able to get access to, but most of the memory should be available to it. The downside is that the RAPIDs Accelerator will be much slower because it will have to call cudaMalloc and cudfFree directly instead of using RMM pooling. The data transfer should be relatively fast, but if it is not we can work with you on doing a prototype with CUDA IPC (see option 4).\r\n2. Statically split the memory between the Python process and the scala process. This will speed up the RAPIDs Accelerator, but a lot less memory will be available for XGBoost to use.The data transfer should still be relatively fast compared to what we have on the CPU. Again if it is not good enough we can look at doing an one off prototype.\r\n3. Disable the RAPIDs Accelerator entirely. This will give all of the GPU memory to XGBoost, but the transfer will be much slower and there isn't much we can do about it. You also don't get any GPU acceleration for reading the data or feature engineering before training.\r\n4. Roll your own solution as a prototype. We would read in the data/do feature engineering/etc and then hand that off to some custom code in scala that we could have python bindings for. It would get an `RDD[Table]` using [this](https://nvidia.github.io/spark-rapids/docs/additional-functionality/ml-integration.html). Then we can put in the barrier, and do a mapPartitions that would launch a custom python process, hopefully using most of the same code that Spark uses for python processes. Then it would handle doing CUDA IPC for each table over to the python process and release memory back as we go. At the end it would release any final remaining memory, tell the python process to do the training and wait for something to be sent back. That is what I have had in the back of my mind for a while on how we could do this, but I have been waiting on the async allocator and  CUDA IPC to be working properly together. From what I understand the async allocator has issues with cuda IPC that they are still working on, but it is not ready yet.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133253204/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133283252",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133283252",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561",
        "id": 1133283252,
        "node_id": "IC_kwDOD7z77c5DjIe0",
        "user": {
            "login": "trivialfis",
            "id": 16746409,
            "node_id": "MDQ6VXNlcjE2NzQ2NDA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/16746409?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/trivialfis",
            "html_url": "https://github.com/trivialfis",
            "followers_url": "https://api.github.com/users/trivialfis/followers",
            "following_url": "https://api.github.com/users/trivialfis/following{/other_user}",
            "gists_url": "https://api.github.com/users/trivialfis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/trivialfis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/trivialfis/subscriptions",
            "organizations_url": "https://api.github.com/users/trivialfis/orgs",
            "repos_url": "https://api.github.com/users/trivialfis/repos",
            "events_url": "https://api.github.com/users/trivialfis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/trivialfis/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-20T19:57:28Z",
        "updated_at": "2022-05-20T19:57:28Z",
        "author_association": "NONE",
        "body": "Thank you for the suggestions! Really appreciate that.\r\n\r\nXGBoost does support drawing memory from RMM so that's not an issue. I think the last point sufficiently shows that there's a way to improve in the future and I have high hope that it will get good performance once those blockers are sorted out.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133283252/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]