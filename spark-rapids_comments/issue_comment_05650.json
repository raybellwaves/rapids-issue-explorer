[
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138555507",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1138555507",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1138555507,
        "node_id": "IC_kwDOD7z77c5D3Ppz",
        "user": {
            "login": "cfangplus",
            "id": 42287875,
            "node_id": "MDQ6VXNlcjQyMjg3ODc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42287875?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cfangplus",
            "html_url": "https://github.com/cfangplus",
            "followers_url": "https://api.github.com/users/cfangplus/followers",
            "following_url": "https://api.github.com/users/cfangplus/following{/other_user}",
            "gists_url": "https://api.github.com/users/cfangplus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cfangplus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cfangplus/subscriptions",
            "organizations_url": "https://api.github.com/users/cfangplus/orgs",
            "repos_url": "https://api.github.com/users/cfangplus/repos",
            "events_url": "https://api.github.com/users/cfangplus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cfangplus/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-26T13:05:54Z",
        "updated_at": "2022-05-26T13:09:42Z",
        "author_association": "NONE",
        "body": "My SQL contains three tables and two joins, I used the SQL Tab of Spark WebUI, analyzed the details of the query, list the time cost of each node of the task within the last spark stage and compared them with CPU mode. I attached it here. It seems that GPU operator like Sort, HashAggregate are quitely faster than those correspoding CPU operator, howerver , GPU mode contains additional operators like GpuShuffleCoalesce and GpuCoaleseBatches, and these two operator cost too much times which slow down the total performance. \r\nI have not used nv nsight system yet, and from the past disscustion there, https://github.com/NVIDIA/spark-rapids/discussions/5394#discussioncomment-2658194, I guess that acquiring the GPU semaphore is the bottleneck. So now my question is, how could I resolve the performance problem and what's the update of RapidsShuffleManager ?\r\n![time costs](https://user-images.githubusercontent.com/42287875/170493268-ec049e5c-2dce-4430-ac09-d02fb0d9378c.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138555507/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138665010",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1138665010",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1138665010,
        "node_id": "IC_kwDOD7z77c5D3qYy",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-26T14:54:22Z",
        "updated_at": "2022-05-26T14:54:22Z",
        "author_association": "MEMBER",
        "body": "The metrics above do not indicate GpuShuffleCoalesce and GpuCoalesceBatches are directly the issue.  For both operations, the `concat batch time total` is the time spent actually doing the operation that node requires (i.e.: concatenating multiple batches together into a larger batch), and in both cases, the time is pretty low (even less than a millisecond in many cases).  The `collect batch time total` metric is the time that node spent waiting for input -- in other words, waiting for the node above it in the plan to produce output.  With the concat metric being low and the collect time being high, that indicates the time was spent waiting for shuffle (or whatever node precedes it in the query plan).\r\n\r\nRegarding the GPU semaphore, which version of the RAPIDS Accelerator are you using?  We have added an explicit metric for time spent waiting for the GPU semaphore in many SQL UI nodes which is available in release 21.12 and after.  It is disabled by default to keep the number of metrics manageable for the driver, but it can be enabled in those releases by setting `spark.rapids.sql.metrics.level=DEBUG`.\r\n\r\nWe have also made strides in recent releases to avoid holding the GPU semaphore while not actively processing data on the GPU (e.g.: #4588 and #4476).  There are still instances where this can happen, and the problem is quite complex.  For example, it would be relatively easy to always release the GPU semaphore when performing shuffle I/O or other host-based operations, but doing so while data remains in GPU memory allows new tasks to start adding data to GPU memory and it can easily lead to an OOM or heavy thrashing with excessive memory spill.  The thrashing can be so bad that it can be _faster_ to hold onto the semaphore and prevent too many tasks from trying to use the GPU simultaneously even though it seems wasteful at first glance.  It all depends on how fast the network is, how fast local disks are, how much memory new tasks will add to the GPU before the I/O completes, etc. etc.  It is a tricky problem with many variables that are difficult to predict.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138665010/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141742173",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1141742173",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1141742173,
        "node_id": "IC_kwDOD7z77c5EDZpd",
        "user": {
            "login": "cfangplus",
            "id": 42287875,
            "node_id": "MDQ6VXNlcjQyMjg3ODc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42287875?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cfangplus",
            "html_url": "https://github.com/cfangplus",
            "followers_url": "https://api.github.com/users/cfangplus/followers",
            "following_url": "https://api.github.com/users/cfangplus/following{/other_user}",
            "gists_url": "https://api.github.com/users/cfangplus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cfangplus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cfangplus/subscriptions",
            "organizations_url": "https://api.github.com/users/cfangplus/orgs",
            "repos_url": "https://api.github.com/users/cfangplus/repos",
            "events_url": "https://api.github.com/users/cfangplus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cfangplus/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T07:03:19Z",
        "updated_at": "2022-05-31T07:03:19Z",
        "author_association": "NONE",
        "body": "I updated the version of RAPIDS Accelerator to 22.04 and run the SQL again.\r\nI found that the collect batch time in GPUShuffleCoalesce and GPUCoalesceBatches disappeared, and yes, the GPU semaphore wait time comes  in at 3.2s/5s = 60%. Here is some spark properties ,spark.rapids.sql.concorrentGpuTasks = 4 while spark.executor.cores = 12",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141742173/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1142607324",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1142607324",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1142607324,
        "node_id": "IC_kwDOD7z77c5EGs3c",
        "user": {
            "login": "mattahrens",
            "id": 5303895,
            "node_id": "MDQ6VXNlcjUzMDM4OTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5303895?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mattahrens",
            "html_url": "https://github.com/mattahrens",
            "followers_url": "https://api.github.com/users/mattahrens/followers",
            "following_url": "https://api.github.com/users/mattahrens/following{/other_user}",
            "gists_url": "https://api.github.com/users/mattahrens/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mattahrens/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mattahrens/subscriptions",
            "organizations_url": "https://api.github.com/users/mattahrens/orgs",
            "repos_url": "https://api.github.com/users/mattahrens/repos",
            "events_url": "https://api.github.com/users/mattahrens/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mattahrens/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T20:25:07Z",
        "updated_at": "2022-05-31T20:25:07Z",
        "author_association": "COLLABORATOR",
        "body": "Work is planned in the 22.08 release cycle to look for improvement opportunities related to semaphore acquisition: https://github.com/NVIDIA/spark-rapids/issues/4568.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1142607324/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149801212",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1149801212",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1149801212,
        "node_id": "IC_kwDOD7z77c5EiJL8",
        "user": {
            "login": "cfangplus",
            "id": 42287875,
            "node_id": "MDQ6VXNlcjQyMjg3ODc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42287875?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cfangplus",
            "html_url": "https://github.com/cfangplus",
            "followers_url": "https://api.github.com/users/cfangplus/followers",
            "following_url": "https://api.github.com/users/cfangplus/following{/other_user}",
            "gists_url": "https://api.github.com/users/cfangplus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cfangplus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cfangplus/subscriptions",
            "organizations_url": "https://api.github.com/users/cfangplus/orgs",
            "repos_url": "https://api.github.com/users/cfangplus/repos",
            "events_url": "https://api.github.com/users/cfangplus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cfangplus/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-08T11:34:51Z",
        "updated_at": "2022-06-08T11:46:45Z",
        "author_association": "NONE",
        "body": "I have another question about the SQL, which mentioned above and contains three tables and two joins. I attched the physical plan here.\r\n![sql physical plan](https://user-images.githubusercontent.com/42287875/172607324-a54da70c-6261-46ee-8469-e21147025db0.png)\r\nThe table grid which shows above provide the time cost for each plan node within the last stage, which contains the two join operations. And as you can see, we can estimate the total time of each task for this stage, 738ms + 827ms + 32ms + 22ms + 1.6s + 3.5s + 10ms + 3.5s + 48ms + 52ms + 5.1s + 1.7s + 2ms + 1.6s + 1.6s = 20.329s. However, the SPARK WEBUI shows that the time cost for task within the last stage is nearly 5s. WHY?\r\n![media task time](https://user-images.githubusercontent.com/42287875/172608120-9402e1da-8745-44d1-9942-ba6a1e74c208.png)\r\n  ",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149801212/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150308446",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1150308446",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1150308446,
        "node_id": "IC_kwDOD7z77c5EkFBe",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-08T19:24:26Z",
        "updated_at": "2022-06-08T19:24:26Z",
        "author_association": "MEMBER",
        "body": "> we can estimate the total time of each task for this stage, 738ms + 827ms + 32ms + 22ms + 1.6s + 3.5s + 10ms + 3.5s + 48ms + 52ms + 5.1s + 1.7s + 2ms + 1.6s + 1.6s = 20.329s\r\n\r\nThere's quite a bit of double-counting in this calculation.  For example, the `collect batch time total` metric for `GpuCoalesceBatches` is timing how long it took to collect all the input batches, which is effectively timing all the nodes _above_ this node in the stage rather than something specific to this node.  In this example, that includes the time for the filter, running window, sort, and shuffle coalesce.  In general, if you're interested in the time associated with just what one node in the plan is doing, you should ignore any \"collect time\" metrics as that is timing how long it took _other_ nodes to produce output rather than this node.  There are other metrics, like `build time total` and `stream time total`, that similarly can end up timing other nodes.  That's why the time spent in build time + stream time for the join towards the end of the stage ends up very close to the total task time.\r\n\r\nThe `op time total` metric will focus on just how long a single node is taking to perform its operation separate from time spent waiting for input iterators.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150308446/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150842827",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1150842827",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1150842827,
        "node_id": "IC_kwDOD7z77c5EmHfL",
        "user": {
            "login": "cfangplus",
            "id": 42287875,
            "node_id": "MDQ6VXNlcjQyMjg3ODc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42287875?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cfangplus",
            "html_url": "https://github.com/cfangplus",
            "followers_url": "https://api.github.com/users/cfangplus/followers",
            "following_url": "https://api.github.com/users/cfangplus/following{/other_user}",
            "gists_url": "https://api.github.com/users/cfangplus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cfangplus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cfangplus/subscriptions",
            "organizations_url": "https://api.github.com/users/cfangplus/orgs",
            "repos_url": "https://api.github.com/users/cfangplus/repos",
            "events_url": "https://api.github.com/users/cfangplus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cfangplus/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-09T08:43:19Z",
        "updated_at": "2022-06-09T08:43:19Z",
        "author_association": "NONE",
        "body": "yea, nice comment, great thx @jlowe \r\nAnother question, as you reply before:\r\n> With the concat metric being low and the collect time being high, that indicates the time was spent waiting for shuffle (or whatever node precedes it in the query plan).\r\n> \r\nNow as I have updated the version of RAPIDS Accelerator from 21.10 to 22.04. I found that the collect batch time in GPUShuffleCoalesce and GPUCoalesceBatches disappeared, and yes, the GPU semaphore wait time comes in at 3.5s, and it comes to be 3.2/5s, nearly 60% proportion in the task time cost. I list the detail here. So the question is, the bottleneck is GPU semaphore or shuffle read?  BTW, I didn't use Rapids shuffle manager here, as before we tested and its result would be worse, maybe after the 22.8 release, we could take a try ? \r\n![semaphore2](https://user-images.githubusercontent.com/42287875/172804665-f1e32dbc-c237-404c-a515-cee6c685ea43.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150842827/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1154012352",
        "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1154012352",
        "issue_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
        "id": 1154012352,
        "node_id": "IC_kwDOD7z77c5EyNTA",
        "user": {
            "login": "jlowe",
            "id": 1360766,
            "node_id": "MDQ6VXNlcjEzNjA3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jlowe",
            "html_url": "https://github.com/jlowe",
            "followers_url": "https://api.github.com/users/jlowe/followers",
            "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
            "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
            "organizations_url": "https://api.github.com/users/jlowe/orgs",
            "repos_url": "https://api.github.com/users/jlowe/repos",
            "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jlowe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-13T14:42:48Z",
        "updated_at": "2022-06-13T14:42:48Z",
        "author_association": "MEMBER",
        "body": "> So the question is, the bottleneck is GPU semaphore or shuffle read?\r\n\r\nClearly semaphore wait is the main bottleneck for this task, but it's difficult to say for sure whether there is shuffle being performed while the semaphore is being held (generally undesirable when this occurs but it does happen in some cases). \r\n An Nsight trace would be able to show whether the semaphore is being held while shuffle is being performed.\r\n\r\nNote that semaphore wait in itself is not necessarily something that needs to be eliminated at all costs, as the semaphore's intent is to prevent situations where adding more concurrent tasks to the GPU may lead to an out-of-memory error on the GPU.  For example, if you have very many concurrent tasks configured for an executor (e.g.: 256 cores) but a relatively small GPU (e.g.: T4 with only 16GB) then it is fully expected to see a relatively high semaphore wait time since most tasks will be waiting their turn to use the GPU.  If all 256 concurrent tasks tried to use the T4 at the same time, it's very likely the GPU will run out of memory.\r\n\r\nThere are a couple of ways to help reduce semaphore wait time.  First is seeing if you can run more concurrent tasks on the GPU by configuring `spark.rapids.sql.concurrentGpuTasks`.  The more concurrent tasks that are allowed on the GPU, the lower the average semaphore wait time per task.  Setting this too high can lead to GPU OOM errors.  Another way to reduce the time is by reducing the number of CPU cores per executor.  This will run fewer tasks concurrently per executor which can hurt query performance for portions that run only on the CPU, but it will also reduce CPU memory pressure from having so many tasks trying to run simultaneously which can occasionally run faster overall.",
        "reactions": {
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1154012352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]