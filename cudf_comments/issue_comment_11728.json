[
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255239864",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255239864",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255239864,
        "node_id": "IC_kwDOBWUGps5K0XC4",
        "user": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T16:04:20Z",
        "updated_at": "2022-09-22T16:04:20Z",
        "author_association": "CONTRIBUTOR",
        "body": "@rjzamora This issue serves as our proposal for the new parsing-context aware CSV reader. We would love to discuss with you (or another Dask stakeholder) in more detail.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255239864/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255354385",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255354385",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255354385,
        "node_id": "IC_kwDOBWUGps5K0zAR",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T17:45:43Z",
        "updated_at": "2022-09-22T17:45:43Z",
        "author_association": "MEMBER",
        "body": "Nice - I can't say I understand how the transition-vector logic works, but the general plan seems reasonable to me. My current understanding is that the corresponding dask graph would look something like this:\r\n\r\n![possible-graph](https://user-images.githubusercontent.com/20461013/191814631-c45be4f7-9a2d-4411-96a4-827e2b2f5d4c.png)\r\n\r\nWe start by selecting overlapping byte ranges from the dataset (or perhaps they don't need to overlap?). Then we map the `read_csv_context` logic to each of these local byte-ranges. Then we do an overlapped mapping of the `merge_row_contexts` logic to update the context for each byte range. Then we do the final `read_csv` mapping to generate our `DataFrame` partitions.\r\n\r\nIs this the general algorithm you have in mind?",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255354385/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255359399",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255359399",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255359399,
        "node_id": "IC_kwDOBWUGps5K00On",
        "user": {
            "login": "upsj",
            "id": 1693511,
            "node_id": "MDQ6VXNlcjE2OTM1MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1693511?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/upsj",
            "html_url": "https://github.com/upsj",
            "followers_url": "https://api.github.com/users/upsj/followers",
            "following_url": "https://api.github.com/users/upsj/following{/other_user}",
            "gists_url": "https://api.github.com/users/upsj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/upsj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/upsj/subscriptions",
            "organizations_url": "https://api.github.com/users/upsj/orgs",
            "repos_url": "https://api.github.com/users/upsj/repos",
            "events_url": "https://api.github.com/users/upsj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/upsj/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T17:50:26Z",
        "updated_at": "2022-09-22T17:50:26Z",
        "author_association": "CONTRIBUTOR",
        "body": "couple of points for completeness:\r\n1. they must be non-overlapping, but adjacent for this to be correct\r\n2. \"read local context\" reads exactly the specified byte range (plus/minus one character to detect whether the byte range starts after a newline)\r\n3. \"combine contexts\" is really lightweight, we are basically passing around an integer, so it could happen via \"allgather + local exclusive prefix-scan\" as well\r\n4. read_csv may read past the end of the byte range to find the end of the last row inside the range, other than that, everything's non-overlapping",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255359399/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255379056",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255379056",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255379056,
        "node_id": "IC_kwDOBWUGps5K05Bw",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T18:10:59Z",
        "updated_at": "2022-09-22T18:11:35Z",
        "author_association": "MEMBER",
        "body": ">1. they must be non-overlapping, but adjacent for this to be correct\r\n>2. \"read local context\" reads exactly the specified byte range (plus/minus one character to detect whether the byte range starts after a newline)\r\n\r\nThanks for clarifying.  We will likely want to use a distinct IO task to physically pull each byte range from a (possibly remote) dataset. This means a given \"read local context\" task will only have access to the specific bytes that were read in the IO task it depends on. It seems like we would want to assign non-overlapping byte ranges to the \"read local context\" tasks, but that the IO tasks they depend on would need to have a bit of overlap (to deal with the \"plus/mins one character\" issue).\r\n\r\n>3. \"combine contexts\" is really lightweight, we are basically passing around an integer, so it could happen via \"allgather + local exclusive prefix-scan\" as well\r\n\r\nLightweight sounds good. A map-overlap pattern may still be preferred (if possible), since allgather opperations are not particularly performant in Dask.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255379056/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255388850",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255388850",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255388850,
        "node_id": "IC_kwDOBWUGps5K07ay",
        "user": {
            "login": "upsj",
            "id": 1693511,
            "node_id": "MDQ6VXNlcjE2OTM1MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1693511?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/upsj",
            "html_url": "https://github.com/upsj",
            "followers_url": "https://api.github.com/users/upsj/followers",
            "following_url": "https://api.github.com/users/upsj/following{/other_user}",
            "gists_url": "https://api.github.com/users/upsj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/upsj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/upsj/subscriptions",
            "organizations_url": "https://api.github.com/users/upsj/orgs",
            "repos_url": "https://api.github.com/users/upsj/repos",
            "events_url": "https://api.github.com/users/upsj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/upsj/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T18:20:43Z",
        "updated_at": "2022-09-22T18:20:43Z",
        "author_association": "CONTRIBUTOR",
        "body": "> This means a given \"read local context\" task will only have access to the specific bytes that were read in the IO task it depends on.\r\n\r\nThis sounds like a problematic limitation - if a byte range ends shortly after the start of a long row, your IO task may provide too little data. Are you worried about reading the same data twice here? Maybe we could also buffer that on the libcudf data_source level? To expand some more, `read_csv` works in two steps:\r\n\r\n1. finding row offsets, this means scanning the byte range for non-quoted/commented out newline characters, as well as finding the next newline after the end of the byte range. The IO here happens in small chunks and will most likely overrun the byte range.\r\n2. loading everything between the first and last row offset computed by 1. into device memory and actually parsing the rows.\r\n\r\nThe very worst case I could imagine would be a single dask task's byte range that is completely contained within a row, so the corresponding output is empty, but its predecessor would need to read past that task's byte range.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255388850/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255415329",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1255415329",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1255415329,
        "node_id": "IC_kwDOBWUGps5K1B4h",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-22T18:45:13Z",
        "updated_at": "2022-09-22T18:45:13Z",
        "author_association": "MEMBER",
        "body": ">if a byte range ends shortly after the start of a long row, your IO task may provide too little data. Are you worried about reading the same data twice here?\r\n\r\nYou are correct that it will be possible to \"break\" the Dask logic by assigning byte ranges that are too small (or by assigning too-little overlap). As far as I understand, this is already potential problem in dask's `read_csv`.  However, in the past the only alternative was to read the entire remote file on every worker.\r\n\r\nNow that I think about it, this is no longer the only alternative, because we are now able to perform partial IO from a file opened with `fsspec` (as a pyarrow `NativeFile`). Therefore, as long as we can handle a pyarrow `NativeFile` source, then we can allow those tasks to access any byte from the file.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1255415329/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1256008355",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1256008355",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1256008355,
        "node_id": "IC_kwDOBWUGps5K3Sqj",
        "user": {
            "login": "upsj",
            "id": 1693511,
            "node_id": "MDQ6VXNlcjE2OTM1MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1693511?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/upsj",
            "html_url": "https://github.com/upsj",
            "followers_url": "https://api.github.com/users/upsj/followers",
            "following_url": "https://api.github.com/users/upsj/following{/other_user}",
            "gists_url": "https://api.github.com/users/upsj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/upsj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/upsj/subscriptions",
            "organizations_url": "https://api.github.com/users/upsj/orgs",
            "repos_url": "https://api.github.com/users/upsj/repos",
            "events_url": "https://api.github.com/users/upsj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/upsj/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-23T09:50:37Z",
        "updated_at": "2022-09-23T10:03:21Z",
        "author_association": "CONTRIBUTOR",
        "body": "If we wanted to do this 100% cleanly, we could also extract row offsets in the parse context stage and exchange them between neighboring tasks/workers. Then the initial IO could be limited to the byte range + 1, and the subsequent parsing IO would know exactly which bytes to load a priori\r\n\r\nComing back to the communication patterns:\r\n> A map-overlap pattern may still be preferred (if possible), since allgather opperations are not particularly performant in Dask.\r\n\r\nOverlaps could potentially work somewhat heuristically in most cases, but I'd be more comfortable with an always safe solution, since the error-proneness across byte range boundaries was the original motivation for this discussion.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1256008355/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1256202929",
        "html_url": "https://github.com/rapidsai/cudf/issues/11728#issuecomment-1256202929",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/11728",
        "id": 1256202929,
        "node_id": "IC_kwDOBWUGps5K4CKx",
        "user": {
            "login": "rjzamora",
            "id": 20461013,
            "node_id": "MDQ6VXNlcjIwNDYxMDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/20461013?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjzamora",
            "html_url": "https://github.com/rjzamora",
            "followers_url": "https://api.github.com/users/rjzamora/followers",
            "following_url": "https://api.github.com/users/rjzamora/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjzamora/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjzamora/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjzamora/subscriptions",
            "organizations_url": "https://api.github.com/users/rjzamora/orgs",
            "repos_url": "https://api.github.com/users/rjzamora/repos",
            "events_url": "https://api.github.com/users/rjzamora/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjzamora/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-23T13:20:48Z",
        "updated_at": "2022-09-23T13:20:48Z",
        "author_association": "MEMBER",
        "body": ">I'd be more comfortable with an always safe solution\r\n\r\nThat makes sense. It would be fine for the first pass at the Dask algorithm to just execute a distinct task graph to calculate the **correct** byte range for each partition, followed by a conventional `read_csv` graph. The map-overlap pattern would allow us to avoid any eager execution, but my opinion is that eager execution is fine when it is well motivated.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1256202929/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]