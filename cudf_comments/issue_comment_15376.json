[
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015690375",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2015690375",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2015690375,
        "node_id": "IC_kwDOBWUGps54JP6H",
        "user": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-22T18:34:50Z",
        "updated_at": "2024-03-22T18:34:50Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hello @gm3g11, thank you for sharing your experience with the parquet chunked reader. I'm having some trouble reading the `txt` file you added, maybe it's a problem on my end but it might help to post the code in a hidden markdown block. \r\n\r\nWould you be willing to share more about your file with us? It's possible the 40-50 ms decompress is expected, or there could be a performance hotspot. If we have more information about the data file, we can help estimate the expected read throughput.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015690375/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015722296",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2015722296",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2015722296,
        "node_id": "IC_kwDOBWUGps54JXs4",
        "user": {
            "login": "nvdbaranec",
            "id": 56695930,
            "node_id": "MDQ6VXNlcjU2Njk1OTMw",
            "avatar_url": "https://avatars.githubusercontent.com/u/56695930?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nvdbaranec",
            "html_url": "https://github.com/nvdbaranec",
            "followers_url": "https://api.github.com/users/nvdbaranec/followers",
            "following_url": "https://api.github.com/users/nvdbaranec/following{/other_user}",
            "gists_url": "https://api.github.com/users/nvdbaranec/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nvdbaranec/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nvdbaranec/subscriptions",
            "organizations_url": "https://api.github.com/users/nvdbaranec/orgs",
            "repos_url": "https://api.github.com/users/nvdbaranec/repos",
            "events_url": "https://api.github.com/users/nvdbaranec/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nvdbaranec/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-22T18:54:15Z",
        "updated_at": "2024-03-22T18:56:07Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hi @gm3g11.  Do you know what compression format this file is using?  Also, how large is this file and what settings are you using when creating the chunked reader object?",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015722296/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015758503",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2015758503",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2015758503,
        "node_id": "IC_kwDOBWUGps54Jgin",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-22T19:17:57Z",
        "updated_at": "2024-03-22T19:50:23Z",
        "author_association": "NONE",
        "body": "> Hello @gm3g11, thank you for sharing your experience with the parquet chunked reader. I'm having some trouble reading the `txt` file you added, maybe it's a problem on my end but it might help to post the code in a hidden markdown block.\r\n> \r\n> Would you be willing to share more about your file with us? It's possible the 40-50 ms decompress is expected, or there could be a performance hotspot. If we have more information about the data file, we can help estimate the expected read throughput.\r\n\r\n@GregoryKimball, thanks for your reply. Here is the code I am using:\r\n\r\n```\r\n#include <cudf/aggregation.hpp>\r\n#include <cudf/groupby.hpp>\r\n#include <cudf/io/csv.hpp>\r\n#include <cudf/table/table.hpp>\r\n\r\n#include <rmm/cuda_device.hpp>\r\n#include <rmm/mr/device/cuda_memory_resource.hpp>\r\n#include <rmm/mr/device/device_memory_resource.hpp>\r\n#include <rmm/mr/device/pool_memory_resource.hpp>\r\n#include <memory>\r\n#include <string>\r\n#include <utility>\r\n#include <vector>\r\n#include <iostream>\r\n#include <string>\r\n#include <vector>\r\n\r\n#include <map>\r\n#include <rmm/device_buffer.hpp>\r\n\r\n#include <cudf/column/column.hpp>\r\n#include <cudf/lists/lists_column_view.hpp>\r\n#include <cudf/lists/count_elements.hpp>\r\n#include <cudf/column/column_view.hpp>\r\n#include <cudf/concatenate.hpp>\r\n#include <cudf/io/parquet.hpp>\r\n#include <cudf/table/table.hpp>\r\n#include <cudf/table/table_view.hpp>\r\n#include <cudf/types.hpp>\r\n// #include <cudf/io/text/detail/multibyte_split.hpp>\r\n\r\n// #include <nvbench/nvbench.cuh>\r\nint main(int argc, char** argv)\r\n{\r\n    rmm::mr::cuda_memory_resource cuda_mr{};\r\n    rmm::mr::pool_memory_resource<rmm::mr::cuda_memory_resource> pool_mr(&cuda_mr, 0, rmm::percent_of_free_device_memory(50));\r\n    rmm::mr::set_current_device_resource(&pool_mr);\r\n\r\n    auto byte_limit = static_cast<std::size_t>(1024 * 1024 * 1024); // 1GB chunk size\r\n    auto read_opts = cudf::io::parquet_reader_options::builder(cudf::io::source_info(\"./dataset/part-00198-tid-3747487300473043810-11eb4400-583b-4bd3-9c6c-a9803c7aeb94-3334-1-c000.snappy.parquet\")).build();\r\n\r\n    // Assuming you have access to a CUDA stream and memory resource\r\n    auto stream = rmm::cuda_stream_default;\r\n    // rmm::cuda_stream stream;\r\n    auto mr = rmm::mr::get_current_device_resource();\r\n    // rmm::cuda_stream stream;\r\n    // rmm::cuda_stream_view stream_view(stream.value());\r\n\r\n    cudf::size_type num_rows_read = 0;\r\n\r\n    \r\n      \r\n    auto reader   = cudf::io::chunked_parquet_reader(0, read_opts,stream,mr);\r\n\r\n\r\n      auto start = std::chrono::steady_clock::now();\r\n      do {        \r\n        auto const result = reader.read_chunk();      \r\n        num_rows_read += result.tbl->num_rows();\r\n      } while (reader.has_next());\r\n\r\n      auto end = std::chrono::steady_clock::now();\r\n      // total_time += end-start;\r\n\r\n    std::cout << \"Chunked reading parquet file time elapsed \"\r\n      << (std::chrono::duration_cast<std::chrono::milliseconds>(end-start)).count()\r\n      << \" ms\\n\";\r\n    return 0;\r\n}\r\n```\r\n\r\nHere is the link for the parquet file:\r\nhttps://drive.google.com/file/d/1_OSLSiBg9afVu8Io4Vt_3OpDWjN1CDc0/view?usp=sharing\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015758503/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015762280",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2015762280",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2015762280,
        "node_id": "IC_kwDOBWUGps54Jhdo",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-22T19:20:51Z",
        "updated_at": "2024-03-22T19:20:51Z",
        "author_association": "NONE",
        "body": "> Hi @gm3g11. Do you know what compression format this file is using? Also, how large is this file and what settings are you using when creating the chunked reader object?\r\n\r\nThe compression format is SNAPPY, and the size of the uncompressed parquet file is: 1.5056GB. I am using unlimited bytes for in and out for the chuked reader object.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2015762280/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2017308776",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2017308776",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2017308776,
        "node_id": "IC_kwDOBWUGps54PbBo",
        "user": {
            "login": "etseidl",
            "id": 25541553,
            "node_id": "MDQ6VXNlcjI1NTQxNTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25541553?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/etseidl",
            "html_url": "https://github.com/etseidl",
            "followers_url": "https://api.github.com/users/etseidl/followers",
            "following_url": "https://api.github.com/users/etseidl/following{/other_user}",
            "gists_url": "https://api.github.com/users/etseidl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/etseidl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/etseidl/subscriptions",
            "organizations_url": "https://api.github.com/users/etseidl/orgs",
            "repos_url": "https://api.github.com/users/etseidl/repos",
            "events_url": "https://api.github.com/users/etseidl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/etseidl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-25T06:22:34Z",
        "updated_at": "2024-03-25T06:22:34Z",
        "author_association": "CONTRIBUTOR",
        "body": "@gm3g11 is this a large corpus you cannot control, or would you be able to change how the files are written? Looking at the data, I think one large takeaway is that compression isn't buying you much except for the first column. In fact, for many data pages compression is making the data larger. For the `alert_id` column, you could achieve almost the same data reduction by using `DELTA_BYTE_ARRAY` encoding (each value is a prefix followed by a UUID...DELTA_BYTE_ARRAY would remove that prefix, nearly halving the size). Similarly, the `envelope_fields_version` and `envelope_fields_generation_counter` columns would likely benefit from `DELTA_BINARY_PACKED` encoding. Right now PLAIN+Snappy is reducing 160k to around 99k per page, but given the range of values (all seemingly 6 decimal digits), I'd think using the delta encoding would only require 20 bits per value. This would give you data pages more like 50k in size.\r\n\r\nI don't remember how much control over encoding options parquet-mr gives you, but I think if you set compression to none, and enabled V2 data page headers (doing so would cause the most troublesome columns to use delta encoding rather than PLAIN), you _might_ see a good improvement in read performance without increasing the overall file sizes significantly.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2017308776/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2018337955",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2018337955",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2018337955,
        "node_id": "IC_kwDOBWUGps54TWSj",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-25T15:54:48Z",
        "updated_at": "2024-03-25T15:54:48Z",
        "author_association": "NONE",
        "body": "@etseidl Thanks for your reply. For this example parquet file, I can do some experiments and try the mentioned encoding scheme. I will let you know if I make some progress.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2018337955/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021371725",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2021371725",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2021371725,
        "node_id": "IC_kwDOBWUGps54e69N",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-26T20:08:54Z",
        "updated_at": "2024-03-26T20:08:54Z",
        "author_association": "NONE",
        "body": "@etseidl \r\nThis method is effective. Without compression, we observed an improvement in Parquet reading performance, decreasing from 123 ms to 45 ms, while the file size did not increase significantly, only from 39M to 50M. \r\n\r\nIn the next step, we want to further accelerate parquet reading with multi-streams and multi-threads. Before we implement it, we extend the parquet file from 240,578 rows into 4,811,560 rows (increase 20 times). There is one thing I want to discuss: my multi-threads method seems work but the multi-streams don't work (the streams are still serial shown as below:)\r\n![image](https://github.com/rapidsai/cudf/assets/36735914/140cddf4-24dc-4d66-ace0-a82a9da0c0c9)\r\n\r\nI don't find too much material about the streams in the cudf::io::read_parquet API, here is how I am using it in my code, and any comment is welcome:\r\n\r\n#include <cudf/io/parquet.hpp>\r\n#include <cudf/table/table.hpp>\r\n#include <rmm/cuda_stream_view.hpp>\r\n#include <rmm/mr/device/cuda_memory_resource.hpp>\r\n#include <rmm/mr/device/per_device_resource.hpp>\r\n#include <chrono>\r\n#include <iostream>\r\n#include <vector>\r\n#include <thread>\r\n#include <numeric>\r\n\r\nvoid read_parquet_chunk(const std::string& file_path, int32_t chunk, int32_t chunk_row_cnt, int32_t total_rows, int32_t chunked_read_num_chunks, std::vector<std::unique_ptr<cudf::table>>& results, rmm::cuda_stream& stream) {\r\n    cudf::io::parquet_reader_options read_options = cudf::io::parquet_reader_options::builder(cudf::io::source_info(file_path))\r\n        .skip_rows(chunk * chunk_row_cnt)\r\n        .num_rows((chunk == chunked_read_num_chunks - 1) ? (total_rows - chunk * chunk_row_cnt) : chunk_row_cnt)\r\n        .build();\r\n    auto result = cudf::io::read_parquet(read_options, stream);\r\n    results[chunk] = std::move(result.tbl);\r\n}\r\n\r\nint main() {\r\n    const std::string file_path = \"./dataset/output_no_compression_4M.parquet\";\r\n    constexpr int32_t total_rows = 4811560;\r\n    constexpr int32_t chunked_read_num_chunks = 20;\r\n    const int32_t chunk_row_cnt = total_rows / chunked_read_num_chunks;\r\n\r\n    // Initialize CUDA memory resource\r\n    rmm::mr::cuda_memory_resource cuda_mr;\r\n    rmm::mr::set_current_device_resource(&cuda_mr);\r\n\r\n    std::vector<rmm::cuda_stream> streams(chunked_read_num_chunks);\r\n    std::vector<std::unique_ptr<cudf::table>> results(chunked_read_num_chunks);\r\n    std::vector<std::thread> threads;\r\n\r\n    auto start_time = std::chrono::high_resolution_clock::now();\r\n\r\n    for (int32_t chunk = 0; chunk < chunked_read_num_chunks; ++chunk) {\r\n        streams[chunk] = rmm::cuda_stream();\r\n        threads.emplace_back(read_parquet_chunk, std::ref(file_path), chunk, chunk_row_cnt, total_rows, chunked_read_num_chunks, std::ref(results), std::ref(streams[chunk]));\r\n    }\r\n\r\n    for (auto& thread : threads) {\r\n        thread.join();\r\n    }\r\n\r\n    // Synchronize all streams\r\n    for (auto& stream : streams) {\r\n        stream.synchronize();\r\n    }\r\n\r\n    auto end_time = std::chrono::high_resolution_clock::now();\r\n    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time).count();\r\n\r\n    cudf::size_type total_rows_read = 0;\r\n    for (const auto& result : results) {\r\n        total_rows_read += result->num_rows();\r\n    }\r\n    std::cout << \"Total rows read: \" << total_rows_read << std::endl;\r\n    std::cout << \"Total read time: \" << duration << \" ms\" << std::endl;\r\n\r\n    return 0;\r\n}",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021371725/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021551401",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2021551401",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2021551401,
        "node_id": "IC_kwDOBWUGps54fm0p",
        "user": {
            "login": "etseidl",
            "id": 25541553,
            "node_id": "MDQ6VXNlcjI1NTQxNTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25541553?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/etseidl",
            "html_url": "https://github.com/etseidl",
            "followers_url": "https://api.github.com/users/etseidl/followers",
            "following_url": "https://api.github.com/users/etseidl/following{/other_user}",
            "gists_url": "https://api.github.com/users/etseidl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/etseidl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/etseidl/subscriptions",
            "organizations_url": "https://api.github.com/users/etseidl/orgs",
            "repos_url": "https://api.github.com/users/etseidl/repos",
            "events_url": "https://api.github.com/users/etseidl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/etseidl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-26T22:05:35Z",
        "updated_at": "2024-03-26T22:05:35Z",
        "author_association": "CONTRIBUTOR",
        "body": "As I've learned the hard way, simply using separate streams is not necessarily going to cause execution to proceed in parallel on a single GPU. For kernel overlap to occur, you'll need sufficient resources (SMs, shared mem, registers, etc). The parquet kernel is pretty resource heavy, so the chances for overlap occurring are minimal. Also note that internal to libcudf, the various decode kernels execute in separate streams, further constraining resources.\r\n\r\nIf you want to parallelize further, you'll need to spread execution across multiple GPUs. You could try rolling your own, or use a parallel execution engine like dask or spark.\r\n\r\n@nvdbaranec please chime in on anything I got horribly wrong :smile: \r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021551401/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021624428",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2021624428",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2021624428,
        "node_id": "IC_kwDOBWUGps54f4ps",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-26T23:16:48Z",
        "updated_at": "2024-03-26T23:16:48Z",
        "author_association": "NONE",
        "body": "@etseidl Thanks for sharing. Based on the document, I see there is a stream option in the cudf::io::read_parquet, and then think we can use multi-streams to accelerate the read_parquet further. Currently, I can only test it on a single GPU.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021624428/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021780856",
        "html_url": "https://github.com/rapidsai/cudf/issues/15376#issuecomment-2021780856",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/15376",
        "id": 2021780856,
        "node_id": "IC_kwDOBWUGps54ge14",
        "user": {
            "login": "gm3g11",
            "id": 36735914,
            "node_id": "MDQ6VXNlcjM2NzM1OTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/36735914?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gm3g11",
            "html_url": "https://github.com/gm3g11",
            "followers_url": "https://api.github.com/users/gm3g11/followers",
            "following_url": "https://api.github.com/users/gm3g11/following{/other_user}",
            "gists_url": "https://api.github.com/users/gm3g11/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gm3g11/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gm3g11/subscriptions",
            "organizations_url": "https://api.github.com/users/gm3g11/orgs",
            "repos_url": "https://api.github.com/users/gm3g11/repos",
            "events_url": "https://api.github.com/users/gm3g11/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gm3g11/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2024-03-27T01:56:55Z",
        "updated_at": "2024-03-27T01:56:55Z",
        "author_association": "NONE",
        "body": "Hi @GregoryKimball, I came across the GitHub link: \"[FEA] Add a Parquet reader benchmark that uses multiple CUDA streams #12700\" (https://github.com/rapidsai/cudf/issues/12700), and noticed that you're leading this feature development. I think this feature is related to my case. Could you provide any progresses on the implementation of multiple CUDA streams in the Parquet reader? Thank you.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/2021780856/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]