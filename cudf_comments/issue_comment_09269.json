[
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/925470928",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-925470928",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 925470928,
        "node_id": "IC_kwDOBWUGps43KZDQ",
        "user": {
            "login": "devavret",
            "id": 3027195,
            "node_id": "MDQ6VXNlcjMwMjcxOTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3027195?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/devavret",
            "html_url": "https://github.com/devavret",
            "followers_url": "https://api.github.com/users/devavret/followers",
            "following_url": "https://api.github.com/users/devavret/following{/other_user}",
            "gists_url": "https://api.github.com/users/devavret/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/devavret/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/devavret/subscriptions",
            "organizations_url": "https://api.github.com/users/devavret/orgs",
            "repos_url": "https://api.github.com/users/devavret/repos",
            "events_url": "https://api.github.com/users/devavret/events{/privacy}",
            "received_events_url": "https://api.github.com/users/devavret/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-23T02:57:01Z",
        "updated_at": "2021-09-23T02:57:01Z",
        "author_association": "CONTRIBUTOR",
        "body": "I'm trying to understand this. So there's broadly two steps where parquet reader spends time:\r\n1.  Reading rowgroup data from the file.\r\n2. Decompressing and decoding said data.\r\n\r\nIn case of rowgroup based filtering, you can ask cudf for the rowgroups to read which means rest of the rowgroups are neither loaded from disk into host/device memory nor decompressed/decoded.\r\nYou can filter based on exact row numbers but that would mean the entire rowgroup is still read from disk but only the necessary pages are decompressed/decoded. So only partial time savings.\r\n\r\nWith this new page level filtering, you're trying to avoid reading unnecessary pages from _within_ the rowgroups, from disk. Is that correct?",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/925470928/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/969318315",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-969318315",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 969318315,
        "node_id": "IC_kwDOBWUGps45xp-r",
        "user": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "created_at": "2021-11-15T21:03:25Z",
        "updated_at": "2021-11-15T21:03:25Z",
        "author_association": "NONE",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/969318315/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1038449941",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1038449941",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1038449941,
        "node_id": "IC_kwDOBWUGps495X0V",
        "user": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "created_at": "2022-02-13T22:03:11Z",
        "updated_at": "2022-02-13T22:03:11Z",
        "author_association": "NONE",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1038449941/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1230983052",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1230983052",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1230983052,
        "node_id": "IC_kwDOBWUGps5JX0-M",
        "user": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-29T23:49:40Z",
        "updated_at": "2022-08-29T23:49:40Z",
        "author_association": "CONTRIBUTOR",
        "body": ">  API where we can send compressed pages and metadata to CUDF for decoding. The metadata would include things like the file and row group that the pages came from and what range of column indicies within those pages we would like to be read.\r\n\r\n@revans2 would you please share more about the API you have in mind? Would the inputs be a new metadata class, or are there existing ones that would suffice? Would the output be a table or an iterable of column fragments?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1230983052/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1251201505",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1251201505",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1251201505,
        "node_id": "IC_kwDOBWUGps5Kk9Hh",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-19T15:44:51Z",
        "updated_at": "2022-09-19T15:44:51Z",
        "author_association": "CONTRIBUTOR",
        "body": "Sorry about the long delay for a response. I see two possible APIs.\r\n\r\nThe first would be that in Spark we add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API so we can properly support security and all of the various pluggable APIs that Spark/Java can support, but CUDF does not yet.\r\n\r\nCUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does, not hard I can show you how to do it. APIs to do predicate push down so row groups can be skipped if needed (and in this case possibly pages within a row group being skipped too). APIs to do column pruning so we can tell you exactly which columns we want read. A callback API so we know just before data is moved to the GPU so we can overlap I/O as much as possible, and not overload the GPU's memory. And an API, if it does not exist already, to be able to group multiple smaller files together so that the GPU can be more efficient when reading the data. Also we can use a thread pool to read data from some file systems, because the bandwidth for a lot of CSP blob stores is limited by each connection or single threaded CPU performance in decompressing/decrypting the data, so having more connections improves the total throughput.\r\n\r\nI like this approach because it allows for more improvement in the future, including things like GPU direct storage and it bypasses having to read/write the footer multiple times.  But we would have to do a bunch of profiling to be sure that the java callback is efficient and not slowing things down, and it would be a lot of large changes to CUDF to add in all of the features that we have been doing directly in scala today.\r\n\r\nThe second approach would be something where we would pass down structured data to CUDF. So instead of passing down a buffer that holds a complete parquet file.  We would parse the footer and the ColumnIndex ourselves. Read the data that we know we need to read, and then pass the information to CUDF as almost a tree, where we would give you the some form of a footer that would point to the columns, row groups, pages, and row ranges that we care about and want read.\r\n\r\nThis approach is a lot more complicated to do, in my option, and is prone to also being slow because sending structured data through JNI is also rather problematic and slow. So I would prefer the first option, but...",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1251201505/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1515743905",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1515743905",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1515743905,
        "node_id": "IC_kwDOBWUGps5aWGqh",
        "user": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-20T05:40:18Z",
        "updated_at": "2023-04-20T05:40:55Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thank you @revans2 for sharing these ideas for improving the IO pipeline in the Spark-RAPIDS plugin. I'd like to pick up this thread again now that I'm scoping predicate pushdown in the libcudf parquet reader.\r\n\r\n> in Spark we [could] add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API\r\n\r\nIs your idea that this glue layer would appear to libcudf as a new `datasource`?\r\n\r\n> CUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does\r\n\r\nFor predicate pushdown we are considering using an AST to represent the predicate. Do you already have tools to convert Spark expressions into libcudf ASTs?\r\n\r\n> possibly pages within a row group being skipped too\r\n\r\nDo you know for sure if Spark does page-level IO filtering with predicates? At least pyarrow, DuckDB and polars only do rowgroup-level filtering with predicates. \r\n\r\n> APIs to do column pruning\r\n\r\nI believe we already support column pruning in `parquet_reader_options._columns`.\r\n\r\n> an API, if it does not exist already, to be able to group multiple smaller files\r\n\r\nI'll need to check on this. We have multi-source support for Parquet reading in cuDF-python, but I'm not sure how that maps into libcudf.\r\n\r\n> Also we can use a thread pool to read data from some file systems\r\n\r\nDoes this have any implications for the API design in libcudf?",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1515743905/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1516477419",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1516477419",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1516477419,
        "node_id": "IC_kwDOBWUGps5aY5vr",
        "user": {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-20T14:55:25Z",
        "updated_at": "2023-04-20T14:55:25Z",
        "author_association": "CONTRIBUTOR",
        "body": "> > in Spark we [could] add in a glue layer that would let the CUDF reader call back into java and the Hadoop File System API\r\n> \r\n> Is your idea that this glue layer would appear to libcudf as a new `datasource`?\r\n\r\nYes. Currently we will use the built in Spark and Parquet java code to do predicate push down and see what columns we need to read based off of statistics in the footer and the section of the file that a given task is supposed to process.  After that we will read those parts of the file, write out a new file + new footer to a memory buffer and hand that memory buffer off to cudf to parse.  This gave us flexibility to do predicate push down, column pruning, partitioning, and even combining multiple small files together without having to wait for CUDF to add these features.  It also let us reduce the amount of data we read and parallelize fetching the data.\r\n\r\nBecause we have built so much outside of CUDF it makes me a little nervous to switch everything around unless we can find a way to get at least feature parity. \r\n \r\n> > CUDF would then need to add in APIs that would allow us to partition the data the same way that Spark does\r\n> \r\n> For predicate pushdown we are considering using an AST to represent the predicate. Do you already have tools to convert Spark expressions into libcudf ASTs?\r\n\r\nAST would be fine. The big issue for me is making sure that the AST can support all of the filtering that we currently support.\r\n\r\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/FilterApi.java\r\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/Operators.java\r\n\r\nThe big differences I see are `binaryColumn`, `in`, and `notIn`. We probably could simulate `in` and `notIn` using equality and binary operations.\r\n\r\n> > possibly pages within a row group being skipped too\r\n> \r\n> Do you know for sure if Spark does page-level IO filtering with predicates? At least pyarrow, DuckDB and polars only do rowgroup-level filtering with predicates.\r\n\r\nThat is what triggered me filing this issue.  Parquet-mr added the ability to do this and spark started to do it. They had a bug in their initial implementation which we found so I filed this to explore what it might take to support it here too. It might not be worth it because of the amount of change needed to support a feature like this.\r\n\r\n> > APIs to do column pruning\r\n> \r\n> I believe we already support column pruning in `parquet_reader_options._columns`.\r\n\r\nYes, but technically we can do sub-column pruning too.  Like what if I have a `foo:struct<a:Int, b:Int>`, but I know that foo.a is never touched, so all I want to load is `foo:struct<b:Int>` and skip `a` all together.\r\n\r\n> > an API, if it does not exist already, to be able to group multiple smaller files\r\n> \r\n> I'll need to check on this. We have multi-source support for Parquet reading in cuDF-python, but I'm not sure how that maps into libcudf.\r\n> \r\n> > Also we can use a thread pool to read data from some file systems\r\n> \r\n> Does this have any implications for the API design in libcudf?\r\n\r\nYes. The current data source API is single threaded/single buffer. Read this one buffer starting at offset X and with a size of Y.  Done? OK now read this buffer at offset X2 and size of Y2... \r\n\r\nIf you could provide an API with something like `std::vector<request>` where `request` looks like `struct{buffer_type, offset, size, dst}` then we could decide how we do the I/O to best fulfill that request in the shortest time possible. It would also be great if we could make it async so it could return a `future` so for the small file issue you could send off requests for all of them in parallel so that we could schedule that appropriately too.\r\n\r\nThe reason that we do this is because blob stores can have very high time to first byte (so for random I/O when doing parquet or ORC it is good to coalesce reads and issues multiple requests at once)\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\r\n\r\n>  These applications can achieve consistent small object latencies (and first-byte-out latencies for larger objects) of roughly 100\u2013200 milliseconds.\r\n\r\nWe have also found that throughput can be throttled per connection and we need multiple open connections to saturate the bandwidth. \r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\r\n\r\n> You can increase your read or write performance by using parallelization.",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1516477419/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1516678975",
        "html_url": "https://github.com/rapidsai/cudf/issues/9269#issuecomment-1516678975",
        "issue_url": "https://api.github.com/repos/rapidsai/cudf/issues/9269",
        "id": 1516678975,
        "node_id": "IC_kwDOBWUGps5aZq8_",
        "user": {
            "login": "etseidl",
            "id": 25541553,
            "node_id": "MDQ6VXNlcjI1NTQxNTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25541553?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/etseidl",
            "html_url": "https://github.com/etseidl",
            "followers_url": "https://api.github.com/users/etseidl/followers",
            "following_url": "https://api.github.com/users/etseidl/following{/other_user}",
            "gists_url": "https://api.github.com/users/etseidl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/etseidl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/etseidl/subscriptions",
            "organizations_url": "https://api.github.com/users/etseidl/orgs",
            "repos_url": "https://api.github.com/users/etseidl/repos",
            "events_url": "https://api.github.com/users/etseidl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/etseidl/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-20T17:14:11Z",
        "updated_at": "2023-04-20T17:14:11Z",
        "author_association": "CONTRIBUTOR",
        "body": "Chiming in here uninvited...\r\n\r\n> Yes, but technically we can do sub-column pruning too. Like what if I have a `foo:struct<a:Int, b:Int>`, but I know that foo.a is never touched, so all I want to load is `foo:struct<b:Int>` and skip `a` all together.\r\n\r\nThe cudf parquet reader supports this as well, you just have to provide the fully quallified path.  You can set `columns` to something like `foo.item.b` (or maybe `foo.element.b` depending on the writer...the schema knows). \r\n\r\nAnd the current API does support multiple input files in the `source_info`, as well as specific row groups from those input files.\r\n\r\nI've experimented with using the column index info on the read side to do page pruning, but for most of my use cases it wasn't worth it.  If I'm reading a range of a single file where the range spans multiple row groups, then I'm only saving a little bit on the first and last row group.  If I'm reading a range from a single row group, there's usually not enough columns being read to saturate the device, so there's no speed up (esp if there are dictionary pages to decompress/decode).  Reading a single row with no dictionaries can be significantly faster, and I'd imagine doing a targeted scan of multiple files would benefit as well.  But since spark can already do page pruning up front (and cudf lacks S3/HDFS support at the c++ layer AFAICT), I don't know if there's an easy win here.\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/rapidsai/cudf/issues/comments/1516678975/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]