{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/13039",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/13039/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13039/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/13039/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/13039",
    "id": 1647923321,
    "node_id": "I_kwDOBWUGps5iOVB5",
    "number": 13039,
    "title": "[QST] `dask_cudf.read_parquet` failed with \"NotImplementedError: large_string\"",
    "user": {
        "login": "stucash",
        "id": 6358866,
        "node_id": "MDQ6VXNlcjYzNTg4NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6358866?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/stucash",
        "html_url": "https://github.com/stucash",
        "followers_url": "https://api.github.com/users/stucash/followers",
        "following_url": "https://api.github.com/users/stucash/following{/other_user}",
        "gists_url": "https://api.github.com/users/stucash/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/stucash/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/stucash/subscriptions",
        "organizations_url": "https://api.github.com/users/stucash/orgs",
        "repos_url": "https://api.github.com/users/stucash/repos",
        "events_url": "https://api.github.com/users/stucash/events{/privacy}",
        "received_events_url": "https://api.github.com/users/stucash/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626564,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjQ=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/question",
            "name": "question",
            "color": "D4C5F9",
            "default": true,
            "description": "Further information is requested"
        },
        {
            "id": 1013987799,
            "node_id": "MDU6TGFiZWwxMDEzOTg3Nzk5",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Waiting%20on%20Author",
            "name": "0 - Waiting on Author",
            "color": "ffb88c",
            "default": false,
            "description": "Waiting for author to respond to review"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/20",
        "html_url": "https://github.com/rapidsai/cudf/milestone/20",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/20/labels",
        "id": 8568142,
        "node_id": "MI_kwDOBWUGps4Agr1O",
        "number": 20,
        "title": "Stabilizing large workflows (OOM, spilling, partitioning)",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 16,
        "closed_issues": 3,
        "state": "open",
        "created_at": "2022-10-21T19:29:07Z",
        "updated_at": "2024-05-22T23:20:04Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 11,
    "created_at": "2023-03-30T16:14:30Z",
    "updated_at": "2024-05-27T13:49:59Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I am a new user of `dask`/`dask_cudf`.\r\nI have parquet files of various sizes (11GB, 2.5GB, 1.1GB), all of which failed with `NotImplementedError: large_string`. \r\n\r\nMy `dask.dataframe` backend is `cudf`. When the backend is `pandas`, `read.parquet` works fine.\r\n\r\nHere's an exerpt of what my data looks like in `csv` format:\r\n\r\n    Symbol,Date,Open,High,Low,Close,Volume\r\n    AADR,17-Oct-2017 09:00,57.47,58.3844,57.3645,58.3844,2094\r\n    AADR,17-Oct-2017 10:00,57.27,57.2856,57.25,57.27,627\r\n    AADR,17-Oct-2017 11:00,56.99,56.99,56.99,56.99,100\r\n    AADR,17-Oct-2017 12:00,56.98,57.05,56.98,57.05,200\r\n    AADR,17-Oct-2017 13:00,57.14,57.16,57.14,57.16,700\r\n    AADR,17-Oct-2017 14:00,57.13,57.13,57.13,57.13,100\r\n    AADR,17-Oct-2017 15:00,57.07,57.07,57.07,57.07,200\r\n    AAMC,17-Oct-2017 09:00,87,87,87,87,100\r\n    AAU,17-Oct-2017 09:00,1.1,1.13,1.0832,1.121,67790\r\n    AAU,17-Oct-2017 10:00,1.12,1.12,1.12,1.12,100\r\n    AAU,17-Oct-2017 11:00,1.125,1.125,1.125,1.125,200\r\n    AAU,17-Oct-2017 12:00,1.1332,1.15,1.1332,1.15,27439\r\n    AAU,17-Oct-2017 13:00,1.15,1.15,1.13,1.13,8200\r\n    AAU,17-Oct-2017 14:00,1.1467,1.1467,1.14,1.1467,1750\r\n    AAU,17-Oct-2017 15:00,1.1401,1.1493,1.1401,1.1493,4100\r\n    AAU,17-Oct-2017 16:00,1.13,1.13,1.13,1.13,100\r\n    ABE,17-Oct-2017 09:00,14.64,14.64,14.64,14.64,200\r\n    ABE,17-Oct-2017 10:00,14.67,14.67,14.66,14.66,1200\r\n    ABE,17-Oct-2017 11:00,14.65,14.65,14.65,14.65,600\r\n    ABE,17-Oct-2017 15:00,14.65,14.65,14.65,14.65,836\r\n\r\n\r\nWhat I did was really simple:\r\n\r\n    import dask.dataframe as dd\r\n    import cudf\r\n    import dask_cudf\r\n    \r\n    # Failed with large_string error\r\n    dask_cudf.read_parquet('path/to/my.parquet')\r\n    # Failed with large_string error\r\n    dd.read_parquet('path/to/my.parquet')\r\n\r\nThe only large string I could think of is the timestamp string.\r\n\r\nIs there a way around this in `cudf` as it is not implemented yet? The format is `2023-03-12 09:00:00+00:00`.\r\n\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/13039/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/13039/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}