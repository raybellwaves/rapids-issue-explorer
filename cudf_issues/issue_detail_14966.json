{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/14966",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/14966/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/14966/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/14966/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/14966",
    "id": 2117262936,
    "node_id": "I_kwDOBWUGps5-Mt5Y",
    "number": 14966,
    "title": "[FEA] Incorporate chunked parquet reading into cuDF-python",
    "user": {
        "login": "GregoryKimball",
        "id": 12725111,
        "node_id": "MDQ6VXNlcjEyNzI1MTEx",
        "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/GregoryKimball",
        "html_url": "https://github.com/GregoryKimball",
        "followers_url": "https://api.github.com/users/GregoryKimball/followers",
        "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
        "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
        "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
        "repos_url": "https://api.github.com/users/GregoryKimball/repos",
        "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
        "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1139741213,
            "node_id": "MDU6TGFiZWwxMTM5NzQxMjEz",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/Python",
            "name": "Python",
            "color": "1d76db",
            "default": false,
            "description": "Affects Python cuDF API."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "galipremsagar",
        "id": 11664259,
        "node_id": "MDQ6VXNlcjExNjY0MjU5",
        "avatar_url": "https://avatars.githubusercontent.com/u/11664259?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/galipremsagar",
        "html_url": "https://github.com/galipremsagar",
        "followers_url": "https://api.github.com/users/galipremsagar/followers",
        "following_url": "https://api.github.com/users/galipremsagar/following{/other_user}",
        "gists_url": "https://api.github.com/users/galipremsagar/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/galipremsagar/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/galipremsagar/subscriptions",
        "organizations_url": "https://api.github.com/users/galipremsagar/orgs",
        "repos_url": "https://api.github.com/users/galipremsagar/repos",
        "events_url": "https://api.github.com/users/galipremsagar/events{/privacy}",
        "received_events_url": "https://api.github.com/users/galipremsagar/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "galipremsagar",
            "id": 11664259,
            "node_id": "MDQ6VXNlcjExNjY0MjU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/11664259?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/galipremsagar",
            "html_url": "https://github.com/galipremsagar",
            "followers_url": "https://api.github.com/users/galipremsagar/followers",
            "following_url": "https://api.github.com/users/galipremsagar/following{/other_user}",
            "gists_url": "https://api.github.com/users/galipremsagar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/galipremsagar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/galipremsagar/subscriptions",
            "organizations_url": "https://api.github.com/users/galipremsagar/orgs",
            "repos_url": "https://api.github.com/users/galipremsagar/repos",
            "events_url": "https://api.github.com/users/galipremsagar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/galipremsagar/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/20",
        "html_url": "https://github.com/rapidsai/cudf/milestone/20",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/20/labels",
        "id": 8568142,
        "node_id": "MI_kwDOBWUGps4Agr1O",
        "number": 20,
        "title": "Stabilizing large workflows (OOM, spilling, partitioning)",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 16,
        "closed_issues": 3,
        "state": "open",
        "created_at": "2022-10-21T19:29:07Z",
        "updated_at": "2024-05-22T23:20:04Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 0,
    "created_at": "2024-02-04T19:10:28Z",
    "updated_at": "2024-05-10T22:32:47Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nlibcudf provides a `chunked_parquet_reader` in its public API. This reader uses new reader options to process the data in a parquet file in sub-file units. The `chunk_read_limit` option limits the table size in bytes to be returned per read by only decoding a subset of pages per chunked read. The `pass_read_limit` option limits the memory used for reading and decompressing data by only decompressing a subset of pages per chunked read.\r\n\r\nThe chunked parquet reader allows cuDF-python to expose two types of useful functionality:\r\n1. an API that acts as an iterator to yield dataframe chunks. This is similar to the `iter_row_groups` behavior in [fastparquet](https://fastparquet.readthedocs.io/en/latest/api.html). This approach would let users work with parquet files that contain more rows than 2.1B rows (see #13159 for more information about the row limit in libcudf). \r\n2. a \"low_memory\" mode that reads the full file, but has a lower peak memory footprint thanks to the smaller sizes of intermediate allocations. This is similar to the the `low_memory` argument in [polars](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html). This approach would make it easier to read large parquet datasets with limited GPU memory.\r\n\r\n**Describe the solution you'd like**\r\nWe should make chunked parquet reading available to cuDF-python users. Perhaps this functionality could be made available to `cudf.pandas` users as well. \r\n\r\n\r\n**Additional context**\r\nPandas does not seem to have a method for chunking parquet reads, and I'm not sure if pandas makes use of the `iter_row_groups` behavior in fastparquet as a pass-through parameter.\r\n\r\n\r\nAPI docs references:\r\n* pandas: [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)\r\n* pyarrow: [parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)\r\n* fastparquet: [read_parquet](https://fastparquet.readthedocs.io/en/latest/api.html)\r\n* polars: [read_parquet](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html)\r\n* cudf: [read_parquet](https://docs.rapids.ai/api/cudf/nightly/user_guide/api_docs/api/cudf.read_parquet/)\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/14966/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/14966/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}