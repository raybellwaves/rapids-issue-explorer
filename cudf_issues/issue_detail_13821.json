{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/13821",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/13821/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13821/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/13821/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/13821",
    "id": 1837201993,
    "node_id": "I_kwDOBWUGps5tgXpJ",
    "number": 13821,
    "title": "[FEA] Reduce page faults when using managed memory",
    "user": {
        "login": "GregoryKimball",
        "id": 12725111,
        "node_id": "MDQ6VXNlcjEyNzI1MTEx",
        "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/GregoryKimball",
        "html_url": "https://github.com/GregoryKimball",
        "followers_url": "https://api.github.com/users/GregoryKimball/followers",
        "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
        "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
        "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
        "repos_url": "https://api.github.com/users/GregoryKimball/repos",
        "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
        "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1013987352,
            "node_id": "MDU6TGFiZWwxMDEzOTg3MzUy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Backlog",
            "name": "0 - Backlog",
            "color": "d4c5f9",
            "default": false,
            "description": "In queue waiting for assignment"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/20",
        "html_url": "https://github.com/rapidsai/cudf/milestone/20",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/20/labels",
        "id": 8568142,
        "node_id": "MI_kwDOBWUGps4Agr1O",
        "number": 20,
        "title": "Stabilizing large workflows (OOM, spilling, partitioning)",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 16,
        "closed_issues": 3,
        "state": "open",
        "created_at": "2022-10-21T19:29:07Z",
        "updated_at": "2024-05-22T23:20:04Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 3,
    "created_at": "2023-08-04T18:50:40Z",
    "updated_at": "2023-08-17T18:14:29Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nIn cuDF-python and RMM, it's easy to opt into [managed memory](https://docs.rapids.ai/api/rmm/stable/api/#rmm.reinitialize) (also known as Unified Memory, UM, and Unified Virtual Memory, UVM). However, libcudf is not optimized for use with managed memory and encounters many \"[just too late](https://www.nextplatform.com/2019/01/24/unified-memory-the-final-piece-of-the-gpu-programming-puzzle/)\" page faults when the \"[oversubscription factor](https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/)\" is >1.\r\n\r\n### Hinting options and strategies\r\n* Use hinting with [cudaMemPrefetchAsync](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42) before operating on a column_view. I believe this hint will eagerly migrate the data to device. Open questions include: does it require an extra sync? do kernels page fault for a while until the data fully migrates? \r\n* Use hinting with [cudaMemAdviseSetAccessedBy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c85750a22279bce0dc29956ad4f257084623). This hinting also does not eagerly migrate the data, and seems to be focused on preventing faults between devices on the same node. It also allows direct memory access (DMA) from the device to pinned host buffers.\r\n* Use hinting with [cudaMemAdviseSetPreferredLocation](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c857a4a2bc3c7d218dcd9a1b425b432759eb). This does not eagerly migrate the data, instead it influences the page migration system. If we set the preferred location to device, I believe this hint would prevent those allocations from being evicted and could lead to poor performance of the page migration engine.  \r\n*  Use hinting with [cudaMemAdviseSetReadMostly](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c857441d911811beda174627f403142d5ff0). For column_view data, processing in libcudf algorithms will be read-only by design. We can communicate this to UVM, but for libcudf's common access pattern - read once and then write a new allocation for the results - I don't think \"read mostly\" hinting will give us higher throughput or reduced faulting.\r\n* Use host-pinned buffers and use direct memory access (DMA) from the device to extend working memory ([see.\"zero-copy\" in this blog](https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/)). To execute this strategy we hint \"preferred location\" to host and \"accessed by\" to device. Memory throughput will be lower using DMA to host than using device memory, but stalling kernels on page faults will be much slower than waiting on DMA. We still need to design how and when we would choose to leave data on the host and access by DMA (always??? except intermediate allocations).\r\n* In addition to preventing page faults, we may also want to prevent evictions by preemptively clearing device memory. There does not appear to be a mechanism for eagerly migrating data from device to host. Perhaps preferred location hinting can also drive evictions on groups of pages instead of one page at a time.\r\n\r\n### Implementation ideas for libcudf\r\n* Where would this hinting be located in the repository? We could implement a RAII \"advisor\" class that takes a (non-owning) reference to a column_view and performs the appropriate hinting. The advisor class would only perform hinting for column_views created using managed memory resources. It may be difficult to add hinting to column_view because the column_view object can't tell if it's underlying data was a managed or unmanaged allocation.\r\n* Is a way to identify from a device pointer if the associated allocation is managed or unmanaged? Perhaps [cuPointerGetAttribute()](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__UNIFIED.html) should return  CU_MEMORYTYPE_UNIFIED as [CU_POINTER_ATTRIBUTE_MEMORY_TYPE](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html#group__CUDA__TYPES_1ggc2cce590e35080745e72633dfc6e0b600409e16293b60b383f30a9b417b2917c) for managed memory. Is there a runtime API for accessing device pointer attributes? (TBD)\r\n\r\n\r\n### Useful reference for cudaMemAdvise\r\n![image](https://github.com/rapidsai/cudf/assets/12725111/9ff97f3c-3e42-4664-b6b6-fcfdfc07dc90)\r\n\r\n<br>\r\n<br>\r\n<br>\r\n\r\n_Please note: Using managed memory in libcudf is in early stages of scoping. This issue will improve over time._\r\n\r\n<br>\r\n<br>\r\n\r\n**Describe the solution you'd like**\r\nI would like to add a libcudf benchmark for studying managed memory performance, and then some targeted experiments (with profiling) to observe the impact of different hinting strategies. When we have identified a promising design, we will open a more targeted issue.\r\n\r\n**Describe alternatives you've considered**\r\nContinue to let Dask and Spark-RAPIDS catch and retry when there are device OOM errors.\r\n\r\n**Additional context**\r\nPlease note that with managed memory pools, the pool allocation is lazy. This is different from unmanaged memory pools where we allocate the full pool upfront, trading slightly longer startup time for much faster algorithm allocations.\r\n\r\nUseful blog posts:\r\nhttps://developer.nvidia.com/blog/unified-memory-cuda-beginners/\r\nhttps://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/\r\nhttps://developer.nvidia.com/blog/maximizing-unified-memory-performance-in-cuda/\r\nhttps://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/13821/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/13821/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}