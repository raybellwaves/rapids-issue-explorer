{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/13733",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/13733/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13733/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/13733/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/13733",
    "id": 1816892483,
    "node_id": "I_kwDOBWUGps5sS5RD",
    "number": 13733,
    "title": "[FEA] Increase maximum characters in strings columns",
    "user": {
        "login": "GregoryKimball",
        "id": 12725111,
        "node_id": "MDQ6VXNlcjEyNzI1MTEx",
        "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/GregoryKimball",
        "html_url": "https://github.com/GregoryKimball",
        "followers_url": "https://api.github.com/users/GregoryKimball/followers",
        "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
        "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
        "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
        "repos_url": "https://api.github.com/users/GregoryKimball/repos",
        "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
        "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1013987352,
            "node_id": "MDU6TGFiZWwxMDEzOTg3MzUy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Backlog",
            "name": "0 - Backlog",
            "color": "d4c5f9",
            "default": false,
            "description": "In queue waiting for assignment"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        },
        {
            "id": 1515616253,
            "node_id": "MDU6TGFiZWwxNTE1NjE2MjUz",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/strings",
            "name": "strings",
            "color": "0e8a16",
            "default": false,
            "description": "strings issues (C++ and Python)"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/30",
        "html_url": "https://github.com/rapidsai/cudf/milestone/30",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/30/labels",
        "id": 9790800,
        "node_id": "MI_kwDOBWUGps4AlWVQ",
        "number": 30,
        "title": "Language model acceleration",
        "description": null,
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 5,
        "closed_issues": 1,
        "state": "open",
        "created_at": "2023-08-14T17:50:04Z",
        "updated_at": "2024-02-15T18:33:18Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 9,
    "created_at": "2023-07-22T20:58:01Z",
    "updated_at": "2024-06-06T13:03:02Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "In libcudf, strings columns have child columns containing character data and offsets, and the offsets child column uses a 32-bit signed size type. This limits strings columns to containing ~2.1 billion characters. For LLM training, documents have up to 1M characters, and a median around 3K characters. Due to the size type limit, LLM training pipelines have to carefully batch the data down to a few thousand rows to stay comfortably within the size type limit. We have a general issue open to explore a 64-bit size type in libcudf (#13159). For size issues with LLM training pipelines, we should consider a targeted change to only address the size limit for strings columns.\r\n\r\n### Requirements\r\n* We must maintain or improve throughput for functions processing strings columns with <2.1 billion characters. This requirement prevents us from using 64-bit offsets for all strings columns. It does not prevent us from using 64-bit offsets for strings columns with >2.1 billion characters.\r\n* We must not introduce a new data type or otherwise increase compile times significantly. This requirement prevents us from dispatching between \"strings\" types and \"large strings\" types. \r\n\r\n### Proposed solution\r\nOne idea that satisfies these requirements would be to represent the character data as an `int64` typed column instead of an `int8` typed column. This would allow us to store 8x more bytes of character data. To access the character bytes, we would use an offset-normalizing iterator (inspired by [\"indexalator\"](https://github.com/rapidsai/cudf/blob/branch-23.08/cpp/include/cudf/detail/indexalator.cuh)) to identify byte positions using an `int64` iterator output. Please note that the row count 32-bit size type would still apply to the proposed \"large strings\" columns.\r\n\r\nWe should also consider an \"unbounded\" character data allocation that is not typed, but rather a single buffer up to 2^64 bytes in size. The 64-bit offset type would be able to index into much larger allocations.\r\n\r\nPlease note that this solution will not impact the offsets for list columns. We believe that the best design to allow for more than 2.1B elements in lists will be to use 64-bit size type in libcudf as discussed in #13159.\r\n\r\n### Creating strings columns\r\nStrings columns factories would choose child column types at the time of column creation, based on the size of the character data. This change would impact strings column factories, as well as algorithms that use strings column utilities or generate their own offsets buffers. At column creation time, the constructor will choose between `int32` offsets with `int8` character data and `int64` offsets with `int64` character data, based on the size of the character data. Any function that calls [make_offsets_child_column](https://github.com/rapidsai/cudf/blob/9e099cef25b11821c6307bb9c231656a2bae700f/cpp/include/cudf/detail/sizes_to_offsets_iterator.cuh#L298-L302) will need to be aware of the alternate child column types for large strings.\r\n\r\n### Accessing strings data\r\nThe offset-normalizing iterator would always return `int64` type so that strings column consumers would not need to support both `int32` and `int64` offset types. See [cudf::detail::sizes_to_offsets_iterator](https://github.com/rapidsai/cudf/pull/12180) for an example of how an iterator operating on `int32` data can output `int64` data.\r\n\r\n### Interoperability with Arrow\r\nThe new strings column variant with `int64` offsets with `int64` character data may already be Arrow-compatible. This requires more testing and some changes to our Arrow interop utilities.\r\n\r\n### Part 1: libcudf changes to support large strings columns\r\n\r\nDefinitions:\r\n\"strings column\": `int8` character data and `int32` offset data (2.1B characters) \r\n\"large strings column\": `int8` character data up to 2^64 bytes and `int64` offset data (18400T characters)\r\n\r\n| Step | PR | Notes | \r\n|---|---|---|\r\n| Replace `offset_type` references with `size_type` | \u2705 #13788 | offsets generated by the offset-normalizing iterator will have type `int64_t` | \r\n| <s> Add new data-size member to `cudf::column_view`, `cudf::mutable_column_view` and `cudf::column_device_view` </s> | \u274c #14031 | solution for character counts greater than `int32` | \r\n| Create an offset-normalizing iterator over character data that always outputs 64-bit offsets| \u2705 #14206 <br> \u2705 #14234 | First step in #14043 |\r\n| * Add the character data buffer to the parent strings column, rather than as a child column <br> * Also refactor algorithms such as concat, contiguous split and gather which access character data <br> * Update code in cuDF-python that interact with character child columns <br> * Update code in cudf-java that interact with character child columns | \u2705 #14202 | See performance blocker resolved in \u2705 #14540 |\r\n| Deprecate unneeded factories and use strings column factories consistently | \u2705 | #14461, #14771, #14695, #14612, +one more | \r\n| Introduce an environment variable to control the threshold for converting to 64-bit indices, to enable testing on smaller strings columns | \u2705 `LIBCUDF_LARGE_STRINGS_THRESHOLD` added | part of #14612 | \r\n| Transition strings APIs to use the offset-normalizing iterator (\"offsetalator\") | \u2705 | See #14611, #14700, #14744, #14745, #14757, #14783, #14824  | \r\n| Remove references to `strings_column_view::offsets_begin()` in libcudf since it hardcodes the return type as int32. | \u2705 | See #15112 #15077  | \r\n| Remove references to `create_chars_child_column` in libcudf since it wraps a column around chars data. | \u2705 | #15241 | \r\n| Change the current `make_strings_children` to return a uvector for chars instead of a column | \u2705 | See #15171  | \r\n| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_ENABLED` to let users force libcudf to throw rather than start using 64-bit offsets, to allow try-catch-repartitioning instead | \u2705 |  #15195  | \r\n| Introduce an environment variable `LIBCUDF_LARGE_STRINGS_THRESHOLD` | \u2705 |  #14612 | \r\n| Rework `concatenate` to produce large strings when `LIBCUDF_LARGE_STRINGS_ENABLED` and character count is above the `LIBCUDF_LARGE_STRINGS_THRESHOLD` | \u2705 |  See #15195  |\r\n| cuDF-python testing. use concat to create a large string column. We should be able to operate on this column, as long as we aren't creating a large string. Can we: (1) returns int/bool, like, contains, (2) slice (3) returns smaller strings. | \ud83d\udd04 | |\r\n| Add an `experimental` version of `make_strings_children` that generates 64-bit offsets when the total character length exceeds the threshold | \u2705 |#15363 | \r\n| Add a large strings test fixture that stores large columns between unit tests and controls the environment variables | \u2705 | #15513  | \r\n| Check appropriate cudf tests pass with `LIBCUDF_LARGE_STRINGS_THRESHOLD` at zero  | | |\r\n| benchmark regressions analyzed and approved | | |\r\n| Spark-RAPIDS tests pass | | |\r\n| Remove `experimental` namespace. Replace `make_strings_children` with implementation with the `experimental` namespace version. | \u2705 | #15702  | \r\n| Live session with cuDF-python expert to start producing and operating on large strings | | |\r\n\r\n### Part 2: cuIO changes to read and write large strings columns\r\n\r\n| Step | PR | Notes |\r\n|---|---|---| \r\n| JSON | Reader: Probably solved by https://github.com/rapidsai/cudf/pull/15930 |  |\r\n| Parquet | Reader and writer Added in #15632 | Still needs to be tested | \r\n| ORC | Partial fix in #15891 | Needs investigation | \r\n| CSV | | This uses factories that are already enabled. Still needs testing. |\r\n| Text | | This uses factories that are already enabled. Still needs testing. |\r\n\r\n\r\n### Part 3: Interop changes to read and write large strings columns\r\n\r\nAlso see https://github.com/rapidsai/cudf/pull/15093 about large_strings compatibility for pandas-2.2\r\n\r\n| Step | PR | Notes |\r\n|---|---|---| \r\n|  to_arrow functions  |  | Use offsets type to set LARGE_STRINGS type. |\r\n|  from_arrow functions  |  |  Honor the env var settings as well. |",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/13733/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/13733/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}