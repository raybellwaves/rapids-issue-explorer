{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/11796",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/11796/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11796/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/11796/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/11796",
    "id": 1388299861,
    "node_id": "I_kwDOBWUGps5Sv8ZV",
    "number": 11796,
    "title": "[QST] OOM issue while loading the 26GB twitter dataset into 128GB GPU memory",
    "user": {
        "login": "parkerzf",
        "id": 1273326,
        "node_id": "MDQ6VXNlcjEyNzMzMjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1273326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/parkerzf",
        "html_url": "https://github.com/parkerzf",
        "followers_url": "https://api.github.com/users/parkerzf/followers",
        "following_url": "https://api.github.com/users/parkerzf/following{/other_user}",
        "gists_url": "https://api.github.com/users/parkerzf/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/parkerzf/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/parkerzf/subscriptions",
        "organizations_url": "https://api.github.com/users/parkerzf/orgs",
        "repos_url": "https://api.github.com/users/parkerzf/repos",
        "events_url": "https://api.github.com/users/parkerzf/events{/privacy}",
        "received_events_url": "https://api.github.com/users/parkerzf/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626564,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjQ=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/question",
            "name": "question",
            "color": "D4C5F9",
            "default": true,
            "description": "Further information is requested"
        },
        {
            "id": 1013987352,
            "node_id": "MDU6TGFiZWwxMDEzOTg3MzUy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Backlog",
            "name": "0 - Backlog",
            "color": "d4c5f9",
            "default": false,
            "description": "In queue waiting for assignment"
        },
        {
            "id": 1185240898,
            "node_id": "MDU6TGFiZWwxMTg1MjQwODk4",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/dask",
            "name": "dask",
            "color": "fcc25d",
            "default": false,
            "description": "Dask issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/20",
        "html_url": "https://github.com/rapidsai/cudf/milestone/20",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/20/labels",
        "id": 8568142,
        "node_id": "MI_kwDOBWUGps4Agr1O",
        "number": 20,
        "title": "Stabilizing large workflows (OOM, spilling, partitioning)",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 16,
        "closed_issues": 3,
        "state": "open",
        "created_at": "2022-10-21T19:29:07Z",
        "updated_at": "2024-05-22T23:20:04Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 4,
    "created_at": "2022-09-27T20:05:54Z",
    "updated_at": "2022-10-21T19:31:39Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Hey I try to load the twitter graph in a AWS `p3.16xlarge` instance, which has 8 16GB memory GPUs, in total 128GB. However, it is OOM. Could you please take a look if I missed anything? Thanks so much!\r\n\r\n```python\r\nimport dask\r\nfrom dask_cuda import LocalCUDACluster\r\nfrom dask.distributed import Client\r\nimport dask_cudf\r\nimport cugraph\r\nimport cugraph.dask as dask_cugraph\r\nfrom cugraph.dask.common.mg_utils import get_visible_devices\r\nfrom cugraph.dask.comms import comms as Comms\r\nimport time\r\n\r\ncsv_file_name = \"twitter-2010.csv\"\r\n\r\nwith dask.config.set(jit_unspill=True):\r\n    with LocalCUDACluster(n_workers=8, device_memory_limit=\"16GB\") as cluster:\r\n        with Client(cluster) as client:\r\n            client.wait_for_workers(len(get_visible_devices()))\r\n            Comms.initialize(p2p=True)\r\n            chunksize = dask_cugraph.get_chunksize(csv_file_name)\r\n            ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\r\n            ddf.compute()\r\n            # G = cugraph.Graph(directed=True)\r\n            # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')\r\n```\r\n\r\nI can't find similar issues,  this [one](https://github.com/rapidsai/cudf/issues/6087) got similar errors but it is because LocalCUDACluster is not used.\r\n\r\nI used the docker approach to install the rapid frameworks:\r\n\r\n```cmd\r\ndocker pull rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9\r\ndocker run --gpus all --rm -it \\\r\n    --shm-size=10g --ulimit memlock=-1 \\\r\n    -p 8888:8888 -p 8787:8787 -p 8786:8786 \\\r\n    rapidsai/rapidsai-dev:22.08-cuda11.5-devel-ubuntu20.04-py3.9\r\n```\r\n\r\nThe error log:\r\n\r\n```\r\n2022-09-27 13:03:21,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n2022-09-27 13:03:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\r\n2022-09-27 13:03:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n/tmp/ipykernel_5947/1798640855.py in <module>\r\n      9             chunksize = dask_cugraph.get_chunksize(csv_file_name)\r\n     10             ddf = dask_cudf.read_csv(csv_file_name, chunksize=chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\r\n---> 11             ddf.compute()\r\n     12             # G = cugraph.Graph(directed=True)\r\n     13             # G.from_dask_cudf_edgelist(ddf, source='src', destination='dst')\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(self, **kwargs)\r\n    313         dask.base.compute\r\n    314         \"\"\"\r\n--> 315         (result,) = compute(self, traverse=False, **kwargs)\r\n    316         return result\r\n    317 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    597 \r\n    598     results = schedule(dsk, keys, **kwargs)\r\n--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    600 \r\n    601 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/base.py in <listcomp>(.0)\r\n    597 \r\n    598     results = schedule(dsk, keys, **kwargs)\r\n--> 599     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    600 \r\n    601 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in finalize(results)\r\n    136 \r\n    137 def finalize(results):\r\n--> 138     return _concat(results)\r\n    139 \r\n    140 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxify_device_objects.py in wrapper(*args, **kwargs)\r\n    167     @functools.wraps(func)\r\n    168     def wrapper(*args, **kwargs):\r\n--> 169         ret = func(*args, **kwargs)\r\n    170         if dask.config.get(\"jit-unspill-compatibility-mode\", default=False):\r\n    171             ret = unproxify_device_objects(ret, skip_explicit_proxies=False)\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/core.py in _concat(args, ignore_index)\r\n    131         args[0]\r\n    132         if not args2\r\n--> 133         else methods.concat(args2, uniform=True, ignore_index=ignore_index)\r\n    134     )\r\n    135 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\r\n     60     else:\r\n     61         func = concat_dispatch.dispatch(type(dfs[0]))\r\n---> 62         return func(\r\n     63             dfs,\r\n     64             axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cuda-22.8.0-py3.9.egg/dask_cuda/proxy_object.py in wrapper(*args, **kwargs)\r\n    900         args = [unproxy(d) for d in args]\r\n    901         kwargs = {k: unproxy(v) for k, v in kwargs.items()}\r\n--> 902         return func(*args, **kwargs)\r\n    903 \r\n    904     return wrapper\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask/dataframe/dispatch.py in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\r\n     60     else:\r\n     61         func = concat_dispatch.dispatch(type(dfs[0]))\r\n---> 62         return func(\r\n     63             dfs,\r\n     64             axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)\r\n     77         def inner(*args, **kwds):\r\n     78             with self._recreate_cm():\r\n---> 79                 return func(*args, **kwds)\r\n     80         return inner\r\n     81 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/dask_cudf/backends.py in concat_cudf(dfs, axis, join, uniform, filter_warning, sort, ignore_index, **kwargs)\r\n    273         )\r\n    274 \r\n--> 275     return cudf.concat(dfs, axis=axis, ignore_index=ignore_index)\r\n    276 \r\n    277 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/reshape.py in concat(objs, axis, join, ignore_index, sort)\r\n    397                 # don't filter out empty df's\r\n    398                 objs = old_objs\r\n--> 399             result = cudf.DataFrame._concat(\r\n    400                 objs,\r\n    401                 axis=axis,\r\n\r\n/opt/conda/envs/rapids/lib/python3.9/contextlib.py in inner(*args, **kwds)\r\n     77         def inner(*args, **kwds):\r\n     78             with self._recreate_cm():\r\n---> 79                 return func(*args, **kwds)\r\n     80         return inner\r\n     81 \r\n\r\n/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/dataframe.py in _concat(cls, objs, axis, join, ignore_index, sort)\r\n   1674         # Concatenate the Tables\r\n   1675         out = cls._from_data(\r\n-> 1676             *libcudf.concat.concat_tables(\r\n   1677                 tables, ignore_index=ignore_index or are_all_range_index\r\n   1678             )\r\n\r\nconcat.pyx in cudf._lib.concat.concat_tables()\r\n\r\nconcat.pyx in cudf._lib.concat.concat_tables()\r\n```\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/11796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/11796/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}