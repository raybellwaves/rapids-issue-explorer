{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/15759",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/15759/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15759/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/15759/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/15759",
    "id": 2297479930,
    "node_id": "I_kwDOBWUGps6I8ML6",
    "number": 15759,
    "title": "[BUG] Data corruption and strange CUDA memory address errors at the same row index, despite manipulating data, when using `.stack()` on large, wide dataset",
    "user": {
        "login": "taureandyernv",
        "id": 46935140,
        "node_id": "MDQ6VXNlcjQ2OTM1MTQw",
        "avatar_url": "https://avatars.githubusercontent.com/u/46935140?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/taureandyernv",
        "html_url": "https://github.com/taureandyernv",
        "followers_url": "https://api.github.com/users/taureandyernv/followers",
        "following_url": "https://api.github.com/users/taureandyernv/following{/other_user}",
        "gists_url": "https://api.github.com/users/taureandyernv/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/taureandyernv/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/taureandyernv/subscriptions",
        "organizations_url": "https://api.github.com/users/taureandyernv/orgs",
        "repos_url": "https://api.github.com/users/taureandyernv/repos",
        "events_url": "https://api.github.com/users/taureandyernv/events{/privacy}",
        "received_events_url": "https://api.github.com/users/taureandyernv/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626559,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NTk=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-05-15T10:22:32Z",
    "updated_at": "2024-05-19T03:47:52Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nWhenever I'm trying to use cudf,stack() on this large wide dataframe, at around the same index location, the data gets corrupted as you stack past that index until it fails to run, or just fails to run.  It happens at index 1159550.   go one index before 1159550, everything is fine. One or two after, you start to see issues or it fails.  Even if you change around the data a bit, it still fails. eventually.  When it fails, it returns `RuntimeError: parallel_for: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered`. \r\n\r\nHappens on both an A100 80GB and H100 running 24.04.  Completes successfully on pandas. Falls back to pandas and successfully completes on cudf.pandas.\r\n\r\n**Steps/Code to reproduce bug**\r\nThis requires a dataset download, handled in the min repro, and a 32GB GPU or larger to test.\r\n\r\nYou can actually see the data getting corrupted at the incrementing runs at the end of the min repro, before it finally fails\r\n\r\n```\r\n!if [ ! -f \"job_skills.csv\" ]; then curl https://storage.googleapis.com/rapidsai/colab-data/job_skills.csv.gz -o job_skills.csv.gz; gunzip job_skills.csv.gz; else echo \"unzipped job data found\"; fi\r\nimport cudf\r\nskills = cudf.read_csv(\"job_skills.csv\")\r\n\r\nb = skills[\"job_skills\"].str.split(\",\", expand=True)\r\n#print(b.iloc[1159550]) # incase you wanted to see what was on that index\r\nprint(b.iloc[1159550])\r\nb2 = b[:1159549]\r\n# b2 = b[:1159550] # Uncommenting this, it will fail\r\nstacked_skills = b2.stack()\r\nprint(stacked_skills.head())\r\n\r\n# this will also fail\r\n# stacked_skills = b.stack().dropna()\r\n\r\n# even if you change the dataframe a bit by moving up the indexes incrementally, it will not really change where it fails, as you can start to see the data start glitch\r\nprint(skills.count())\r\nskills = skills.dropna()\r\nprint(skills.count())\r\nb = skills[\"job_skills\"].str.split(\",\", expand=True)\r\nprint(b.iloc[1159550]) # in case you wanted to see what was on that index\r\nb2 = b[:1159549]\r\nstacked_skills = b2.stack()\r\nprint(1159549)\r\nprint(stacked_skills.head())\r\nb2 = b[:1159550]\r\nstacked_skills = b2.stack()\r\nprint(1159550)\r\nprint(stacked_skills.head()) # you can start to see data corruption or it just fails\r\nb2 = b[:1159551]\r\nstacked_skills = b2.stack()\r\nprint(1159551)\r\nprint(stacked_skills.head())\r\nb2 = b[:1159552]\r\nstacked_skills = b2.stack()\r\nprint(1159552)\r\nprint(stacked_skills.head())\r\nb2 = b[:1159553]\r\nstacked_skills = b2.stack()\r\nprint(1159553)\r\nprint(stacked_skills.head())\r\nb2 = b[:1159554]\r\nstacked_skills = b2.stack()\r\nprint(1159554)\r\nprint(stacked_skills.head()) # by here it should fail\r\n```\r\nOutputs:\r\n```\r\n0                         Anesthesiology\r\n1                        Medical license\r\n2                      BLS certification\r\n3                       DEA registration\r\n4       Controlled Substance Certificate\r\n                     ...                \r\n458                                 <NA>\r\n459                                 <NA>\r\n460                                 <NA>\r\n461                                 <NA>\r\n462                                 <NA>\r\nName: 1159550, Length: 463, dtype: object\r\n0  0    Building Custodial Services\r\n   1                       Cleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\njob_link      1296381\r\njob_skills    1294346\r\ndtype: int64\r\njob_link      1294346\r\njob_skills    1294346\r\ndtype: int64\r\n0      Project Management\r\n1           Communication\r\n2           Collaboration\r\n3              Leadership\r\n4          ProblemSolving\r\n              ...        \r\n458                  <NA>\r\n459                  <NA>\r\n460                  <NA>\r\n461                  <NA>\r\n462                  <NA>\r\nName: 1161237, Length: 463, dtype: object\r\n1159549\r\n0  0    Building Custodial Services\r\n   1                       Cleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\n1159550\r\n0  0     PCUeel Nurseendek Services\r\n   1                       Cleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\n1159551\r\n0  0     PCUeel Nursenndek Services\r\n   1                       Cleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\n1159552\r\n0  0     FoUd Safetyeg certificatio\r\n   1                      nCleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[1], line 40\r\n     38 print(stacked_skills.head())\r\n     39 b2 = b[:1159553]\r\n---> 40 stacked_skills = b2.stack()\r\n     41 print(1159553)\r\n     42 print(stacked_skills.head())\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    113 @wraps(func)\r\n    114 def inner(*args, **kwargs):\r\n    115     libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116     result = func(*args, **kwargs)\r\n    117     libnvtx_pop_range(self.domain.handle)\r\n    118     return result\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:7079, in DataFrame.stack(self, level, dropna, future_stack)\r\n   7073     # homogenize the dtypes of the columns\r\n   7074     homogenized = [\r\n   7075         col.astype(common_type) if col is not None else all_nulls()\r\n   7076         for col in columns\r\n   7077     ]\r\n-> 7079     stacked.append(libcudf.reshape.interleave_columns(homogenized))\r\n   7081 # Construct the resulting dataframe / series\r\n   7082 if not has_unnamed_levels:\r\n\r\nFile /opt/conda/lib/python3.10/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)\r\n     76 @wraps(func)\r\n     77 def inner(*args, **kwds):\r\n     78     with self._recreate_cm():\r\n---> 79         return func(*args, **kwds)\r\n\r\nFile reshape.pyx:26, in cudf._lib.reshape.interleave_columns()\r\n\r\nRuntimeError: parallel_for: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered\r\n```\r\n\r\n**Expected behavior**\r\nThis should just work, as it does in pandas, without ay data corruption\r\n```\r\n!if [ ! -f \"job_skills.csv\" ]; then curl https://storage.googleapis.com/rapidsai/colab-data/job_skills.csv.gz -o job_skills.csv.gz; gunzip job_skills.csv.gz; else echo \"unzipped job data found\"; fi\r\nimport pandas as pd\r\nskills = pd.read_csv(\"job_skills.csv\")\r\n\r\nb = skills[\"job_skills\"].str.split(\",\", expand=True)\r\nprint(b.iloc[1159550])\r\nb2 = b # just to keep the copying similar.  it doesn't matter.\r\nstacked_skills = b2.stack()\r\nprint(stacked_skills.head())\r\n```\r\nOutputs:\r\n```\r\n0                         Anesthesiology\r\n1                        Medical license\r\n2                      BLS certification\r\n3                       DEA registration\r\n4       Controlled Substance Certificate\r\n                     ...                \r\n458                                 None\r\n459                                 None\r\n460                                 None\r\n461                                 None\r\n462                                 None\r\nName: 1159550, Length: 463, dtype: object\r\n0  0    Building Custodial Services\r\n   1                       Cleaning\r\n   2            Janitorial Services\r\n   3             Materials Handling\r\n   4                   Housekeeping\r\ndtype: object\r\n```\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: Docker\r\n   - If method of install is [Docker], docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 9888:8888 -p 9787:8787 -p 9786:8786 -p 9999:9999 rapidsai/notebooks:24.04-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root\r\n\r\n\r\n**Environment details**\r\nRAPIDS 24.04 cuda 11.8, py 3.9 and 3.10 Docker on ARM SBSA machines\r\n\r\n**Additional context**\r\nWhen running cudf.pandas, this will succeed, but at the costs of taking nearly 30-40% longer than pandas alone.  If and when it succeeds (by reducing it to the last row where it succeeds, it would be 50x+ faster.  I have not done a data integrity test just yet, to see if the corruption happens earlier.\r\n@vyasr fyi.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/15759/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/15759/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}