{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/15057",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/15057/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15057/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/15057/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/15057",
    "id": 2135232901,
    "node_id": "I_kwDOBWUGps5_RRGF",
    "number": 15057,
    "title": "[FEA] Update chunked parquet reader benchmarks to include `pass_read_limit`",
    "user": {
        "login": "GregoryKimball",
        "id": 12725111,
        "node_id": "MDQ6VXNlcjEyNzI1MTEx",
        "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/GregoryKimball",
        "html_url": "https://github.com/GregoryKimball",
        "followers_url": "https://api.github.com/users/GregoryKimball/followers",
        "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
        "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
        "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
        "repos_url": "https://api.github.com/users/GregoryKimball/repos",
        "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
        "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1013987352,
            "node_id": "MDU6TGFiZWwxMDEzOTg3MzUy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Backlog",
            "name": "0 - Backlog",
            "color": "d4c5f9",
            "default": false,
            "description": "In queue waiting for assignment"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        },
        {
            "id": 1185244142,
            "node_id": "MDU6TGFiZWwxMTg1MjQ0MTQy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/cuIO",
            "name": "cuIO",
            "color": "fef2c0",
            "default": false,
            "description": "cuIO issue"
        },
        {
            "id": 1405146975,
            "node_id": "MDU6TGFiZWwxNDA1MTQ2OTc1",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/Spark",
            "name": "Spark",
            "color": "7400ff",
            "default": false,
            "description": "Functionality that helps Spark RAPIDS"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/22",
        "html_url": "https://github.com/rapidsai/cudf/milestone/22",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/22/labels",
        "id": 8672393,
        "node_id": "MI_kwDOBWUGps4AhFSJ",
        "number": 22,
        "title": "Parquet continuous improvement",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 38,
        "closed_issues": 37,
        "state": "open",
        "created_at": "2022-11-19T18:08:31Z",
        "updated_at": "2024-06-06T18:40:39Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 0,
    "created_at": "2024-02-14T21:27:40Z",
    "updated_at": "2024-02-16T23:45:28Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThe `BM_parquet_read_chunks` benchmark in `benchmarks/io/parquet/parquet_reader_input.cpp` includes a `byte_limit` nvbench axis. This axis controls the `chunk_read_limit`. With the new features added in #14360, there is a new `chunked_parquet_reader` API that exposes both `chunk_read_limit` and `pass_read_limit` parameters to control reader behavior. We currently do not have a method for benchmarking `pass_read_limit` values.\r\n\r\n**Describe the solution you'd like**\r\n- [ ] Add a new benchmark, such as `BM_parquet_read_subrowgroup_chunks`, that provides nvbench axes for both `chunk_read_limit` and `pass_read_limit`\r\n- [ ] Rename `byte_limit` to `chunk_read_limit` in `BM_parquet_read_chunks` for clarity, now that we have both input and output byte limits in chunked parquet reading.\r\n- [ ] Also, please consider adding an nvbench axis for `data_size` for at least the chunked parquet reader benchmarks. It would be useful to allow the benchmarks to operate on tables larger than 536 MB.\r\n\r\n**Describe alternatives you've considered**\r\nn/a",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/15057/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/15057/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}