{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/6089",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/6089/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6089/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/6089/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/6089",
    "id": 685674967,
    "node_id": "MDU6SXNzdWU2ODU2NzQ5Njc=",
    "number": 6089,
    "title": "[FEA] Support packing to a max input sequence length with cudf-subword tokenizer",
    "user": {
        "login": "VibhuJawa",
        "id": 4837571,
        "node_id": "MDQ6VXNlcjQ4Mzc1NzE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4837571?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/VibhuJawa",
        "html_url": "https://github.com/VibhuJawa",
        "followers_url": "https://api.github.com/users/VibhuJawa/followers",
        "following_url": "https://api.github.com/users/VibhuJawa/following{/other_user}",
        "gists_url": "https://api.github.com/users/VibhuJawa/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/VibhuJawa/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/VibhuJawa/subscriptions",
        "organizations_url": "https://api.github.com/users/VibhuJawa/orgs",
        "repos_url": "https://api.github.com/users/VibhuJawa/repos",
        "events_url": "https://api.github.com/users/VibhuJawa/events{/privacy}",
        "received_events_url": "https://api.github.com/users/VibhuJawa/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        },
        {
            "id": 1139741213,
            "node_id": "MDU6TGFiZWwxMTM5NzQxMjEz",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/Python",
            "name": "Python",
            "color": "1d76db",
            "default": false,
            "description": "Affects Python cuDF API."
        },
        {
            "id": 1515616253,
            "node_id": "MDU6TGFiZWwxNTE1NjE2MjUz",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/strings",
            "name": "strings",
            "color": "0e8a16",
            "default": false,
            "description": "strings issues (C++ and Python)"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "davidwendt",
        "id": 45795991,
        "node_id": "MDQ6VXNlcjQ1Nzk1OTkx",
        "avatar_url": "https://avatars.githubusercontent.com/u/45795991?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/davidwendt",
        "html_url": "https://github.com/davidwendt",
        "followers_url": "https://api.github.com/users/davidwendt/followers",
        "following_url": "https://api.github.com/users/davidwendt/following{/other_user}",
        "gists_url": "https://api.github.com/users/davidwendt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/davidwendt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/davidwendt/subscriptions",
        "organizations_url": "https://api.github.com/users/davidwendt/orgs",
        "repos_url": "https://api.github.com/users/davidwendt/repos",
        "events_url": "https://api.github.com/users/davidwendt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/davidwendt/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "davidwendt",
            "id": 45795991,
            "node_id": "MDQ6VXNlcjQ1Nzk1OTkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/45795991?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/davidwendt",
            "html_url": "https://github.com/davidwendt",
            "followers_url": "https://api.github.com/users/davidwendt/followers",
            "following_url": "https://api.github.com/users/davidwendt/following{/other_user}",
            "gists_url": "https://api.github.com/users/davidwendt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/davidwendt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/davidwendt/subscriptions",
            "organizations_url": "https://api.github.com/users/davidwendt/orgs",
            "repos_url": "https://api.github.com/users/davidwendt/repos",
            "events_url": "https://api.github.com/users/davidwendt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/davidwendt/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 1,
    "created_at": "2020-08-25T17:48:07Z",
    "updated_at": "2024-02-23T18:43:26Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently, the tokenized string is shorter than max_length, output is be padded with 0s.  So if `max( tokenized string lengths)` < `max_length`, it  leads to performance penalties as the compute time for `Transformer` models is often proportional to the sequence length of the input . \r\n\r\nHuggingFace's tokenizer defaults to padding to max input sequence length if `max_length` and `pad_to_max_length` are not provided . We should try to follow that, this is especially beneficial for streaming cases that feature https://github.com/rapidsai/cudf/issues/5868 will help. \r\n\r\n\r\n##### See below example: \r\n\r\n#### Padding to max sequence length.(Proposed Default Behaviour)  \r\n```python\r\nfrom transformers import BertTokenizerFast\r\n\r\n\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\r\ndata = ['a', 'a b', 'a b c d']\r\noutput = tokenizer.batch_encode_plus(data,padding=True,add_special_tokens=False, return_tensors = 'pt')\r\noutput['input_ids']\r\n\r\ntensor([[1037,    0,    0,    0],\r\n        [1037, 1038,    0,    0],\r\n        [1037, 1038, 1039, 1040]])\r\n```\r\n\r\n####  Padding to max_length (Current Default Behavior) \r\n```python\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\r\noutput = tokenizer.batch_encode_plus(\r\n        data, truncation=True, max_length=64, pad_to_max_length=True,\r\n        add_special_tokens=False, return_tensors = 'pt'\r\n    )\r\noutput['input_ids']\r\n```\r\n```\r\ntensor([[1037,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0],\r\n        [1037, 1038,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0],\r\n        [1037, 1038, 1039, 1040,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\r\n            0,    0,    0,    0]])\r\n```\r\n\r\n\r\n\r\n**Related Implications:**\r\n\r\na. We might have to switch from returning one-dimensional cupy arrays to 2-dimensional arrays for token-ids and attention masks which we allready do for most workflow cases so should not have performance penalties.   \r\n\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, a user can do the tokenization twice. \r\n\r\n1. First time to get maximum sequence length, do this without a `to_dlpack` call. \r\n2. Inputting that sequence length to tokenizer again and then convert to tensors using `dlpack` \r\n\r\nI do above for gpu-bdb q27 HF. \r\n), As most of the time is spent doing `to_dlpack` so this workaround should not have big performance implications. \r\n\r\n\r\n\r\nCC: @raykallen , @randerzander , @davidwendt \r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/6089/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/6089/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}