{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/13040",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/13040/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13040/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/13040/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/13040",
    "id": 1647938844,
    "node_id": "I_kwDOBWUGps5iOY0c",
    "number": 13040,
    "title": "[QST] convert column of datetime string to column of datetime object",
    "user": {
        "login": "stucash",
        "id": 6358866,
        "node_id": "MDQ6VXNlcjYzNTg4NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6358866?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/stucash",
        "html_url": "https://github.com/stucash",
        "followers_url": "https://api.github.com/users/stucash/followers",
        "following_url": "https://api.github.com/users/stucash/following{/other_user}",
        "gists_url": "https://api.github.com/users/stucash/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/stucash/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/stucash/subscriptions",
        "organizations_url": "https://api.github.com/users/stucash/orgs",
        "repos_url": "https://api.github.com/users/stucash/repos",
        "events_url": "https://api.github.com/users/stucash/events{/privacy}",
        "received_events_url": "https://api.github.com/users/stucash/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626564,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjQ=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/question",
            "name": "question",
            "color": "D4C5F9",
            "default": true,
            "description": "Further information is requested"
        },
        {
            "id": 1013987799,
            "node_id": "MDU6TGFiZWwxMDEzOTg3Nzk5",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Waiting%20on%20Author",
            "name": "0 - Waiting on Author",
            "color": "ffb88c",
            "default": false,
            "description": "Waiting for author to respond to review"
        },
        {
            "id": 1185240898,
            "node_id": "MDU6TGFiZWwxMTg1MjQwODk4",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/dask",
            "name": "dask",
            "color": "fcc25d",
            "default": false,
            "description": "Dask issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-03-30T16:25:12Z",
    "updated_at": "2023-06-05T18:36:24Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "I am a new user of Dask and RapidsAI.\r\nAn exerpt of my data (in `csv` format):\r\n\r\n    Symbol,Date,Open,High,Low,Close,Volume\r\n    AADR,17-Oct-2017 09:00,57.47,58.3844,57.3645,58.3844,2094\r\n    AADR,17-Oct-2017 10:00,57.27,57.2856,57.25,57.27,627\r\n    AADR,17-Oct-2017 11:00,56.99,56.99,56.99,56.99,100\r\n    AADR,17-Oct-2017 12:00,56.98,57.05,56.98,57.05,200\r\n    AADR,17-Oct-2017 13:00,57.14,57.16,57.14,57.16,700\r\n    AADR,17-Oct-2017 14:00,57.13,57.13,57.13,57.13,100\r\n    AADR,17-Oct-2017 15:00,57.07,57.07,57.07,57.07,200\r\n    AAMC,17-Oct-2017 09:00,87,87,87,87,100\r\n    AAU,17-Oct-2017 09:00,1.1,1.13,1.0832,1.121,67790\r\n    AAU,17-Oct-2017 10:00,1.12,1.12,1.12,1.12,100\r\n    AAU,17-Oct-2017 11:00,1.125,1.125,1.125,1.125,200\r\n    AAU,17-Oct-2017 12:00,1.1332,1.15,1.1332,1.15,27439\r\n    AAU,17-Oct-2017 13:00,1.15,1.15,1.13,1.13,8200\r\n    AAU,17-Oct-2017 14:00,1.1467,1.1467,1.14,1.1467,1750\r\n    AAU,17-Oct-2017 15:00,1.1401,1.1493,1.1401,1.1493,4100\r\n    AAU,17-Oct-2017 16:00,1.13,1.13,1.13,1.13,100\r\n    ABE,17-Oct-2017 09:00,14.64,14.64,14.64,14.64,200\r\n    ABE,17-Oct-2017 10:00,14.67,14.67,14.66,14.66,1200\r\n    ABE,17-Oct-2017 11:00,14.65,14.65,14.65,14.65,600\r\n    ABE,17-Oct-2017 15:00,14.65,14.65,14.65,14.65,836\r\n\r\nNote `Date` column is of type string.\r\n\r\nI have some example stock market timeseries data (i.e., DOHLCV) in csv files and I read them into a `dask_cudf` dataframe (my `dask.dataframe` backend is cudf and `read.csv` is a creation dispacther that conveniently gives me a `cudf.dataframe`). \r\n\r\n    import dask_cudf \r\n    import cudf\r\n    from dask import dataframe as dd\r\n    \r\n    ddf = dd.read_csv('path/to/my/data/*.csv')\r\n    ddf\r\n    # output\r\n    <dask_cudf.DataFrame | 450 tasks | 450 npartitions>\r\n    \r\n    \r\n    # test csv data above can be retrieved using following statements\r\n    # df = pd.read_clipboard(sep=\",\")\r\n    # cdf = cudf.from_pandas(df)\r\n    # ddf = dask_cudf.from_cudf(cdf, npartitions=2)\r\n\r\nI then try to convert datetime string into real datetime object (`np.datetime64[ns]` or anything equivalent in `cudf`/`dask` world). I then failed with error.\r\n\r\n    df[\"Date\"] = dd.to_datetime(df[\"Date\"], format=\"%d-%b-%Y %H:%M\").head(5)\r\n    df.set_index(\"Date\", inplace=True) # This failed with different error, will raise in a different SO thread.\r\n    # Following statement gives me same error.\r\n    # cudf.to_datetime(df[\"Date\"], format=\"%d-%b-%Y %H:%M\")\r\n\r\nFull error log is to the end.\r\n\r\nThe error message seems to suggest that I'd need to `compute` the `dask_cudf.dataframe`, turning it into a real `cudf` object, then I\r\ncan do as I would in `pandas`:\r\n\r\n    df[\"Date\"] = cudf.to_datetime(df.Date)\r\n    df = df.set_index(df.Date)\r\n\r\nThis apparently isn't ideal and it very much is the thing that `dask` is for: we'd delay this and only calculate the ultimate number we need.\r\n\r\nwhat is the `dask`/`dask_cudf` way to convert a string column to datetime column in `dask_cudf`? As far as I can see, if the backend is `pandas`, the conversion is done smoothly and rarely has problem. \r\n\r\nOr, is it that `cudf` or GPU world in general, is not supposed to do much with date types like `datetime`, `string` ? (e.g., ideally GPU is geared towards expensive numerical computations). \r\n\r\nMy use case involves some filtering to do with `string` and `datetime`, therefore I need to set up the `dataframe` with proper `datetime` object.\r\n\r\n#### Error Log\r\n\r\n    TypeError                                 Traceback (most recent call last)\r\n    Cell In[52], line 1\r\n    ----> 1 dd.to_datetime(df[\"Date\"], format=\"%d-%b-%Y %H:%M\").head(2)\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:1268, in _Frame.head(self, n, npartitions, compute)\r\n       1266 # No need to warn if we're already looking at all partitions\r\n       1267 safe = npartitions != self.npartitions\r\n    -> 1268 return self._head(n=n, npartitions=npartitions, compute=compute, safe=safe)\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:1302, in _Frame._head(self, n, npartitions, compute, safe)\r\n       1297 result = new_dd_object(\r\n       1298     graph, name, self._meta, [self.divisions[0], self.divisions[npartitions]]\r\n       1299 )\r\n       1301 if compute:\r\n    -> 1302     result = result.compute()\r\n       1303 return result\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/base.py:314, in DaskMethodsMixin.compute(self, **kwargs)\r\n        290 def compute(self, **kwargs):\r\n        291     \"\"\"Compute this dask collection\r\n        292 \r\n        293     This turns a lazy Dask collection into its in-memory equivalent.\r\n       (...)\r\n        312     dask.base.compute\r\n        313     \"\"\"\r\n    --> 314     (result,) = compute(self, traverse=False, **kwargs)\r\n        315     return result\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/base.py:599, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n        596     keys.append(x.__dask_keys__())\r\n        597     postcomputes.append(x.__dask_postcompute__())\r\n    --> 599 results = schedule(dsk, keys, **kwargs)\r\n        600 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/threaded.py:89, in get(dsk, keys, cache, num_workers, pool, **kwargs)\r\n         86     elif isinstance(pool, multiprocessing.pool.Pool):\r\n         87         pool = MultiprocessingPoolExecutor(pool)\r\n    ---> 89 results = get_async(\r\n         90     pool.submit,\r\n         91     pool._max_workers,\r\n         92     dsk,\r\n         93     keys,\r\n         94     cache=cache,\r\n         95     get_id=_thread_get_id,\r\n         96     pack_exception=pack_exception,\r\n         97     **kwargs,\r\n         98 )\r\n        100 # Cleanup pools associated to dead threads\r\n        101 with pools_lock:\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:511, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\r\n        509         _execute_task(task, data)  # Re-execute locally\r\n        510     else:\r\n    --> 511         raise_exception(exc, tb)\r\n        512 res, worker_id = loads(res_info)\r\n        513 state[\"cache\"][key] = res\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:319, in reraise(exc, tb)\r\n        317 if exc.__traceback__ is not tb:\r\n        318     raise exc.with_traceback(tb)\r\n    --> 319 raise exc\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/local.py:224, in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n        222 try:\r\n        223     task, data = loads(task_info)\r\n    --> 224     result = _execute_task(task, data)\r\n        225     id = get_id()\r\n        226     result = dumps((result, id))\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)\r\n        115     func, args = arg[0], arg[1:]\r\n        116     # Note: Don't assign the subtask results to a variable. numpy detects\r\n        117     # temporaries by their reference count and can execute certain\r\n        118     # operations in-place.\r\n    --> 119     return func(*(_execute_task(a, cache) for a in args))\r\n        120 elif not ishashable(arg):\r\n        121     return arg\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/optimization.py:990, in SubgraphCallable.__call__(self, *args)\r\n        988 if not len(args) == len(self.inkeys):\r\n        989     raise ValueError(\"Expected %d args, got %d\" % (len(self.inkeys), len(args)))\r\n    --> 990 return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:149, in get(dsk, out, cache)\r\n        147 for key in toposort(dsk):\r\n        148     task = dsk[key]\r\n    --> 149     result = _execute_task(task, cache)\r\n        150     cache[key] = result\r\n        151 result = _execute_task(out, cache)\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/core.py:119, in _execute_task(arg, cache, dsk)\r\n        115     func, args = arg[0], arg[1:]\r\n        116     # Note: Don't assign the subtask results to a variable. numpy detects\r\n        117     # temporaries by their reference count and can execute certain\r\n        118     # operations in-place.\r\n    --> 119     return func(*(_execute_task(a, cache) for a in args))\r\n        120 elif not ishashable(arg):\r\n        121     return arg\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/utils.py:72, in apply(func, args, kwargs)\r\n         41 \"\"\"Apply a function given its positional and keyword arguments.\r\n         42 \r\n         43 Equivalent to ``func(*args, **kwargs)``\r\n       (...)\r\n         69 >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\r\n         70 \"\"\"\r\n         71 if kwargs:\r\n    ---> 72     return func(*args, **kwargs)\r\n         73 else:\r\n         74     return func(*args)\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/dask/dataframe/core.py:6821, in apply_and_enforce(*args, **kwargs)\r\n       6819 func = kwargs.pop(\"_func\")\r\n       6820 meta = kwargs.pop(\"_meta\")\r\n    -> 6821 df = func(*args, **kwargs)\r\n       6822 if is_dataframe_like(df) or is_series_like(df) or is_index_like(df):\r\n       6823     if not len(df):\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1100, in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\r\n       1098         result = _convert_and_box_cache(argc, cache_array)\r\n       1099     else:\r\n    -> 1100         result = convert_listlike(argc, format)\r\n       1101 else:\r\n       1102     result = convert_listlike(np.array([arg]), format)[0]\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:413, in _convert_listlike_datetimes(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\r\n        410         return idx\r\n        411     raise\r\n    --> 413 arg = ensure_object(arg)\r\n        414 require_iso8601 = False\r\n        416 if infer_datetime_format and format is None:\r\n    \r\n    File pandas/_libs/algos_common_helper.pxi:33, in pandas._libs.algos.ensure_object()\r\n    \r\n    File ~/Live-usb-storage/projects/python/alpha/lib/python3.10/site-packages/cudf/core/frame.py:451, in Frame.__array__(self, dtype)\r\n        450 def __array__(self, dtype=None):\r\n    --> 451     raise TypeError(\r\n        452         \"Implicit conversion to a host NumPy array via __array__ is not \"\r\n        453         \"allowed, To explicitly construct a GPU matrix, consider using \"\r\n        454         \".to_cupy()\\nTo explicitly construct a host matrix, consider \"\r\n        455         \"using .to_numpy().\"\r\n        456     )\r\n    \r\n    TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\r\n    To explicitly construct a host matrix, consider using .to_numpy().\r\n\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/13040/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/13040/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}