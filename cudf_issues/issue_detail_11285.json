{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/11285",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/11285/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11285/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/11285/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/11285",
    "id": 1307833357,
    "node_id": "I_kwDOBWUGps5N8_QN",
    "number": 11285,
    "title": "[FEA] Potential missing performance in `partitioning.partition` (compared to `hash.hash_partition`)",
    "user": {
        "login": "wence-",
        "id": 1126981,
        "node_id": "MDQ6VXNlcjExMjY5ODE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1126981?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/wence-",
        "html_url": "https://github.com/wence-",
        "followers_url": "https://api.github.com/users/wence-/followers",
        "following_url": "https://api.github.com/users/wence-/following{/other_user}",
        "gists_url": "https://api.github.com/users/wence-/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/wence-/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/wence-/subscriptions",
        "organizations_url": "https://api.github.com/users/wence-/orgs",
        "repos_url": "https://api.github.com/users/wence-/repos",
        "events_url": "https://api.github.com/users/wence-/events{/privacy}",
        "received_events_url": "https://api.github.com/users/wence-/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        },
        {
            "id": 1322252617,
            "node_id": "MDU6TGFiZWwxMzIyMjUyNjE3",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/Performance",
            "name": "Performance",
            "color": "C2E0C6",
            "default": false,
            "description": "Performance related issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "bdice",
        "id": 3943761,
        "node_id": "MDQ6VXNlcjM5NDM3NjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3943761?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bdice",
        "html_url": "https://github.com/bdice",
        "followers_url": "https://api.github.com/users/bdice/followers",
        "following_url": "https://api.github.com/users/bdice/following{/other_user}",
        "gists_url": "https://api.github.com/users/bdice/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/bdice/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/bdice/subscriptions",
        "organizations_url": "https://api.github.com/users/bdice/orgs",
        "repos_url": "https://api.github.com/users/bdice/repos",
        "events_url": "https://api.github.com/users/bdice/events{/privacy}",
        "received_events_url": "https://api.github.com/users/bdice/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "bdice",
            "id": 3943761,
            "node_id": "MDQ6VXNlcjM5NDM3NjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3943761?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bdice",
            "html_url": "https://github.com/bdice",
            "followers_url": "https://api.github.com/users/bdice/followers",
            "following_url": "https://api.github.com/users/bdice/following{/other_user}",
            "gists_url": "https://api.github.com/users/bdice/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bdice/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bdice/subscriptions",
            "organizations_url": "https://api.github.com/users/bdice/orgs",
            "repos_url": "https://api.github.com/users/bdice/repos",
            "events_url": "https://api.github.com/users/bdice/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bdice/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 2,
    "created_at": "2022-07-18T11:46:41Z",
    "updated_at": "2023-04-02T22:40:23Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "A common pattern in dask is to shuffle distributed data around by some hash-based index. For example, this comes up in merging dataframes. Since the determination of index buckets is typically carried out independently from the splitting of the dataframe, this turns into calls to `libcudf.partitioning.partition`. The other option for this particular case would be to call `libcudf.hash.hash_partition`. The latter appears  to be signficantly (~5x) faster for large dataframes (code attached below, which partitions a dataframe with row columns and 100_000_000 rows on the first column into a configurable number of partitions, for the results below I used 10). Typical numbers of partitions for this use case are likely O(10-1000). Although this performance difference is not the order one cost in a distributed shuffle, flipping the switch from partition by index to partition by hash in dask-cuda provides a 10% speedup in some benchmarks (see rapidsai/dask-cuda#952).\r\n\r\n```\r\n$ python scatter-test.py\r\npartition-by-indices: 52ms\r\npartition-by-hash: 9.5ms\r\n```\r\n\r\nTo help the timings for the `partition-by-indices` case, I only compute the indices to partition on once. Profiling with nsight shows this takes ~2.7ms. The `partition` call takes 52 ms (of which `scatter` takes 22ms), in contrast `hash-and-scatter` in one go via `hash_partition` takes 9.5ms. Since `partition` by indices needs to read an extra column (the indices), I might expect things to be a bit slower, but this large difference was a bit surprising.\r\n\r\nThere's a note in the partitioning code that it might make sense to avoid atomics:\r\n\r\nhttps://github.com/rapidsai/cudf/blob/ec0b32bf73fc725982f62b0932782718d3886125/cpp/src/partitioning/partitioning.cu#L631\r\n\r\nAside: the pathological case of `npartitions == 1` is a factor of 2x slower for the partition-by-indices case, and 10x slower for partition-by-hash (probably worthwhile dispatching into a fast-path copy for that).\r\n\r\n```python\r\nimport rmm\r\nimport cudf\r\nimport cudf._lib as libcudf\r\nimport cupy\r\nimport time\r\n\r\n\r\ndef build_dataframe(nrows):\r\n    return cudf.DataFrame({\"key\": cupy.arange(nrows),\r\n                           \"value\": cupy.arange(nrows)})\r\n\r\n\r\ndef partition_by_indices(df, indices, npartitions):\r\n    cols, offsets = libcudf.partitioning.partition(\r\n        list(df._columns),\r\n        indices,\r\n        npartitions\r\n    )\r\n    return cols, offsets\r\n\r\n\r\ndef partition_by_hash(df, key, npartitions):\r\n    cols, offsets = libcudf.hash.hash_partition(\r\n        list(df._columns),\r\n        [df._column_names.index(key)],\r\n        npartitions\r\n    )\r\n    return cols, offsets\r\n\r\n\r\ndef run(*, nrows=10**8, with_pool=True, npartitions=10):\r\n    rmm.reinitialize(pool_allocator=with_pool)\r\n    df = build_dataframe(nrows)\r\n    key = \"key\"\r\n    indices = (libcudf.hash.hash([df[key]._column], \"murmur3\") % npartitions)\r\n    start = time.time()\r\n    for _ in range(100):\r\n        _ = partition_by_indices(df, indices, npartitions)\r\n    end = time.time()\r\n    print(f\"partition-by-indices: {(end - start)*10:.3g}ms\")\r\n    start = time.time()\r\n    for _ in range(100):\r\n        _ = partition_by_hash(df, key, npartitions)\r\n    end = time.time()\r\n    print(f\"partition-by-hash: {(end - start)*10:.3g}ms\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run(nrows=10**8, with_pool=True, npartitions=10)\r\n```\r\n\r\ncc: @bdice, @shwina \r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/11285/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/11285/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}