{
    "url": "https://api.github.com/repos/rapidsai/cudf/issues/13828",
    "repository_url": "https://api.github.com/repos/rapidsai/cudf",
    "labels_url": "https://api.github.com/repos/rapidsai/cudf/issues/13828/labels{/name}",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13828/comments",
    "events_url": "https://api.github.com/repos/rapidsai/cudf/issues/13828/events",
    "html_url": "https://github.com/rapidsai/cudf/issues/13828",
    "id": 1840105109,
    "node_id": "I_kwDOBWUGps5trcaV",
    "number": 13828,
    "title": "[FEA] Increase reader throughput by pipelining IO and compute",
    "user": {
        "login": "GregoryKimball",
        "id": 12725111,
        "node_id": "MDQ6VXNlcjEyNzI1MTEx",
        "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/GregoryKimball",
        "html_url": "https://github.com/GregoryKimball",
        "followers_url": "https://api.github.com/users/GregoryKimball/followers",
        "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
        "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
        "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
        "repos_url": "https://api.github.com/users/GregoryKimball/repos",
        "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
        "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 599626561,
            "node_id": "MDU6TGFiZWw1OTk2MjY1NjE=",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 1013987352,
            "node_id": "MDU6TGFiZWwxMDEzOTg3MzUy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/0%20-%20Backlog",
            "name": "0 - Backlog",
            "color": "d4c5f9",
            "default": false,
            "description": "In queue waiting for assignment"
        },
        {
            "id": 1139740666,
            "node_id": "MDU6TGFiZWwxMTM5NzQwNjY2",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/libcudf",
            "name": "libcudf",
            "color": "c5def5",
            "default": false,
            "description": "Affects libcudf (C++/CUDA) code."
        },
        {
            "id": 1185244142,
            "node_id": "MDU6TGFiZWwxMTg1MjQ0MTQy",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/cuIO",
            "name": "cuIO",
            "color": "fef2c0",
            "default": false,
            "description": "cuIO issue"
        },
        {
            "id": 1405146975,
            "node_id": "MDU6TGFiZWwxNDA1MTQ2OTc1",
            "url": "https://api.github.com/repos/rapidsai/cudf/labels/Spark",
            "name": "Spark",
            "color": "7400ff",
            "default": false,
            "description": "Functionality that helps Spark RAPIDS"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": {
        "url": "https://api.github.com/repos/rapidsai/cudf/milestones/22",
        "html_url": "https://github.com/rapidsai/cudf/milestone/22",
        "labels_url": "https://api.github.com/repos/rapidsai/cudf/milestones/22/labels",
        "id": 8672393,
        "node_id": "MI_kwDOBWUGps4AhFSJ",
        "number": 22,
        "title": "Parquet continuous improvement",
        "description": "",
        "creator": {
            "login": "GregoryKimball",
            "id": 12725111,
            "node_id": "MDQ6VXNlcjEyNzI1MTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12725111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GregoryKimball",
            "html_url": "https://github.com/GregoryKimball",
            "followers_url": "https://api.github.com/users/GregoryKimball/followers",
            "following_url": "https://api.github.com/users/GregoryKimball/following{/other_user}",
            "gists_url": "https://api.github.com/users/GregoryKimball/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GregoryKimball/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GregoryKimball/subscriptions",
            "organizations_url": "https://api.github.com/users/GregoryKimball/orgs",
            "repos_url": "https://api.github.com/users/GregoryKimball/repos",
            "events_url": "https://api.github.com/users/GregoryKimball/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GregoryKimball/received_events",
            "type": "User",
            "site_admin": false
        },
        "open_issues": 38,
        "closed_issues": 37,
        "state": "open",
        "created_at": "2022-11-19T18:08:31Z",
        "updated_at": "2024-06-06T18:40:39Z",
        "due_on": null,
        "closed_at": null
    },
    "comments": 0,
    "created_at": "2023-08-07T19:43:53Z",
    "updated_at": "2024-02-23T17:56:36Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "-- this is a draft, please do not comment yet -- \r\n\r\nThe end-to-end throughput of a file reader is limited by the sequential read speed of the underlying data source. We can use \"pipelining\" to overlap processing data on the device with reading data from the data source. Pipelining works by processing the data in batches, so that the previous chunk can be processed as the next chunk is reading. Pipelined readers show higher end-to-end throughput if the overlap between reading and processing is greater than the overhead from processing smaller batches. \r\n\r\nIn cuIO, `multibyte_split` used a pipelined design that reads text data in ~33 MB chunks (2^25 bytes) into a pinned host buffer, copies the data to device, and then generates offsets data. Here's a profile reading \"Common Crawl\" document data with `cudf.read_text` from a 410 MB file:\r\n![image](https://github.com/rapidsai/cudf/assets/12725111/0c2c8b2d-f23b-4688-8ef7-fe3286da4a72)\r\n\r\nNote how the `get_next_chunk` function includes the OS `read` and `Memcopy HtoD`, and how the `Memcpy HtoD` overlaps with the next OS `read`. Stream-ordered kernel launches also overlap with the next OS `read`. For each 10 ms OS `read`, there is 1.5 ms of overlapping copy/compute work and 0.2 ms of overhead between each OS `read`. \r\n\r\nWe can applying pipelining to the Parquet reader as well. Parquet reading includes several major steps: raw IO, header decoding, decompression, and data decoding. The runtime of each step varies based on the properties of the data in the file, including the data types, encoding efficiency, and compression efficiency. Furthermore Parquet files have [internal row group and page structure](https://github.com/apache/parquet-format#file-format) that restricts how the file can be split. Here is an example profile reading the same \"Common Crawl\" data as above, but from a 240 MB Snappy-compressed Parquet file:\r\n![image](https://github.com/rapidsai/cudf/assets/12725111/a1f6700a-ab9f-42bd-bd70-eb285a042770)\r\n\r\nNote how 90 ms is spent in OS read on the file and ~20 ms is spent processing, with decompression taking most (11.5 ms) of the processing time. Also note the GPU utilization data during the `read_parquet` function, with zero GPU utilization during the copy followed by good good SM utilization and moderate warp utilization during the compute.\r\n\r\nWe've completed prototyping work in #12358, experimenting with several approaches for pipelining the Parquet reader. Here are some performance analysis ideas for the next time we tackle this feature:\r\n* Curate a library of real world (not generated) data files and use that to evaluate the performance of pipelining approach\r\n* Analyze the copying, decompression, decoding times in the curated library and track which files show the biggest benefit from pipelining\r\n* Consider setting a floor (such as 200 MB of compressed data) before pipelining kicks in, to make sure we aren't accruing too much overhead\r\n* Evaluate network-attached storage in addition to local NVMe data sources\r\n\r\nAs far as pipelining approaches, here are some areas to consider:\r\n| Stream usage | Chunking pattern | Notes | \r\n|---|---|---|\r\n| entire read per stream | row group | tbd |\r\n| decompression stream and decoding stream | row group | tbd |\r\n\r\n-- this is a draft, please do not comment yet -- ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/rapidsai/cudf/issues/13828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/rapidsai/cudf/issues/13828/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}