{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8652",
    "id": 1786326216,
    "node_id": "I_kwDOD7z77c5qeSzI",
    "number": 8652,
    "title": "[BUG] array_item test failures on Spark 3.3.x",
    "user": {
        "login": "jlowe",
        "id": 1360766,
        "node_id": "MDQ6VXNlcjEzNjA3NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jlowe",
        "html_url": "https://github.com/jlowe",
        "followers_url": "https://api.github.com/users/jlowe/followers",
        "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
        "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
        "organizations_url": "https://api.github.com/users/jlowe/orgs",
        "repos_url": "https://api.github.com/users/jlowe/repos",
        "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jlowe/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-07-03T14:47:01Z",
    "updated_at": "2023-07-18T20:06:37Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "The following tests are failing on Spark 3.3.x because it expects an exception to be thrown but it is not thrown by the CPU Spark session.  Saw these failures on Spark 3.3.0 and Spark 3.3.2.\r\n```\r\n==================================================================== FAILURES =====================================================================\r\n___________________________________________________ test_array_item_with_strict_index[-2-True] ____________________________________________________\r\n[gw4] linux -- Python 3.8.10 /home/jlowe/miniconda3/envs/cudf_dev/bin/python\r\n\r\nstrict_index_enabled = True, index = -2\r\n\r\n    @pytest.mark.skipif(not is_spark_33X() or is_databricks_runtime(), reason=\"'strictIndexOperator' is introduced from Spark 3.3.0 and removed in Spark 3.4.0 and DB11.3\")\r\n    @pytest.mark.parametrize('strict_index_enabled', [True, False])\r\n    @pytest.mark.parametrize('index', [-2, 100, array_neg_index_gen, array_out_index_gen], ids=idfn)\r\n    def test_array_item_with_strict_index(strict_index_enabled, index):\r\n        message = \"SparkArrayIndexOutOfBoundsException\"\r\n        if isinstance(index, int):\r\n            test_df = lambda spark: unary_op_df(spark, ArrayGen(int_gen)).select(col('a')[index])\r\n        else:\r\n            test_df = lambda spark: two_col_df(spark, ArrayGen(int_gen), index).selectExpr('a[b]')\r\n    \r\n        test_conf = copy_and_update(ansi_enabled_conf, {'spark.sql.ansi.strictIndexOperator': strict_index_enabled})\r\n    \r\n        if strict_index_enabled:\r\n>           assert_gpu_and_cpu_error(\r\n                lambda spark: test_df(spark).collect(),\r\n                conf=test_conf,\r\n                error_message=message)\r\n\r\n../../src/main/python/array_test.py:136: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../src/main/python/asserts.py:626: in assert_gpu_and_cpu_error\r\n    assert_spark_exception(lambda: with_cpu_session(df_fun, conf), error_message)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nfunc = <function assert_gpu_and_cpu_error.<locals>.<lambda> at 0x7f91f460e790>, error_message = 'SparkArrayIndexOutOfBoundsException'\r\n\r\n    def assert_spark_exception(func, error_message):\r\n        \"\"\"\r\n        Assert that a specific Java exception is thrown\r\n        :param func: a function to be verified\r\n        :param error_message: a string such as the one produce by java.lang.Exception.toString\r\n        :return: Assertion failure if no exception matching error_message has occurred.\r\n        \"\"\"\r\n        with pytest.raises(Exception) as excinfo:\r\n>           func()\r\nE           Failed: DID NOT RAISE <class 'Exception'>\r\n\r\n../../src/main/python/asserts.py:613: Failed\r\n___________________________________________________ test_array_item_with_strict_index[100-True] ___________________________________________________\r\n[gw4] linux -- Python 3.8.10 /home/jlowe/miniconda3/envs/cudf_dev/bin/python\r\n\r\nstrict_index_enabled = True, index = 100\r\n\r\n    @pytest.mark.skipif(not is_spark_33X() or is_databricks_runtime(), reason=\"'strictIndexOperator' is introduced from Spark 3.3.0 and removed in Spark 3.4.0 and DB11.3\")\r\n    @pytest.mark.parametrize('strict_index_enabled', [True, False])\r\n    @pytest.mark.parametrize('index', [-2, 100, array_neg_index_gen, array_out_index_gen], ids=idfn)\r\n    def test_array_item_with_strict_index(strict_index_enabled, index):\r\n        message = \"SparkArrayIndexOutOfBoundsException\"\r\n        if isinstance(index, int):\r\n            test_df = lambda spark: unary_op_df(spark, ArrayGen(int_gen)).select(col('a')[index])\r\n        else:\r\n            test_df = lambda spark: two_col_df(spark, ArrayGen(int_gen), index).selectExpr('a[b]')\r\n    \r\n        test_conf = copy_and_update(ansi_enabled_conf, {'spark.sql.ansi.strictIndexOperator': strict_index_enabled})\r\n    \r\n        if strict_index_enabled:\r\n>           assert_gpu_and_cpu_error(\r\n                lambda spark: test_df(spark).collect(),\r\n                conf=test_conf,\r\n                error_message=message)\r\n\r\n../../src/main/python/array_test.py:136: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../src/main/python/asserts.py:626: in assert_gpu_and_cpu_error\r\n    assert_spark_exception(lambda: with_cpu_session(df_fun, conf), error_message)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nfunc = <function assert_gpu_and_cpu_error.<locals>.<lambda> at 0x7f91cdb680d0>, error_message = 'SparkArrayIndexOutOfBoundsException'\r\n\r\n    def assert_spark_exception(func, error_message):\r\n        \"\"\"\r\n        Assert that a specific Java exception is thrown\r\n        :param func: a function to be verified\r\n        :param error_message: a string such as the one produce by java.lang.Exception.toString\r\n        :return: Assertion failure if no exception matching error_message has occurred.\r\n        \"\"\"\r\n        with pytest.raises(Exception) as excinfo:\r\n>           func()\r\nE           Failed: DID NOT RAISE <class 'Exception'>\r\n\r\n../../src/main/python/asserts.py:613: Failed\r\n___________________________________________________ test_array_item_ansi_fail_invalid_index[-2] ___________________________________________________\r\n[gw4] linux -- Python 3.8.10 /home/jlowe/miniconda3/envs/cudf_dev/bin/python\r\n\r\nindex = -2\r\n\r\n    @pytest.mark.parametrize('index', [-2, 100, array_neg_index_gen, array_out_index_gen], ids=idfn)\r\n    def test_array_item_ansi_fail_invalid_index(index):\r\n        message = \"SparkArrayIndexOutOfBoundsException\" if (is_databricks104_or_later() or is_spark_330_or_later()) else \"java.lang.ArrayIndexOutOfBoundsException\"\r\n        if isinstance(index, int):\r\n            test_func = lambda spark: unary_op_df(spark, ArrayGen(int_gen)).select(col('a')[index]).collect()\r\n        else:\r\n            test_func = lambda spark: two_col_df(spark, ArrayGen(int_gen), index).selectExpr('a[b]').collect()\r\n>       assert_gpu_and_cpu_error(\r\n            test_func,\r\n            conf=ansi_enabled_conf,\r\n            error_message=message)\r\n\r\n../../src/main/python/array_test.py:153: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../src/main/python/asserts.py:626: in assert_gpu_and_cpu_error\r\n    assert_spark_exception(lambda: with_cpu_session(df_fun, conf), error_message)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nfunc = <function assert_gpu_and_cpu_error.<locals>.<lambda> at 0x7f91cf36dee0>, error_message = 'SparkArrayIndexOutOfBoundsException'\r\n\r\n    def assert_spark_exception(func, error_message):\r\n        \"\"\"\r\n        Assert that a specific Java exception is thrown\r\n        :param func: a function to be verified\r\n        :param error_message: a string such as the one produce by java.lang.Exception.toString\r\n        :return: Assertion failure if no exception matching error_message has occurred.\r\n        \"\"\"\r\n        with pytest.raises(Exception) as excinfo:\r\n>           func()\r\nE           Failed: DID NOT RAISE <class 'Exception'>\r\n\r\n../../src/main/python/asserts.py:613: Failed\r\n__________________________________________________ test_array_item_ansi_fail_invalid_index[100] ___________________________________________________\r\n[gw4] linux -- Python 3.8.10 /home/jlowe/miniconda3/envs/cudf_dev/bin/python\r\n\r\nindex = 100\r\n\r\n    @pytest.mark.parametrize('index', [-2, 100, array_neg_index_gen, array_out_index_gen], ids=idfn)\r\n    def test_array_item_ansi_fail_invalid_index(index):\r\n        message = \"SparkArrayIndexOutOfBoundsException\" if (is_databricks104_or_later() or is_spark_330_or_later()) else \"java.lang.ArrayIndexOutOfBoundsException\"\r\n        if isinstance(index, int):\r\n            test_func = lambda spark: unary_op_df(spark, ArrayGen(int_gen)).select(col('a')[index]).collect()\r\n        else:\r\n            test_func = lambda spark: two_col_df(spark, ArrayGen(int_gen), index).selectExpr('a[b]').collect()\r\n>       assert_gpu_and_cpu_error(\r\n            test_func,\r\n            conf=ansi_enabled_conf,\r\n            error_message=message)\r\n\r\n../../src/main/python/array_test.py:153: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../src/main/python/asserts.py:626: in assert_gpu_and_cpu_error\r\n    assert_spark_exception(lambda: with_cpu_session(df_fun, conf), error_message)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nfunc = <function assert_gpu_and_cpu_error.<locals>.<lambda> at 0x7f91cdb65820>, error_message = 'SparkArrayIndexOutOfBoundsException'\r\n\r\n    def assert_spark_exception(func, error_message):\r\n        \"\"\"\r\n        Assert that a specific Java exception is thrown\r\n        :param func: a function to be verified\r\n        :param error_message: a string such as the one produce by java.lang.Exception.toString\r\n        :return: Assertion failure if no exception matching error_message has occurred.\r\n        \"\"\"\r\n        with pytest.raises(Exception) as excinfo:\r\n>           func()\r\nE           Failed: DID NOT RAISE <class 'Exception'>\r\n\r\n../../src/main/python/asserts.py:613: Failed\r\n---- generated xml file: /home/jlowe/src/spark-rapids/integration_tests/target/run_dir-20230703093946-9jfJ/TEST-pytest-1688395186391743284.xml ----\r\n============================================================= short test summary info =============================================================\r\nFAILED ../../src/main/python/array_test.py::test_array_item_with_strict_index[-2-True][INJECT_OOM] - Failed: DID NOT RAISE <class 'Exception'>\r\nFAILED ../../src/main/python/array_test.py::test_array_item_with_strict_index[100-True] - Failed: DID NOT RAISE <class 'Exception'>\r\nFAILED ../../src/main/python/array_test.py::test_array_item_ansi_fail_invalid_index[-2] - Failed: DID NOT RAISE <class 'Exception'>\r\nFAILED ../../src/main/python/array_test.py::test_array_item_ansi_fail_invalid_index[100] - Failed: DID NOT RAISE <class 'Exception'>\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8652/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}