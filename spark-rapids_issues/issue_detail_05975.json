{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5975",
    "id": 1299386637,
    "node_id": "I_kwDOD7z77c5NcxEN",
    "number": 5975,
    "title": "[BUG] significant slow down with ParquetCachedBatchSerializer and pyspark CrossValidator",
    "user": {
        "login": "eordentlich",
        "id": 36281329,
        "node_id": "MDQ6VXNlcjM2MjgxMzI5",
        "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/eordentlich",
        "html_url": "https://github.com/eordentlich",
        "followers_url": "https://api.github.com/users/eordentlich/followers",
        "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
        "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
        "organizations_url": "https://api.github.com/users/eordentlich/orgs",
        "repos_url": "https://api.github.com/users/eordentlich/repos",
        "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
        "received_events_url": "https://api.github.com/users/eordentlich/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2022-07-08T19:41:59Z",
    "updated_at": "2022-07-12T20:20:49Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nFirst observed when attempting to run pyspark's CrossValidator + VectorAssembler + pyspark version of XGBoost under review in this PR: https://github.com/dmlc/xgboost/pull/8020.    Parts of this should fall back to cpu due to the use of VectorUDT injected by VectorAssembler.   Running time of certain steps however jumps from a few minutes to over an hour when ParquetCachedBatchSerializer is enabled vs disabled, with spark-rapids plugin enabled in both cases.     Attempted to reproduce in a more self-contained manner per below code snippet that incorporates some of the relevant logic from CrossValidator and XGboost.\r\n\r\n**Steps/Code to reproduce bug**\r\n```scala\r\nimport org.apache.spark.ml.feature.VectorAssembler\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nval df = spark.range(0, 10000000).toDF(\"col_1\")\r\nval df2 = df.withColumn(\"rand1\",rand()).withColumn(\"rand2\",rand()).withColumn(\"rand3\",rand())\r\nval va = new VectorAssembler().setInputCols(Array(\"rand1\",\"rand2\",\"rand3\")).setOutputCol(\"vector\")\r\nval df3 = va.transform(df2).withColumn(\"filter\",rand()).filter($\"filter\" < 0.5)\r\ndf3.cache()\r\nval df4 = df3.repartition(2)\r\ndf4.count\r\n```\r\nIn my environment, this bit of code takes a few seconds to run in `spark-shell` with ParquetCachedBatchSerializer *disabled* but almost 2 min when *enabled*.\r\n\r\nAnother issue with this example is that if the line `val df3 = ...` is replaced with\r\n`val df3 = df2.withColumn(\"filter\",rand()).filter($\"filter\" < 0.5)`  (i.e. no VectorUDT column added), an Array index out of bounds exception is encountered with ParquetCachedBatchSerializer enabled, while no error with it disabled.\r\n\r\nA pyspark version of the above example shows similar behavior.\r\n\r\n**Expected behavior**\r\nMuch smaller performance penalty with ParquetCachedBatchSerializer enabled in this example, which should resolve the main issue encountered with pyspark CrossValidator.\r\n\r\n**Environment details (please complete the following information)**\r\n - Environment location: Standalone, local server, single gpu\r\n - Spark configuration settings related to the issue:\r\n ```bash\r\n$SPARK_HOME/bin/pyspark --master ${SPARK_URL} --deploy-mode client --driver-memory 10G --executor-memory 60G --num-executors 1 --executor-cores 12 --conf spark.cores.max=96 --conf spark.task.cpus=1 --conf spark.locality.wait=0 --conf spark.yarn.maxAppAttempts=1 --conf spark.sql.files.maxPartitionBytes=1024m --conf spark.task.resource.gpu.amount=0.08 --conf spark.executor.resource.gpu.amount=1 --conf spark.executor.resource.gpu.discoveryScript=./getGpusResources.sh --conf spark.executorEnv.CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps \\\r\n                             --conf spark.plugins=com.nvidia.spark.SQLPlugin \\\r\n                             --conf spark.rapids.sql.enabled=true \\\r\n                             --conf spark.sql.files.maxPartitionBytes=1G \\\r\n                             --conf spark.sql.shuffle.partitions=192 \\\r\n                             --conf spark.rapids.sql.explain=ALL \\\r\n                             --conf spark.rapids.sql.incompatibleOps.enabled=true \\\r\n                             --conf spark.rapids.sql.batchSizeBytes=512M \\\r\n                             --conf spark.rapids.sql.reader.batchSizeBytes=768M \\\r\n                             --conf spark.rapids.sql.rowBasedUDF.enabled=true \\\r\n                             --conf spark.rapids.sql.variableFloatAgg.enabled=true \\\r\n                             --conf spark.rapids.sql.hasNans=false \\\r\n                             --conf spark.rapids.memory.gpu.minAllocFraction=0.0001 \\\r\n                             --conf spark.rapids.memory.gpu.maxAllocFraction=0.5 \\\r\n                             --conf spark.rapids.memory.gpu.allocFraction=0.5 \\\r\n                             --conf spark.sql.adaptive.enabled=false \\\r\n                             --conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer \\\r\n                             --conf spark.executorEnv.CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log --files $SPARK_HOME/examples/src/main/scripts/getGpusResources.sh --jars ${SPARK_RAPIDS_PLUGIN_JAR}\r\n```\r\nI then remove `--conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer` to disable ParquetCachedBatchSerializer.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}