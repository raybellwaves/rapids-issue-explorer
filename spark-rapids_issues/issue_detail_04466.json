{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4466",
    "id": 1095529385,
    "node_id": "I_kwDOD7z77c5BTHOp",
    "number": 4466,
    "title": "[FEA] explore \"is not null\" filter optimization more",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 2586576266,
            "node_id": "MDU6TGFiZWwyNTg2NTc2MjY2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/task",
            "name": "task",
            "color": "65abf7",
            "default": false,
            "description": "Work required that improves the product but is not user facing"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2022-01-06T17:25:46Z",
    "updated_at": "2022-03-16T21:56:11Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "Spark tends to do a lot of filters right after reading in a data file where it is filtering out nulls.  This often happens because of joins where a null key will never match so Spark will push a filter up as far as it can to get rid of nulls before processing.  Because this is so prevalent @jlowe had the idea that we could try and do a short circuit and just look at the nullCounts for the columns involved to see if we need to do any type of calculation at all, because hopefully the null count is already set in the columns returned by reading a file.\r\n\r\nTo explore this I wrote a quick patch to see what the performance impact is for filters that are looking at just a single column for isNotNull https://github.com/revans2/spark-rapids/tree/is_not_null_filter_exploration.  This ends up being a non-trivial number in TPC-DS, but because of exchange reuse it is not huge, but should hopefully give us an idea of what we could do with this.\r\n\r\nWhen I ran several TPC-DS queries in local mode with this change an decimal data with a scale factor of 200 I saw a 25% performance improvement for the filters involved. But that only translated to somewhere around 0.18% to 0.045% total runtime change.  There were also cases where the new approach was slower than the old approach.  Because this is such a small amount of improvement I thought it would be best to come back to this later and explore some other more general areas that might have bigger performance impacts.\r\n\r\nThe next steps would be\r\n1. We would want to capture some traces to see why some of the time this is slower than the existing solution.\r\n2. Try to extend this to include other cases where we are checking on several columns all of which are combined with ANDs.\r\n\r\nIn theory this should never be a loss if we have already calculated the null count when reading the files, but as we look more at AST it might become less and less of an issue.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4466/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}