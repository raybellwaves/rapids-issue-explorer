{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7998",
    "id": 1652135197,
    "node_id": "I_kwDOD7z77c5ieZUd",
    "number": 7998,
    "title": "[FEA] Go back and do cache serialization differently",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-04-03T14:02:45Z",
    "updated_at": "2023-04-04T20:30:25Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen we started to look at serialization in 2020 I wanted to try and match as closely as possible to what Spark does for serialization.  But at the time we didn't want to take the time to write customer kernels for the compression formats used by Spark.  Instead we decided to go with parquet as the format because it was something that both the CPU and GPU supported well enough that we could make it work in both places without writing anything new.  I think we should go back and revisit that decision. The ParquetCachedBatchSerializer is a performance improvement in many cases when using the GPU, but is clearly a performance regression in many more cases when using the CPU. It does produce better compression, but at a cost.\r\n\r\n**Describe the solution you'd like**\r\nWe should go back again and look at what it would take to support the same compression algorithms that the CPU uses.\r\n\r\n - PassThrough (Really no compression)\r\n -  RunLengthEncoding\r\n - DictionaryEncoding\r\n - BooleanBitSet\r\n - IntDelta\r\n - LongDelta\r\n \r\nWith special handling for NullType which is really just a row count.\r\n\r\nIf we can match exactly what Spark does, then we know that the CPU encoding/decoding time will not be any slower than it is today and the GPU will likely always be a win.  If we need to make some small modifications for performance/compressions reasons afterwards we can. It might even be good to expose some of this to the user to let them turn up the compression level at the cost of some performance.  I mean doing things like compressing nested sub-columns, not just simple top level columns which Spark does.\r\n\r\nOne of the things we might want to look at changing early on is nested types. They are stored in Spark's row format. This is something that we don't really support and I don't think we want to spend the time to try and support it. We might just fall back to the CPU in the short term until we can test to see if a columnar version would be better.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7998/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}