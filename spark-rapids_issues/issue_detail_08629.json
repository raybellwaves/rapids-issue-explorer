{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8629",
    "id": 1779782525,
    "node_id": "I_kwDOD7z77c5qFVN9",
    "number": 8629,
    "title": "[BUG] Parquet decimals encoded as binary load on GPU but not CPU",
    "user": {
        "login": "jlowe",
        "id": 1360766,
        "node_id": "MDQ6VXNlcjEzNjA3NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jlowe",
        "html_url": "https://github.com/jlowe",
        "followers_url": "https://api.github.com/users/jlowe/followers",
        "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
        "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
        "organizations_url": "https://api.github.com/users/jlowe/orgs",
        "repos_url": "https://api.github.com/users/jlowe/repos",
        "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jlowe/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2061735878,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc4",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/documentation",
            "name": "documentation",
            "color": "0075ca",
            "default": true,
            "description": "Improvements or additions to documentation"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-06-28T21:11:22Z",
    "updated_at": "2023-07-05T21:52:52Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "The RAPIDS Accelerator is loading Parquet decimal data encoded as BINARY when Spark CPU throws an error for this.  For example:\r\n```\r\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///home/jlowe/src/spark-rapids/thirdparty/parquet-testing/data/byte_array_decimal.parquet. Column: [value], Expected: decimal(4,2), Found: BINARY.\r\n  at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:868)\r\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:298)\r\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)\r\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [value], physicalType: BINARY, logicalType: decimal(4,2)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\r\n  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\r\n  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:328)\r\n  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)\r\n  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)\r\n  ... 21 more\r\n```\r\n\r\nEasy way to reproduce this is to try to load the [byte_array_decimal.parquet file from parquet-testing](https://github.com/apache/parquet-testing/blob/master/data/byte_array_decimal.parquet).\r\n\r\nIf we want to replicate the Spark behavior, we could check for this case and throw a similar exception. Alternatively, we could document the difference in the compatibility documentation.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}