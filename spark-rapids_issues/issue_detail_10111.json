{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10111",
    "id": 2058728402,
    "node_id": "I_kwDOD7z77c56tbPS",
    "number": 10111,
    "title": "[FEA] figure out how to do collect_list in a window operation.",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-12-28T18:49:13Z",
    "updated_at": "2024-01-03T19:22:34Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n`collect_list` and `collect_set` are really not good from a reliability standpoint when run in the context of a window operation. Because of this I filed https://github.com/NVIDIA/spark-rapids/issues/10110 to disable them by default. But long term we need to come up with a much better solution to the problem.\r\n\r\nFundamentally `collect_list` is going to take a lot of memory to materialize the result, especially if the window size is large. I think the only long term solution here is to only allow it for windows where we know that they are small enough that we can make it work, and even then we probably need some heuristics going into the window operation itself so that we can make the input batches small enough that we don't go over memory limits when doing the window operation.\r\n\r\nWe might be able to get CUDF to make some changes too, where we have a \"chunked\" window exec where it will just calculate a subset of the rows at a time. That way we would keep around the input table and calculate smaller output batches that don't violate the memory constraints.\r\n\r\nNow that being said unbounded preceding to unbounded following, despite it being the worst case situation for actual memory usage on the output we might be able to play some games similar to what we want to do in https://github.com/NVIDIA/spark-rapids/issues/9303 where we can compute the result using a group by aggregation, and then when we go to put the result into the output table we can split it so that we don't violate the memory limits.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10111/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}