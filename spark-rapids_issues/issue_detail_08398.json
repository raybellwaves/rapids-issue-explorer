{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8398",
    "id": 1725975467,
    "node_id": "I_kwDOD7z77c5m4Eur",
    "number": 8398,
    "title": "[FEA] Explore sort based aggregations for large numbers of aggregates",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-05-25T14:47:08Z",
    "updated_at": "2023-06-27T21:46:48Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThis is a follow on issue for https://github.com/NVIDIA/spark-rapids/issues/8141. From it we learned that for 512 decimal sum aggregations we don't need to sort the data and do only a few aggregations at a time.  Instead we can split the data into very small batches and do the aggregations.  But we don't know from a performance standpoint if that is the best solution. \r\n\r\nThere are a few other possibilities that we want to test from a performance standpoint and possibly implement to improve the very cases I am concerned about.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/issues/8391 and https://github.com/NVIDIA/spark-rapids/issues/8390\r\n\r\nBut it would be good to understand in what situations is a sort worth it or not, especially for really large numbers of aggregations.\r\n\r\nWe need to be careful with this though. CUDF does not guarantee that they will not reorder the keys, even if we tell them that the data is already sorted. If the numbers look really good we would have to get more guarantees from them on this before we could ship anything.\r\n\r\nThe current code, at least prior to https://github.com/NVIDIA/spark-rapids/issues/8391 and https://github.com/NVIDIA/spark-rapids/issues/8390 will do a first pass aggregation per-batch, and save away the results. If the size of all of the intermediate results are larger than a target batch size, then we fall back to sorting the data. When that happens we will sort the data using the out of core sort iterator, partition it on key group boundaries, and then do a merge aggregation on the sorted/partitioned batch outputs. Otherwise we just do a merge aggregation on all of the data.\r\n\r\nAfter those two proposed changes we might be doing hash partitioning instead of the sort and key batching on the assumption that hash aggregation is much cheaper than sorting is.\r\n\r\nThere are a few situations and code modifications I would like to explore. Unfortunately to get all the data/metrics we want we probably need to write a full patch for the plugin instead of doing some micro benchmarks. Also there is a combinatorics explosion in all of the experiments I would like to do 6 datasets * 8 aggregations * (3 patches + the baseline) = 192 tests.  Hopefully a lot of these can be automated to run nights and over weekends, but it is still a lot.\r\n\r\n**Data to test with:**\r\n1. cardinality is high and is highly grouped (the data is almost sorted by the key, should get good combining initially, but not after a first pass and not in the final aggregation)\r\n2. cardinality is high and is randomly distributed (should get almost no combining or final)\r\n3. cardinality is low and is highly grouped\r\n4. cardinality is low and is randomly distributed\r\n5. cardinality is medium and is high grouped\r\n6. cardinality is medium and is randomly distributed\r\n\r\nBy high cardinality I mean we each key shows up 2 to 3 times in the entire dataset, for medium 200 to 300 times, and for low 20,000 to 30,000 times. But we want enough data that a single task cannot hold all of it in GPU memory. At least a few hundred GiB of data, uncompressed.  And the almost sorted should really be that. The data is essentially sorted by key, but may have a few minor permutations in it.\r\n\r\n**Aggregations to test with:**\r\nGenerally there are just a few aggregations being done but we have customers that do up to a few hundred aggregations.  Lets go an order of magnitude above that to see if we can handle it effectively. I would like to see numbers for 1, 10, 100, 200, 300, 500, 1000, and 2000 aggregations.\r\n\r\n**Code changes to explore:**\r\n\r\nA. We sort the data by batch as it comes in, no need to do an out of core sort.  We split the number of aggregations up into evenly sized groups with at most 100 aggregations per group, but it might be nice to play around with this number. We then can do the first pass one group of aggs at a time, make the output spillable and go on to the next chunk. We probably want to save the output groupby columns separately from the aggregated columns so that we only save them once and can keep the memory usage lower that it otherwise would be. We have to be sure that we told CUDF that the data is already sorted so they they don't try to build a hash table or reorder keys or anything like that. Then when we are done with all of the groups we can put the columns back together again.  For the merge pass I think we just want to insert it directly into a out of core sort iterator instead of saving it on our own and trying to see if it is big enough or not to do the sort fallback. It would be good if we had a way to tell it that the data is already sorted so it does not have to do the first sort, it just splits it and saves it away. Then the merge pass would be similar to before we do a key grouping split on the output of the sort iterator and do the merge in groups before outputting the result.\r\nB. We sort the data fully (out of core sort) as it comes in.  We can then do the first pass aggregations in groups like with code change A, but instead of putting the output into an out of core sort iterator we feed it directly into the key group batching iterator.  The merge pass would then take the output of that iterator and would act just like in code change A.\r\nC. We sort the data fully (out of core sort) and do a key grouping iterator on the input data.  We can then do the aggregations fully per batch.  We don't need to do any merge aggregations because a single pass through the data should be good enough.\r\n\r\nTo be clear we want to compare these to whatever the existing solution is.  Specifically after https://github.com/NVIDIA/spark-rapids/issues/8382 goes in.  We would want the aggregation groupings to happen before the preprocess stage so that #8382 does not make the batches super small in terms of both rows and then we make them smaller in terms of columns.\r\n\r\nCode change C is essentially what the CPU does for sort based aggregations, except that they read the data in one row at a time so they don't have to worry about batching. The batching could cause some problems for skewed data so we would not want to push it into production, but it could inform us if we needed/wanted to do a hybrid of B and C, where we would start out of C, but if we see a key group that is too large to fit in a single batch, then we tag it and do B for that batch and the following batch.\r\n\r\nThe downside of C and B is that we have to sort all of the data before we reduce the size of the data.  A is intended to address that, by doing the aggregation first, but if this is a partial aggregation and nothing is getting combined (the https://github.com/NVIDIA/spark-rapids/issues/7404 situation) then it might be good to see if we could sort all of the input data.\r\n\r\nThe thing is that the common case is that we see a single batch of input and produce a single batch of output most commonly. We also want to be sure that any final solution we come up with does not get in the way of that.\r\n\r\n**Metrics to look at:**\r\nIn addition to the total run time I would like to see things broken down by.\r\n\r\n1. How much data was shuffled, both pre and post compression.  The pre-compression numbers should not change, but by sorting the data it might have a big impact on the compressed data.\r\n2. How much data was spilled.\r\n3. The op time for each the partial aggregation and the final aggregation.\r\n4. Ideally the time spent sorting for both the the partial aggregation and the final aggregation.\r\n\r\n**Hardware and configurations:**\r\nThe hardware is not too important. I think a single GPU is good enough to test this with, but we want to limit it to 16 GiB of memory which is the most common. A T4 would be ideal because it is the most common GPU used by out customers.\r\n\r\nI would like to see a parallelism of 2, a batch size of 1 GiB (which are the out of the box defaults and good for a 16 GiB GPU).  Lets set the max partition bytes to 512 MiB, which should give us a decent amount of input data to process per task, especially because we will read all of the data.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}