{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5460",
    "id": 1232675898,
    "node_id": "I_kwDOD7z77c5JeSQ6",
    "number": 5460,
    "title": "[FEA] Column reordering for columnar write utility",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735878,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc4",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/documentation",
            "name": "documentation",
            "color": "0075ca",
            "default": true,
            "description": "Improvements or additions to documentation"
        },
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2022-05-11T13:49:59Z",
    "updated_at": "2022-05-17T21:09:51Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "When there are a large number of columns for file formats like Parquet and ORC the size of the contiguous data for each individual column can be very small. This can result in doing lots of very small random reads to the file system to read the data for the subset of columns that are needed.\r\n\r\nI would like to propose a new utility that will take various SQL queries or data frames along with an input table/directory and instead of executing them, it will get the read schema for them all and try to come up with an ordering for the columns to maximize the contiguous read size for those queries.\r\n\r\nThe utility could then save away that ordering and when writing new files to that data set it would provide a way to inject a project that would reorder the columns to match the desired order.\r\n\r\nWe could optionally also provide a way for it to reorder existing files without decoding and re-encoding them, but that is a lot more complicated to do.\r\n\r\nWe have seen, even when running in local mode with all of the data in the page cache and backed by an NVMe that this can have a huge speed up. We saw over a 30% improvement in total query time.\r\n\r\nWe also need to decide if this is something that we keep/do ourselves or if it is something that we should push back to Spark.\r\n\r\nIt might be interesting to see if the performance tool could detect that there are a large number of columns and only a small set of them are being read very slowly and then suggest this as an improvement.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}