{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4405",
    "id": 1085319800,
    "node_id": "I_kwDOD7z77c5AsKp4",
    "number": 4405,
    "title": "[FEA] ReplicateRows -  Performance comparison while calculating lower bound cutoff",
    "user": {
        "login": "nartal1",
        "id": 50492963,
        "node_id": "MDQ6VXNlcjUwNDkyOTYz",
        "avatar_url": "https://avatars.githubusercontent.com/u/50492963?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/nartal1",
        "html_url": "https://github.com/nartal1",
        "followers_url": "https://api.github.com/users/nartal1/followers",
        "following_url": "https://api.github.com/users/nartal1/following{/other_user}",
        "gists_url": "https://api.github.com/users/nartal1/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/nartal1/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/nartal1/subscriptions",
        "organizations_url": "https://api.github.com/users/nartal1/orgs",
        "repos_url": "https://api.github.com/users/nartal1/repos",
        "events_url": "https://api.github.com/users/nartal1/events{/privacy}",
        "received_events_url": "https://api.github.com/users/nartal1/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2021-12-21T00:32:28Z",
    "updated_at": "2021-12-21T21:11:12Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThis is a follow on from the discussion in the PR https://github.com/NVIDIA/spark-rapids/pull/4388#discussion_r772527786 .\r\nThis is to experiment the best approach to get the cutoff:\r\n\r\nWe need to do the performance comparison between the current approach and something where we first estimate how many batches we need. And then we calculate the actual lower bound cutoff of those batches similar to how we do it for joins.\r\n\r\n    val rowSizes = withResource(GpuColumnVector.from(inputBatch)) { table =>\r\n      val inputBitCounts = withResource(table.rowBitCount()) { bits =>\r\n        bits.castTo(DType.INT64)\r\n      }\r\n      withResource(inputBitCounts) { inputBitCounts =>\r\n          inputBitCounts.mult(vectors(generatorOffset))\r\n      }\r\n    }\r\n    val runningTotal = withResource(rowSizes) { rowSizes =>\r\n        rowSizes.prefixSum()\r\n    }\r\n    // Then get the cutoffs based off of the estimated number of rows needed.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4405/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}