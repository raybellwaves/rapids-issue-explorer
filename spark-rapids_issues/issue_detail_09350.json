{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9350",
    "id": 1919676230,
    "node_id": "I_kwDOD7z77c5ya-9G",
    "number": 9350,
    "title": "[BUG] rounding float/double in some cases for large scales produces different results.",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-09-29T17:39:00Z",
    "updated_at": "2023-11-29T21:47:13Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nIn our compatibility docs we explicitly say that round and bround can be off in many cases.  But I think we can detect and fix these cases for the most part, or at least in a lot of cases.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/branch-23.10/docs/compatibility.md#floating-point\r\n\r\nSpark when they do rounding will convert the float/double to a Decimal number with unlimited precision and then round it to the given scale and convert the answer back. This is fine and all, but it is also really slow, and can need up to 128 bytes to hold the number. In almost all cases it looks like the round becomes a noop on the CPU.  The output is exactly the same as the input. For us we end up doing a bunch of floating point math and get a result that is very close, but slightly off from that Spark CPU gets.\r\n\r\nIt appears to happen when the exponent of the floating point number - the scale we are rounding to is larger than what the fraction could hold. For floats that is about 24-bit (8 digits) and for doubles it is about 53-bits (16 digits)\r\n\r\nI don't think this would fix everything in all cases, but it should get us a lot closer, and feels like something we could actually do.\r\n\r\n**Steps/Code to reproduce bug**\r\n\r\nI applied this patch and found at least one row that was off for one floating point test that was larger than what approximate float thought was good. I ma sure there are a lot more.\r\n\r\n```diff\r\ndiff --git a/integration_tests/src/main/python/arithmetic_ops_test.py b/integration_tests/src/main/python/arithmetic_ops_test.py\r\nindex 9749208d5..5201365f3 100644\r\n--- a/integration_tests/src/main/python/arithmetic_ops_test.py\r\n+++ b/integration_tests/src/main/python/arithmetic_ops_test.py\r\n@@ -626,6 +626,8 @@ _arith_data_gens_for_round = numeric_gens + _arith_decimal_gens_no_neg_scale_38_\r\n     decimal_gen_32bit_neg_scale,\r\n     DecimalGen(precision=15, scale=-8),\r\n     DecimalGen(precision=30, scale=-5),\r\n+    DecimalGen(precision=38, scale=37),\r\n+    DecimalGen(precision=37, scale=36),\r\n     pytest.param(_decimal_gen_36_neg5, marks=pytest.mark.skipif(\r\n         is_spark_330_or_later(), reason='This case overflows in Spark 3.3.0+')),\r\n     pytest.param(_decimal_gen_38_neg10, marks=pytest.mark.skipif(\r\n@@ -635,15 +637,12 @@ _arith_data_gens_for_round = numeric_gens + _arith_decimal_gens_no_neg_scale_38_\r\n @incompat\r\n @approximate_float\r\n @pytest.mark.parametrize('data_gen', _arith_data_gens_for_round, ids=idfn)\r\n-def test_decimal_bround(data_gen):\r\n+@pytest.mark.parametrize('scale', [-1, 0, 1, 2, 10, 11, 13, 15, 16, 17], ids=idfn)\r\n+def test_decimal_bround(data_gen, scale):\r\n     assert_gpu_and_cpu_are_equal_collect(\r\n-            lambda spark: unary_op_df(spark, data_gen).selectExpr(\r\n-                'bround(a)',\r\n-                'bround(1.234, 2)',\r\n-                'bround(a, -1)',\r\n-                'bround(a, 1)',\r\n-                'bround(a, 2)',\r\n-                'bround(a, 10)'))\r\n+            lambda spark: unary_op_df(spark, data_gen, length=4096).selectExpr(\r\n+                'a',\r\n+                'bround(a, {}) as b'.format(scale)))\r\n\r\n```\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}