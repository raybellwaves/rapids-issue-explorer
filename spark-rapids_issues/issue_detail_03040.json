{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/3040",
    "id": 954128927,
    "node_id": "MDU6SXNzdWU5NTQxMjg5Mjc=",
    "number": 3040,
    "title": "[BUG] prevent going above `vm.max_map_count` OS limit with UCX",
    "user": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2096543664,
            "node_id": "MDU6TGFiZWwyMDk2NTQzNjY0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/shuffle",
            "name": "shuffle",
            "color": "67fc73",
            "default": false,
            "description": "things that impact the shuffle plugin"
        },
        {
            "id": 2223784776,
            "node_id": "MDU6TGFiZWwyMjIzNzg0Nzc2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P1",
            "name": "P1",
            "color": "fbca04",
            "default": false,
            "description": "Nice to have for release"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "abellina",
            "id": 1901059,
            "node_id": "MDQ6VXNlcjE5MDEwNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abellina",
            "html_url": "https://github.com/abellina",
            "followers_url": "https://api.github.com/users/abellina/followers",
            "following_url": "https://api.github.com/users/abellina/following{/other_user}",
            "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
            "organizations_url": "https://api.github.com/users/abellina/orgs",
            "repos_url": "https://api.github.com/users/abellina/repos",
            "events_url": "https://api.github.com/users/abellina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abellina/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 2,
    "created_at": "2021-07-27T17:59:48Z",
    "updated_at": "2021-08-10T21:06:12Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "We saw an issue where q72 at 3TB with 8 GPUs was causing requests with thousands (~7K) of small blocks (sized around ~1KB), leading to us running above `max_map_count`.\r\n\r\nNormally, the `RapidsShuffleServer` queues all copies it needs to do on the CUDA stream and then waits for a single CUDA synchronize call. If there are several requests that successfully acquired a bounce buffer, they all get handled, and then they all wait for that CUDA synchronize call. The problem isn't the waiting, but mostly the fact that each waiting copy obtained a reference for a `RapidsBuffer`. If the `RapidsBuffer` is in the disk, it also means we are holding on to the `mmap`ed region as well.\r\n\r\nAs of today here are some ways users can work around this: \r\n\r\n- change `vm.max_map_count` to get more breathing room, \r\n- change the `spark.sql.shuffle.partitions` (smaller than the 200 default)\r\n\r\nBut these workarounds make the plugin harder to use, hence this issue.\r\n\r\nOne way to handle this is to coalesce the blocks written to disk (bigger files). We can then `mmap` larger regions (but keeping in mind that we don't want to `mmap` huge chunks either), additionally we can add better handling when an `mmap` failure does happen, so we can handle exceptions better, which the current code doesn't do well.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}