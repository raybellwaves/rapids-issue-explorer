{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9248",
    "id": 1901048440,
    "node_id": "I_kwDOD7z77c5xT7J4",
    "number": 9248,
    "title": "[FEA] Increase the window size of scale test query 41",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        },
        {
            "id": 5893831059,
            "node_id": "LA_kwDOD7z77c8AAAABX0y5kw",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/scale%20test",
            "name": "scale test",
            "color": "006b75",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-09-18T14:16:51Z",
    "updated_at": "2023-09-19T19:10:41Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen I wrote the original scale tests I was just pulling numbers out of the air to try and tax aspects of the plugin that I knew could be problematic.  But I didn't do the math on them to see how bad it could be. I am doing a bit more of that now and I think we need to adjust the query 41 to make it a little harder, at least from a memory standpoint.\r\n\r\nThe query is\r\n\r\n```\r\n        s\"select \" +\r\n            s\"collect_list(f_data_low_unique_1) \" +\r\n            s\"OVER (PARTITION BY f_key4_1 order by f_data_row_num_1 \" +\r\n            s\"ROWS BETWEEN ${config.complexity} PRECEDING and CURRENT ROW ) \" +\r\n            s\"as f_data_low_unique_1_list \" +\r\n            s\"from f_facts\",\r\n```\r\n\r\nThe key that we partition by is highly skewed, there are only 5 values, so we are going to end up with 5 tasks. The number of rows is not too large `scaleFactor * 10000`.\r\n\r\nSo, on average each task should get about 200,000 rows at scale factor 100. The query only touches 3 columns, an int to partition by `f_key4_1`, a long as the input `f_data_low_unique_1`, and a long as the order by column `f_data_row_num_1`. That adds up to about 20 bytes per row or 3.8 MiB of input per task. With the complexity of 300 it means that we are going to create a list that has about 300 values in it. Even with the offsets and everything that still only comes up to about 463 MiB of output per task. Even on a 16 GiB GPU we should be able to handle 35 tasks in parallel, from a memory standpoint. Ideally we would want to take this a bit more.\r\n\r\nIf I do the math and we want to tax a 16 GiB GPU (90% of the GPU's memory to hold the output here), at scale factor 100, we would need about  9000 entries per array on average.\r\n\r\n```\r\n16*1024*1024*1024 * 0.9 ~ 15,461,882,265 (target output batch size)\r\n15,461,882,265 / 200,000 ~ 77,309 (bytes per row of output)\r\n77,309 - (8+8+4+4) = 77,285 (bytes per row of the array output, we removed the input data size and the offset entry)\r\n77,285 / 8 ~ 9,661 (entries per array)\r\n9,661 / 300 ~ 32\r\n```\r\nSo instead of having a window that is just complexity preceding I would like to see a window that is somewhere around 32 * the complexity preceding and current row.  32 would get us close to hitting the maximum that a 16 GiB GPU could handle without some serious algorithm changes and 16 would put us closer to what I would expect a 16 GiB card to do without any issues, but because these are scale tests I think we want to look at 32.  Just because it might force us to deal with some algorithm changes that would be needed to not only predict the output size on `collect_list`, but also have algorithms that can chunk the input to a row window operation.  These really come down to knowing how many rows to keep around between batches on both the preceding side and following side along with knowing how much to throw away from the output batch that is not complete, or might have been output earlier.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9248/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}