{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7950",
    "id": 1643962902,
    "node_id": "I_kwDOD7z77c5h_OIW",
    "number": 7950,
    "title": "[FEA] Add a metric to see how much data we overspill because of a race",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-03-28T13:32:53Z",
    "updated_at": "2023-03-28T20:33:11Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nFirst off, I don't think this is very critical. Second the ultimate end goal of creating this metric is to be able to monitor things, not to try and make the value 0. I think we would end up being much slower if we did make it 0.\r\n\r\nThere is a race in spill that is by design. If there are two threads and one of them wants to spill because it needs more memory, but the other one has a spillable batch that it is about to use it is possible for the first thread to start spilling the data and in the middle of that the second thread ends up grabbing a reference to the original buffer. We did this by design because we thought it would be better to let the second thread \"win\" the race and just use the data instead of waiting for the data to spill and read it back in again.\r\n\r\nThe idea is to add in a metric to see how much memory we end up spilling, but is never read back after it was spilled.  This is the size of all SpillableColumnarBatches that after the data is spilled it is never asked for a batch again before it is closed. We don't care/worry about shuffle data with UCX because we know that Spark is going to keep shuffle data around that is not going to be read again.\r\n\r\nThe reason it is interesting to monitor is because when we started to put in the retry logic it has increased the probability of this type of race happening. We make a lot more data spillable and then turn around right afterwards and read in that data.  This is a very common pattern.\r\n\r\nThe theory is that this will not matter in the real world. First off spilling should be fairly rare. Second the priority on these batches that we are actively using should be high enough that other things should be spilled before these are. So it would only happen when we really truly are out of memory on the system.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7950/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}