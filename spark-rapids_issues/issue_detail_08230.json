{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8230",
    "id": 1696460843,
    "node_id": "I_kwDOD7z77c5lHfAr",
    "number": 8230,
    "title": "[FEA] take ordered and project has limitations compared to the CPU",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-05-04T17:59:22Z",
    "updated_at": "2023-05-11T21:06:15Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nTakeOrderedAndProject is translated into a GpuTopN (a little bit of a simplification).  In Spark 3.4.0 TakeOrderedAndProject started to support an offset in addition to a limit. But even without that if a user specified a large limit then we could quickly run out of memory and crash.\r\n\r\nThis is because we read an input batch. Sort it. Then slice it be the limit at most. Read the next batch. Concat it to the end of the original input. Sort it all. Then slice it to the limit at most. Repeat until all of the batches have been read in. \r\n\r\nWe could fix this by using the out of core sort as a base instead and pull sorted batches until we hit the limit, and then throw away all of the other data.  We could even make it more efficient by having the out of core sort know what the limit is and throw away data early that it knows would never be hit.\r\n\r\nThat said, I don't see this as that big of a deal. We have not run into any situations where users want a very large limit. Also reading the CPU code it is not able to spill to disk. They use guava [Ordering.leastOf](https://github.com/google/guava/blob/624c17fc201d441c1d8ac6b7382e2939c99b67ac/guava/src/com/google/common/collect/Ordering.java#L785) to do this on a row by row basis. If it is a large amount of data that is being returned. All of the data from the input iterator is collected to a List, then it is sorted and limit of them are returned.  (That is a lot like our algorithm, but less memory efficient, and on the CPU) In the other cases they they do a similarly not that space efficient algorithm to get the sorted limit.\r\n\r\nAgain because there is no spilling they are limited to heap memory in java so it cannot be that large of a limit.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8230/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}