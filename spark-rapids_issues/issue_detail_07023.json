{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7023",
    "id": 1440859001,
    "node_id": "I_kwDOD7z77c5V4cN5",
    "number": 7023,
    "title": "File readers could reuse input stream connection between filtering and copy data",
    "user": {
        "login": "tgravescs",
        "id": 4563792,
        "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/tgravescs",
        "html_url": "https://github.com/tgravescs",
        "followers_url": "https://api.github.com/users/tgravescs/followers",
        "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
        "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
        "organizations_url": "https://api.github.com/users/tgravescs/orgs",
        "repos_url": "https://api.github.com/users/tgravescs/repos",
        "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
        "received_events_url": "https://api.github.com/users/tgravescs/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2022-11-08T20:27:15Z",
    "updated_at": "2022-11-08T20:27:15Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nOur  readers could reuse the input stream connection between where we read the file footer information and get the blocks to copy, to where we do the predicate pushdown and then again when we copy the blocks over into host memory buffers rather then opening the file potentially 3 different times.\r\n\r\nThe way we can do this is use the parquet api's that take a org.apache.parquet.io.InputFile. We can have a custom InputFile that would just open a single stream and seek around in that stream.\r\n\r\nI did a small test with this using the multithread multi-file parquet reader and it was showing a slight improvement when reading data from s3.  median task time went down from 11seconds to 10 seconds and wallclock went  down 6 seconds when doing just a read of 300,000 small files. The overall task time went down by 6 minutes.\r\n\r\na prototype of this is on branch: https://github.com/tgravescs/spark-rapids/tree/reuseInputStreamFilterRead\r\n\r\nit would need more testing and cleanup and then possibly applying this to other reader types and file types.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7023/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}