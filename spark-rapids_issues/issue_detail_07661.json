{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7661",
    "id": 1570346978,
    "node_id": "I_kwDOD7z77c5dmZfi",
    "number": 7661,
    "title": "Investigate treating scalars in a column batch as dictionary columns",
    "user": {
        "login": "jlowe",
        "id": 1360766,
        "node_id": "MDQ6VXNlcjEzNjA3NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jlowe",
        "html_url": "https://github.com/jlowe",
        "followers_url": "https://api.github.com/users/jlowe/followers",
        "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
        "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
        "organizations_url": "https://api.github.com/users/jlowe/orgs",
        "repos_url": "https://api.github.com/users/jlowe/repos",
        "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jlowe/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 2586576266,
            "node_id": "MDU6TGFiZWwyNTg2NTc2MjY2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/task",
            "name": "task",
            "color": "65abf7",
            "default": false,
            "description": "Work required that improves the product but is not user facing"
        },
        {
            "id": 2710265788,
            "node_id": "MDU6TGFiZWwyNzEwMjY1Nzg4",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/cudf_dependency",
            "name": "cudf_dependency",
            "color": "7400FF",
            "default": false,
            "description": "An issue or PR with this label depends on a new feature in cudf"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2023-02-03T19:38:43Z",
    "updated_at": "2024-03-14T21:28:18Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "There are a number of cases where we end up wanting to treat a scalar as a column in a columnar batch or cudf table, and that forces us to replicate the scalar for every row in the batch/table so it can be a column.  This can be very wasteful when the scalar is of a significant size (e.g.: a long string, a complex type, etc.)\r\n\r\nOne potential way to help improve this situation would be to use dictionary columns, where we create a dictionary of one entry, the scalar, and the indices are all the same, pointing to that one entry for each row.  If we can preserve that dictionary through operations (e.g.: gather on the column results in another dictionary column rather than exploding the dictionary column out into a standard column), we could achieve some memory and memory bandwidth savings.\r\n\r\nThis could be particularly useful for batches containing the input filename or other partition-like values that are large and currently exploded out for every row.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}