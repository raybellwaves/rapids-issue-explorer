{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/6661",
    "id": 1391942955,
    "node_id": "I_kwDOD7z77c5S910r",
    "number": 6661,
    "title": "[BUG] proper policy for repeated GPU OOM event handling ",
    "user": {
        "login": "gerashegalov",
        "id": 3187938,
        "node_id": "MDQ6VXNlcjMxODc5Mzg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gerashegalov",
        "html_url": "https://github.com/gerashegalov",
        "followers_url": "https://api.github.com/users/gerashegalov/followers",
        "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
        "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
        "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
        "repos_url": "https://api.github.com/users/gerashegalov/repos",
        "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2022-09-30T07:00:41Z",
    "updated_at": "2022-10-04T20:15:55Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nWe have a logic allowing the plugin to survive GPU OOM. For instance, HashJoin iterator may split the stream side of the join into smaller batches to reduce the GPU memory footprint after hitting a GPU OOM. \r\n\r\nOn the other hand we allow the user to specify `spark.rapids.memory.gpu.oomDumpDir` to generate Java Heap Dumps. They heap dump file name has the form gpu-oom-<Executor JVM PID>.hprof . After the first OOM event, all subsequent attempts to generate heap dumps fail\r\n\r\n```\r\n22/09/30 06:15:24 WARN DeviceMemoryEventHandler: Dumping heap to /home/user/ooms/gpu-oom-14038.hprof\r\n22/09/30 06:15:24 ERROR DeviceMemoryEventHandler: Error handling allocation failure\r\njava.io.IOException: File exists\r\n```\r\n\r\n<details><summary>stack trace</summary>\r\n<pre>\r\n\tat sun.management.HotSpotDiagnostic.dumpHeap0(Native Method)\r\n\tat sun.management.HotSpotDiagnostic.dumpHeap(HotSpotDiagnostic.java:61)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)\r\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:276)\r\n\tat com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\r\n\tat com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\r\n\tat com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\r\n\tat com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\r\n\tat com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\r\n\tat com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)\r\n\tat com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)\r\n\tat javax.management.StandardMBean.invoke(StandardMBean.java:405)\r\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)\r\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)\r\n\tat com.sun.jmx.mbeanserver.MXBeanProxy$InvokeHandler.invoke(MXBeanProxy.java:150)\r\n\tat com.sun.jmx.mbeanserver.MXBeanProxy.invoke(MXBeanProxy.java:167)\r\n\tat javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:258)\r\n\tat com.sun.proxy.$Proxy25.dumpHeap(Unknown Source)\r\n\tat com.nvidia.spark.rapids.DeviceMemoryEventHandler.heapDump(DeviceMemoryEventHandler.scala:92) \\\r\n</pre>\r\n</details>\r\n\r\n\r\n**Steps/Code to reproduce bug**\r\nSee the [broadcast join notebook][1] for the repro steps\r\n\r\nSet 'spark.rapids.memory.gpu.allocSize' to `512m` and run in the local or single executor mode the broadcast hash join with 100-integer column lhs and 500000-integer column on the rhs.\r\n```Python\r\ndf = spark.read.parquet(dfgen_path)\\\r\n    .limit(100)\r\n# bigger table on the right\r\nrhs = spark.read.parquet(dfgen_path)\\\r\n    .limit(500000)\r\nbdf = broadcast(rhs)\r\nq3 = df\\\r\n        .join(bdf.withColumnRenamed('a', 'b'), col('a') == col('b'))\\\r\n        .join(bdf.withColumnRenamed('a', 'c'), col('a') == col('c'))\r\n```\r\n\r\n[1]: https://github.com/gerashegalov/spark-rapids-examples/blob/e3a30cbc025128754b4b742c4caae8c7cd1a3370/examples/SQL+DF-Examples/micro-benchmarks/notebooks/spillExperimenmts.ipynb\r\n\r\n**Expected behavior**\r\nNeed some useful options such as \r\n- retaining last N heap dumps \r\n- retain first N heap dumps\r\n\r\n**Environment details (please complete the following information)**\r\nAny\r\n\r\n**Additional context**\r\n#836 ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6661/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}