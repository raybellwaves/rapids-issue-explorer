{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10612",
    "id": 2196414083,
    "node_id": "I_kwDOD7z77c6C6p6D",
    "number": 10612,
    "title": "[FEA] Optimize Shuffle coalesce performance when handling very small sized partitioned batches ",
    "user": {
        "login": "winningsix",
        "id": 2278268,
        "node_id": "MDQ6VXNlcjIyNzgyNjg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2278268?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/winningsix",
        "html_url": "https://github.com/winningsix",
        "followers_url": "https://api.github.com/users/winningsix/followers",
        "following_url": "https://api.github.com/users/winningsix/following{/other_user}",
        "gists_url": "https://api.github.com/users/winningsix/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/winningsix/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/winningsix/subscriptions",
        "organizations_url": "https://api.github.com/users/winningsix/orgs",
        "repos_url": "https://api.github.com/users/winningsix/repos",
        "events_url": "https://api.github.com/users/winningsix/events{/privacy}",
        "received_events_url": "https://api.github.com/users/winningsix/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2024-03-20T01:20:17Z",
    "updated_at": "2024-04-04T19:03:03Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently shuffle will partition the columnar batch into partitioned batches.\r\nhttps://github.com/NVIDIA/spark-rapids/blob/65ec25d85503484874d3b157d4e2775159187436/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuHashPartitioningBase.scala#L35-L47\r\n\r\nAnd those partitioned batches will go through ```GpuColumnarBatchSerializer``` in ```GpuShuffleExchangeExec``` to serialize it.\r\nhttps://github.com/NVIDIA/spark-rapids/blob/65ec25d85503484874d3b157d4e2775159187436/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/execution/GpuShuffleExchangeExecBase.scala#L341-L344\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/branch-24.04/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuColumnarBatchSerializer.scala#L170-L174\r\n\r\nAnd the deserialization path, using shuffle hash join as an example, it will first go through the path: deserialization -> concat+load -> filter (e.g. null filtering masking or RequireSingleBatch) to meet ```CoalesceSizeGoal```.\r\nhttps://github.com/NVIDIA/spark-rapids/blob/65ec25d85503484874d3b157d4e2775159187436/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuShuffleCoalesceExec.scala#L213-L218\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/65ec25d85503484874d3b157d4e2775159187436/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuShuffledHashJoinExec.scala#L295\r\n\r\nIf we had a large number of shuffle partition number and output batch number from the preceding operator, then each partitioned batch will be very small. For example, 1GB batch size with 2000 shuffle partition number, then 1GB/2000 = 500 KB for each serialized batch. We should introduce a coalescing shuffle writer to buffering those small batches.\r\n\r\nAlso if the shuffle write total size is not big enough, saying 30MB with 8 batches while we had 2000 partition number, shuffle write side will generate 8 * 2000 total batches to go through process above. And each batch is ~ 15.36 KB. From nsys traces below, there're clear bubbles for both shuffle write and read side.\r\n\r\nThe shuffle write does have some overlap with GPU computation but since those small sized batch serialization making a long tailing time after GPU finished all his work.\r\n![Screenshot 2024-03-20 at 08 49 18](https://github.com/NVIDIA/spark-rapids/assets/2278268/d58ed42c-c910-445f-b251-775ef79e8100)\r\nAnd the below shown that serialization on each batch will all write offset as well as data but with lots of overhead (the yellow box \"serialize batch\" besides the green box \"write sliced\"). And with lots of small batches, the overhead of serialization getting significant while the data write itself is not that much. We should figure out a way to improve this performance issue. \r\n![image](https://github.com/NVIDIA/spark-rapids/assets/2278268/94f73768-e067-4655-884d-6abf2b7f1b0a)\r\n\r\nOn the other hand in deserialization, the first stream batch was blocked until we met the build goal. And lots of time is spent in read head + read batch with lots of iterations.\r\n![image](https://github.com/NVIDIA/spark-rapids/assets/2278268/437a0762-9c3e-4fac-ae13-a4b55b9eba4d)\r\n\r\n**Describe the solution you'd like**\r\nThere're two ideas in general could help this:\r\n1. Introduce a ```write side coalesce``` mechanism before flushing out those small batches. To achieve this goal, we need to maintain some buffer to threshold the batch serialization other than immediately flushing them out.\r\n2. For serializer optimization, one possible option is about leveraging Arrow's serializer to see whether we can directly serialize batch. But also exist some extra work around encryption and compression work.  https://github.com/apache/arrow/blob/e52017a72735d502c3ac3323d9d1fc61a15a6ae0/cpp/src/arrow/ipc/writer.cc#L1597\r\n3. Optimize shuffle coalesce performance by some of the CPU work onto GPU. Tracked in https://github.com/NVIDIA/spark-rapids/issues/10402.\r\n\r\n**Describe alternatives you've considered**\r\nAQE may not help too much as we still need the optimize both shuffle write and read with those small batches with the same shuffle partition number for shuffle write.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}