{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/6917",
    "id": 1423168816,
    "node_id": "I_kwDOD7z77c5U09Uw",
    "number": 6917,
    "title": "[BUG] rereading a written csv file drops rows ",
    "user": {
        "login": "eordentlich",
        "id": 36281329,
        "node_id": "MDQ6VXNlcjM2MjgxMzI5",
        "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/eordentlich",
        "html_url": "https://github.com/eordentlich",
        "followers_url": "https://api.github.com/users/eordentlich/followers",
        "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
        "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
        "organizations_url": "https://api.github.com/users/eordentlich/orgs",
        "repos_url": "https://api.github.com/users/eordentlich/repos",
        "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
        "received_events_url": "https://api.github.com/users/eordentlich/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [
        {
            "login": "firestarman",
            "id": 7280411,
            "node_id": "MDQ6VXNlcjcyODA0MTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7280411?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/firestarman",
            "html_url": "https://github.com/firestarman",
            "followers_url": "https://api.github.com/users/firestarman/followers",
            "following_url": "https://api.github.com/users/firestarman/following{/other_user}",
            "gists_url": "https://api.github.com/users/firestarman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/firestarman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/firestarman/subscriptions",
            "organizations_url": "https://api.github.com/users/firestarman/orgs",
            "repos_url": "https://api.github.com/users/firestarman/repos",
            "events_url": "https://api.github.com/users/firestarman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/firestarman/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 1,
    "created_at": "2022-10-25T22:02:10Z",
    "updated_at": "2022-11-02T21:03:40Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nReading a csv file results in a dataframe with 120000 rows.   Writing the dataframe to a new csv file and then rereading with spark-rapids results in a dataframe with fewer rows (119470 if the write is done by spark rapids, 117308 if the write is done with pyspark).   No error is thrown.\r\n\r\n**Steps/Code to reproduce bug**\r\nThis was observed with the csv file downloadable [here](https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv).\r\n\r\nNote this csv file uses quotes for the second column to allow commas in the fields, and also has escaped quotes.\r\n\r\nUse pyspark api, tested in Databricks python notebook.   Haven't checked local.\r\n\r\nDownload and save the file to `dbfs:/news_category_train.csv`.\r\n\r\nRun the following:\r\n```\r\nnews_df = spark.read.option(\"header\", True).csv(\"dbfs:/news_category_train.csv\")\r\nprint(news_df.count())\r\nnews_df.write.csv(\"dbfs:/news_category_train_rapids.csv\",header=True)\r\nnews_df_reread = spark.read.option(\"header\", True).csv(\"dbfs:/news_category_train_rapids.csv\")\r\nprint(news_df_reread.count())\r\n```\r\nThe two counts were observed to be different with the first one at 120000 and the second one at 119470.   Similar behavior if first read and write are done without spark-rapids, and the second read is via spark rapids.\r\n\r\nBaseline spark yields 120000 rows in all cases, even when reading spark-rapids written csv.\r\n\r\n**Expected behavior**\r\nNo dropping of rows when writing and rereading a Dataframe in csv mode, originally read from a csv file.   \r\n\r\n**Environment details (please complete the following information)**\r\n - Environment location: Azure Databricks notebook, spark-rapids 22.10, 2 T4 worker nodes.\r\n - Spark configuration settings related to the issue\r\n ```\r\nspark.task.resource.gpu.amount 1\r\nspark.task.cpus 8\r\nspark.serializer org.apache.spark.serializer.KryoSerializer\r\nspark.databricks.delta.preview.enabled true\r\nspark.kryoserializer.buffer.max 2000M\r\nspark.executorEnv.PYTHONPATH /databricks/jars/rapids-4-spark_2.12-22.10.0.jar:/databricks/spark/python\r\nspark.jsl.settings.pretrained.cache_folder dbfs:/eordentlich/spark-nlp/cached_pretrained\r\nspark.sql.execution.arrow.maxRecordsPerBatch 100000\r\nspark.jsl.settings.annotator.log_folder dbfs:/eordentlich/spark-nlp/annotator_logs\r\nspark.executor.cores 8\r\nspark.rapids.memory.gpu.minAllocFraction 0.0001\r\nspark.rapids.memory.gpu.allocFraction 0.25\r\nspark.plugins com.nvidia.spark.SQLPlugin\r\nspark.locality.wait 0s\r\nspark.rapids.memory.gpu.pooling.enabled true\r\nspark.rapids.sql.explain ALL\r\nspark.kryo.registrator com.nvidia.spark.rapids.GpuKryoRegistrator\r\nspark.rapids.memory.gpu.reserve 20\r\nspark.rapids.sql.python.gpu.enabled true\r\nspark.rapids.memory.pinnedPool.size 2G\r\nspark.python.daemon.module rapids.daemon_databricks\r\nspark.rapids.sql.batchSizeBytes 128m\r\nspark.sql.adaptive.enabled false\r\nspark.rapids.sql.enabled true\r\nspark.databricks.delta.optimizeWrite.enabled false\r\nspark.rapids.sql.concurrentGpuTasks 2\r\n```\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}