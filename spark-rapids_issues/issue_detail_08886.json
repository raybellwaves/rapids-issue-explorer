{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8886",
    "id": 1829905134,
    "node_id": "I_kwDOD7z77c5tEiLu",
    "number": 8886,
    "title": "[FEA] Add Host Memory Retry Columnar To Row Conversion",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2586576266,
            "node_id": "MDU6TGFiZWwyNTg2NTc2MjY2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/task",
            "name": "task",
            "color": "65abf7",
            "default": false,
            "description": "Work required that improves the product but is not user facing"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "gerashegalov",
        "id": 3187938,
        "node_id": "MDQ6VXNlcjMxODc5Mzg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gerashegalov",
        "html_url": "https://github.com/gerashegalov",
        "followers_url": "https://api.github.com/users/gerashegalov/followers",
        "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
        "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
        "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
        "repos_url": "https://api.github.com/users/gerashegalov/repos",
        "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "gerashegalov",
            "id": 3187938,
            "node_id": "MDQ6VXNlcjMxODc5Mzg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gerashegalov",
            "html_url": "https://github.com/gerashegalov",
            "followers_url": "https://api.github.com/users/gerashegalov/followers",
            "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
            "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
            "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
            "repos_url": "https://api.github.com/users/gerashegalov/repos",
            "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-07-31T19:46:59Z",
    "updated_at": "2023-12-19T18:11:27Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nGpuColumnarToRowExec is a little complicated because we have a number of different optimization around it. Ultimately we need a good way to limit the amount of host memory that it can use.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/issues/9862 should give us hard limits, but we also want to be able to retry the allocation if it fails like with the GPU retry framework.  This is to add in that retry where needed.\r\n\r\nThe accelerated transpose case (AcceleratedColumnarToRowIterator) converts the data to one or more HostColumnVector instances that are lists of bytes, which hold a row format similar to UnsafeRow in Spark.\r\n\r\nThe non-accelerated case (ColumnarToRowIterator) will just copy the data to the host and then walk that data one row at a time.\r\n\r\nThe primary goal here is to limit the host memory without deadlocking. The easiest way to do this would be to take the HostColumnVectors and make them spillable. Then we would get the wrapped column vector each time we needed to read some data and release it when we were done.  This would work, but it is far from ideal, especially if we have a lot of columns and spilling is happening regularly.\r\n\r\nIt would be better if we could chunk the data on demand into smaller chunks.\r\n\r\nFor the accelerated case I think we could adjust the limits it currently has in place so we could have a target size that we pass down to the kernels. They would then return chunks of rows that are about that size.  When we copy them back to the host, there would be more overhead on the heap to keep track of more objects, but it would also reduce the maximum amount of non-spillable memory that we have at any point in time, and it would also reduce the amount of memory that might need to be read back in each time.\r\n\r\nFor the non-accelerated case I think we would need a contig split like API or something similar. I am not super happy with the idea that we would need to do more computation, but perhaps we could do this dynamically, like we do for spill with bounce buffers and the chunked pack API there. This might need to be a follow on piece of work, as it is likely a lot more complicated.\r\n```[tasklist]\r\n### Tasks\r\n- [ ] https://github.com/NVIDIA/spark-rapids-jni/pull/1637\r\n- [ ] https://github.com/NVIDIA/spark-rapids/pull/10013\r\n```\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8886/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}