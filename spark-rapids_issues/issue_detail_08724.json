{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8724",
    "id": 1805097571,
    "node_id": "I_kwDOD7z77c5rl5pj",
    "number": 8724,
    "title": "[FEA] Create a BigDataChecking tool",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094874947,
            "node_id": "MDU6TGFiZWwyMDk0ODc0OTQ3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/test",
            "name": "test",
            "color": "60d6d4",
            "default": false,
            "description": "Only impacts tests"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2023-07-14T15:50:26Z",
    "updated_at": "2023-09-12T18:39:25Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nhttps://github.com/NVIDIA/spark-rapids/pull/8706 adds a tool to generate large data sets for scale testing, and possibly some performance testing for very difficult corner cases.  But ultimately we need to also verify that the results after running that generated data through a query are correct. TPC- benchmarks provide a validation tool that can be used to check if the results are correct, but for us, our definition of correct is that it matches Spark on the CPU. Warts and all.\r\n\r\nThe other problem is that the data produced by scale testing can be very very large (both in terms of rows, but also column count, and nesting). So we need to come up with a way to validate that the results are correct in a way that is going to scale. This itself may be a really massive scale test too because we are likely to double the number of rows and columns by trying to compare two large data sets.\r\n\r\nOne way to do this is to group by all of the columns, do a count on them (so that all duplicates are removed), for both the expected data set and the test data set. Then to do two anti-joins on them. A left-anti-join to know if there are rows on the LHS that are not in the right, and a right-anti-join to know if there are rows in the RHS that are not in the left.\r\n\r\n```\r\nval expected_count = expected.groupBy(expected.schema.names: _*).count\r\nval test_count = test.groupBy(test.schema.names: _*).count\r\nval just_in_expected = expected_count.join(test_count, expected_count.schema.names.toSeq, \"left_anti\").withColumn(\"from\", lit(0))\r\nval just_in_test = test_count.join(expected_count, expected_count.schema.names.toSeq, \"left_anti\").withColumn(\"from\", lit(1))\r\njust_in_expected.union(just_in_test).show()\r\n```\r\n\r\nThis works if all of the values produced will be bit for bit identical to those on the CPU, and are types that Spark can join on. For floating point types, especially with aggregations that is not guaranteed.  Perhaps that is good enough. We just avoid floats in our scale tests and we are good, but I am not sure we can do that especially if we want to reproduce issues from customer queries.\r\n\r\nWe might need to normalize maps to make this work and turn them into a combinations of structs and arrays with the data sorted by the key.\r\n\r\nFor floating point values or anywhere that we would want to do an approximate match, I just don't know of a good way to do it that is distributed and scales well.\r\n\r\nIf we try to normalize the floating point values by rounding of any kind there are inflection points where if we are off by a little the rounding would just exacerbate the difference. We could provide a UDF to do a fuzzy match on the floating point values as a part of the anti-join, but we could not do the group by any more to deal with duplicates because we might not even be self consistent, and aggregations don't support a fuzzy match.\r\n\r\nThe only option I can think of is to sort all the data from both data sets and produce a row number.  This takes a bit of work with monotonically_increasing_id and partition_id, along with writing out the intermediate data, but it is possible.  After that we can join on the row numbers and do a fuzzy match in a UDF for each column that contains floating point values, or maps. Not ideal but it would work.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}