{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8332",
    "id": 1717623923,
    "node_id": "I_kwDOD7z77c5mYNxz",
    "number": 8332,
    "title": "[BUG] Host memory leak in ParquetWriterSuite \"Old timestamps millis in EXCEPTION mode\"",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "revans2",
            "id": 3441321,
            "node_id": "MDQ6VXNlcjM0NDEzMjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/revans2",
            "html_url": "https://github.com/revans2",
            "followers_url": "https://api.github.com/users/revans2/followers",
            "following_url": "https://api.github.com/users/revans2/following{/other_user}",
            "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
            "organizations_url": "https://api.github.com/users/revans2/orgs",
            "repos_url": "https://api.github.com/users/revans2/repos",
            "events_url": "https://api.github.com/users/revans2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/revans2/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-05-19T18:04:18Z",
    "updated_at": "2023-05-23T20:23:00Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nWhen I run the test I get the following memory leak.\r\n\r\n```\r\n23/05/19 12:24:04.596 shutdown-hook-0 ERROR HostMemoryBuffer: A HOST BUFFER WAS LEAKED (ID: 13 7f6bc819f9a0)\r\n23/05/19 12:24:04.602 shutdown-hook-0 ERROR MemoryCleaner: Leaked host buffer (ID: 13): 2023-05-19 17:24:01.0774 UTC: INC\r\njava.lang.Thread.getStackTrace(Thread.java:1564)\r\nai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:333)\r\nai.rapids.cudf.MemoryCleaner$Cleaner.addRef(MemoryCleaner.java:91)\r\nai.rapids.cudf.MemoryBuffer.incRefCount(MemoryBuffer.java:275)\r\nai.rapids.cudf.MemoryBuffer.<init>(MemoryBuffer.java:117)\r\nai.rapids.cudf.HostMemoryBuffer.<init>(HostMemoryBuffer.java:196)\r\nai.rapids.cudf.HostMemoryBuffer.<init>(HostMemoryBuffer.java:192)\r\nai.rapids.cudf.HostMemoryBuffer.allocate(HostMemoryBuffer.java:144)\r\nai.rapids.cudf.Table.writeParquetBufferBegin(Native Method)\r\nai.rapids.cudf.Table.access$600(Table.java:41)\r\nai.rapids.cudf.Table$ParquetTableWriter.<init>(Table.java:1314)\r\nai.rapids.cudf.Table$ParquetTableWriter.<init>(Table.java:1291)\r\nai.rapids.cudf.Table.writeParquetChunked(Table.java:1372)\r\ncom.nvidia.spark.rapids.GpuParquetWriter.<init>(GpuParquetFileFormat.scala:399)\r\ncom.nvidia.spark.rapids.GpuParquetFileFormat$$anon$1.newInstance(GpuParquetFileFormat.scala:287)\r\norg.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.newOutputWriter(GpuFileFormatDataWriter.scala:161)\r\norg.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.<init>(GpuFileFormatDataWriter.scala:143)\r\norg.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:408)\r\norg.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:136)\r\n```\r\n\r\nDigging into it this is caused by the `GpuParquetWriter` not closing the native `TableWriter` because the native code is holding onto the buffer and will close it when things get shut down properly, and they are not getting shut down properly.\r\n\r\nParquet has two different options that are conflicting in this code. The first option is rebasing timestamps before writing. We do not support this so we throw an exception if we cannot do it before we do the write.\r\n\r\nThere is a second option to write timestamps out as milliseconds instead of microseconds.\r\n\r\nThe rebase code only works on microseconds. If it sees milliseconds it throws an assertion error.\r\n\r\nWhen the retry code was added to the parquet writer the location where we transform the microseconds to millisecond changed relative to the location where we check to see if we need to rebase the timestamps.\r\n\r\nThis now caused us to throw an exception any time we try to write out milliseconds instead of microseconds and also rebase is turned on.\r\n\r\nThe reason why this caused a memory leak is because when we shut down the writer, if we didn't output any data to the table writer, then we write out an empty batch. CUDF for some formats will output a corrupt file if we don't do that.  Well that happens before we shut down the writer, so we end up leaking the writer in those cases.\r\n\r\nWe need to fix when/how we do transformations to ideally do the transformation/checks in a single callback that takes a table as input and produces a table as output. It will make things simpler for other places that also use this transformation like API.  Then we can check for the rebase if needed before we transform the timestamp types and finally write out the data.\r\n\r\nThen we also should try and have a simpler way to recover from the empty batch write throwing an exception so we at least close the writer.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8332/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}