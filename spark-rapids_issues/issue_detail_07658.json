{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7658",
    "id": 1570119106,
    "node_id": "I_kwDOD7z77c5dlh3C",
    "number": 7658,
    "title": "[FEA] We should have regular validation jobs for parquet with zstd compression",
    "user": {
        "login": "jbrennan333",
        "id": 69316431,
        "node_id": "MDQ6VXNlcjY5MzE2NDMx",
        "avatar_url": "https://avatars.githubusercontent.com/u/69316431?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jbrennan333",
        "html_url": "https://github.com/jbrennan333",
        "followers_url": "https://api.github.com/users/jbrennan333/followers",
        "following_url": "https://api.github.com/users/jbrennan333/following{/other_user}",
        "gists_url": "https://api.github.com/users/jbrennan333/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jbrennan333/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jbrennan333/subscriptions",
        "organizations_url": "https://api.github.com/users/jbrennan333/orgs",
        "repos_url": "https://api.github.com/users/jbrennan333/repos",
        "events_url": "https://api.github.com/users/jbrennan333/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jbrennan333/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094874947,
            "node_id": "MDU6TGFiZWwyMDk0ODc0OTQ3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/test",
            "name": "test",
            "color": "60d6d4",
            "default": false,
            "description": "Only impacts tests"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-02-03T16:39:31Z",
    "updated_at": "2023-02-07T21:58:54Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "We currently run benchmarks and validation for parquet/snappy decompression.  I don't think we are currently doing regular validation of snappy compression, except for the small amount done when writing out the results of queries.\r\n\r\nAs the devtech team continues to make improvements for compression/decompression in nvcomp, and cudf team continues to make improvements to parquet/orc reading/writing, we should have validation jobs that run regularly for the spark-rapids plugin to validate reading/writing zstd data.\r\n\r\ndevtech and nvcomp have their own test pipelines, but we have often found that running large query sets like nds2.0 at scale 3000 in spark can shake out bugs that escape earlier testing.   We also want to ensure that we catch any data inconsistencies as soon as possible so they can be fixed earlier in the release cycle.\r\n\r\nWe have done zstd compression/decompression validation by hand for the last couple releases.  A plan for this type of testing that was done for 22.12 can be found in this issue: https://github.com/NVIDIA/spark-rapids/issues/3037.  We did the same set of steps for 23.02.\r\n\r\nI don't think we need to do all of what is in that plan, but it gives a good outline of things we may want to test for.\r\nSome things we might want are:\r\n\r\n- Run NDS2.0 benchmarks on GPU at scale 3000 using zstd data (produced by CPU) on a regular basis (maybe weekly) so we can track performance changes.\r\n- Run a job to convert raw nds2.0 data to parquet with zstd compression, and validate that the data matches the cpu data - we don't need to regenerate the CPU data every time.  (we might also want to do this for snappy, as it is more heavily used)\r\n- Run NDS2.0 power run on GPU at scale 3000 using the GPU-generated parquet-zstd data and validate results (this particular case found a bug in decompression in 23.02)\r\n- Run NDS2.0 power run on CPU at scale using GPU-generated data (this ensures our GPU generated data is still readable on CPU)\r\n\r\nI'm assuming we would want to do these for parquet data.  Ideally we would do it for ORC as well if resources allow.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7658/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}