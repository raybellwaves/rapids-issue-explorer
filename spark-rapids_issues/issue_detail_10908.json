{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10908",
    "id": 2318855627,
    "node_id": "I_kwDOD7z77c6KNu3L",
    "number": 10908,
    "title": "[BUG] Cast string to decimal won't return null for out of range floats",
    "user": {
        "login": "thirtiseven",
        "id": 7326403,
        "node_id": "MDQ6VXNlcjczMjY0MDM=",
        "avatar_url": "https://avatars.githubusercontent.com/u/7326403?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/thirtiseven",
        "html_url": "https://github.com/thirtiseven",
        "followers_url": "https://api.github.com/users/thirtiseven/followers",
        "following_url": "https://api.github.com/users/thirtiseven/following{/other_user}",
        "gists_url": "https://api.github.com/users/thirtiseven/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/thirtiseven/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/thirtiseven/subscriptions",
        "organizations_url": "https://api.github.com/users/thirtiseven/orgs",
        "repos_url": "https://api.github.com/users/thirtiseven/repos",
        "events_url": "https://api.github.com/users/thirtiseven/events{/privacy}",
        "received_events_url": "https://api.github.com/users/thirtiseven/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-05-27T10:47:34Z",
    "updated_at": "2024-06-04T20:14:16Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nCast string to decimal won't return null for out-of-range floats, it will return the value instead of null or throw an exception.\r\n\r\nFor example `1.23E+21` cast to decimal(15,-5) will return `1.2300000000000000E+21` on GPU, but null (ansi off) or `SparkArithmeticException: Decimal(expanded, 1.23E+21, 3, -19) cannot be represented as Decimal(15, -5)` (ansi on) on CPU.\r\n\r\n**Steps/Code to reproduce bug**\r\n```\r\nscala> val df = Seq(\"1.23E+21\").toDF(\"col\")\r\ndf: org.apache.spark.sql.DataFrame = [col: string]\r\n\r\nscala> df.write.mode(\"OVERWRITE\").parquet(\"TEMP\")\r\n24/05/27 10:35:34 WARN GpuOverrides:\r\n*Exec <DataWritingCommandExec> will run on GPU\r\n  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\r\n  ! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\r\n    @Expression <AttributeReference> col#4 could run on GPU\r\n\r\n\r\nscala> import org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.types._\r\n\r\nscala> spark.conf.set(\"spark.sql.legacy.allowNegativeScaleOfDecimal\", \"true\")\r\n\r\nscala>\r\n\r\nscala> spark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\r\n\r\nscala> val decType = DataTypes.createDecimalType(15, -5)\r\ndecType: org.apache.spark.sql.types.DecimalType = DecimalType(15,-5)\r\n\r\nscala> spark.read.parquet(\"TEMP\").select(col(\"col\").cast(decType)).show(false)\r\n24/05/27 10:36:42 WARN GpuOverrides:\r\n!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\r\n  @Partitioning <SinglePartition$> could run on GPU\r\n  *Exec <ProjectExec> will run on GPU\r\n    *Expression <Alias> cast(cast(col#7 as decimal(15,-5)) as string) AS col#13 will run on GPU\r\n      *Expression <Cast> cast(cast(col#7 as decimal(15,-5)) as string) will run on GPU\r\n        *Expression <Cast> cast(col#7 as decimal(15,-5)) will run on GPU\r\n    *Exec <FileSourceScanExec> will run on GPU\r\n\r\n+----------------------+\r\n|col                   |\r\n+----------------------+\r\n|1.2300000000000000E+21|\r\n+----------------------+\r\n\r\n\r\nscala> spark.conf.set(\"spark.rapids.sql.enabled\", \"false\")\r\n\r\nscala> spark.read.parquet(\"TEMP\").select(col(\"col\").cast(decType)).show(false)\r\njava.lang.AssertionError: assertion failed:\r\n  Decimal$DecimalIsFractional\r\n     while compiling: <console>\r\n        during phase: globalPhase=terminal, enteringPhase=jvm\r\n     library version: version 2.12.15\r\n    compiler version: version 2.12.15\r\n  reconstructed args: -classpath /home/haoyangl/spark-rapids/dist/target/rapids-4-spark_2.12-24.08.0-SNAPSHOT-cuda11.jar -Yrepl-class-based -Yrepl-outdir /tmp/spark-026c13e7-fb35-46ff-a225-22ac468cd6e1/repl-47525fc2-08a4-4b4b-b850-a00c64aa2c3b\r\n\r\n  last tree to typer: TypeTree(class Byte)\r\n       tree position: line 6 of <console>\r\n            tree tpe: Byte\r\n              symbol: (final abstract) class Byte in package scala\r\n   symbol definition: final abstract class Byte extends  (a ClassSymbol)\r\n      symbol package: scala\r\n       symbol owners: class Byte\r\n           call site: constructor $eval in object $eval in package $line22\r\n\r\n== Source file context for tree position ==\r\n\r\n     3\r\n     4 object $eval {\r\n     5   lazy val $result = $line22.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res5\r\n     6   lazy val $print: _root_.java.lang.String =  {\r\n     7     $line22.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw\r\n     8\r\n     9 \"\"\r\n\tat scala.reflect.internal.SymbolTable.throwAssertionError(SymbolTable.scala:185)\r\n\tat scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1525)\r\n\tat scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)\r\n\tat scala.reflect.internal.Symbols$Symbol.flatOwnerInfo(Symbols.scala:2353)\r\n\tat scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:3346)\r\n\tat scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:3348)\r\n\tat scala.reflect.internal.Symbols$ModuleClassSymbol.sourceModule(Symbols.scala:3487)\r\n\tat scala.reflect.internal.Symbols.$anonfun$forEachRelevantSymbols$1$adapted(Symbols.scala:3802)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat scala.reflect.internal.Symbols.markFlagsCompleted(Symbols.scala:3799)\r\n\tat scala.reflect.internal.Symbols.markFlagsCompleted$(Symbols.scala:3805)\r\n\tat scala.reflect.internal.SymbolTable.markFlagsCompleted(SymbolTable.scala:28)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1(UnPickler.scala:324)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:342)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef(UnPickler.scala:645)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:413)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$readSymbol$10(UnPickler.scala:357)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:188)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:357)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$run$1(UnPickler.scala:96)\r\n\tat scala.reflect.internal.pickling.UnPickler$Scan.run(UnPickler.scala:88)\r\n\tat scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:47)\r\n\tat scala.tools.nsc.symtab.classfile.ClassfileParser.unpickleOrParseInnerClasses(ClassfileParser.scala:1186)\r\n\tat scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:468)\r\n\tat scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$2(ClassfileParser.scala:161)\r\n\tat scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$1(ClassfileParser.scala:147)\r\n\tat scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:130)\r\n\tat scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:343)\r\n\tat scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:250)\r\n\tat scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:269)\r\n\tat scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:1104)\r\n\tat scala.reflect.internal.Symbols$Symbol.toOption(Symbols.scala:2609)\r\n\tat scala.tools.nsc.interpreter.IMain.translateSimpleResource(IMain.scala:340)\r\n\tat scala.tools.nsc.interpreter.IMain$TranslatingClassLoader.findAbstractFile(IMain.scala:354)\r\n\tat scala.reflect.internal.util.AbstractFileClassLoader.findResource(AbstractFileClassLoader.scala:76)\r\n\tat java.lang.ClassLoader.getResource(ClassLoader.java:1089)\r\n\tat java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1300)\r\n\tat scala.reflect.internal.util.RichClassLoader$.classAsStream$extension(ScalaClassLoader.scala:89)\r\n\tat scala.reflect.internal.util.RichClassLoader$.classBytes$extension(ScalaClassLoader.scala:81)\r\n\tat scala.reflect.internal.util.ScalaClassLoader.classBytes(ScalaClassLoader.scala:131)\r\n\tat scala.reflect.internal.util.ScalaClassLoader.classBytes$(ScalaClassLoader.scala:131)\r\n\tat scala.reflect.internal.util.AbstractFileClassLoader.classBytes(AbstractFileClassLoader.scala:41)\r\n\tat scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:70)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:405)\r\n\tat org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:348)\r\n\tat org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)\r\n\tat org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)\r\n\tat org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8618)\r\n\tat org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:8838)\r\n\tat org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:8529)\r\n\tat org.codehaus.janino.UnitCompiler.reclassify(UnitCompiler.java:8388)\r\n\tat org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6900)\r\n\tat org.codehaus.janino.UnitCompiler.access$14600(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6518)\r\n\tat org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6515)\r\n\tat org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4429)\r\n\tat org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6515)\r\n\tat org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6511)\r\n\tat org.codehaus.janino.Java$Lvalue.accept(Java.java:4353)\r\n\tat org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6511)\r\n\tat org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)\r\n\tat org.codehaus.janino.Java$Rvalue.accept(Java.java:4321)\r\n\tat org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6490)\r\n\tat org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9110)\r\n\tat org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5055)\r\n\tat org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4482)\r\n\tat org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4455)\r\n\tat org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5286)\r\n\tat org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4455)\r\n\tat org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5683)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2581)\r\n\tat org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1506)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\r\n\tat org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\r\n\tat org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\r\n\tat org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1661)\r\n\tat org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1499)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$ForStatement.accept(Java.java:3187)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\r\n\tat org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1848)\r\n\tat org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1501)\r\n\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.Java$WhileStatement.accept(Java.java:3245)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\r\n\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3420)\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1362)\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1335)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:807)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:975)\r\n\tat org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:392)\r\n\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:384)\r\n\tat org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1445)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\r\n\tat org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1312)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:833)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)\r\n\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)\r\n\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)\r\n\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)\r\n\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\r\n\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)\r\n\tat org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)\r\n\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)\r\n\tat org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)\r\n\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)\r\n\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)\r\n\tat org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)\r\n\tat org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)\r\n\tat org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)\r\n\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)\r\n\tat org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1490)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1587)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1584)\r\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\r\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\r\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\r\n\tat org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\r\n\tat org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1437)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:340)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:473)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:810)\r\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:787)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:35)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:37)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:39)\r\n\tat $line22.$read$$iw$$iw$$iw$$iw.<init>(<console>:41)\r\n\tat $line22.$read$$iw$$iw$$iw.<init>(<console>:43)\r\n\tat $line22.$read$$iw$$iw.<init>(<console>:45)\r\n\tat $line22.$read$$iw.<init>(<console>:47)\r\n\tat $line22.$read.<init>(<console>:49)\r\n\tat $line22.$read$.<init>(<console>:53)\r\n\tat $line22.$read$.<clinit>(<console>)\r\n\tat $line22.$eval$.$print$lzycompute(<console>:7)\r\n\tat $line22.$eval$.$print(<console>:6)\r\n\tat $line22.$eval.$print(<console>)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\r\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\r\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\r\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\r\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\r\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\r\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\r\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\r\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\r\n\tat scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)\r\n\tat scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)\r\n\tat scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)\r\n\tat scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)\r\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)\r\n\tat org.apache.spark.repl.Main$.doMain(Main.scala:78)\r\n\tat org.apache.spark.repl.Main$.main(Main.scala:58)\r\n\tat org.apache.spark.repl.Main.main(Main.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nerror: error while loading Decimal, class file '/home/haoyangl/spark-3.3.0-bin-hadoop3.2/jars/spark-catalyst_2.12-3.3.0.jar(org/apache/spark/sql/types/Decimal.class)' is broken\r\n(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:\r\n  Decimal$DecimalIsFractional\r\n     while compiling: <console>\r\n        during phase: globalPhase=terminal, enteringPhase=jvm\r\n     library version: version 2.12.15\r\n    compiler version: version 2.12.15\r\n  reconstructed args: -classpath /home/haoyangl/spark-rapids/dist/target/rapids-4-spark_2.12-24.08.0-SNAPSHOT-cuda11.jar -Yrepl-class-based -Yrepl-outdir /tmp/spark-026c13e7-fb35-46ff-a225-22ac468cd6e1/repl-47525fc2-08a4-4b4b-b850-a00c64aa2c3b\r\n\r\n  last tree to typer: TypeTree(class Byte)\r\n       tree position: line 6 of <console>\r\n            tree tpe: Byte\r\n              symbol: (final abstract) class Byte in package scala\r\n   symbol definition: final abstract class Byte extends  (a ClassSymbol)\r\n      symbol package: scala\r\n       symbol owners: class Byte\r\n           call site: constructor $eval in object $eval in package $line22\r\n\r\n== Source file context for tree position ==\r\n\r\n     3\r\n     4 object $eval {\r\n     5   lazy val $result = res5\r\n     6   lazy val $print: _root_.java.lang.String =  {\r\n     7     $iw\r\n     8\r\n     9 \"\" )\r\n24/05/27 10:36:56 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\r\norg.apache.spark.SparkArithmeticException: Decimal(expanded, 1.23E+21, 3, -19) cannot be represented as Decimal(15, -5). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala:108)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n24/05/27 10:36:56 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (spark-haoyang executor driver): org.apache.spark.SparkArithmeticException: Decimal(expanded, 1.23E+21, 3, -19) cannot be represented as Decimal(15, -5). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala:108)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\r\n24/05/27 10:36:56 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (spark-haoyang executor driver): org.apache.spark.SparkArithmeticException: Decimal(expanded, 1.23E+21, 3, -19) cannot be represented as Decimal(15, -5). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala:108)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n  at scala.Option.foreach(Option.scala:407)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:810)\r\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:787)\r\n  ... 49 elided\r\nCaused by: org.apache.spark.SparkArithmeticException: Decimal(expanded, 1.23E+21, 3, -19) cannot be represented as Decimal(15, -5). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala:108)\r\n  at org.apache.spark.sql.errors.QueryExecutionErrors.cannotChangeDecimalPrecisionError(QueryExecutionErrors.scala)\r\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:750)\r\n```\r\n**Expected behavior**\r\nResults on CPU and GPU should match.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10908/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}