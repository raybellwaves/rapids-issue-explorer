{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10147",
    "id": 2064568878,
    "node_id": "I_kwDOD7z77c57DtIu",
    "number": 10147,
    "title": "[BUG] test_dpp_reuse_broadcast_exchange failed",
    "user": {
        "login": "jlowe",
        "id": 1360766,
        "node_id": "MDQ6VXNlcjEzNjA3NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jlowe",
        "html_url": "https://github.com/jlowe",
        "followers_url": "https://api.github.com/users/jlowe/followers",
        "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
        "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
        "organizations_url": "https://api.github.com/users/jlowe/orgs",
        "repos_url": "https://api.github.com/users/jlowe/repos",
        "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jlowe/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "NVnavkumar",
        "id": 97137715,
        "node_id": "U_kgDOBco0Mw",
        "avatar_url": "https://avatars.githubusercontent.com/u/97137715?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NVnavkumar",
        "html_url": "https://github.com/NVnavkumar",
        "followers_url": "https://api.github.com/users/NVnavkumar/followers",
        "following_url": "https://api.github.com/users/NVnavkumar/following{/other_user}",
        "gists_url": "https://api.github.com/users/NVnavkumar/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NVnavkumar/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NVnavkumar/subscriptions",
        "organizations_url": "https://api.github.com/users/NVnavkumar/orgs",
        "repos_url": "https://api.github.com/users/NVnavkumar/repos",
        "events_url": "https://api.github.com/users/NVnavkumar/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NVnavkumar/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "NVnavkumar",
            "id": 97137715,
            "node_id": "U_kgDOBco0Mw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97137715?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/NVnavkumar",
            "html_url": "https://github.com/NVnavkumar",
            "followers_url": "https://api.github.com/users/NVnavkumar/followers",
            "following_url": "https://api.github.com/users/NVnavkumar/following{/other_user}",
            "gists_url": "https://api.github.com/users/NVnavkumar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/NVnavkumar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/NVnavkumar/subscriptions",
            "organizations_url": "https://api.github.com/users/NVnavkumar/orgs",
            "repos_url": "https://api.github.com/users/NVnavkumar/repos",
            "events_url": "https://api.github.com/users/NVnavkumar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/NVnavkumar/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 12,
    "created_at": "2024-01-03T19:54:10Z",
    "updated_at": "2024-05-14T19:04:14Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "From a recent nightly test run:\r\n```\r\n[2024-01-03T17:41:18.525Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-parquet][DATAGEN_SEED=1704297021, INJECT_OOM, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains.\r\n[2024-01-03T17:41:18.525Z] : java.lang.AssertionError: assertion failed: Could not find DynamicPruningExpression in the Spark plan\r\n[2024-01-03T17:41:18.525Z] AdaptiveSparkPlan isFinalPlan=true\r\n[2024-01-03T17:41:18.525Z] +- == Final Plan ==\r\n[2024-01-03T17:41:18.525Z]    LocalTableScan <empty>, [key#37135, max(value)#37151L]\r\n[2024-01-03T17:41:18.525Z] +- == Initial Plan ==\r\n[2024-01-03T17:41:18.525Z]    Sort [key#37135 ASC NULLS FIRST, max(value)#37151L ASC NULLS FIRST], true, 0\r\n[2024-01-03T17:41:18.525Z]    +- Exchange rangepartitioning(key#37135 ASC NULLS FIRST, max(value)#37151L ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=109491]\r\n[2024-01-03T17:41:18.525Z]       +- HashAggregate(keys=[key#37135], functions=[max(value#37136L)], output=[key#37135, max(value)#37151L])\r\n[2024-01-03T17:41:18.525Z]          +- Exchange hashpartitioning(key#37135, 4), ENSURE_REQUIREMENTS, [plan_id=109488]\r\n[2024-01-03T17:41:18.525Z]             +- HashAggregate(keys=[key#37135], functions=[partial_max(value#37136L)], output=[key#37135, max#37199L])\r\n[2024-01-03T17:41:18.525Z]                +- Union\r\n[2024-01-03T17:41:18.525Z]                   :- Project [key#37054 AS key#37135, value#37140L AS value#37136L]\r\n[2024-01-03T17:41:18.525Z]                   :  +- BroadcastHashJoin [key#37054], [key#37056], Inner, BuildRight, false\r\n[2024-01-03T17:41:18.526Z]                   :     :- HashAggregate(keys=[key#37054], functions=[sum(value#37053)], output=[key#37054, value#37140L])\r\n[2024-01-03T17:41:18.526Z]                   :     :  +- Exchange hashpartitioning(key#37054, 4), ENSURE_REQUIREMENTS, [plan_id=109473]\r\n[2024-01-03T17:41:18.526Z]                   :     :     +- HashAggregate(keys=[key#37054], functions=[partial_sum(value#37053)], output=[key#37054, sum#37201L])\r\n[2024-01-03T17:41:18.526Z]                   :     :        +- Project [value#37053, key#37054]\r\n[2024-01-03T17:41:18.526Z]                   :     :           +- Filter (isnotnull(value#37053) AND (value#37053 > 0))\r\n[2024-01-03T17:41:18.526Z]                   :     :              +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_0[value#37053,key#37054,skey#37055] Batched: true, DataFilters: [isnotnull(value#37053), (value#37053 > 0)], Format: Parquet, Location: InMemoryFileIndex(50 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-git..., PartitionFilters: [isnotnull(key#37054), dynamicpruningexpression(key#37054 IN dynamicpruning#37196)], PushedFilters: [IsNotNull(value), GreaterThan(value,0)], ReadSchema: struct<value:int>\r\n[2024-01-03T17:41:18.526Z]                   :     :                    +- SubqueryAdaptiveBroadcast dynamicpruning#37196, 0, true, Project [key#37056], [key#37056]\r\n[2024-01-03T17:41:18.526Z]                   :     :                       +- AdaptiveSparkPlan isFinalPlan=false\r\n[2024-01-03T17:41:18.526Z]                   :     :                          +- Project [key#37056]\r\n[2024-01-03T17:41:18.526Z]                   :     :                             +- Filter ((((isnotnull(ex_key#37058) AND isnotnull(filter#37060)) AND (ex_key#37058 = 3)) AND (filter#37060 = 1458)) AND isnotnull(key#37056))\r\n[2024-01-03T17:41:18.526Z]                   :     :                                +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_1[key#37056,ex_key#37058,filter#37060] Batched: true, DataFilters: [isnotnull(ex_key#37058), isnotnull(filter#37060), (ex_key#37058 = 3), (filter#37060 = 1458), isn..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.526Z]                   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=109476]\r\n[2024-01-03T17:41:18.526Z]                   :        +- Project [key#37056]\r\n[2024-01-03T17:41:18.526Z]                   :           +- Filter ((((isnotnull(ex_key#37058) AND isnotnull(filter#37060)) AND (ex_key#37058 = 3)) AND (filter#37060 = 1458)) AND isnotnull(key#37056))\r\n[2024-01-03T17:41:18.526Z]                   :              +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_1[key#37056,ex_key#37058,filter#37060] Batched: true, DataFilters: [isnotnull(ex_key#37058), isnotnull(filter#37060), (ex_key#37058 = 3), (filter#37060 = 1458), isn..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.526Z]                   +- Project [key#37184, value#37187L]\r\n[2024-01-03T17:41:18.526Z]                      +- BroadcastHashJoin [key#37184], [key#37188], Inner, BuildRight, false\r\n[2024-01-03T17:41:18.526Z]                         :- HashAggregate(keys=[key#37184], functions=[sum(value#37183)], output=[key#37184, value#37187L])\r\n[2024-01-03T17:41:18.526Z]                         :  +- Exchange hashpartitioning(key#37184, 4), ENSURE_REQUIREMENTS, [plan_id=109479]\r\n[2024-01-03T17:41:18.526Z]                         :     +- HashAggregate(keys=[key#37184], functions=[partial_sum(value#37183)], output=[key#37184, sum#37203L])\r\n[2024-01-03T17:41:18.526Z]                         :        +- Project [value#37183, key#37184]\r\n[2024-01-03T17:41:18.526Z]                         :           +- Filter (isnotnull(value#37183) AND (value#37183 > 0))\r\n[2024-01-03T17:41:18.526Z]                         :              +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_0[value#37183,key#37184,skey#37185] Batched: true, DataFilters: [isnotnull(value#37183), (value#37183 > 0)], Format: Parquet, Location: InMemoryFileIndex(50 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-git..., PartitionFilters: [isnotnull(key#37184), dynamicpruningexpression(key#37184 IN dynamicpruning#37197)], PushedFilters: [IsNotNull(value), GreaterThan(value,0)], ReadSchema: struct<value:int>\r\n[2024-01-03T17:41:18.526Z]                         :                    +- SubqueryAdaptiveBroadcast dynamicpruning#37197, 0, true, Project [key#37188], [key#37188]\r\n[2024-01-03T17:41:18.526Z]                         :                       +- AdaptiveSparkPlan isFinalPlan=false\r\n[2024-01-03T17:41:18.526Z]                         :                          +- Project [key#37188]\r\n[2024-01-03T17:41:18.526Z]                         :                             +- Filter ((((isnotnull(ex_key#37190) AND isnotnull(filter#37192)) AND (ex_key#37190 = 3)) AND (filter#37192 = 1458)) AND isnotnull(key#37188))\r\n[2024-01-03T17:41:18.526Z]                         :                                +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_1[key#37188,ex_key#37190,filter#37192] Batched: true, DataFilters: [isnotnull(ex_key#37190), isnotnull(filter#37192), (ex_key#37190 = 3), (filter#37192 = 1458), isn..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.526Z]                         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=109482]\r\n[2024-01-03T17:41:18.526Z]                            +- Project [key#37188]\r\n[2024-01-03T17:41:18.526Z]                               +- Filter ((((isnotnull(ex_key#37190) AND isnotnull(filter#37192)) AND (ex_key#37190 = 3)) AND (filter#37192 = 1458)) AND isnotnull(key#37188))\r\n[2024-01-03T17:41:18.526Z]                                  +- FileScan parquet spark_catalog.default.tmp_table_gw1_470723338_1[key#37188,ex_key#37190,filter#37192] Batched: true, DataFilters: [isnotnull(ex_key#37190), isnotnull(filter#37192), (ex_key#37190 = 3), (filter#37192 = 1458), isn..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.526Z] \r\n[2024-01-03T17:41:18.526Z] \tat scala.Predef$.assert(Predef.scala:223)\r\n[2024-01-03T17:41:18.526Z] \tat org.apache.spark.sql.rapids.ShimmedExecutionPlanCaptureCallbackImpl.assertContains(ShimmedExecutionPlanCaptureCallbackImpl.scala:170)\r\n[2024-01-03T17:41:18.526Z] \tat org.apache.spark.sql.rapids.ShimmedExecutionPlanCaptureCallbackImpl.assertContains(ShimmedExecutionPlanCaptureCallbackImpl.scala:175)\r\n[2024-01-03T17:41:18.526Z] \tat org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback$.assertContains(ExecutionPlanCaptureCallback.scala:76)\r\n[2024-01-03T17:41:18.526Z] \tat org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains(ExecutionPlanCaptureCallback.scala)\r\n[2024-01-03T17:41:18.526Z] \tat sun.reflect.GeneratedMethodAccessor140.invoke(Unknown Source)\r\n[2024-01-03T17:41:18.526Z] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n[2024-01-03T17:41:18.526Z] \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.Gateway.invoke(Gateway.java:282)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n[2024-01-03T17:41:18.526Z] \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n[2024-01-03T17:41:18.527Z] \tat java.lang.Thread.run(Thread.java:750)\r\n[2024-01-03T17:41:18.527Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-orc][DATAGEN_SEED=1704297021, INJECT_OOM, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains.\r\n[2024-01-03T17:41:18.527Z] : java.lang.AssertionError: assertion failed: Could not find DynamicPruningExpression in the Spark plan\r\n[2024-01-03T17:41:18.527Z] AdaptiveSparkPlan isFinalPlan=true\r\n[2024-01-03T17:41:18.527Z] +- == Final Plan ==\r\n[2024-01-03T17:41:18.527Z]    LocalTableScan <empty>, [key#37628, max(value)#37644L]\r\n[2024-01-03T17:41:18.527Z] +- == Initial Plan ==\r\n[2024-01-03T17:41:18.527Z]    Sort [key#37628 ASC NULLS FIRST, max(value)#37644L ASC NULLS FIRST], true, 0\r\n[2024-01-03T17:41:18.527Z]    +- Exchange rangepartitioning(key#37628 ASC NULLS FIRST, max(value)#37644L ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=111165]\r\n[2024-01-03T17:41:18.527Z]       +- HashAggregate(keys=[key#37628], functions=[max(value#37629L)], output=[key#37628, max(value)#37644L])\r\n[2024-01-03T17:41:18.527Z]          +- Exchange hashpartitioning(key#37628, 4), ENSURE_REQUIREMENTS, [plan_id=111162]\r\n[2024-01-03T17:41:18.527Z]             +- HashAggregate(keys=[key#37628], functions=[partial_max(value#37629L)], output=[key#37628, max#37692L])\r\n[2024-01-03T17:41:18.527Z]                +- Union\r\n[2024-01-03T17:41:18.527Z]                   :- Project [key#37547 AS key#37628, value#37633L AS value#37629L]\r\n[2024-01-03T17:41:18.527Z]                   :  +- BroadcastHashJoin [key#37547], [key#37549], Inner, BuildRight, false\r\n[2024-01-03T17:41:18.527Z]                   :     :- HashAggregate(keys=[key#37547], functions=[sum(value#37546)], output=[key#37547, value#37633L])\r\n[2024-01-03T17:41:18.527Z]                   :     :  +- Exchange hashpartitioning(key#37547, 4), ENSURE_REQUIREMENTS, [plan_id=111147]\r\n[2024-01-03T17:41:18.527Z]                   :     :     +- HashAggregate(keys=[key#37547], functions=[partial_sum(value#37546)], output=[key#37547, sum#37694L])\r\n[2024-01-03T17:41:18.527Z]                   :     :        +- Project [value#37546, key#37547]\r\n[2024-01-03T17:41:18.527Z]                   :     :           +- Filter (isnotnull(value#37546) AND (value#37546 > 0))\r\n[2024-01-03T17:41:18.527Z]                   :     :              +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_0[value#37546,key#37547,skey#37548] Batched: true, DataFilters: [isnotnull(value#37546), (value#37546 > 0)], Format: ORC, Location: InMemoryFileIndex(50 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-git..., PartitionFilters: [isnotnull(key#37547), dynamicpruningexpression(key#37547 IN dynamicpruning#37689)], PushedFilters: [IsNotNull(value), GreaterThan(value,0)], ReadSchema: struct<value:int>\r\n[2024-01-03T17:41:18.527Z]                   :     :                    +- SubqueryAdaptiveBroadcast dynamicpruning#37689, 0, true, Project [key#37549], [key#37549]\r\n[2024-01-03T17:41:18.527Z]                   :     :                       +- AdaptiveSparkPlan isFinalPlan=false\r\n[2024-01-03T17:41:18.527Z]                   :     :                          +- Project [key#37549]\r\n[2024-01-03T17:41:18.527Z]                   :     :                             +- Filter ((((isnotnull(ex_key#37551) AND isnotnull(filter#37553)) AND (ex_key#37551 = 3)) AND (filter#37553 = 1458)) AND isnotnull(key#37549))\r\n[2024-01-03T17:41:18.527Z]                   :     :                                +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_1[key#37549,ex_key#37551,filter#37553] Batched: true, DataFilters: [isnotnull(ex_key#37551), isnotnull(filter#37553), (ex_key#37551 = 3), (filter#37553 = 1458), isn..., Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.527Z]                   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=111150]\r\n[2024-01-03T17:41:18.527Z]                   :        +- Project [key#37549]\r\n[2024-01-03T17:41:18.527Z]                   :           +- Filter ((((isnotnull(ex_key#37551) AND isnotnull(filter#37553)) AND (ex_key#37551 = 3)) AND (filter#37553 = 1458)) AND isnotnull(key#37549))\r\n[2024-01-03T17:41:18.527Z]                   :              +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_1[key#37549,ex_key#37551,filter#37553] Batched: true, DataFilters: [isnotnull(ex_key#37551), isnotnull(filter#37553), (ex_key#37551 = 3), (filter#37553 = 1458), isn..., Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.527Z]                   +- Project [key#37677, value#37680L]\r\n[2024-01-03T17:41:18.527Z]                      +- BroadcastHashJoin [key#37677], [key#37681], Inner, BuildRight, false\r\n[2024-01-03T17:41:18.527Z]                         :- HashAggregate(keys=[key#37677], functions=[sum(value#37676)], output=[key#37677, value#37680L])\r\n[2024-01-03T17:41:18.527Z]                         :  +- Exchange hashpartitioning(key#37677, 4), ENSURE_REQUIREMENTS, [plan_id=111153]\r\n[2024-01-03T17:41:18.527Z]                         :     +- HashAggregate(keys=[key#37677], functions=[partial_sum(value#37676)], output=[key#37677, sum#37696L])\r\n[2024-01-03T17:41:18.527Z]                         :        +- Project [value#37676, key#37677]\r\n[2024-01-03T17:41:18.527Z]                         :           +- Filter (isnotnull(value#37676) AND (value#37676 > 0))\r\n[2024-01-03T17:41:18.527Z]                         :              +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_0[value#37676,key#37677,skey#37678] Batched: true, DataFilters: [isnotnull(value#37676), (value#37676 > 0)], Format: ORC, Location: InMemoryFileIndex(50 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-git..., PartitionFilters: [isnotnull(key#37677), dynamicpruningexpression(key#37677 IN dynamicpruning#37690)], PushedFilters: [IsNotNull(value), GreaterThan(value,0)], ReadSchema: struct<value:int>\r\n[2024-01-03T17:41:18.527Z]                         :                    +- SubqueryAdaptiveBroadcast dynamicpruning#37690, 0, true, Project [key#37681], [key#37681]\r\n[2024-01-03T17:41:18.527Z]                         :                       +- AdaptiveSparkPlan isFinalPlan=false\r\n[2024-01-03T17:41:18.527Z]                         :                          +- Project [key#37681]\r\n[2024-01-03T17:41:18.527Z]                         :                             +- Filter ((((isnotnull(ex_key#37683) AND isnotnull(filter#37685)) AND (ex_key#37683 = 3)) AND (filter#37685 = 1458)) AND isnotnull(key#37681))\r\n[2024-01-03T17:41:18.527Z]                         :                                +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_1[key#37681,ex_key#37683,filter#37685] Batched: true, DataFilters: [isnotnull(ex_key#37683), isnotnull(filter#37685), (ex_key#37683 = 3), (filter#37685 = 1458), isn..., Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.527Z]                         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=111156]\r\n[2024-01-03T17:41:18.527Z]                            +- Project [key#37681]\r\n[2024-01-03T17:41:18.527Z]                               +- Filter ((((isnotnull(ex_key#37683) AND isnotnull(filter#37685)) AND (ex_key#37683 = 3)) AND (filter#37685 = 1458)) AND isnotnull(key#37681))\r\n[2024-01-03T17:41:18.527Z]                                  +- FileScan orc spark_catalog.default.tmp_table_gw1_1272856583_1[key#37681,ex_key#37683,filter#37685] Batched: true, DataFilters: [isnotnull(ex_key#37683), isnotnull(filter#37685), (ex_key#37683 = 3), (filter#37685 = 1458), isn..., Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/home/jenkins/agent/workspace/jenkins-rapids_integration-dev-gith..., PartitionFilters: [], PushedFilters: [IsNotNull(ex_key), IsNotNull(filter), EqualTo(ex_key,3), EqualTo(filter,1458), IsNotNull(key)], ReadSchema: struct<key:int,ex_key:int,filter:int>\r\n[2024-01-03T17:41:18.528Z] \r\n[2024-01-03T17:41:18.528Z] \tat scala.Predef$.assert(Predef.scala:223)\r\n[2024-01-03T17:41:18.528Z] \tat org.apache.spark.sql.rapids.ShimmedExecutionPlanCaptureCallbackImpl.assertContains(ShimmedExecutionPlanCaptureCallbackImpl.scala:170)\r\n[2024-01-03T17:41:18.528Z] \tat org.apache.spark.sql.rapids.ShimmedExecutionPlanCaptureCallbackImpl.assertContains(ShimmedExecutionPlanCaptureCallbackImpl.scala:175)\r\n[2024-01-03T17:41:18.528Z] \tat org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback$.assertContains(ExecutionPlanCaptureCallback.scala:76)\r\n[2024-01-03T17:41:18.528Z] \tat org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains(ExecutionPlanCaptureCallback.scala)\r\n[2024-01-03T17:41:18.528Z] \tat sun.reflect.GeneratedMethodAccessor140.invoke(Unknown Source)\r\n[2024-01-03T17:41:18.528Z] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n[2024-01-03T17:41:18.528Z] \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.Gateway.invoke(Gateway.java:282)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n[2024-01-03T17:41:18.528Z] \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n[2024-01-03T17:41:18.528Z] \tat java.lang.Thread.run(Thread.java:750)\r\n[2024-01-03T17:41:18.528Z] = 2 failed, 20760 passed, 933 skipped, 417 xfailed, 388 xpassed, 9504 warnings in 6656.39s (1:50:56) =\r\n```",
    "closed_by": {
        "login": "NVnavkumar",
        "id": 97137715,
        "node_id": "U_kgDOBco0Mw",
        "avatar_url": "https://avatars.githubusercontent.com/u/97137715?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NVnavkumar",
        "html_url": "https://github.com/NVnavkumar",
        "followers_url": "https://api.github.com/users/NVnavkumar/followers",
        "following_url": "https://api.github.com/users/NVnavkumar/following{/other_user}",
        "gists_url": "https://api.github.com/users/NVnavkumar/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NVnavkumar/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NVnavkumar/subscriptions",
        "organizations_url": "https://api.github.com/users/NVnavkumar/orgs",
        "repos_url": "https://api.github.com/users/NVnavkumar/repos",
        "events_url": "https://api.github.com/users/NVnavkumar/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NVnavkumar/received_events",
        "type": "User",
        "site_admin": false
    },
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147/timeline",
    "performed_via_github_app": null,
    "state_reason": "reopened"
}