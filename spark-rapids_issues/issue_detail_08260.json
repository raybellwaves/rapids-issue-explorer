{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8260",
    "id": 1702370736,
    "node_id": "I_kwDOD7z77c5leB2w",
    "number": 8260,
    "title": "Add a CI step for merging configs propagated from getPrivateConfigs",
    "user": {
        "login": "gerashegalov",
        "id": 3187938,
        "node_id": "MDQ6VXNlcjMxODc5Mzg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gerashegalov",
        "html_url": "https://github.com/gerashegalov",
        "followers_url": "https://api.github.com/users/gerashegalov/followers",
        "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
        "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
        "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
        "repos_url": "https://api.github.com/users/gerashegalov/repos",
        "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2137414845,
            "node_id": "MDU6TGFiZWwyMTM3NDE0ODQ1",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/build",
            "name": "build",
            "color": "0052cc",
            "default": false,
            "description": "Related to CI / CD or cleanly building"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-05-09T16:34:04Z",
    "updated_at": "2023-05-09T16:34:04Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "When an \"extra-*configs\" provider class adds a public configuration key  the current verify logic in pre-merge designed only for the configs internal to the spark-rapids repo fails:\r\n\r\nExample: https://github.com/NVIDIA/spark-rapids/actions/runs/4925346478/jobs/8799470121#step:2:28242\r\n\r\nLocal repro:\r\n\r\n```bash\r\nmvn verify -DskipTests -Ppre-merge -Dincluded_buildvers=311\r\n[INFO] --- exec-maven-plugin:3.0.0:exec (if_modified_files) @ rapids-4-spark_2.12 ---\r\nfound modified files during mvn verify:\r\n M docs/configs.md\r\n[ERROR] Command execution failed.\r\norg.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)\r\n```\r\n\r\n```diff\r\ngit diff\r\ndiff --git a/docs/configs.md b/docs/configs.md\r\nindex 554a98f97..f476eea2b 100644\r\n--- a/docs/configs.md\r\n+++ b/docs/configs.md\r\n@@ -42,6 +42,7 @@ Name | Description | Default Value | Applicable at\r\n <a name=\"alluxio.slow.disk\"></a>spark.rapids.alluxio.slow.disk|Indicates whether the disks used by Alluxio are slow. If it's true and reading S3 large files, Rapids Accelerator reads from S3 directly instead of reading from Alluxio caches. Refer to spark.rapids.alluxio.large.file.threshold which defines a threshold that identifying whether files are large. Typically, it's slow disks if speed is less than 300M/second. If using convert time spark.rapids.alluxio.replacement.algo, this may not apply to all file types like Delta files|true|Runtime\r\n <a name=\"alluxio.user\"></a>spark.rapids.alluxio.user|Alluxio user is set on the Alluxio client, which is used to mount or get information. By default it should be the user that running the Alluxio processes. The default value is ubuntu.|ubuntu|Runtime\r\n <a name=\"cloudSchemes\"></a>spark.rapids.cloudSchemes|Comma separated list of additional URI schemes that are to be considered cloud based filesystems. Schemes already included: abfs, abfss, dbfs, gs, s3, s3a, s3n, wasbs. Cloud based stores generally would be total separate from the executors and likely have a higher I/O read cost. Many times the cloud filesystems also get better throughput when you have multiple readers in parallel. This is used with spark.rapids.sql.format.parquet.reader.type|None|Runtime\r\n+<a name=\"filecache.checkStale\"></a>spark.rapids.filecache.checkStale|Controls whether the cached is checked for being out of date with respect to the input file. When enabled, the data that has been cached locally for a file will be invalidated if the file is updated after being cached. This feature is only necessary if an input file for a Spark application can be changed during the lifetime of the application. If an individual input file will not be overwritten during the Spark application then performance may be improved by setting this to false.|false|Startup\r\n <a name=\"filecache.enabled\"></a>spark.rapids.filecache.enabled|Controls whether the caching of input files is enabled. When enabled, input datais cached to the same local directories configured for the Spark application. The cache will use up to half the available space by default. To set an absolute cache size limit, see the spark.rapids.filecache.maxBytes configuration setting. Currently only data from Parquet files are cached.|false|Startup\r\n <a name=\"filecache.maxBytes\"></a>spark.rapids.filecache.maxBytes|Controls the maximum amount of data that will be cached locally. If left unspecified, it will use half of the available disk space detected on startup for the configured Spark local disks.|None|Startup\r\n <a name=\"gpu.resourceName\"></a>spark.rapids.gpu.resourceName|The name of the Spark resource that represents a GPU that you want the plugin to use if using custom resources with Spark.|gpu|Startup\r\n ```\r\n ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8260/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}