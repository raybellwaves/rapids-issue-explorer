{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/2863",
    "id": 935805733,
    "node_id": "MDU6SXNzdWU5MzU4MDU3MzM=",
    "number": 2863,
    "title": "[TASK] Explore a pseudo repartition instead of sort for key grouped operations",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 2223784867,
            "node_id": "MDU6TGFiZWwyMjIzNzg0ODY3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P2",
            "name": "P2",
            "color": "8ff7b9",
            "default": false,
            "description": "Not required for release"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2021-07-02T14:08:51Z",
    "updated_at": "2022-05-11T20:47:57Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWe have a number of out of core algorithms, aggregations and joins, that we are working on that rely on sort when the data is too large to fit into a single batch.\r\n\r\nSorting data is rather slow. This is especially true when there are more than one key or the key is variable length. It would be nice to explore alternative algorithms that do not involve sorting the data in order to group similar keys into corresponding batches. For aggregations and many join types the data can be hash partitioned before being operated on. Spark does this to distribute the data to multiple tasks to process the data in parallel.  If we run into a situation where the data is too large to fit into memory after this initial partitioning we should look at running similar partition code again over the data to split it up into even smaller groups, which we can then operate on.  If needed this process could be repeated again if the sizes are still too large. The main drawback to this compared to sorting is that we need a way to know when to stop in the face of data skew. \r\n\r\nAs such it would be nice to do a prototype of this and see how the performance compares to sorting. If we see a lot of performance potential then we can look at working with cudf to figure out the right way to stop when there is a lot of data skew.\r\n\r\nWhen doing the performance comparison we should also pay attention to aggregations that are sort based in cudf. If the data is already sorted, then cudf will not need to sort it again. In those cases we might lose all of our gains from trying to avoid sorting.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2863/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}