{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/2439",
    "id": 894801898,
    "node_id": "MDU6SXNzdWU4OTQ4MDE4OTg=",
    "number": 2439,
    "title": "[FEA] Incorporate concurrency into CBO cost models",
    "user": {
        "login": "andygrove",
        "id": 934084,
        "node_id": "MDQ6VXNlcjkzNDA4NA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/934084?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/andygrove",
        "html_url": "https://github.com/andygrove",
        "followers_url": "https://api.github.com/users/andygrove/followers",
        "following_url": "https://api.github.com/users/andygrove/following{/other_user}",
        "gists_url": "https://api.github.com/users/andygrove/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/andygrove/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/andygrove/subscriptions",
        "organizations_url": "https://api.github.com/users/andygrove/orgs",
        "repos_url": "https://api.github.com/users/andygrove/repos",
        "events_url": "https://api.github.com/users/andygrove/events{/privacy}",
        "received_events_url": "https://api.github.com/users/andygrove/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2021-05-18T21:33:25Z",
    "updated_at": "2021-08-23T17:25:25Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nThe current cost models simply try and predict total compute time per operator and do not take concurrency into account. \r\n\r\nConcurrency can be very different between CPU and GPU. For example, we could be running in an environment with 64 CPU cores and 8 GPUs, where we support 64 concurrent tasks on CPU and 16 concurrent tasks on GPU (2 concurrent tasks per GPU). This is an oversimplification but we would need the GPU tasks to be 4x faster than CPU to see any benefit here.\r\n\r\n**Describe the solution you'd like**\r\nI think we need to break queries down into stages and tasks, predict the CPU vs GPU cost of each task and then model how the query would actually be executed, taking task concurrency into account.\r\n\r\nThis would allow us to predict wall-clock time for each task, stage, and the overall query.\r\n\r\nNote that there are two main levels of task concurrency. There is the Spark task concurrency based on configuration settings including `spark.executor.cores` and `spark.task.cpus` and then there is the GPU-level `spark.rapids.sql.concurrentGpuTasks`. Tasks can still be productive, performing I/O, while waiting on GPU semaphores.\r\n\r\nPotentially we could even leverage stage level scheduling to determine resources for a stage. If we are falling back to CPU for a stage we may want to increase CPU cores for that stage.\r\n\r\n**Describe alternatives you've considered**\r\nNone.\r\n\r\n**Additional context**\r\nNone.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2439/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}