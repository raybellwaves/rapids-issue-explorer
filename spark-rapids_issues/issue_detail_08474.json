{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8474",
    "id": 1736966426,
    "node_id": "I_kwDOD7z77c5niAEa",
    "number": 8474,
    "title": "[BUG] significant slow down with VectorUDT and ParquetCachedBatchSerializer",
    "user": {
        "login": "eordentlich",
        "id": 36281329,
        "node_id": "MDQ6VXNlcjM2MjgxMzI5",
        "avatar_url": "https://avatars.githubusercontent.com/u/36281329?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/eordentlich",
        "html_url": "https://github.com/eordentlich",
        "followers_url": "https://api.github.com/users/eordentlich/followers",
        "following_url": "https://api.github.com/users/eordentlich/following{/other_user}",
        "gists_url": "https://api.github.com/users/eordentlich/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/eordentlich/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/eordentlich/subscriptions",
        "organizations_url": "https://api.github.com/users/eordentlich/orgs",
        "repos_url": "https://api.github.com/users/eordentlich/repos",
        "events_url": "https://api.github.com/users/eordentlich/events{/privacy}",
        "received_events_url": "https://api.github.com/users/eordentlich/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 9,
    "created_at": "2023-06-01T19:40:14Z",
    "updated_at": "2023-06-06T20:17:05Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nThe evaluation step in this example notebook: https://github.com/NVIDIA/spark-rapids-examples/blob/main/examples/XGBoost-Examples/mortgage/notebooks/python/MortgageETL%2BXGBoost.ipynb?short_path=f801328#L1035\r\nis about 600x slower when `ParquetCachedBatchSerializer` is enabled.\r\nThe `result` DataFrame being processed here has several columns of type VectorUDT.   If these columns are either dropped or converted to array type using `psypark.ml.functions.vector_to_array` before cacheing and then converted back upon read using `array_to_vector`, the slow down can be avoided.    This indicates that `ParquetCachedBatchSerializer` has an issue with processing `VectorUDT` columns on the read side.\r\n\r\n**Steps/Code to reproduce bug**\r\nRun the above example notebook with `ParquetCachedBatchSerializer` enabled.\r\n\r\n**Expected behavior**\r\nNo 600x slow down.\r\n\r\n**Environment details (please complete the following information)**\r\n Standalone.\r\n```\r\npyspark                             \\\r\n--master ${SPARK_URL}            \\\r\n--jars ${PWD}/rapids-4-spark_2.12-23.04.0.jar \\\r\n--conf spark.plugins=com.nvidia.spark.SQLPlugin \\\r\n--conf spark.rapids.memory.gpu.allocFraction=0.5 --conf spark.rapids.memory.pinnedPool.size=2g \\\r\n--conf spark.executor.resource.gpu.amount=1 \\\r\n--conf spark.executor.cores=1 \\\r\n--conf spark.task.resource.gpu.amount=1 \\\r\n--conf spark.sql.execution.arrow.maxRecordsPerBatch=200000 \\\r\n--conf spark.executor.resource.gpu.discoveryScript=./getGpusResources.sh \\\r\n--files $SPARK_HOME/examples/src/main/scripts/getGpusResources.sh --conf spark.executor.memory=10g --conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer\r\n```\r\n\r\n**Additional context**\r\nRelated issue, with likely same underlying problem:  https://github.com/NVIDIA/spark-rapids/issues/5975\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}