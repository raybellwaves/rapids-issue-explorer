{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10113",
    "id": 2058742281,
    "node_id": "I_kwDOD7z77c56teoJ",
    "number": 10113,
    "title": "[FEA] figure out how to do collect_set in a window operation.",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-12-28T19:05:27Z",
    "updated_at": "2024-01-03T19:22:26Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n`collect_list` and `collect_set` are really not good from a reliability standpoint when run in the context of a window operation. Because of this I filed https://github.com/NVIDIA/spark-rapids/issues/10110 to disable them by default. But long term we need to come up with a much better solution to the problem.\r\n\r\n`collect_list` has some fundamental problems that make it very hard to work with. Which is why I filed  https://github.com/NVIDIA/spark-rapids/issues/10111 for it to be handled separately.\r\n\r\nIn some ways `collect_set` is better because in the common case the cardinality of the values being collected are likely to be small. But in the worst case it is exactly the same as `collect_list` in terms of size.\r\n\r\nSo we could try and do the same things proposed in https://github.com/NVIDIA/spark-rapids/issues/10111, with keeping the dedupe stage separate.\r\n\r\nWe could also look at making changes to how CUDF implements `collect_set` to make the common case much more memory efficient. Right now `collect_set` is implemented in terms of `collect_list` with a sort/dedupe stage. This works but makes the memory usage just as bad as `collect_list` in all cases. I am not sure what we could do to make it better though. The CPU essentially allocates a set per row and recalculates everything as it goes. This is not likely to work for the GPU because we cannot do dynamic memory allocation from the GPU itself, which means we would need to allocate memory for a set per row up front. I think there are things we could do with this by doing a distinct count on the entire column in the table and using it as an estimate to decide what to do next, but we are going to need to sit down with some cuda experts to see what we can come up with.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10113/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}