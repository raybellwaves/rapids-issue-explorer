{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5650",
    "id": 1249259121,
    "node_id": "I_kwDOD7z77c5Kdi5x",
    "number": 5650,
    "title": "What's the update of RapidsShuffleManager to resolve the bottleneck for waiting to acquire the semaphore",
    "user": {
        "login": "cfangplus",
        "id": 42287875,
        "node_id": "MDQ6VXNlcjQyMjg3ODc1",
        "avatar_url": "https://avatars.githubusercontent.com/u/42287875?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/cfangplus",
        "html_url": "https://github.com/cfangplus",
        "followers_url": "https://api.github.com/users/cfangplus/followers",
        "following_url": "https://api.github.com/users/cfangplus/following{/other_user}",
        "gists_url": "https://api.github.com/users/cfangplus/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/cfangplus/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/cfangplus/subscriptions",
        "organizations_url": "https://api.github.com/users/cfangplus/orgs",
        "repos_url": "https://api.github.com/users/cfangplus/repos",
        "events_url": "https://api.github.com/users/cfangplus/events{/privacy}",
        "received_events_url": "https://api.github.com/users/cfangplus/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735893,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODkz",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/question",
            "name": "question",
            "color": "d876e3",
            "default": true,
            "description": "Further information is requested"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 8,
    "created_at": "2022-05-26T08:22:42Z",
    "updated_at": "2022-06-13T14:42:49Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "The main bottleneck during this portion of the query is waiting to acquire the semaphore.  However the tasks owning the semaphore are not making full use of the GPU.  The main portion of time they are spending is decompressing shuffle data on the CPU and copying it down to the GPU.  They need to own the GPU semaphore during this phase because they are placing data onto the GPU.  The whole point of the GPU semaphore is to prevent too many tasks from placing data onto the GPU at the same time and exhausting the GPU's memory.\r\n\r\nEssentially the main bottleneck in that stage is dealing with the shuffle data and transfer to the GPU, because that's what's taking so long for the tasks holding the GPU semaphore to release it.  Once the shuffle data is loaded on the GPU, the rest of the stage processing is quite fast.  The RapidsShuffleManager was designed explicitly to target this shuffle problem, as it tries to keep shuffle targets in GPU memory and not rely on the CPU for compression/decompression which can be a bottleneck.  Unfortunately there are a number of issues with RapidsShuffleManager that prevent it from working well in all situations, but we're actively working on improving it.  Our goal is to eventually have that shuffle manager be the preferred shuffle when using the RAPIDS Accelerator, even if the cluster does not have RDMA-capable hardware.\r\n\r\n> if i use 2 gpu like T4, Performance will be significantly improved ?\r\n\r\nIf you add more GPUs (and thus more executors, since an executor with the plugin can only control 1 GPU), yes, performance should be improved to a point.  This would be similar to adding executors to a CPU job.  If you have enough GPUs in your cluster so that CPU_cores_per_executor == concurrent_GPU_tasks then no task will ever wait on the GPU semaphore.\r\n\r\n_Originally posted by @jlowe in https://github.com/NVIDIA/spark-rapids/discussions/5394#discussioncomment-2658194_",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}