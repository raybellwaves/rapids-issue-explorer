{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/102",
    "id": 629479197,
    "node_id": "MDU6SXNzdWU2Mjk0NzkxOTc=",
    "number": 102,
    "title": "[FEA] pyspark limit integration tests",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2061735887,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/good%20first%20issue",
            "name": "good first issue",
            "color": "7057ff",
            "default": true,
            "description": "Good for newcomers"
        },
        {
            "id": 2094499852,
            "node_id": "MDU6TGFiZWwyMDk0NDk5ODUy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/SQL",
            "name": "SQL",
            "color": "8f4df9",
            "default": false,
            "description": "part of the SQL/Dataframe plugin"
        },
        {
            "id": 2094874947,
            "node_id": "MDU6TGFiZWwyMDk0ODc0OTQ3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/test",
            "name": "test",
            "color": "60d6d4",
            "default": false,
            "description": "Only impacts tests"
        },
        {
            "id": 2223784867,
            "node_id": "MDU6TGFiZWwyMjIzNzg0ODY3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P2",
            "name": "P2",
            "color": "8ff7b9",
            "default": false,
            "description": "Not required for release"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2020-06-02T20:06:30Z",
    "updated_at": "2022-07-26T06:48:08Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWe need some integration tests to really work on limit.  The issue is that limit has a lot of very specialized optimizations, and is not deterministic in all cases.\r\n\r\nSorting the data before we do a limit can result in a TakeOrderedAndProject, which we don't support on the GPU yet, but other types of limits are not guaranteed to produce a consistent result.  Because of this we may need to play some games with the query, like to reparation it to a single partition and then sort within the partition before doing the limit.  Either way we need to look at the plans and verify that we are covering the options that we expect.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/102/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}