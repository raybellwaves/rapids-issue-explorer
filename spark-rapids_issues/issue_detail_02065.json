{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/2065",
    "id": 848682463,
    "node_id": "MDU6SXNzdWU4NDg2ODI0NjM=",
    "number": 2065,
    "title": "[BUG] fall back to CPU if columnNameofCorruptRecord is in the CSV schema",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2223784586,
            "node_id": "MDU6TGFiZWwyMjIzNzg0NTg2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P0",
            "name": "P0",
            "color": "d93f0b",
            "default": false,
            "description": "Must have for release"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2021-04-01T18:12:48Z",
    "updated_at": "2022-04-12T21:55:25Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nSpark has this option to deal with parsing bad data.  It is kind of convoluted, but there is a config where you can set the name of a column that will deal with corrupt data.  Then if spark sees this column name appear in the schema for the CSV data being read Spark will place anything that it thinks is corrupt data in that string column.  I don't see a lot of value in having our code support this, but we should fall back to the CPU if we see it.\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/2065/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}