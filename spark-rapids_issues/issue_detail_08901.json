{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8901",
    "id": 1831303023,
    "node_id": "I_kwDOD7z77c5tJ3dv",
    "number": 8901,
    "title": "[FEA] Stress test UCX shuffle after Host memory limits are in place",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094874947,
            "node_id": "MDU6TGFiZWwyMDk0ODc0OTQ3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/test",
            "name": "test",
            "color": "60d6d4",
            "default": false,
            "description": "Only impacts tests"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-08-01T13:33:50Z",
    "updated_at": "2023-08-08T20:37:38Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI don't think we need to do anything special to support UCX shuffle to limit host memory. It should really only be on the GPU or when spilling.  But even with the spilling it uses bounce buffers for CPU data transfers. The problem here really comes down to memory pressure and thundering herds. Shuffle is by definition a thundering herd. If executor A has a very large shuffle buffer that we need to send to executor B, and B has a similarly large one to send to A. There could be a situation where both A and B have already read in the host memory buffer so that they can send the data to the other, and then A and B both try to allocate a device buffer to receive it. This causes the spill to kick in, but we cannot spill because A and B are holding onto too much host memory for the spill to allocate the full buffer. This could result in a deadlock. There are two ways to work around this, and I think we need to implement both of them.\r\n\r\nFirst we want to be sure that any receiving allocation happens before the sending data is brought into memory. When the send data is brought into memory it is copied to the bounce buffer and then made spillable again. That should be enough to prevent a deadlock. It would be great to also add in some better APIs so that we don\u2019t necessarily have to read back in all of the data from disk if we are going to a bounce buffer, and are not likely to touch some of the data ever again. But that is an optimization.\r\n\r\nI think in practice the order of operations for UCX + Spill is happening today, but we need to be sure of that and we need to have it documented that it is not optional.\r\n\r\nThis is to really stress test UCX shuffle and see that we are not in a case where we can get deadlocks.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8901/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}