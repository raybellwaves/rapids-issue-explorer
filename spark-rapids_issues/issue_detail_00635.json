{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/635",
    "id": 689964999,
    "node_id": "MDU6SXNzdWU2ODk5NjQ5OTk=",
    "number": 635,
    "title": "[FEA] Dynamic spark.rapids.sql.concurrentGpuTasks and spark.rapids.sql.batchSizeBytes",
    "user": {
        "login": "JustPlay",
        "id": 5866501,
        "node_id": "MDQ6VXNlcjU4NjY1MDE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5866501?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/JustPlay",
        "html_url": "https://github.com/JustPlay",
        "followers_url": "https://api.github.com/users/JustPlay/followers",
        "following_url": "https://api.github.com/users/JustPlay/following{/other_user}",
        "gists_url": "https://api.github.com/users/JustPlay/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/JustPlay/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/JustPlay/subscriptions",
        "organizations_url": "https://api.github.com/users/JustPlay/orgs",
        "repos_url": "https://api.github.com/users/JustPlay/repos",
        "events_url": "https://api.github.com/users/JustPlay/events{/privacy}",
        "received_events_url": "https://api.github.com/users/JustPlay/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 8,
    "created_at": "2020-09-01T09:03:15Z",
    "updated_at": "2020-09-06T09:38:42Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\n(A).    In the current spark-rapids (0.1 and 0.2), `spark.rapids.sql.concurrentGpuTasks` and `spark.rapids.sql.batchSizeBytes` are static and across through the whole query (and may even across multi-query in some benchmark workload), and what's more, it's  difficult for end-user to find a good setting values (particular when the user had to run multiple queries with very different runtime patten)\r\n(B).    Radical values for those two conf may lead to OOM\uff0cbut conservative values will put the GPU into an inefficient state (low efficiency)\r\n(C).    Different query, even different query-stage in the same query have very different runtime pattern, so we need different `concurrent` and `batch-size` setting. but we can't change by using the `--conf` command line options (e.g. when we running TPC-DS benchmark, we can only give a fixed set of command line --conf when we submit the job with 103 queries)\r\n(D).   So, rapids need the ability of `dynamically change those two options at runtime (transparent to users)`, which i call it `Dynamic spark.rapids.sql.concurrentGpuTasks and spark.rapids.sql.batchSizeBytes`\r\n\r\n**Describe the solution you'd like**\r\nFor `spark.rapids.sql.batchSizeBytes`, I think we can rely on the AQE framework, for different query-stage, we set different value to `spark.rapids.sql.batchSizeBytes` based on the statistical information from the upstream stage (for this I do not have more detailed idea, But i think AQE is a good start)\r\n\r\nFor `spark.rapids.sql.concurrentGpuTasks`, I think we can do  some thing by extend the `GpuSemaphore`\r\n1. when a task want to acquire gpu\uff0cwe can dynamic judge whether it can be allowed to use the gpu based-on the gpu memory pressure (i.e. total memory, free memory, and the number of tasks currently running on gpu or use some history information)\r\n2. when we judge that the free gpu memory is enough, we  then put the task on gpu, if not, we put the task into a `wait-list`\r\n3. when other task release gpu or exit, we weak up one or more task that waiting in the wait-list\r\n\r\nBy using `dymamic concurrentGpuTasks`\uff0c the user can give a bigger concurrent setting for better perf for most query\uff0cand for some corner query that will OOM when concurrent is bigger\uff0crapids will dynamic limit the concurrent to minimum the OOM risc\r\n\r\nBased on my understanding of the rapids and rmm code , three thing need to be done:\r\n1. impl the get_mem_info (or other API) in RMM's pool_memory_resource\r\n2. add some code in the RMM JNI code in rapids \r\n3. extends rapids's GpuSemaphore (add some counter\uff0cadd a judge logic based on memory pressure\uff0cadd a wait-list)\r\n\r\nI think the introduced overhead will be negligible\uff0cbecause we can let the task first acquire the gpu-semaphore then do the judgement\r\n\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context, code examples, or references to existing implementations about the feature request here.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/635/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}