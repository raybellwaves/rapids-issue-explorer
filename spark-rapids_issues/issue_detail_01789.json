{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/1789",
    "id": 813783326,
    "node_id": "MDU6SXNzdWU4MTM3ODMzMjY=",
    "number": 1789,
    "title": "[FEA] Support batched window operations",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2589743725,
            "node_id": "MDU6TGFiZWwyNTg5NzQzNzI1",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/epic",
            "name": "epic",
            "color": "0E8A16",
            "default": false,
            "description": "Issue that encompasses a significant feature or body of work"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2021-02-22T19:12:02Z",
    "updated_at": "2022-02-01T17:40:47Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThis is a general request based off the specific issues seen in #1642 \r\n\r\nCurrently window operations on the GPU require all of the data for a partition to be in a single GPU batch.  This limits the amount of data we can process because of limitations on the size of GPU batches.  This is actually not a requirement, but it made implementing the operator much simpler.\r\n\r\nWe should implement window processing in such a way that we don't have to have all of the data on the GPU at once, only what is required.\r\n\r\n**Describe the solution you'd like**\r\nThere really are two different types of window operations that are defined by how the window is specified.\r\n\r\nRow based window operations (look at a window that is N rows before the current row and M rows after it) \r\nRange based window operations (look at window made up of all rows there the order by key is between the current rows value - X and the current row's value + Y)\r\n\r\nRow based window operations also have an important sub-variant for rank operations which I will explain more on later.\r\n\r\nWindow operations are typically partitioned, and as such in the worst case we need to have all of the data for a single partition in memory at once. If there are many groups in the partition by columns we could look at the partition by columns for the last row and do a lower bounds check to figure out how much of the batch is complete and can be processed.  We then can wait until we have the rest of that group to process it. But in many cases we can actually do better than that, and in cases like #1642 there are only 3 groups total and we can run out of memory even in a single group.\r\n\r\nUltimately what we need is information about how much of the data (row number) is done and can be output as complete and how much of the data needs to be retained for the next batch so we can use it to get the correct answer for other rows.  In the worst case, UNBOUNDED preceding or UNBOUNDED following, this would end up splitting the data on group/partition by boundaries.\r\n\r\nFor row based queries we can calculate this ourselves.  If we look at the partition by keys for the last row in a batch we can then do a lower bound on it to find the starting point for the window.  We then add the number of rows following to that row to know where the cutoff would be for output (we have all the needed data for processing).  We then could subtract the number of rows preceding from the completed index to know how much data we have to keep around for the next batch.  In either case if it is unbounded then we should fall back to chunk it by the group by/partition by keys.\r\n\r\nFor rank queries we can do even better. Rank queries include `rank`, `dense_range` and `row_number`.  In this case we don't need all of the data from a window we just need the last row of the previous batch.  With that we can add any offsets needed to the new batch to pick up where we left off. For example a `row_number` query would only need the partition by keys and the final row number from the previous batch. After the new batch finishes processing it would look for all rows that had the same partition by keys in them (are a part of the same group) and then add the row_number from the last row of the previous batch to it.  Rank and dense rank can do similar tricks to make it work. This is important because technically these queries have a window that is unlimited preceding and 0 following. Without this optimization we would still need all of the data for a single group to be in memory at once.\r\n\r\nFor range based queries we will need help from cudf to be able to support this. We would need to be able to get back from running a window query the same information.  What row number has completed processing and has all of the data needed to produce a correct answer and what row number on needs to be retained from the current batch so we can calculate the correct results on the next batch.  Technically we cannot know the row number needed to be kept because we don't know what value will be in the next row, but we do know that it cannot be smaller than the current value, so we can keep enough data to support the last row in the current batch.\r\n\r\nWe probably want to maintain special cases for unlimited preceding/following where we just fall back to splitting on group boundaries because we know up front that processing partial groups is a waste of time.\r\n\r\n**Describe alternatives you've considered**\r\nThere really isn't another option if we want to make this work.\r\n\r\n**Additional context**\r\nWe could split this up into separate optimization.\r\n\r\n- [x]  Chunking/Splitting by partition keys. #1856\r\n- [x] Rank based optimization when these are all of the queries in an operation. #1859 \r\n- [ ] Row based optimization when there are no unlimited window bounds. #1860\r\n- [ ] Range based optimization when there are no unlimited window bounds and once cudf can support this. ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}