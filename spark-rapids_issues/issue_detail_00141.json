{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/141",
    "id": 636190303,
    "node_id": "MDU6SXNzdWU2MzYxOTAzMDM=",
    "number": 141,
    "title": "[FEA] Need reader tests for special case parquet files",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094499852,
            "node_id": "MDU6TGFiZWwyMDk0NDk5ODUy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/SQL",
            "name": "SQL",
            "color": "8f4df9",
            "default": false,
            "description": "part of the SQL/Dataframe plugin"
        },
        {
            "id": 2223784776,
            "node_id": "MDU6TGFiZWwyMjIzNzg0Nzc2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P1",
            "name": "P1",
            "color": "fbca04",
            "default": false,
            "description": "Nice to have for release"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2020-06-10T12:03:58Z",
    "updated_at": "2020-07-22T21:42:19Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThere are a number of parquet read configs for interoperability with other environments.  We need to find some of these files and test that the reader works properly with these.  Some of these we cannot support right now and we should disable the plugin for these if we cannot support a config.\r\n\r\n`PARQUET_BINARY_AS_STRING`\r\n`spark.sql.parquet.binaryAsString`\r\nSome other Parquet-producing systems, in particular Impala and older versions of \r\nSpark SQL, do not differentiate between binary data and strings when writing out the \r\nParquet schema. This flag tells Spark SQL to interpret binary data as a string to provide \r\ncompatibility with these systems.\r\n\r\n`PARQUET_INT96_AS_TIMESTAMP`\r\n`spark.sql.parquet.int96AsTimestamp`\r\nSome Parquet-producing systems, in particular Impala, store Timestamp into INT96. \r\nSpark would also store Timestamp as INT96 because we need to avoid precision lost of the \r\nnanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to \r\nprovide compatibility with these systems.\r\n\r\n`PARQUET_INT96_TIMESTAMP_CONVERSION`\r\n`spark.sql.parquet.int96TimestampConversion`\r\nThis controls whether timestamp adjustments should be applied to INT96 data when \r\nconverting to timestamps, for data written by Impala.  This is necessary because Impala \r\nstores INT96 data with a different timezone offset than Hive & Spark.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/141/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}