{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/620",
    "id": 687718002,
    "node_id": "MDU6SXNzdWU2ODc3MTgwMDI=",
    "number": 620,
    "title": "[BUG] Correct fix for partial-only HashAggregate with multiple distinct and non-distinct functions",
    "user": {
        "login": "nartal1",
        "id": 50492963,
        "node_id": "MDQ6VXNlcjUwNDkyOTYz",
        "avatar_url": "https://avatars.githubusercontent.com/u/50492963?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/nartal1",
        "html_url": "https://github.com/nartal1",
        "followers_url": "https://api.github.com/users/nartal1/followers",
        "following_url": "https://api.github.com/users/nartal1/following{/other_user}",
        "gists_url": "https://api.github.com/users/nartal1/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/nartal1/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/nartal1/subscriptions",
        "organizations_url": "https://api.github.com/users/nartal1/orgs",
        "repos_url": "https://api.github.com/users/nartal1/repos",
        "events_url": "https://api.github.com/users/nartal1/events{/privacy}",
        "received_events_url": "https://api.github.com/users/nartal1/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2020-08-28T05:12:45Z",
    "updated_at": "2020-09-04T17:14:32Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "\r\n**Describe the bug**\r\nThis is follow on issue of https://github.com/NVIDIA/spark-rapids/issues/153. We currently closed that issue by falling back to CPU for that case. This issue is to figure out proper fix where we can run that case in GPU. \r\n\r\n**Steps/Code to reproduce bug**\r\nPlease provide a list of steps or a code sample to reproduce the issue.\r\nComment out this section of the code in  tagPlanforGpu() in aggregate.scala file to reproduce the bug(disable falling back to CPU).\r\n```\r\nhashAggReplaceMode match {\r\n      case \"partial\" => {\r\n        // In partial mode, if there are non distinct functions and multiple distinct functions\r\n        // non distinct functions are computed using First operator. The final result would be\r\n        // incorrect for non distinct functions for partition size > 1. Reason for this is - if the\r\n        // first batch computed and sent to CPU doesn't contain all the rows required to compute\r\n        // non distinct function(s), then Spark would consider that value as final result(due to\r\n        // First)Falling back to CPU for this special case.\r\n        if (agg.aggregateExpressions.exists(e => e.aggregateFunction.isInstanceOf[First])) {\r\n          val count = agg.aggregateExpressions.flatMap(expr =>\r\n            expr.children.flatMap {\r\n              _.collect {\r\n                case a: Count => a\r\n                case b: Count => b\r\n              }\r\n            })\r\n          if (count.size > 1) {\r\n            willNotWorkOnGpu(\"Aggregate of non distinct functions with multiple distinct \" +\r\n              \"functions is non deterministic for non distinct functions as it is computed using \" +\r\n              \"First.\")\r\n          }\r\n        }\r\n      }\r\n      case _ =>\r\n    }\r\n```\r\n and run this test in hash_aggregate.py:\r\n```\r\n@approximate_float\r\n@ignore_order\r\n@incompat\r\n@pytest.mark.parametrize('data_gen', _init_list_no_nans, ids=idfn)\r\n@pytest.mark.parametrize('conf', get_params(_confs, params_markers_for_confs), ids=idfn)\r\ndef test_hash_multiple_mode_query(data_gen, conf):\r\n    print_params(data_gen)\r\n    assert_gpu_and_cpu_are_equal_collect(\r\n        lambda spark: debug_df(gen_df(spark, data_gen, length=100)\r\n            .groupby('a')\r\n            .agg(f.count('a'),\r\n                 f.avg('b'),\r\n                 f.avg('a'),\r\n                 f.countDistinct('b'),\r\n                 f.sum('a'),\r\n                 f.min('a'),\r\n                 f.max('a'),\r\n                 f.countDistinct('c')\r\n                )), conf=conf)\r\n```\r\n\r\n**Expected behavior**\r\nCorrect result should be produced for non-distinct functions in this case. \r\n```\r\nGpu Count = 1,\r\nCurrently it is : Gpu Count = 0\r\n```\r\n\r\n**Additional context**\r\nCurrently while running on GPU, if the first batch doesn't contain all the rows(gid's corresponding to non distinct function) , then incorrect output is produced for non-distincts. Reason being when there are non-distinct and multiple distincts, Spark computes non-distinct functions using `First` operator. In case of GPU computation in ColumnarBatch, if the first batch which does the computation doesn't contain all the rows required to compute non distinct function,  then Spark considers that as the final result(due to `First`) and produces incorrect result. `First` is non-deterministic in this case. \r\nThings to do in this issue:\r\n1) Try to repro the case in CPU if possible. If not, then figure out how Spark is producing correct result despite `First` being non-deterministic.\r\n2) Look for options to fix based on Step 1. \r\n One of the solution is to consider concating all batches before doing a `First`.\r\n \r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/620/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}