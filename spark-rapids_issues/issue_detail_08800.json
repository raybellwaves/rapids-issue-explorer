{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8800",
    "id": 1820550560,
    "node_id": "I_kwDOD7z77c5sg2Wg",
    "number": 8800,
    "title": "[FEA] Remove batch size row and byte limits for parquet, and always use the chunked reader",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 3444490915,
            "node_id": "LA_kwDOD7z77c7NTsKj",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/tech%20debt",
            "name": "tech debt",
            "color": "0052cc",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-07-25T15:05:55Z",
    "updated_at": "2023-07-25T20:49:16Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "`spark.rapids.sql.reader.batchSizeRows` and `spark.rapids.sql.reader.batchSizeBytes` were our first attempt to avoid going over size limits for CUDF when reading data from input files. We now have a chunked reader in parquet and I am not sure that there is any value in having these limits, especially as configs, any more. It might be good to update things to not use them when we have the chunked reader enabled. And while we are at it, should we remove the config for chunked reader and in the future add some temporary configs that are file format specific when adding in a new chunked reader? The parquet chunked reader appears to be stable, and performant now. I don't see much reason to have the config anymore.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8800/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}