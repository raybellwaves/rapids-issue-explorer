{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8391",
    "id": 1724778431,
    "node_id": "I_kwDOD7z77c5mzge_",
    "number": 8391,
    "title": "[FEA] Do a hash based re-partition instead of a sort based fallback for hash aggregate",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-05-24T21:45:27Z",
    "updated_at": "2023-05-31T18:17:06Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently if the intermediate data for a hash aggregate is too large we fall back to sorting the data, splitting it on key boundaries and then merging the intermediate results.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/9d9d491267c49c91c013f424dc6af1a62327360d/sql-plugin/src/main/scala/com/nvidia/spark/rapids/aggregate.scala#L745-L772\r\n\r\nThis works, but we know that sort is very expensive, especially compared to doing hash partitioning.\r\n\r\nhttps://github.com/NVIDIA/spark-rapids/issues/8390 was filed to see if we can find a cheaper way to avoid spilling as much memory as we build up batches, and also as a way to avoid doing the sort/key partitioning.\r\n\r\nThis is here to try and replace the sort/key partitioning entirely, and may even replace some of #8390 assuming that it works out well.\r\n\r\n**Describe the solution you'd like**\r\nI would like a patch that replaces the `outOfCoreIter` and `keyBatchingIter` with something closer to what happens in the `GpuSubPartitionHashJoin` \r\n\r\nhttps://github.com/NVIDIA/spark-rapids/blob/branch-23.06/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/execution/GpuSubPartitionHashJoin.scala\r\n\r\nThe idea would be to partition each intermediate batch using a hash seed that is different from the one used to partition the data in a hash partitioning. Unlike join I think we could use a different partitioning implementation because we don't need to join keys of possibly different types together, but it might be nice to keep the code common if possible.\r\n\r\nI was thinking that as soon as we see intermediate batches are larger than the target batch size, and we are not going to just output the data without a merge because of #7404, then we start to re-partition the intermediate results. \r\n\r\nIn the first version of this we do it just like join. We do one pass through the data and partition it 16 ways.  After that first pass we can combine small partitions together and process them to produce a result that we can output.  If there are any partitions larger than the target batch size we can split them further with a heuristic that should be fairly accurate based off of the size of the data, but again it should use a different seed that the previous passes. Then we can merge them and output them from the second pass.\r\n\r\nIf #8390 looks good it might me worth thinking about playing some games with merging intermediate results after we do the partitioning instead of doing a second pass at partitioning, but at that point we likely have 4 GiB of data cached (512 MiB per partition with 16 partitions) and I don't know how likely all of that has spilled, and if it has spilled is the cost to read back in that data is worth the savings in merging it.\r\n\r\nFor now I would say just stick with something simple and then we can modify it as we see needs.\r\n\r\nAs for testing I want to see a number of different situations that we test from both a performance and a reliability standpoint.  All of these would be from the standpoint of a lot of data going into a small number of tasks. Like we had way too few shuffle partitions for the size of the data.  A lot of this is going to really be about the cardinality and ordering of the grouping keys.\r\n\r\nI want to see what happens when the key's\r\n    1. cardinality is high and is highly grouped (the data is almost sorted by the key, should get good combining initially, but not after a first pass) \r\n    2. cardinality is high and is randomly distributed (should get almost no combining in the partial)\r\n    3. cardinality is low and is highly grouped\r\n    4. cardinality is low and is randomly distributed\r\n    5. cardinality is medium and is high grouped\r\n    6. cardinality is medium and is randomly distributed\r\n\r\nBy high cardinality I mean we each key shows up 2 to 3 times in the entire dataset, for medium 200 to 300 times, and for low 20,000 to 30,000 times.  But we want enough data that a single task cannot hold all of it in GPU memory. At least a few hundred GiB of data.\r\n\r\nWith this I would also like to see how it behaves with large numbers of aggregations. That can increase the size of the input data by adding lots of new columns, but also when combined with https://github.com/NVIDIA/spark-rapids/issues/8382 we might get lots of batches with relatively few rows in each.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8391/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}