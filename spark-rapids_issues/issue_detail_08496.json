{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8496",
    "id": 1742166969,
    "node_id": "I_kwDOD7z77c5n11u5",
    "number": 8496,
    "title": "[FEA] Look into supporting GPU caching of UDT",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-06-05T16:54:19Z",
    "updated_at": "2023-06-06T20:24:48Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThis is a follow on issue to https://github.com/NVIDIA/spark-rapids/pull/8495 and https://github.com/NVIDIA/spark-rapids/issues/8474\r\n\r\nI used a modified version of the repro case in #8474 \r\n\r\n```\r\n#!/bin/env python\r\nfrom pyspark.conf import SparkConf\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import rand, element_at, sum, col\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.functions import vector_to_array\r\nimport timeit\r\n\r\nconf = SparkConf()\r\nconf.setAppName(\"Cache Benchmark\")\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\n\r\nspark.sparkContext.setLogLevel(\"WARN\")\r\n\r\n#df = spark.range(20000000)\r\ndf = spark.range(100000000)\r\ndf = df.select(rand(0).alias(\"r0\"),rand(1).alias(\"r1\"))\r\n\r\ndf_vec = VectorAssembler(inputCols=[\"r0\",\"r1\"],outputCol=\"vec\").transform(df).drop(\"r0\",\"r1\")\r\nt = timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\nprint(f\"NO CACHE COLD UDT: {t}\")\r\nt = timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\nprint(f\"NO CACHE WARM UDT: {t}\")\r\ndf_vec.cache()\r\nt = timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\nprint(f\"POPULATE CACHE UDT: {t}\")\r\nt = timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(\"vec\"), 1))).collect()), number=1)\r\nprint(f\"USE CACHE UDT: {t}\")\r\n\r\n\r\nprint()\r\ndf_arr = VectorAssembler(inputCols=[\"r0\",\"r1\"],outputCol=\"vec\").transform(df).drop(\"r0\",\"r1\").select(vector_to_array(\"vec\").alias(\"arr\"))\r\nt = timeit.timeit(lambda: print(df_arr.select(sum(element_at(col(\"arr\"), 1))).collect()), number=1)\r\nprint(f\"NO CACHE ARR: {t}\")\r\ndf_arr.cache()\r\nt = timeit.timeit(lambda: print(df_arr.select(sum(element_at(col(\"arr\"), 1))).collect()), number=1)\r\nprint(f\"POPULATE CACHE ARR: {t}\")\r\nt = timeit.timeit(lambda: print(df_arr.select(sum(element_at(col(\"arr\"), 1))).collect()), number=1)\r\nprint(f\"USE CACHE ARR: {t}\")\r\n```\r\n\r\nI tested the default serializer vs PCBS and GPU enabled vs CPU only.  The really interresting part was the time to read the data out of the cache. The GPU is really fast when reading PCBS data (I am using the second set of tests where the data is first transformed to an array before it is cached, because for the others we are falling back to the CPU for the processing).\r\n\r\n|Run on Cached Data|GPU|CPU|\r\n|-|-|-|\r\n|**Default**| 1.754 (fell back to CPU) | 0.560 |\r\n|**PCBS**| 0.357 | 4.221 |\r\n\r\nHere the GPU doing PCBS is 11.8x faster than the CPU when using PCBS and 1.7x faster than the CPU using the Default serializer (Which is actually just copying the data because it does not try to compress nested data).  So if we can automatically transform the UDTs to SQL types (something that the Default serializer has to do anyways) we should have performance that is close to that of the Default serializer for reads.  Which it is hard to beat noop.\r\n\r\nWrites are harder to isolate because of everything else that is happening, but the numbers for populating the cache are.\r\n\r\n|Populate Cache UDT | GPU (but falls back to CPU for caching) | CPU |\r\n|-|-|-|\r\n|**Default** | 12.036 | 11.613 |\r\n|**PCBS** | 25.717 | 24.889 |\r\n\r\n|Populate Cache Array | GPU | CPU |\r\n|-|-|-|\r\n|**Default** | 12.605 (fell back to CPU) | 11.906 |\r\n|**PCBS** | 9.971 | 17.286 |\r\n\r\nBut it is clear that the fastest implementation is still PCBS on the GPU. We could probably save a few seconds over the default implementation on the CPU by doing the transformation from a UDT to SQL on the CPU and then caching the data on the GPU.\r\n\r\nThis would also be a very interesting experiment to see what it would take to support UDT in the plugin. Not doing any processing of the data on the GPU, but instead allowing these to go along for the ride in joins/etc.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8496/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}