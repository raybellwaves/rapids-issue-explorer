{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/5796",
    "id": 1265411945,
    "node_id": "I_kwDOD7z77c5LbKdp",
    "number": 5796,
    "title": "[BUG] RapidsShuffleManager and external packages for Spark Standalone",
    "user": {
        "login": "gerashegalov",
        "id": 3187938,
        "node_id": "MDQ6VXNlcjMxODc5Mzg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gerashegalov",
        "html_url": "https://github.com/gerashegalov",
        "followers_url": "https://api.github.com/users/gerashegalov/followers",
        "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
        "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
        "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
        "repos_url": "https://api.github.com/users/gerashegalov/repos",
        "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2096543664,
            "node_id": "MDU6TGFiZWwyMDk2NTQzNjY0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/shuffle",
            "name": "shuffle",
            "color": "67fc73",
            "default": false,
            "description": "things that impact the shuffle plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2022-06-08T23:24:11Z",
    "updated_at": "2022-07-12T23:20:12Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nDocument all the caveats regarding the Plugin jar deployment. \r\n\r\nProbably due to a Spark bug it looks like one cannot use --jars to specify a package from which we intend to use the shuffle manager. See if we need to contribute a fix to upstream Apache Spark.\r\n\r\nOn the other hand we face issues when an external package such as spark-avro is used via --jars or --packages whereas our plugin is added to Spark internal classpath. Either\r\n- by placing the plugin jar in $SPARK_HOME/jars \r\n- or by using driver-class-path/spark.*.extraClassPath \r\n\r\nas noted in #5758\r\n\r\n\r\n**Steps/Code to reproduce bug**\r\nTo repro the shuffle manager issue specifically:\r\n\r\nBROKEN:\r\n```bash\r\n$SPARK_HOME/bin/pyspark --jars dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar \\\r\n  --conf spark.plugins=com.nvidia.spark.SQLPlugin \\\r\n  --conf spark.shuffle.manager=com.nvidia.spark.rapids.spark321.RapidsShuffleManager \\\r\n  --conf spark.shuffle.service.enabled=false \\\r\n  --conf spark.dynamicAllocation.enabled=false \\\r\n  --conf spark.executorEnv.UCX_ERROR_SIGNALS= \\\r\n  --conf spark.executorEnv.UCX_MEMTYPE_CACHE=n \\\r\n  --master spark://localhost:7077 \\\r\n  --num-executors 1\r\n```\r\nand observe the executor instances crashing with\r\n```\r\n22/06/08 15:41:59 INFO TransportClientFactory: Successfully created connection to /10.0.0.132:33207 after 1 ms (0 ms spent in bootstraps)\r\nException in thread \"main\" java.lang.reflect.UndeclaredThrowableException\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)\r\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)\r\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:419)\r\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:408)\r\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)\r\nCaused by: java.lang.ClassNotFoundException: com.nvidia.spark.rapids.spark321.RapidsShuffleManager\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:348)\r\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:216)\r\n\tat org.apache.spark.util.Utils$.instantiateSerializerOrShuffleManager(Utils.scala:2642)\r\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:315)\r\n\tat org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:207)\r\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$7(CoarseGrainedExecutorBackend.scala:468)\r\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:62)\r\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\r\n\t... 4 more\r\n```\r\n\r\nWORKS\r\n\r\nHowever, substituting --jars with the combo --driver-class-path/spark.executor.extraClassPath works fine:\r\n```\r\n$SPARK_HOME/bin/pyspark \\\r\n   --driver-class-path $PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar \r\n   --conf spark.executor.extraClassPath=$PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar \\\r\n   --conf spark.plugins=com.nvidia.spark.SQLPlugin \\ \r\n   --conf spark.shuffle.manager=com.nvidia.spark.rapids.spark321.RapidsShuffleManager \\\r\n   --conf spark.shuffle.service.enabled=false \\\r\n   --conf spark.dynamicAllocation.enabled=false \\\r\n   --conf spark.executorEnv.UCX_ERROR_SIGNALS= \\\r\n   --conf spark.executorEnv.UCX_MEMTYPE_CACHE=n \\\r\n   --master spark://localhost:7077 \\ \r\n   --num-executors 1\r\n```\r\n\r\nTo reconcile this and #5758, placing all jars on the Spark's initial classpath works:\r\n\r\n```bash\r\n$SPARK_HOME/bin/pyspark \\\r\n  --driver-class-path $PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar:$HOME/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar \\\r\n  --conf spark.executor.extraClassPath=$PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar:$HOME/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar \\\r\n  --conf spark.plugins=com.nvidia.spark.SQLPlugin \\\r\n  --conf spark.rapids.sql.explain=ALL \\\r\n  --conf spark.rapids.sql.format.avro.enabled=true \\\r\n  --conf spark.rapids.sql.format.avro.read.enabled=true \\\r\n  --conf spark.shuffle.manager=com.nvidia.spark.rapids.spark321.RapidsShuffleManager \\\r\n  --conf spark.shuffle.service.enabled=false \\\r\n  --conf spark.dynamicAllocation.enabled=false \\\r\n  --conf spark.executorEnv.UCX_ERROR_SIGNALS= \\\r\n  --conf spark.executorEnv.UCX_MEMTYPE_CACHE=n \\\r\n  --conf spark.rapids.memory.gpu.minAllocFraction=0 \\\r\n  --conf spark.rapids.memory.gpu.allocFraction=0.2 \\\r\n  --executor-cores=2 \\\r\n  --total-executor-cores=4 \\\r\n  --master spark://localhost:7077\r\n```\r\n```python\r\n>>> spark.read.format('avro').load('/tmp/a.avro').selectExpr('AVG(a)').collect()\r\n22/06/08 16:48:57 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/08 16:48:57 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/08 16:48:57 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n  *Exec <ShuffleExchangeExec> will run on GPU\r\n    *Partitioning <SinglePartition$> will run on GPU\r\n    *Exec <HashAggregateExec> will run on GPU\r\n      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n        *Expression <Average> avg(a#0) will run on GPU\r\n      *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/08 16:48:57 WARN GpuOverrides: \r\n*Exec <ShuffleExchangeExec> will run on GPU\r\n  *Partitioning <SinglePartition$> will run on GPU\r\n  *Exec <HashAggregateExec> will run on GPU\r\n    *Expression <AggregateExpression> partial_avg(a#0) will run on GPU\r\n      *Expression <Average> avg(a#0) will run on GPU\r\n    *Exec <FileSourceScanExec> will run on GPU\r\n\r\n22/06/08 16:48:58 WARN GpuOverrides:                                (0 + 3) / 3]\r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n\r\n22/06/08 16:48:58 WARN GpuOverrides: \r\n*Exec <HashAggregateExec> will run on GPU\r\n  *Expression <AggregateExpression> avg(a#0) will run on GPU\r\n    *Expression <Average> avg(a#0) will run on GPU\r\n  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU\r\n\r\n[Row(avg(a)=5.0)] \r\n```\r\n\r\n**Expected behavior**\r\nA coherent doc, ideally providing just one  supported method of deploying jars that always works. \r\nOtherwise explain why / when one or the other way is necessary\r\n\r\n**Environment details (please complete the following information)**\r\n- local, cloud providers\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/5796/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}