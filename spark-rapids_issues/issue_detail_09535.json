{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9535",
    "id": 1960987712,
    "node_id": "I_kwDOD7z77c504kxA",
    "number": 9535,
    "title": "[QST] Unable to Run Windowing with Skew Benchmark on RAPIDS",
    "user": {
        "login": "rohitreddy1698",
        "id": 29453278,
        "node_id": "MDQ6VXNlcjI5NDUzMjc4",
        "avatar_url": "https://avatars.githubusercontent.com/u/29453278?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/rohitreddy1698",
        "html_url": "https://github.com/rohitreddy1698",
        "followers_url": "https://api.github.com/users/rohitreddy1698/followers",
        "following_url": "https://api.github.com/users/rohitreddy1698/following{/other_user}",
        "gists_url": "https://api.github.com/users/rohitreddy1698/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/rohitreddy1698/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/rohitreddy1698/subscriptions",
        "organizations_url": "https://api.github.com/users/rohitreddy1698/orgs",
        "repos_url": "https://api.github.com/users/rohitreddy1698/repos",
        "events_url": "https://api.github.com/users/rohitreddy1698/events{/privacy}",
        "received_events_url": "https://api.github.com/users/rohitreddy1698/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735893,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODkz",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/question",
            "name": "question",
            "color": "d876e3",
            "default": true,
            "description": "Further information is requested"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-10-25T09:47:36Z",
    "updated_at": "2023-10-31T20:03:59Z",
    "closed_at": null,
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "**What is your question?**\r\nI am using spark : spark-3.2.3-bin-hadoop3.2  and RAPIDS version : rapids-4-spark_2.12-23.02.0.jar\r\n\r\nI am trying to run the benchmarks here : https://github.com/NVIDIA/spark-rapids-examples/blob/branch-23.02/examples/SQL%2BDF-Examples/micro-benchmarks/notebooks/micro-benchmarks-gpu.ipynb\r\n\r\nI am specifically trying to run the **Windowing with Skew** benchmark \r\n\r\n`query = '''\r\nselect ss_customer_sk,avg(avg_price) as avg_price\r\nfrom\r\n(\r\nSELECT ss_customer_sk ,avg(ss_sales_price) OVER (PARTITION BY ss_customer_sk order by ss_sold_date_sk ROWS BETWEEN 50 PRECEDING AND 50 FOLLOWING ) as avg_price\r\nFROM store_sales\r\n) group by ss_customer_sk order by 2 desc \r\n'''\r\nprint(\"-\"*50)`\r\n\r\nI am using the following configuration :\r\n\r\n`spark.driver.memory 25g\r\nspark.driver.cores 10\r\nspark.driver.memoryOverhead 5g\r\nspark.dynamicAllocation.enabled false\r\nspark.executor.memory 16g\r\nspark.executor.cores 8\r\nspark.executor.instances 2\r\nspark.executor.resource.gpu.amount 1\r\nspark.rapids.sql.concurrentGpuTasks 4\r\nspark.executor.resource.gpu.discoveryScript /root/getGpusResources.sh\r\nspark.sql.files.maxPartitionBytes 1g\r\nspark.rapids.memory.pinnedPool.size 25g\r\n\r\nspark.locality.wait 0\r\nspark.sql.adaptive.enabled true\r\nspark.eventLog.enabled true\r\nspark.eventLog.dir /root/eventLogs\r\nspark.history.fs.cleaner.enabled true\r\nspark.history.fs.cleaner.interval 48h\r\nspark.history.fs.cleaner.maxAge 30d\r\nspark.history.provider org.apache.spark.deploy.history.FsHistoryProvider\r\nspark.history.ui.maxApplications 10\r\nspark.history.ui.port 18080\r\nspark.history.fs.logDirectory /root/eventLogs\r\nspark.task.resource.gpu.amount 0.20\r\nspark.rapids.sql.enabled true\r\nspark.plugins com.nvidia.spark.SQLPlugin\r\nspark.rapids.sql.variableFloatAgg.enabled truespark.hadoop.fs.conductor.impl com.apple.aiml.hadoop.fs.conductor.ConductorS3AImpl\r\nspark.hadoop.fs.conductor.endpoint https://conductor.data.apple.com/\r\nspark.hadoop.fs.conductor.access.key fe2b1b3c-57bc-4ad5-8cab-9a9f44ecbcc2 \r\nspark.hadoop.fs.conductor.secret.key rmothe\r\nspark.hadoop.fs.conductor.path.style.access true\r\nspark.shuffle.service.enabled true\r\nspark.sql.shuffle.partitions 1000\r\nspark.sql.sources.bucketing.enabled TRUE\r\n\r\nspark.driver.extraClassPath /root/rapids-jars/rapids-4-spark_2.12-23.02.0.jar:/root/spark-3.2.3-bin-hadoop3.2/jars/hadoop-aws-3.3.4.jar:/root/spark-3.2.3-bin-hadoop3.2/jars/aws-java-sdk-bundle-1.11.999.jar:/root/spark-3.2.3-bin-hadoop3.2/jars/s3a-extension-0.0.1.jar\r\n\r\nspark.executor.extraClassPath /root/rapids-jars/rapids-4-spark_2.12-23.02.0.jar:/root/spark-3.2.3-bin-hadoop3.2/jars/hadoop-aws-3.3.4.jar:/root/spark-3.2.3-bin-hadoop3.2/jars/aws-java-sdk-bundle-1.11.999.jar:root/spark-3.2.3-bin-hadoop3.2/jars/s3a-extension-0.0.1.jar\r\n\r\nspark.jars /root/rapids-jars/rapids-4-spark_2.12-23.02.0.jar,/root/spark-3.2.3-bin-hadoop3.2/jars/hadoop-aws-3.3.4.jar,/root/spark-3.2.3-bin-hadoop3.2/jars/aws-java-sdk-bundle-1.11.999.jar,/root/spark-3.2.3-bin-hadoop3.2/jars/s3a-extension-0.0.1.jar`\r\n\r\nI am getting the following error :\r\n\r\n`>>> runMicroBenchmark(spark,\"Windowing with skew\",query,2)\r\n10:09:28.291 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 0 on 35128b17-9401-41ca-a466-d24e48894345: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\r\n10:09:37.592 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 6 on 2e7c73c5-b567-4b47-9d8b-d9d97dff6632: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\r\n10:09:49.223 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 1 on 35128b17-9401-41ca-a466-d24e48894345: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\r\n10:09:53.811 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 42 in stage 29.0 failed 4 times; aborting job\r\nTraceback (most recent call last):========================>    (911 + 8) / 1000]\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 8, in runMicroBenchmark\r\n  File \"/root/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\", line 494, in show\r\n    print(self._jdf.showString(n, 20, vertical))\r\n  File \"/root/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\r\n  File \"/root/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 111, in deco\r\n    return f(*a, **kw)\r\n  File \"/root/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o104.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 29.0 failed 4 times, most recent failure: Lost task 42.3 in stage 29.0 (TID 10727) (2e7c73c5-b567-4b47-9d8b-d9d97dff6632 executor 2): java.lang.OutOfMemoryError: Could not allocate native memory: std::bad_alloc: out_of_memory: RMM failure at:/home/jenkins/agent/workspace/jenkins-spark-rapids-jni-release-7-cuda11/thirdparty/cudf/cpp/build/_deps/rmm-src/include/rmm/mr/device/limiting_resource_adaptor.hpp:143: Exceeded memory limit\r\n\tat ai.rapids.cudf.Rmm.allocInternal(Native Method)\r\n\tat ai.rapids.cudf.Rmm.alloc(Rmm.java:326)\r\n\tat ai.rapids.cudf.DeviceMemoryBuffer.allocate(DeviceMemoryBuffer.java:143)\r\n\tat ai.rapids.cudf.DeviceMemoryBuffer.allocate(DeviceMemoryBuffer.java:133)\r\n\tat com.nvidia.spark.rapids.RapidsBufferStore$RapidsBufferBase.$anonfun$getDeviceMemoryBuffer$6(RapidsBufferStore.scala:384)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.RapidsBufferStore$RapidsBufferBase.withResource(RapidsBufferStore.scala:270)\r\n\tat com.nvidia.spark.rapids.RapidsBufferStore$RapidsBufferBase.getDeviceMemoryBuffer(RapidsBufferStore.scala:383)\r\n\tat com.nvidia.spark.rapids.RapidsBufferStore$RapidsBufferBase.getColumnarBatch(RapidsBufferStore.scala:325)\r\n\tat com.nvidia.spark.rapids.SpillableColumnarBatchImpl.$anonfun$getColumnarBatch$1(SpillableColumnarBatch.scala:106)\r\n\tat com.nvidia.spark.rapids.SpillableColumnarBatchImpl.$anonfun$withRapidsBuffer$1(SpillableColumnarBatch.scala:89)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.SpillableColumnarBatchImpl.withResource(SpillableColumnarBatch.scala:76)\r\n\tat com.nvidia.spark.rapids.SpillableColumnarBatchImpl.withRapidsBuffer(SpillableColumnarBatch.scala:88)\r\n\tat com.nvidia.spark.rapids.SpillableColumnarBatchImpl.getColumnarBatch(SpillableColumnarBatch.scala:104)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$concatPending$3(GpuKeyBatchingIterator.scala:126)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$concatPending$3$adapted(GpuKeyBatchingIterator.scala:125)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$concatPending$2(GpuKeyBatchingIterator.scala:125)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:64)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:62)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$concatPending$1(GpuKeyBatchingIterator.scala:123)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.concatPending(GpuKeyBatchingIterator.scala:122)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$5(GpuKeyBatchingIterator.scala:187)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:55)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:53)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$4(GpuKeyBatchingIterator.scala:185)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$2(GpuKeyBatchingIterator.scala:184)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$2$adapted(GpuKeyBatchingIterator.scala:162)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$1(GpuKeyBatchingIterator.scala:162)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.$anonfun$next$1$adapted(GpuKeyBatchingIterator.scala:157)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.withResource(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.next(GpuKeyBatchingIterator.scala:157)\r\n\tat com.nvidia.spark.rapids.GpuKeyBatchingIterator.next(GpuKeyBatchingIterator.scala:34)\r\n\tat com.nvidia.spark.rapids.GpuWindowIterator.next(GpuWindowExec.scala:1264)\r\n\tat com.nvidia.spark.rapids.GpuWindowIterator.next(GpuWindowExec.scala:1249)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.aggregateInputBatches(aggregate.scala:285)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.$anonfun$next$2(aggregate.scala:240)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.next(aggregate.scala:237)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.next(aggregate.scala:182)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.aggregateInputBatches(aggregate.scala:285)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.$anonfun$next$2(aggregate.scala:240)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.next(aggregate.scala:237)\r\n\tat com.nvidia.spark.rapids.GpuHashAggregateIterator.next(aggregate.scala:182)\r\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:241)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.withResource(GpuColumnarToRowExec.scala:187)\r\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:238)\r\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:215)\r\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:255)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\r\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628)\r\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)`\r\n\t\r\n\t\r\n\tHowever I am able to run the windowing without skew query and am able to see performance gains of 18 x.\r\n\t\r\n\t",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}