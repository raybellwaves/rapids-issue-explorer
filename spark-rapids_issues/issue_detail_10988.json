{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10988",
    "id": 2338511881,
    "node_id": "I_kwDOD7z77c6LYtwJ",
    "number": 10988,
    "title": "[AUDIT][SPARK-48019] Fix incorrect behavior in ColumnVector/ColumnarArray with dictionary and nulls",
    "user": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2094235324,
            "node_id": "MDU6TGFiZWwyMDk0MjM1MzI0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/?%20-%20Needs%20Triage",
            "name": "? - Needs Triage",
            "color": "e99695",
            "default": false,
            "description": "Need team to review and classify"
        },
        {
            "id": 2920816854,
            "node_id": "MDU6TGFiZWwyOTIwODE2ODU0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/audit",
            "name": "audit",
            "color": "B4ED94",
            "default": false,
            "description": "General label for audit related tasks"
        },
        {
            "id": 5444121041,
            "node_id": "LA_kwDOD7z77c8AAAABRH6x0Q",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/audit_3.5.0",
            "name": "audit_3.5.0",
            "color": "09B791",
            "default": false,
            "description": ""
        },
        {
            "id": 6138265837,
            "node_id": "LA_kwDOD7z77c8AAAABbd6A7Q",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/audit_4.0.0",
            "name": "audit_4.0.0",
            "color": "bfd4f2",
            "default": false,
            "description": "Audit related tasks for 4.0.0"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-06-06T15:16:43Z",
    "updated_at": "2024-06-06T15:18:58Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "This is a bug that exists in spark specific implementations of `ColumnarArray`, `OffHeapColumnVector` and `OnHeapColumnVector` where they were not handling null rows correctly. There was a follow on as well were the `copy()` method for `ArrayData` was changed to not use object arrays when we encounter nulls, instead it's using native arrays.\r\n\r\nhttps://github.com/apache/spark/commit/bf2e25459fe\r\n\r\nI think in most cases we provide our own implementations here based on cuDF vectors. That said, I am not 100% sure how the `.cache` stuff works as I see references to the `OffHeapColumnVector`. I see a reference to `ArrayData` in the collect set operation and it's not clear to me if we need to spend time looking at this. Filing as part of a 4.0 audit. Note that the issue was backported to 3.5",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10988/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}