{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7529",
    "id": 1538606806,
    "node_id": "I_kwDOD7z77c5btUbW",
    "number": 7529,
    "title": "[BUG] low cardinality joins on the GPU are really bad.",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 11,
    "created_at": "2023-01-18T19:47:39Z",
    "updated_at": "2024-05-29T21:38:37Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nI recently was playing around with testing a patch and ended up crating a join that ran absolutely horribly on the GPU, but not too bad on the CPU. This is not common so I spent a little bit of time trying to debug it and found that it really comes down to the cardinality of the build side (RHS in a left outer join or the smaller table in an inner join).\r\n\r\n**Steps/Code to reproduce bug**\r\n\r\nFor Inner join\r\n```\r\nval lhs = spark.range(200000000L).selectExpr(\"CAST(id DIV 100 as STRING) as something\")\r\nval rhs = spark.range(20000000L).selectExpr(\"CAST(id % 100 as STRING) as more\")\r\nspark.time(lhs.join(rhs, lhs(\"something\") === rhs(\"more\")).count())\r\n```\r\n\r\nPartitions | GPU Time (ms) | CPU Time (ms) | approximate GPU join op time (ms)\r\n-- | -- | -- | --\r\n4 | 151,469 | 31,828 | 594,000\r\n8 | 74,058 | 25,810 | 288,000\r\n16 | 39,506 | 22,395 | 150,000\r\n32 | 23,240 | 20,167 | 84,000\r\n64 | 14,685 | 19,776 | 52,300\r\n128 | 9,941 | 19,037 | 32,800\r\n256 | 6,507 | 21,247 | 20,300\r\n512 | 4,565 | 21,288 | 12,500\r\n1,024 | 3,755 | 21,450 | 8,300\r\n2,048 | 3,586 | 23,769 | 5,700\r\n4,096 | 4,721 | 24,560 | 4,200\r\n8,192 | 8,295 | 30,102 | 3,700\r\n\r\n\r\n![GPU vs CPU inner join scaling](https://user-images.githubusercontent.com/3441321/213275932-0f253e26-ee5d-485e-9755-0781beec276f.png)\r\n\r\nFor left outer join (which is an order of magnitude slower than inner in this case so I had to make the input sizes 1/10th that of the inner join)\r\n```\r\nval lhs = spark.range(20000000L).selectExpr(\"CAST(id DIV 100 as STRING) as something\")\r\nval rhs = spark.range(2000000L).selectExpr(\"CAST(id % 100 as STRING) as more\")\r\nspark.time(lhs.join(rhs, lhs(\"something\") === rhs(\"more\"), jointType=\"leftouter\").count())\r\n```\r\n\r\nPartitions | GPU Time (ms) | CPU Time (ms) | approximate GPU join op time (ms)\r\n-- | -- | -- | --\r\n4 | 51,179 | 4,295 | 138,000\r\n8 | 29,732 | 2,706 | 78,000\r\n16 | 20,851 | 2,305 | 57,000\r\n32 | 13,894 | 2,235 | 41,900\r\n64 | 8,978 | 2,229 | 30,000\r\n128 | 6,153 | 2,151 | 20,500\r\n256 | 4,461 | 3,011 | 14,400\r\n512 | 3,563 | 3,077 | 10,500\r\n1,024 | 3,489 | 3,948 | 8,700\r\n2,048 | 4,214 | 5,323 | 8,500\r\n4,096 | 5,885 | 8,618 | 9,400\r\n8,192 | 9,688 | 15,527 | 11,800\r\n\r\n![GPU vs CPU left outer join scaling](https://user-images.githubusercontent.com/3441321/213276174-f4ad4a23-d77c-402f-ae9b-efc4afde240d.png)\r\n\r\nI tested with longs instead of strings as the join keys and it made a very small difference, but not enough to worry about.\r\nI verified that none of the operators were spilling. In fact the input data is small enough I could have set the concurrent GPU tasks to 12 and gotten a bit more speedup out of the GPU.\r\n\r\nIt ended up being very directly related to the cardinality of the build side.\r\n\r\n```\r\nval lhs = spark.range(20000000L).selectExpr(\"CAST(id DIV 200 as STRING) as something\")\r\nval rhs = spark.range(2000000L).selectExpr(\"CAST(id % X as STRING) as more\")\r\nspark.time(lhs.join(rhs, lhs(\"something\") === rhs(\"more\"), jointType=\"leftouter\").count())\r\n```\r\n\r\nWhere `X` is the cardinality. I set the concurrent level to 4. There are 12 cores and 12 threads. This was also on an a6000 GPU.\r\n\r\nCardinality | GPU Time (ms)\r\n-- | --\r\n50 | 82,272\r\n100 | 23,873\r\n200 | 7,216\r\n400 | 2,257\r\n800 | 831\r\n1,600 | 396\r\n3,200 | 271\r\n\r\n![GPU left outer join scaling](https://user-images.githubusercontent.com/3441321/213276866-27b3ba0f-80d0-412a-b8dd-a70017488806.png)\r\n\r\n\r\nIncreasing the cardinality has a 300x speedup.  The other interesting part is that we can also have a 14x speedup on a cardinality of 100 if we subdivide the input data even smaller pieces. This is something we were looking into to deal with out of memory issues, but here it shows some big performance improvements. I am not sure exactly what is happening or if this is something that would impact a real world use case, but I thought I should document this at the least. I think it is related to making the build table and collisions in the hash table, specifically with the need to walk more of the tree on a collision vs a hash aggregation that can use atomics to do the operations.\r\n\r\nWe are doing an estimate with a gorupby for the output size using the build side table it we might be able to use that to figure out if we are in a bad case like this and possibly decide if there is an alternative join path we could take.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}