{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9135",
    "id": 1872514120,
    "node_id": "I_kwDOD7z77c5vnExI",
    "number": 9135,
    "title": "[BUG] GC/OOM on `parquet_test.py::test_small_file_memory`",
    "user": {
        "login": "mythrocks",
        "id": 5607330,
        "node_id": "MDQ6VXNlcjU2MDczMzA=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mythrocks",
        "html_url": "https://github.com/mythrocks",
        "followers_url": "https://api.github.com/users/mythrocks/followers",
        "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
        "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
        "organizations_url": "https://api.github.com/users/mythrocks/orgs",
        "repos_url": "https://api.github.com/users/mythrocks/repos",
        "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mythrocks/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "mythrocks",
        "id": 5607330,
        "node_id": "MDQ6VXNlcjU2MDczMzA=",
        "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mythrocks",
        "html_url": "https://github.com/mythrocks",
        "followers_url": "https://api.github.com/users/mythrocks/followers",
        "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
        "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
        "organizations_url": "https://api.github.com/users/mythrocks/orgs",
        "repos_url": "https://api.github.com/users/mythrocks/repos",
        "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mythrocks/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "mythrocks",
            "id": 5607330,
            "node_id": "MDQ6VXNlcjU2MDczMzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5607330?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mythrocks",
            "html_url": "https://github.com/mythrocks",
            "followers_url": "https://api.github.com/users/mythrocks/followers",
            "following_url": "https://api.github.com/users/mythrocks/following{/other_user}",
            "gists_url": "https://api.github.com/users/mythrocks/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mythrocks/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mythrocks/subscriptions",
            "organizations_url": "https://api.github.com/users/mythrocks/orgs",
            "repos_url": "https://api.github.com/users/mythrocks/repos",
            "events_url": "https://api.github.com/users/mythrocks/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mythrocks/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 14,
    "created_at": "2023-08-29T21:34:29Z",
    "updated_at": "2023-09-28T04:00:55Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "The `parquet_test.py::test_small_file_memory` test runs out of memory on CDH (at least on the equivalent of Spark 3.3):\r\n```\r\n$ SPARK_HOME=$SPARK_HOME ./run_pyspark_from_build.sh -k test_small_file_memory\r\n```\r\n```\r\n...\r\n23/08/29 21:17:50 ERROR executor.Executor: [Executor task launch worker for task 2.0 in stage 5.0 (TID 2015)]: Exception in task 2.0 in stage 5.0 (TID 2015)\r\njava.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_322]\r\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_322]\r\n\tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7(GpuMultiFileReader.scala:1216) ~[spark3xx-common/:?]\r\n\tat com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7$adapted(GpuMultiFileReader.scala:1215) ~[spark3xx-common/:?]\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]\r\n...\r\n```\r\nThe failing test takes about a minute to run on my workstation, but considerably longer on CDH, before it fails.\r\n\r\nAs indicated above, this test was run manually on CDH.  As a control group, I was able to run the `window_function_test.py` on the same setup, without failures.\r\nAdditionally, `parquet_test.py::test_small_file_memory` runs properly on Apache Spark 3.2.x:\r\n\r\n```\r\n$ SPARK_HOME=$SPARK_HOME ./run_pyspark_from_build.sh -k test_small_file_memory\r\n...\r\n../../src/main/python/parquet_test.py::test_small_file_memory[] 23/08/29 21:20:32 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\r\n23/08/29 21:20:32 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\r\n23/08/29 21:20:42 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\r\n23/08/29 21:20:42 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\r\nPASSED   [ 50%]\r\n../../src/main/python/parquet_test.py::test_small_file_memory[parquet] 23/08/29 21:20:45 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\r\n23/08/29 21:20:45 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\r\n23/08/29 21:20:53 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\r\n23/08/29 21:20:53 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\r\nPASSED [100%]\r\n```\r\nIt appears that something is indeed up.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}