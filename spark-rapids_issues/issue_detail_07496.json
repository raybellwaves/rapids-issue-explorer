{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7496",
    "id": 1528111281,
    "node_id": "I_kwDOD7z77c5bFSCx",
    "number": 7496,
    "title": "[BUG] q67 appears to run out of system memory and also GPU memory using multi-threaded shuffle in Databricks",
    "user": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 2096543664,
            "node_id": "MDU6TGFiZWwyMDk2NTQzNjY0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/shuffle",
            "name": "shuffle",
            "color": "67fc73",
            "default": false,
            "description": "things that impact the shuffle plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": {
        "login": "tgravescs",
        "id": 4563792,
        "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/tgravescs",
        "html_url": "https://github.com/tgravescs",
        "followers_url": "https://api.github.com/users/tgravescs/followers",
        "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
        "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
        "organizations_url": "https://api.github.com/users/tgravescs/orgs",
        "repos_url": "https://api.github.com/users/tgravescs/repos",
        "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
        "received_events_url": "https://api.github.com/users/tgravescs/received_events",
        "type": "User",
        "site_admin": false
    },
    "assignees": [
        {
            "login": "tgravescs",
            "id": 4563792,
            "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tgravescs",
            "html_url": "https://github.com/tgravescs",
            "followers_url": "https://api.github.com/users/tgravescs/followers",
            "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
            "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
            "organizations_url": "https://api.github.com/users/tgravescs/orgs",
            "repos_url": "https://api.github.com/users/tgravescs/repos",
            "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tgravescs/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 2,
    "created_at": "2023-01-10T22:41:21Z",
    "updated_at": "2023-06-05T17:50:07Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "@mattahrens ran some NDS @ 3TB benchmarks using T4s in Databricks with 8 g4dn.2xlarge workers.\r\n\r\nIt is worth mentioning that a bigger node (g4dn.4xlarge) works. This bigger machine has 64GB of RAM, as opposed to the 32GB available to the one with the issues. When looking at the Environment tab in the Spark UI, Xmx for the executors was ~20GB, and we are using the following configs (other than to turn on the shuffle):\r\n\r\n```\r\n\"spark.task.resource.gpu.amount\": \".01\",\r\n\"spark.plugins\": \"com.nvidia.spark.SQLPlugin\",\r\n\"spark.rapids.memory.pinnedPool.size\": \"4G\",\r\n\"spark.sql.files.maxPartitionBytes\": \"1G\",\r\n\"spark.rapids.sql.multiThreadedRead.numThreads\": \"100\",\r\n\"spark.rapids.sql.concurrentGpuTasks\": \"2\"\r\n```\r\n\r\nSo we have 5GB (4GB pinned + 1GB pageable), and that would put the process at ~25GB baseline, leaving us ~7GB free for other things, including direct byte buffers and HostMemoryBuffer instances. It seems that this was not enough.\r\n\r\nFirst, a task was interrupted with the exception below:\r\n\r\n```\r\n23/01/10 17:13:52 ERROR TaskContextImpl: Error in TaskCompletionListener\r\norg.apache.spark.TaskKilledException\r\n\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:246)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\r\n\tat org.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:469)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuExec.scala:193)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuExec.scala:192)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.RebaseHelper$.withResource(RebaseHelper.scala:26)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuExec.scala:192)\r\n\tat com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:291)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuExec.scala:193)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuExec.scala:192)\r\n\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tat com.nvidia.spark.RebaseHelper$.withResource(RebaseHelper.scala:26)\r\n\tat com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuExec.scala:192)\r\n\tat scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:1089)\r\n\tat scala.collection.BufferedIterator.headOption(BufferedIterator.scala:32)\r\n\tat scala.collection.BufferedIterator.headOption$(BufferedIterator.scala:32)\r\n\tat scala.collection.Iterator$$anon$22.headOption(Iterator.scala:1076)\r\n\tat com.nvidia.spark.rapids.CloseableBufferedIterator.headOption(CloseableBufferedIterator.scala:36)\r\n\tat com.nvidia.spark.rapids.CloseableBufferedIterator.close(CloseableBufferedIterator.scala:41)\r\n\tat com.nvidia.spark.rapids.CloseableBufferedIterator.$anonfun$new$2(CloseableBufferedIterator.scala:32)\r\n\tat com.nvidia.spark.rapids.CloseableBufferedIterator.$anonfun$new$2$adapted(CloseableBufferedIterator.scala:32)\r\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\r\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\r\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\r\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\r\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\r\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\r\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nThen ~30 seconds later we OOM on the GPU as well (we tried to spill then oom).\r\n\r\nI do not know why a task was interrupted, or why we kept going. It seems we need to dig a bit deeper here to figure out what could be going on in this environment, we probably want to add some heap dumps to figure out the number of HostMemoryBuffers and direct buffers that are in flight.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}