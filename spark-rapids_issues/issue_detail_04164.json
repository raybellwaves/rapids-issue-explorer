{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/4164",
    "id": 1058768344,
    "node_id": "I_kwDOD7z77c4_G4XY",
    "number": 4164,
    "title": "[FEA] dynamic sizing of input data",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 3,
    "created_at": "2021-11-19T17:40:05Z",
    "updated_at": "2022-05-17T18:33:10Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "This is something that might be interesting to push back to Spark too. When we are trying to figure out the splits for reading, can we look at the read schema and the file schema to get an idea of how much data is going to be thrown out when we do a read so we can better size the splits. I can see a few options for this.\r\n\r\nOne option is where we look at a small amount of data (1 or 2 files at most) to try and determine the file schema.  The other option is to launch an actual job that all it does is read the file schema to get better knowledge about exactly what is happening.  Option 1 is nice because it can be fast and probably can give us a decent estimate on what things will look like. Option 2 fits more with things like delta lake that will read and cache metadata information before running a much larger query.  We might even be able to switch between the two based off of the number of files, or the number of directories involved in a single query.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}