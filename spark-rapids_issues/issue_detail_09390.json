{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9390",
    "id": 1929018946,
    "node_id": "I_kwDOD7z77c5y-n5C",
    "number": 9390,
    "title": "[BUG] Plugin should throw an exception rather than return nulls on invalid timestamps when reading CSV",
    "user": {
        "login": "andygrove",
        "id": 934084,
        "node_id": "MDQ6VXNlcjkzNDA4NA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/934084?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/andygrove",
        "html_url": "https://github.com/andygrove",
        "followers_url": "https://api.github.com/users/andygrove/followers",
        "following_url": "https://api.github.com/users/andygrove/following{/other_user}",
        "gists_url": "https://api.github.com/users/andygrove/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/andygrove/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/andygrove/subscriptions",
        "organizations_url": "https://api.github.com/users/andygrove/orgs",
        "repos_url": "https://api.github.com/users/andygrove/repos",
        "events_url": "https://api.github.com/users/andygrove/events{/privacy}",
        "received_events_url": "https://api.github.com/users/andygrove/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        },
        {
            "id": 4073796705,
            "node_id": "LA_kwDOD7z77c7y0TRh",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/audit_3.4.0",
            "name": "audit_3.4.0",
            "color": "8310E8",
            "default": false,
            "description": "Audit related tasks for 3.4.0"
        },
        {
            "id": 5444121041,
            "node_id": "LA_kwDOD7z77c8AAAABRH6x0Q",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/audit_3.5.0",
            "name": "audit_3.5.0",
            "color": "09B791",
            "default": false,
            "description": ""
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2023-10-05T20:39:18Z",
    "updated_at": "2023-10-27T20:56:19Z",
    "closed_at": null,
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\n\r\nWhen reading a CSV file containing timestamps that are not compatible with the provided `timestampFormat`, Spark throws a parser error, but the plugin returns null values instead.\r\n\r\nI tested this with Spark 3.4.0 and plugin 23.12-SNAPSHOT\r\n\r\n**Steps/Code to reproduce bug**\r\n\r\nCreate `timestamps.csv` with these contents:\r\n\r\n```\r\n2884-06-24T02:45:51.138\r\n2884-06-24T02:45:51.138\r\n2884-06-24T02:45:51.138\r\n```\r\n\r\n# Spark RAPIDS\r\n\r\n```\r\nscala> import org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.types._\r\n\r\nscala> val schema = StructType(Seq(StructField(\"ts\", DataTypes.TimestampType, true)))\r\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(ts,TimestampType,true))\r\n\r\nscala> val df = spark.read.option(\"timestampFormat\", \"yyyy-MM-dd'T'HH:mm:ss\").schema(schema).csv(\"/tmp/timestamps.csv\")\r\ndf: org.apache.spark.sql.DataFrame = [ts: timestamp]\r\n\r\nscala> df.show\r\n23/10/05 20:31:31 WARN GpuOverrides: \r\n!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\r\n  @Partitioning <SinglePartition$> could run on GPU\r\n  *Exec <ProjectExec> will run on GPU\r\n    *Expression <Alias> cast(ts#0 as string) AS ts#3 will run on GPU\r\n      *Expression <Cast> cast(ts#0 as string) will run on GPU\r\n    *Exec <FileSourceScanExec> will run on GPU\r\n\r\n+----+                                                                          \r\n|  ts|\r\n+----+\r\n|null|\r\n|null|\r\n|null|\r\n+----+\r\n```\r\n\r\n## Spark\r\n\r\n```\r\nscala> spark.conf.set(\"spark.rapids.sql.enabled\", false)\r\n\r\nscala> df.show\r\n23/10/05 20:31:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\r\norg.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\r\nFail to parse '2884-06-24T02:45:51.138' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n```\r\n\r\n**Expected behavior**\r\nWe should throw an exception in this case.\r\n\r\n**Environment details (please complete the following information)**\r\nN/A\r\n\r\n**Additional context**\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}