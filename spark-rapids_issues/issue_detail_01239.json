{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/1239",
    "id": 755410218,
    "node_id": "MDU6SXNzdWU3NTU0MTAyMTg=",
    "number": 1239,
    "title": "[FEA] move away from the single queue in UCXShuffleTransport",
    "user": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 2096543664,
            "node_id": "MDU6TGFiZWwyMDk2NTQzNjY0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/shuffle",
            "name": "shuffle",
            "color": "67fc73",
            "default": false,
            "description": "things that impact the shuffle plugin"
        },
        {
            "id": 2223784867,
            "node_id": "MDU6TGFiZWwyMjIzNzg0ODY3",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/P2",
            "name": "P2",
            "color": "8ff7b9",
            "default": false,
            "description": "Not required for release"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2020-12-02T16:11:36Z",
    "updated_at": "2022-04-27T15:55:00Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "Currently in `UCXShuffleTransport` we pool all requests from every client after metadata has been resolved from peers.\r\n\r\nThe requests are put into a `HashedPriorityQueue`, and sorted by length. This was done early on as one way to schedule transfers  in a fair way for all peers.\r\n\r\nIn release 0.3 of the plugin we moved towards packing bounce buffers. This means we want to go to this queue and pop as many things as would fit a bounce buffer, to make the send/receives more efficient. This conflicts with the single queue.\r\n\r\nFor example: there could be two requests, one with small length and one with larger length, that would both fit the same bounce buffer. We want to get both of these requests together and put them in the same buffer for the transfer, rather than poorly utilize the bounce buffer for the small request, and then later on go onto the larger request with a _different_ bounce buffer, and hence a different receive request, and more RPC.\r\n\r\nIn order to accomplish the above, currently the code iterates over the whole `HashedPriorityQueue` in priority order as long as we haven't exceeded the in-flight limit. Then it \"skips\" some requests in order to go from that first request to the second request that would fit. It also only knows to get 1 bounce buffer per client, so this is another optimization... if all we have is requests for 1 client, why not throw the sink at it? \r\n\r\nThis issue tracks improvements for 0.4 to clean this up and make it more performant.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/1239/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}