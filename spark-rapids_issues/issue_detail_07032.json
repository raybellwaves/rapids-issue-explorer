{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7032",
    "id": 1441241502,
    "node_id": "I_kwDOD7z77c5V55me",
    "number": 7032,
    "title": "[BUG] OOM of NDS2 queries with Iceberg on Dataproc 2.1-preview(Spark-3.3.0)",
    "user": {
        "login": "wjxiz1992",
        "id": 20476954,
        "node_id": "MDQ6VXNlcjIwNDc2OTU0",
        "avatar_url": "https://avatars.githubusercontent.com/u/20476954?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/wjxiz1992",
        "html_url": "https://github.com/wjxiz1992",
        "followers_url": "https://api.github.com/users/wjxiz1992/followers",
        "following_url": "https://api.github.com/users/wjxiz1992/following{/other_user}",
        "gists_url": "https://api.github.com/users/wjxiz1992/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/wjxiz1992/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/wjxiz1992/subscriptions",
        "organizations_url": "https://api.github.com/users/wjxiz1992/orgs",
        "repos_url": "https://api.github.com/users/wjxiz1992/repos",
        "events_url": "https://api.github.com/users/wjxiz1992/events{/privacy}",
        "received_events_url": "https://api.github.com/users/wjxiz1992/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2022-11-09T02:24:04Z",
    "updated_at": "2022-12-09T20:17:47Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\nI see several NDS queries run into OOM error w/ Iceberg enabled. While the query w/o Iceberg but w/ pure Parquet can run successfully.\r\nDataset scale factor: 3000\r\n\r\nThe failed queries are:\r\n\r\n- [ ] 4\r\n- [ ] 14a,b\r\n- [ ] 19\r\n- [ ] 24a,b\r\n- [ ] 37\r\n- [ ] 72\r\n- [ ] 82\r\n- [ ] 84\r\n\r\n\r\n\r\n**Steps/Code to reproduce bug**\r\n1. build jar locally with the following diff:\r\n```\r\n   diff --git a/pom.xml b/pom.xml\r\nindex 6d155840c..05fc3daa3 100644\r\n--- a/pom.xml\r\n+++ b/pom.xml\r\n\r\n@@ -1116,7 +1116,7 @@\r\n-        <iceberg.version>0.13.2</iceberg.version>\r\n+        <iceberg.version>0.14.1</iceberg.version>\r\n```\r\n3. create a Dataproc cluster with 2.1-preview image\r\n```\r\ngcloud dataproc clusters import CLUSTER_NAME --region=us-central1 --source=dataproc-cluster.yaml\r\n```\r\nwith the following yaml file\r\n```\r\n---\r\nprojectId: rapids-spark\r\nclusterName: cluster-b275\r\nconfig:\r\n  configBucket: ''\r\n  gceClusterConfig:\r\n    networkUri: ''\r\n    subnetworkUri: default\r\n    internalIpOnly: false\r\n    zoneUri: us-central1-b\r\n    metadata: {}\r\n    tags: []\r\n    shieldedInstanceConfig:\r\n      enableSecureBoot: false\r\n      enableVtpm: false\r\n      enableIntegrityMonitoring: false\r\n  masterConfig:\r\n    numInstances: 1\r\n    machineTypeUri: n1-standard-32\r\n    diskConfig:\r\n      bootDiskType: pd-standard\r\n      bootDiskSizeGb: 500\r\n      numLocalSsds: 0\r\n      localSsdInterface: NVME\r\n    minCpuPlatform: ''\r\n    imageUri: ''\r\n  softwareConfig:\r\n    imageVersion: preview-ubuntu\r\n    properties: {}\r\n    optionalComponents: []\r\n  lifecycleConfig: {}\r\n  initializationActions:\r\n  - executableFile: gs://nds2-dataset/spark-rapids.sh\r\n  encryptionConfig:\r\n    gcePdKmsKeyName: ''\r\n  autoscalingConfig:\r\n    policyUri: ''\r\n  endpointConfig:\r\n    enableHttpPortAccess: true\r\n  securityConfig:\r\n    kerberosConfig: {}\r\n  workerConfig:\r\n    numInstances: '4'\r\n    machineTypeUri: n1-standard-32\r\n    diskConfig:\r\n      bootDiskType: pd-standard\r\n      bootDiskSizeGb: 500\r\n      numLocalSsds: 8\r\n      localSsdInterface: NVME\r\n    accelerators:\r\n    - acceleratorTypeUri: nvidia-tesla-t4\r\n      acceleratorCount: 2\r\n    minCpuPlatform: 'Intel Skylake'\r\n    imageUri: ''\r\n  secondaryWorkerConfig:\r\n    numInstances: 0\r\nlabels: {}\r\nstatus: {}\r\nstatusHistory:\r\n- {}\r\nmetrics: {}\r\n```\r\nNote, please remove all built-in jars first:\r\n```\r\nfor i in {0..3}\r\ndo\r\n    gcloud compute ssh --zone \"us-central1-b\" \"CLUSTER_NAME-w-$i\"  --project \"rapids-spark\" --command 'sudo rm /usr/lib/spark/jars/rapids-4-spark_2.12-22.08.0.jar'\r\ndone\r\n\r\ngcloud compute ssh --zone \"us-central1-b\" \"CLUSTER_NAME-m\"  --project \"rapids-spark\" --command 'sudo rm /usr/lib/spark/jars/rapids-4-spark_2.12-22.08.0.jar'\r\n```\r\n\r\n5. get spark-rapids-benchmarks repo on the cluster\r\n6. set dataset path and generate query stream files(please ping me offline and I can share the gs bucket path with you including stream files)\r\n\r\n**Environment details (please complete the following information)**\r\ncluster config: see yaml file above\r\nspark app config: \r\n```\r\nexport SPARK_HOME=${SPARK_HOME:-/usr/lib/spark}\r\nexport SPARK_MASTER=${SPARK_MASTER:-yarn}\r\nexport DRIVER_MEMORY=${DRIVER_MEMORY:-30G}\r\nexport EXECUTOR_CORES=${EXECUTOR_CORES:-16}\r\nexport NUM_EXECUTORS=${NUM_EXECUTORS:-8}\r\nexport EXECUTOR_MEMORY=${EXECUTOR_MEMORY:-32G}\r\n\u00a0\r\nexport CONCURRENT_GPU_TASKS=${CONCURRENT_GPU_TASKS:-1}\r\nexport SHUFFLE_PARTITIONS=${SHUFFLE_PARTITIONS:-800}\r\n\u00a0\r\nexport SPARK_CONF=(\"--master\" \"${SPARK_MASTER}\"\r\n--deploy-mode \"client\"\r\n--conf \"spark.driver.maxResultSize=2GB\"\r\n--conf \"spark.driver.memory=${DRIVER_MEMORY}\"\r\n--conf \"spark.executor.cores=${EXECUTOR_CORES}\"\r\n--conf \"spark.executor.instances=${NUM_EXECUTORS}\"\r\n--conf \"spark.executor.memory=${EXECUTOR_MEMORY}\"\r\n--conf \"spark.executor.memoryOverhead=16G\"\r\n--conf \"spark.sql.shuffle.partitions=${SHUFFLE_PARTITIONS}\"\r\n--conf \"spark.sql.autoBroadcastJoinThreshold=10m\"\r\n--conf \"spark.sql.files.maxPartitionBytes=512mb\"\r\n--conf \"spark.sql.adaptive.enabled=true\"\r\n--conf \"spark.executor.resource.gpu.amount=1\"\r\n--conf \"spark.executor.resource.gpu.discoveryScript=./getGpusResources.sh\"\r\n--conf \"spark.task.resource.gpu.amount=0.0625\"\r\n--conf \"spark.plugins=com.nvidia.spark.SQLPlugin\"\r\n--conf \"spark.rapids.memory.host.spillStorageSize=32G\"\r\n--conf \"spark.rapids.memory.pinnedPool.size=8g\"\r\n--conf \"spark.rapids.sql.concurrentGpuTasks=${CONCURRENT_GPU_TASKS}\"\r\n--packages \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.1\"\r\n--conf \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\r\n--conf \"spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog\"\r\n--conf \"spark.sql.catalog.spark_catalog.type=hadoop\"\r\n--files \"$SPARK_HOME/examples/src/main/scripts/getGpusResources.sh\"\r\n```\r\n\r\nsubmit the job by\r\n```\r\n/usr/lib/spark/bin/spark-submit \\\r\n--master yarn --deploy-mode client \\\r\n--conf spark.driver.maxResultSize=2GB \\\r\n--conf spark.driver.memory=30G \\\r\n--conf spark.executor.cores=16 \\\r\n--conf spark.executor.instances=8 \\\r\n--conf spark.ecutor.memory=50G \\\r\n--conf spark.executor.memoryOverhead=16G \\\r\n--conf spark.sql.shuffle.partitions=800 \\\r\n--conf spark.sql.autoBroadcastJoinThreshold=10m \\\r\n--conf spark.sql.files.maxPartitionBytes=512mb \\\r\n--conf spark.sql.adaptive.enabled=true \\\r\n--conf spark.executor.resource.gpu.amount=1 \\\r\n--conf srk.executor.resource.gpu.discoveryScript=./getGpusResources.sh \\\r\n--conf spark.task.resource.gpu.amount=0.0625 \\\r\n--conf spark.plugins=com.nvidia.spark.SQLPlugin \\\r\n--conf spark.rapids.memory.host.spillStorageSize=32G \\\r\n--conf spark.rapids.memory.pinnedPool.size=8g \\\r\n--conf spark.rapids.sql.concuentGpuTasks=1 \\\r\n--packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:0.14.1 \\\r\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\r\n--conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\r\n--conf spark.sql.catalogpark_catalog.type=hadoop \\\r\n--files /usr/lib/spark/examples/src/main/scripts/getGpusResources.sh \\\r\n--jars /home/allxu_nvidia_com/rapids-4-spark_2.12-22.12.0-SNAPSHOT-cuda11.jar \\\r\nnds_power.py \\\r\ngs://nds2-dataset/iceberg-warehouse-3k bench_streams/query19.sql \\\r\npower_test_1101_gpu.csv \\\r\n--input_foat iceberg \\\r\n--property_file properties/aqe-on.properties\r\n```\r\n\r\n\r\n\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7032/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}