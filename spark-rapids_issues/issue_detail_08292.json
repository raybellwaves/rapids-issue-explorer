{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8292",
    "id": 1710424573,
    "node_id": "I_kwDOD7z77c5l8wH9",
    "number": 8292,
    "title": "[FEA] multi-threaded shuffle above 200 partitions",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2023-05-15T16:23:17Z",
    "updated_at": "2023-06-02T16:10:16Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThe current Spark shuffle has two modes. For tasks with less than 200 partitions, configurable. It will write the shuffle data out to a temp file per partition, and then in a second pass it will merge the files together in sorted order. For larger numbers of shuffle partitions the data is sorted by the shuffle partition and written out in sorted order. This might involve spilling if the data is too large. The problem is that it is not a graceful transition between the two. If I run NDS scale factor 3k on dataproc with 256 shuffle partitions, not multi-threaded shuffle, I see a 4.7% total run time difference between the two approaches. Going from 200 to 201 partitions is likely to cause a large performance regression.\r\n\r\nAlso we cannot just configure the shuffle to always use the merge-bypass shuffle. It uses much more system resources when the number of partitions is high. For example the same NDS 3k runs show a 5.88% performance loss when using merge-bypass shuffle at 768 partitions. Going to 896 partitions caused NDS to fail. \r\n\r\nOur current multi-threaded shuffle implementation only deals with the 200 partitions or smaller use case. It is really a multi-threaded merge-bypass shuffle. This is likely to compound the performance issues, and it is not clear to an end user that there is a lot more performance to be had if they need a few more shuffle partitions. It is also not clear to the end user that the multi-threaded shuffle is not used when the number of partitions gets too high. It would really be nice to have a way for the performance to transition gracefully, and to have an accelerated version of the sort based shuffle as well.\r\n\r\nIn addition to this for larger jobs and larger clusters we are likely to want to have more partitions.\r\n\r\nIt would really be even better if we could optimize for some common cases when a single \"batch\" of data is output by a task.\r\n\r\n**Describe the solution you'd like**\r\nI am not 100% sure what we want to do. I have a few ideas, but we need to do some experiments to find out what might and what might not work.\r\n\r\nWhen evaluation a shuffle implementation we need to think about a few factors.\r\n\r\n1. write amplification - how much more data do we end up writing than was handed to us. But we are going to measure it in total amount of data written to disk.\r\n2. read amplification - how much more data do we end up reading than was handed to us. But we are going to measure it in total amount of data read from \"disk\" (could include just the page cache)\r\n3. I/O access pattern - is this sequential I/O or random I/O, and how many IOPs can the hardware support.\r\n4. compression parallelism - can we split the compression/decompression up between multiple threads.\r\n5. Memory usage\r\n\r\nSo if we evaluate the various shuffles it can help to show what is happening here.\r\n\r\n|shuffle|write amplification|read amplification|I/O pattern|comp parallel|memory usage|\r\n|-|-|-|-|-|-|\r\n|merge-bypass|2 * compressed size|2 * compressed size|random write + sequential read|task parallelism|(Compression codec + file handle + buffer) * partition * task|\r\n|multi-threaded|2 * compressed size|2 * compressed size|random write + sequential read|up to partition parallelism|(Compression codec + file handle + buffer) * partition * task|\r\n|sort-shuffle|spill + compressed size|spill + compressed size|sequential write & read|task parallelism|Compression codec + sort buffer size|\r\n\r\nThe buffer size is managed by Spark and can get rather complicated, but it is by default at most `(heap - 300 MiB) * 0.6`, but it is shared by all tasks.\r\n\r\nFrom this I have a few ideas that would be nice to play with and test out.\r\n\r\nThe sort in Spark is really designed around a lot of small rows. It is also designed to be generic for all types of RDDs where they key may not be the partition. But we typically have a few number of large-ish sub-batches. Also the number of partitions is going to be something we can reasonably handle in memory (not the data, but the partitions). We also have a clear separation between the partition key and the data. It would be interesting to see if we could take a hybrid approach here. \r\n\r\nWe keep the data in memory, but we \"sort\" it by keeping a `List<LinkedList<SubShuffleBatch>>` or something like that. The key to the top level list is the partition and then we can tack more sub-batches onto the each partition as we see them. This is to keep the data in the same order it was output by the task. That is not strictly a requirement but would be nice. When we run out of memory and need to spill, we can start by compressing different partitions in parallel and writing them out to disk.  When we write them out to disk we can compress partitions in parallel (up to N Codecs and file handles where N is configurable, but probably 200 by default) and we would replace them with pointers to where they were written out. So for example if I had 400 shuffle partitions to spill and 200 codecs/files to use I would compress the even ones first and write them out to 200 files at the beginning of the files, then I would create new compression codecs and do the odd partitions writing them after the even ones. When all of the data is added we can do a final pass and decide how to build up the final output file.\r\n\r\nFor any partition where all of the data for it is fully in memory we can compress them in parallel and write them out at the appropriate time to the final location in the file. This would hopefully help with the common case of a small shuffle by not having any read or write amplification.\r\n\r\nFor any partition where all of its data was written out to a file in a single chunk, we can move it just like we do today for the multi-threaded shuffle.  We might not be able to send an entire file, but we can send a large chunk of it.\r\n\r\nFor any partition where the data is not all in a single location we are going to have to read in back in and compress it in order.  We can do some compression in multiple threads, but we are going to have to pay attention to memory usage and buffer sizes. If we are okay with some write amplification we can write some number of them in parallel to different files and we try to get the final size and them move them to the final location.\r\n\r\nWe would have to do some profiling and testing to see what is best and especially on what kind of disks.\r\n",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}