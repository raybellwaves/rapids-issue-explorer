{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/7493",
    "id": 1528034773,
    "node_id": "I_kwDOD7z77c5bE_XV",
    "number": 7493,
    "title": "test_nested_pruning_and_case_insensitive failed in Databricks 10.4 (321db) premerge ",
    "user": {
        "login": "gerashegalov",
        "id": 3187938,
        "node_id": "MDQ6VXNlcjMxODc5Mzg=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/gerashegalov",
        "html_url": "https://github.com/gerashegalov",
        "followers_url": "https://api.github.com/users/gerashegalov/followers",
        "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
        "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
        "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
        "repos_url": "https://api.github.com/users/gerashegalov/repos",
        "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
        "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-01-10T21:31:59Z",
    "updated_at": "2023-01-10T21:47:17Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "See #7475  https://github.com/NVIDIA/spark-rapids/actions/runs/3881241731 \r\n\r\n```\r\n[2023-01-10T07:43:15.800Z] =========================== short test summary info ============================\r\n\r\n[2023-01-10T07:43:15.800Z] FAILED ../../src/main/python/parquet_test.py::test_nested_pruning_and_case_insensitive[true--reader_confs5-[['a', Struct(['c_1', String],['c_2', Long],['c_3', Short])]]-[['a', Struct(['c_1', String],['c_3', Short])]]] - py4j.protocol.Py4JJavaError: An error occurred while calling o339761.parquet.\r\n```\r\n\r\n<details>\r\n<summary>full failure report</summary>\r\n<pre>\r\n[2023-01-10T07:43:15.793Z] _ test_nested_pruning_and_case_insensitive[true--reader_confs5-[['a', Struct(['c_1', String],['c_2', Long],['c_3', Short])]]-[['a', Struct(['c_1', String],['c_3', Short])]]] _\r\n\r\n\r\n[2023-01-10T07:43:15.793Z] [gw0] linux -- Python 3.8.15 /databricks/conda/envs/cudf-udf/bin/python\r\n\r\n[2023-01-10T07:43:15.793Z] \r\n\r\n[2023-01-10T07:43:15.793Z] spark_tmp_path = '/tmp/pyspark_tests//0110-052512-bkgwfty7-10-2-128-4-gw0-5955-1525658496/'\r\n\r\n[2023-01-10T07:43:15.793Z] data_gen = [['a', Struct(['c_1', String],['c_2', Long],['c_3', Short])]]\r\n\r\n[2023-01-10T07:43:15.793Z] read_schema = [['a', Struct(['c_1', String],['c_3', Short])]]\r\n\r\n[2023-01-10T07:43:15.793Z] reader_confs = {'spark.rapids.sql.format.parquet.reader.type': 'PERFILE'}\r\n\r\n[2023-01-10T07:43:15.793Z] v1_enabled_list = '', nested_enabled = 'true'\r\n\r\n[2023-01-10T07:43:15.793Z] \r\n\r\n[2023-01-10T07:43:15.793Z]     @pytest.mark.parametrize('data_gen,read_schema', _nested_pruning_schemas, ids=idfn)\r\n\r\n[2023-01-10T07:43:15.793Z]     @pytest.mark.parametrize('reader_confs', reader_opt_confs)\r\n\r\n[2023-01-10T07:43:15.794Z]     @pytest.mark.parametrize('v1_enabled_list', [\"\", \"parquet\"])\r\n\r\n[2023-01-10T07:43:15.794Z]     @pytest.mark.parametrize('nested_enabled', [\"true\", \"false\"])\r\n\r\n[2023-01-10T07:43:15.794Z]     def test_nested_pruning_and_case_insensitive(spark_tmp_path, data_gen, read_schema, reader_confs, v1_enabled_list, nested_enabled):\r\n\r\n[2023-01-10T07:43:15.794Z]         data_path = spark_tmp_path + '/PARQUET_DATA'\r\n\r\n[2023-01-10T07:43:15.794Z] >       with_cpu_session(\r\n\r\n[2023-01-10T07:43:15.794Z]                 lambda spark : gen_df(spark, data_gen).write.parquet(data_path),\r\n\r\n[2023-01-10T07:43:15.794Z]                 conf=rebase_write_corrected_conf)\r\n\r\n[2023-01-10T07:43:15.794Z] \r\n\r\n[2023-01-10T07:43:15.794Z] ../../src/main/python/parquet_test.py:676: \r\n\r\n[2023-01-10T07:43:15.794Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n[2023-01-10T07:43:15.794Z] ../../src/main/python/spark_session.py:113: in with_cpu_session\r\n\r\n[2023-01-10T07:43:15.794Z]     return with_spark_session(func, conf=copy)\r\n\r\n[2023-01-10T07:43:15.794Z] ../../src/main/python/spark_session.py:97: in with_spark_session\r\n\r\n[2023-01-10T07:43:15.794Z]     ret = func(_spark)\r\n\r\n[2023-01-10T07:43:15.794Z] ../../src/main/python/parquet_test.py:677: in <lambda>\r\n\r\n[2023-01-10T07:43:15.794Z]     lambda spark : gen_df(spark, data_gen).write.parquet(data_path),\r\n\r\n[2023-01-10T07:43:15.794Z] /databricks/spark/python/pyspark/sql/readwriter.py:885: in parquet\r\n\r\n[2023-01-10T07:43:15.794Z]     self._jwrite.parquet(path)\r\n\r\n[2023-01-10T07:43:15.794Z] /databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py:1304: in __call__\r\n\r\n[2023-01-10T07:43:15.794Z]     return_value = get_return_value(\r\n\r\n[2023-01-10T07:43:15.794Z] /databricks/spark/python/pyspark/sql/utils.py:117: in deco\r\n\r\n[2023-01-10T07:43:15.794Z]     return f(*a, **kw)\r\n\r\n[2023-01-10T07:43:15.794Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n[2023-01-10T07:43:15.794Z] \r\n\r\n[2023-01-10T07:43:15.794Z] answer = 'xro339762'\r\n\r\n[2023-01-10T07:43:15.794Z] gateway_client = <py4j.clientserver.JavaClient object at 0x7f71f4099bb0>\r\n\r\n[2023-01-10T07:43:15.794Z] target_id = 'o339761', name = 'parquet'\r\n\r\n[2023-01-10T07:43:15.794Z] \r\n\r\n[2023-01-10T07:43:15.794Z]     def get_return_value(answer, gateway_client, target_id=None, name=None):\r\n\r\n[2023-01-10T07:43:15.794Z]         \"\"\"Converts an answer received from the Java gateway into a Python object.\r\n\r\n[2023-01-10T07:43:15.794Z]     \r\n\r\n[2023-01-10T07:43:15.794Z]         For example, string representation of integers are converted to Python\r\n\r\n[2023-01-10T07:43:15.794Z]         integer, string representation of objects are converted to JavaObject\r\n\r\n[2023-01-10T07:43:15.794Z]         instances, etc.\r\n\r\n[2023-01-10T07:43:15.794Z]     \r\n\r\n[2023-01-10T07:43:15.794Z]         :param answer: the string returned by the Java gateway\r\n\r\n[2023-01-10T07:43:15.794Z]         :param gateway_client: the gateway client used to communicate with the Java\r\n\r\n[2023-01-10T07:43:15.794Z]             Gateway. Only necessary if the answer is a reference (e.g., object,\r\n\r\n[2023-01-10T07:43:15.794Z]             list, map)\r\n\r\n[2023-01-10T07:43:15.794Z]         :param target_id: the name of the object from which the answer comes from\r\n\r\n[2023-01-10T07:43:15.794Z]             (e.g., *object1* in `object1.hello()`). Optional.\r\n\r\n[2023-01-10T07:43:15.794Z]         :param name: the name of the member from which the answer comes from\r\n\r\n[2023-01-10T07:43:15.794Z]             (e.g., *hello* in `object1.hello()`). Optional.\r\n\r\n[2023-01-10T07:43:15.794Z]         \"\"\"\r\n\r\n[2023-01-10T07:43:15.794Z]         if is_error(answer)[0]:\r\n\r\n[2023-01-10T07:43:15.794Z]             if len(answer) > 1:\r\n\r\n[2023-01-10T07:43:15.794Z]                 type = answer[1]\r\n\r\n[2023-01-10T07:43:15.794Z]                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n\r\n[2023-01-10T07:43:15.794Z]                 if answer[1] == REFERENCE_TYPE:\r\n\r\n[2023-01-10T07:43:15.794Z] >                   raise Py4JJavaError(\r\n\r\n[2023-01-10T07:43:15.794Z]                         \"An error occurred while calling {0}{1}{2}.\\n\".\r\n\r\n[2023-01-10T07:43:15.794Z]                         format(target_id, \".\", name), value)\r\n\r\n[2023-01-10T07:43:15.794Z] E                   py4j.protocol.Py4JJavaError: An error occurred while calling o339761.parquet.\r\n\r\n[2023-01-10T07:43:15.794Z] E                   : org.apache.spark.SparkException: Job aborted.\r\n\r\n[2023-01-10T07:43:15.794Z] E                   \tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:606)\r\n\r\n[2023-01-10T07:43:15.794Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:360)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:427)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:892)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat sun.reflect.GeneratedMethodAccessor304.invoke(Unknown Source)\r\n\r\n[2023-01-10T07:43:15.795Z] E                   \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.Gateway.invoke(Gateway.java:295)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4314.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4314.0 (TID 17994) (10.2.128.4 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed): \r\n\r\n[2023-01-10T07:43:15.796Z] E                   \r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:735)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:720)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:909)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.scheduler.Task.run(Task.scala:95)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\r\n\r\n[2023-01-10T07:43:15.796Z] E                   \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   Caused by: java.io.EOFException\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \t... 29 more\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \r\n\r\n[2023-01-10T07:43:15.797Z] E                   Driver stacktrace:\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.Option.foreach(Option.scala:407)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2657)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2640)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:325)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \t... 44 more\r\n\r\n[2023-01-10T07:43:15.797Z] E                   Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed): \r\n\r\n[2023-01-10T07:43:15.797Z] E                   \r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:735)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:720)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:909)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\r\n[2023-01-10T07:43:15.797Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.scheduler.Task.run(Task.scala:95)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \t... 1 more\r\n\r\n[2023-01-10T07:43:15.798Z] E                   Caused by: java.io.EOFException\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:894)\r\n\r\n[2023-01-10T07:43:15.798Z] E                   \t... 29 more\r\n</pre>\r\n</details>\r\n\r\nEnvitonment: CI/ 321db\r\n\r\n_Originally posted by @revans2 in https://github.com/NVIDIA/spark-rapids/issues/7475#issuecomment-1377369232_\r\n      ",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/7493/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}