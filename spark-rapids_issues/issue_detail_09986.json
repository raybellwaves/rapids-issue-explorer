{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9986",
    "id": 2030881848,
    "node_id": "I_kwDOD7z77c55DMw4",
    "number": 9986,
    "title": "[FEA] explore maximum memory usage for full window operations",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2586576266,
            "node_id": "MDU6TGFiZWwyNTg2NTc2MjY2",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/task",
            "name": "task",
            "color": "65abf7",
            "default": false,
            "description": "Work required that improves the product but is not user facing"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-12-07T14:31:50Z",
    "updated_at": "2024-04-12T19:07:16Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nRecently as a part of testing https://github.com/NVIDIA/spark-rapids/pull/9973 I found that I needed at least 6 GiB of memory for a single thread to sort the data for the query.\r\n\r\n```\r\nspark.time(spark.range(1000000000).selectExpr(\"id\", \"SUM(id) OVER (ORDER BY id ROWS BETWEEN 0 PRECEDING AND 2 FOLLOWING) as l2f\", \"SUM(id) OVER (ORDER BY id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as l_run\").orderBy(desc(\"id\")).show())\r\n```\r\n\r\nAnd even then the running window was able to complete the work properly, but the bounded window could not. For that I needed about 8 GiB of memory.\r\n\r\nIdeally I would like to see us be able to support this query with just 4 GiB of memory, because that is what our docs say we should be able to do. 4x the target batch size.  This was run with the async allocator, which avoids fragmentation issues so this would not be valid on arena.\r\n\r\n`range` should produce input batches exactly 1 GiB in size. The shuffle is not going to drop the size at all because the data is being sent to a single task. After that the sort should also output batches very close to 1 GiB in size too. So the output of the running window operation should make batches that are 2 GiB in size because it added a new column to the 1 GiB batches.  This means that the range based window would end up likely producing batches that are 3 GiB in size.  So 8 GiB for a 2 GiB input batch is what we would expect to work, but I think we want a way to actually do a split and retry on some of these larger input batches, but if there are other ideas I am open to them.  What I really want to understand is what is happening with sort that causes it to need so much more memory.\r\n\r\nNote that this is not to fix the issues. Just to understand exactly what is happening and file follow on issue so we can fix them.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9986/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}