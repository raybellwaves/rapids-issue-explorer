{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/10761",
    "id": 2275981521,
    "node_id": "I_kwDOD7z77c6HqLjR",
    "number": 10761,
    "title": "[FEA] Improved handling of a large Parquet rowgroup",
    "user": {
        "login": "jlowe",
        "id": 1360766,
        "node_id": "MDQ6VXNlcjEzNjA3NjY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1360766?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jlowe",
        "html_url": "https://github.com/jlowe",
        "followers_url": "https://api.github.com/users/jlowe/followers",
        "following_url": "https://api.github.com/users/jlowe/following{/other_user}",
        "gists_url": "https://api.github.com/users/jlowe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jlowe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jlowe/subscriptions",
        "organizations_url": "https://api.github.com/users/jlowe/orgs",
        "repos_url": "https://api.github.com/users/jlowe/repos",
        "events_url": "https://api.github.com/users/jlowe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jlowe/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 2094500742,
            "node_id": "MDU6TGFiZWwyMDk0NTAwNzQy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/performance",
            "name": "performance",
            "color": "d845b1",
            "default": false,
            "description": "A performance related task/issue"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-05-02T16:36:02Z",
    "updated_at": "2024-05-02T19:25:42Z",
    "closed_at": null,
    "author_association": "MEMBER",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently if a task tries to load a very large rowgroup, either in terms of large number of rows and/or large number of columns, we leverage the sub-rowgroup reader in libcudf to read the rowgroup in batches. However because the on-GPU state of the sub-rowgroup reader is opaque and not spillable, we must iterate to fully load the rowgroup, making each resulting sub-rowgroup batch spillable as we go, to free the on-GPU state of the sub-rowgroup reader and finally proceed with processing the first batch returned from the read.\r\n\r\nThis works fine in practice when the GPU has enough memory to hold the entire rowgroup without spilling.  If it does not, this can perform poorly due to excessive spilling.  This case should be handled better.\r\n\r\n**Describe the solution you'd like**\r\nWhen there isn't enough GPU memory, we could load a single batch via the sub-rowgroup reader and then _close the reader to free the GPU state_.  We then send the batch down the stage iterators for processing.  When it's time to produce the next input batch, we create a new sub-rowgroup reader instance but this time pass a starting row offset matching the last row we left off from the previous batch.  This allows us to process a subset of the rowgroup at a time without needing to manifest the entire rowgroup data at once, potentially spilling heavily during the process.  The downside of course is that we will redundantly transfer and decode some column pages to the GPU, but this may be much faster overall than spilling since it can avoid hitting disks.\r\n\r\n**Describe alternatives you've considered**\r\nAnother approach would be to make the GPU sub-rowgroup state spillable, but this would be a more involved approach requiring changes to the libcudf sub-rowgroup reader.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/10761/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}