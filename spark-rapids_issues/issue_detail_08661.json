{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8661",
    "id": 1789625553,
    "node_id": "I_kwDOD7z77c5qq4TR",
    "number": 8661,
    "title": "[BUG] hash_aggregate_test.py::test_groupby_first_last failed in the UCX build",
    "user": {
        "login": "abellina",
        "id": 1901059,
        "node_id": "MDQ6VXNlcjE5MDEwNTk=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1901059?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abellina",
        "html_url": "https://github.com/abellina",
        "followers_url": "https://api.github.com/users/abellina/followers",
        "following_url": "https://api.github.com/users/abellina/following{/other_user}",
        "gists_url": "https://api.github.com/users/abellina/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abellina/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abellina/subscriptions",
        "organizations_url": "https://api.github.com/users/abellina/orgs",
        "repos_url": "https://api.github.com/users/abellina/repos",
        "events_url": "https://api.github.com/users/abellina/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abellina/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-07-05T13:56:34Z",
    "updated_at": "2023-07-11T20:29:19Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "We saw two hash aggregate failures in the UCX build:\r\n\r\n```\r\n[2023-07-05T00:09:53.931Z] FAILED ../../src/main/python/hash_aggregate_test.py::test_groupby_first_last[Array(Decimal(20,2))][IGNORE_ORDER({'local': True})]\r\n[2023-07-05T00:09:53.931Z] FAILED ../../src/main/python/hash_aggregate_test.py::test_groupby_first_last[Array(Array(Short))][IGNORE_ORDER({'local': True})]\r\n```\r\n\r\nI do NOT see a peer connection issue as described here https://github.com/NVIDIA/spark-rapids/issues/7940. So the problem in this issue seems to be different. I think to debug it we would need to figure out if this test fails with repetition to see if we can get it to happen consistently.\r\n\r\n```\r\n[2023-07-04T20:09:57.430Z] 23/07/04 20:09:50 WARN TaskSetManager: Lost task 3.0 in stage 28708.0 (TID 814183) (10.136.6.4 executor 2): org.apache.spark.shuffle.rapids.RapidsShuffleTimeoutException: Timed out after 120 seconds while waiting for a shuffle batch.\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.shuffle.RapidsShuffleIterator.next(RapidsShuffleIterator.scala:392)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.shuffle.RapidsShuffleIterator.next(RapidsShuffleIterator.scala:51)\r\n[2023-07-04T20:09:57.430Z]      at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n[2023-07-04T20:09:57.430Z]      at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\r\n[2023-07-04T20:09:57.430Z]      at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$next$1(GpuExec.scala:183)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.CollectTimeIterator.next(GpuExec.scala:182)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:310)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:326)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.DynamicGpuPartialSortAggregateIterator.$anonfun$hasNext$4(aggregate.scala:1904)\r\n[2023-07-04T20:09:57.430Z]      at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n[2023-07-04T20:09:57.430Z]      at scala.Option.getOrElse(Option.scala:189)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.DynamicGpuPartialSortAggregateIterator.hasNext(aggregate.scala:1904)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:256)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\r\n[2023-07-04T20:09:57.430Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\r\n[2023-07-04T20:09:57.430Z]      at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n[2023-07-04T20:09:57.430Z]      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n[2023-07-04T20:09:57.430Z]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[2023-07-04T20:09:57.430Z]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[2023-07-04T20:09:57.430Z]      at java.lang.Thread.run(Thread.java:750)\r\n[2023-07-04T20:09:57.430Z]\r\n[2023-07-04T20:09:57.430Z] 23/07/04 20:09:50 ERROR TaskSetManager: Task 3 in stage 28708.0 failed 1 times; aborting job\r\n[2023-07-04T20:09:57.430Z] 23/07/04 20:09:50 WARN TaskSetManager: Lost task 1.0 in stage 28708.0 (TID 814181) (10.136.6.4 executor 3): TaskKilled (Stage cancelled)\r\n[2023-07-04T20:09:57.430Z] ^[[31mFAILED^[[0m^[[31m [ 36%]^[[0m\r\n[2023-07-04T20:09:57.430Z] ../../src/main/python/hash_aggregate_test.py::test_groupby_first_last[Array(Array(Short))][IGNORE_ORDER({'local': True})] 23/07/04 20:09:51 WARN GpuOverrides:\r\n[2023-07-04T20:09:57.430Z]           ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\r\n[2023-07-04T20:09:57.430Z]             @Expression <AttributeReference> a#684387L could run on GPU\r\n[2023-07-04T20:09:57.430Z]             @Expression <AttributeReference> b#684388 could run on GPU\r\n[2023-07-04T20:09:57.430Z]\r\n[2023-07-04T20:12:03.904Z] 23/07/04 20:11:52 WARN TaskSetManager: Lost task 2.0 in stage 28712.0 (TID 814192) (10.136.6.4 executor 2): org.apache.spark.shuffle.rapids.RapidsShuffleTimeoutException: Timed out after 120 seconds while waiting for a shuffle batch.\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.shuffle.RapidsShuffleIterator.next(RapidsShuffleIterator.scala:392)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.shuffle.RapidsShuffleIterator.next(RapidsShuffleIterator.scala:51)\r\n[2023-07-04T20:12:03.904Z]      at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n[2023-07-04T20:12:03.904Z]      at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\r\n[2023-07-04T20:12:03.904Z]      at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$next$1(GpuExec.scala:183)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.CollectTimeIterator.next(GpuExec.scala:182)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.getHasOnDeck(GpuCoalesceBatches.scala:310)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:326)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.DynamicGpuPartialSortAggregateIterator.$anonfun$hasNext$4(aggregate.scala:1904)\r\n[2023-07-04T20:12:03.904Z]      at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n[2023-07-04T20:12:03.904Z]      at scala.Option.getOrElse(Option.scala:189)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.DynamicGpuPartialSortAggregateIterator.hasNext(aggregate.scala:1904)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:256)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\r\n[2023-07-04T20:12:03.904Z]      at com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\r\n[2023-07-04T20:12:03.904Z]      at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n[2023-07-04T20:12:03.904Z]      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n[2023-07-04T20:12:03.904Z]      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[2023-07-04T20:12:03.904Z]      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[2023-07-04T20:12:03.904Z]      at java.lang.Thread.run(Thread.java:750)\r\n[2023-07-04T20:12:03.904Z]\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8661/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}