{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/9988",
    "id": 2030952957,
    "node_id": "I_kwDOD7z77c55DeH9",
    "number": 9988,
    "title": "[FEA] Make GpuBatchedBoundedWindowIterator aware of the partition boundaries",
    "user": {
        "login": "revans2",
        "id": 3441321,
        "node_id": "MDQ6VXNlcjM0NDEzMjE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/3441321?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/revans2",
        "html_url": "https://github.com/revans2",
        "followers_url": "https://api.github.com/users/revans2/followers",
        "following_url": "https://api.github.com/users/revans2/following{/other_user}",
        "gists_url": "https://api.github.com/users/revans2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/revans2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/revans2/subscriptions",
        "organizations_url": "https://api.github.com/users/revans2/orgs",
        "repos_url": "https://api.github.com/users/revans2/repos",
        "events_url": "https://api.github.com/users/revans2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/revans2/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735884,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODg0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/feature%20request",
            "name": "feature request",
            "color": "a2eeef",
            "default": false,
            "description": "New feature or request"
        },
        {
            "id": 4029093938,
            "node_id": "LA_kwDOD7z77c7wJxgy",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/reliability",
            "name": "reliability",
            "color": "2654AF",
            "default": false,
            "description": "Features to improve reliability or bugs that severly impact the reliability of the plugin"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2023-12-07T15:05:00Z",
    "updated_at": "2023-12-13T19:18:31Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Is your feature request related to a problem? Please describe.**\r\nhttps://github.com/NVIDIA/spark-rapids/pull/9973 adds in a new optimization for window operations where it can optimize window operations with a relatively small row based window. It even has a config to be able to control what \"small\" means here.  But this solution does not cover \"large\" windows when in the common case we probably could cover them, and only need a bit more memory when there is skew in the partition by column(s).\r\n\r\nWhat I am proposing is a slight modification of the current `GpuBatchedBoundedWindowIterator`. Currently it is not aware of the partition by column(s) at all. It will just worry about how large the window is and if it has enough data to produce correct answers based off of the window. \r\n\r\n1) Pull in batches until it has enough rows to cover the current window or it all of the data has been read\r\n2) Concat the input with anything that was left over from previous batches\r\n3) Save off as much of the input as will be needed to compute the next batch properly.\r\n4) process the input\r\n5) output as much of it as we know was computed properly\r\n\r\nThe new could would be aware of the partitions and target batch size too.\r\n\r\n1) pull in batches until is has enough rows to cover the current window or the size is larger than the target batch size and we can slice it on a partition boundary.\r\n2) If the size is larger than the target batch size and we can to slice it up by partition boundaries, then do it.\r\n3) process and output anything that has full partition support (including saved data from previous batches including removing rows that were only there to get the right answer).\r\n4) if the size is larger than the target batch size and we cannot slice it by partition boundaries, keep going and hope that we have enough memory to do it.\r\n\r\nThis would ideally let us remove the config that tries to limit the size of the window for the bounded window optimization and just hope that we can fit all of the data into memory for very large windows, because it would be no worse than what we have today, which is what we do for the key grouped iterator.",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/9988/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}