{
    "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210",
    "repository_url": "https://api.github.com/repos/NVIDIA/spark-rapids",
    "labels_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210/labels{/name}",
    "comments_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210/comments",
    "events_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210/events",
    "html_url": "https://github.com/NVIDIA/spark-rapids/issues/8210",
    "id": 1691094904,
    "node_id": "I_kwDOD7z77c5kzA94",
    "number": 8210,
    "title": "[BUG] Databricks 11.3 exception - Multithreaded parquet reader - Missing Credential Scope",
    "user": {
        "login": "tgravescs",
        "id": 4563792,
        "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/tgravescs",
        "html_url": "https://github.com/tgravescs",
        "followers_url": "https://api.github.com/users/tgravescs/followers",
        "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
        "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
        "organizations_url": "https://api.github.com/users/tgravescs/orgs",
        "repos_url": "https://api.github.com/users/tgravescs/repos",
        "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
        "received_events_url": "https://api.github.com/users/tgravescs/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 2061735874,
            "node_id": "MDU6TGFiZWwyMDYxNzM1ODc0",
            "url": "https://api.github.com/repos/NVIDIA/spark-rapids/labels/bug",
            "name": "bug",
            "color": "d73a4a",
            "default": true,
            "description": "Something isn't working"
        }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [
        {
            "login": "tgravescs",
            "id": 4563792,
            "node_id": "MDQ6VXNlcjQ1NjM3OTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4563792?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tgravescs",
            "html_url": "https://github.com/tgravescs",
            "followers_url": "https://api.github.com/users/tgravescs/followers",
            "following_url": "https://api.github.com/users/tgravescs/following{/other_user}",
            "gists_url": "https://api.github.com/users/tgravescs/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tgravescs/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tgravescs/subscriptions",
            "organizations_url": "https://api.github.com/users/tgravescs/orgs",
            "repos_url": "https://api.github.com/users/tgravescs/repos",
            "events_url": "https://api.github.com/users/tgravescs/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tgravescs/received_events",
            "type": "User",
            "site_admin": false
        }
    ],
    "milestone": null,
    "comments": 6,
    "created_at": "2023-05-01T17:26:22Z",
    "updated_at": "2023-08-11T19:09:34Z",
    "closed_at": null,
    "author_association": "COLLABORATOR",
    "active_lock_reason": null,
    "body": "**Describe the bug**\r\n\r\nSeeing an exception running on Databricks 11.3 when reading parquet files, I tried with both 23.04 released jar and 23.06 snapshot jar, here it is using the multithreaded reader. Fails with the coalescing reader as well, but the PERFILE worked.\r\n\r\n```\r\norg.apache.spark.util.TaskCompletionListenerException: org.apache.spark.SparkException: Missing Credential Scope.\r\n\r\nPrevious exception in task: org.apache.spark.SparkException: Missing Credential Scope.\r\n\tjava.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tjava.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.readReadyFiles(GpuMultiFileReader.scala:658)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.getNextBuffersAndMetaAndCombine(GpuMultiFileReader.scala:670)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.getNextBuffersAndMeta(GpuMultiFileReader.scala:706)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.$anonfun$next$1(GpuMultiFileReader.scala:765)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.$anonfun$next$1$adapted(GpuMultiFileReader.scala:740)\r\n\tcom.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tcom.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\tcom.nvidia.spark.rapids.FilePartitionReaderBase.withResource(GpuMultiFileReader.scala:375)\r\n\tcom.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.next(GpuMultiFileReader.scala:740)\r\n\tcom.nvidia.spark.rapids.PartitionIterator.hasNext(dataSourceUtil.scala:29)\r\n\tcom.nvidia.spark.rapids.MetricsBatchIterator.hasNext(dataSourceUtil.scala:46)\r\n\tcom.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.$anonfun$hasNext$1(GpuDataSourceRDD.scala:61)\r\n\tcom.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(GpuDataSourceRDD.scala:61)\r\n\tscala.Option.exists(Option.scala:376)\r\n\tcom.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:61)\r\n\tcom.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.advanceToNextIter(GpuDataSourceRDD.scala:86)\r\n\tcom.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.hasNext(GpuDataSourceRDD.scala:61)\r\n\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\torg.apache.spark.sql.rapids.GpuFileSourceScanExec$$anon$1.hasNext(GpuFileSourceScanExec.scala:469)\r\n\torg.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:317)\r\n\torg.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:340)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.$anonfun$write$2(RapidsShuffleInternalManagerBase.scala:281)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.$anonfun$write$2$adapted(RapidsShuffleInternalManagerBase.scala:274)\r\n\tcom.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tcom.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.withResource(RapidsShuffleInternalManagerBase.scala:234)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.$anonfun$write$1(RapidsShuffleInternalManagerBase.scala:274)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.$anonfun$write$1$adapted(RapidsShuffleInternalManagerBase.scala:273)\r\n\tcom.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\tcom.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.withResource(RapidsShuffleInternalManagerBase.scala:234)\r\n\torg.apache.spark.sql.rapids.RapidsShuffleThreadedWriterBase.write(RapidsShuffleInternalManagerBase.scala:273)\r\n\torg.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\torg.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\r\n\tcom.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\torg.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\r\n\tcom.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\torg.apache.spark.scheduler.Task.doRunTask(Task.scala:169)\r\n\torg.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\r\n\tcom.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\r\n\tcom.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\r\n\tcom.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\r\n\tscala.util.Using$.resource(Using.scala:269)\r\n\tcom.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\r\n\torg.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\r\n\tcom.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\torg.apache.spark.scheduler.Task.run(Task.scala:96)\r\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\r\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\r\n\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\r\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tcom.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\r\n\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tjava.lang.Thread.run(Thread.java:750)\r\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\r\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\r\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\r\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\r\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\r\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\r\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\r\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\r\n\tat scala.util.Using$.resource(Using.scala:269)\r\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\r\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\r\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\tSuppressed: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Missing Credential Scope.\r\n\t\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\t\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n\t\tat com.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.$anonfun$close$1(GpuMultiFileReader.scala:844)\r\n\t\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\t\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\t\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\t\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\t\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\t\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\t\tat com.nvidia.spark.rapids.MultiFileCloudPartitionReaderBase.close(GpuMultiFileReader.scala:842)\r\n\t\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.$anonfun$advanceToNextIter$1(GpuDataSourceRDD.scala:83)\r\n\t\tat com.nvidia.spark.rapids.shims.GpuDataSourceRDD$$anon$1.$anonfun$advanceToNextIter$1$adapted(GpuDataSourceRDD.scala:82)\r\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\r\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\r\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\r\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\r\n\t\t... 21 more\r\n\tCaused by: org.apache.spark.SparkException: Missing Credential Scope.\r\n\t\tat com.databricks.unity.UCSExecutor.$anonfun$currentScope$1(UCSExecutor.scala:68)\r\n\t\tat scala.Option.getOrElse(Option.scala:189)\r\n\t\tat com.databricks.unity.UCSExecutor.currentScope(UCSExecutor.scala:68)\r\n\t\tat com.databricks.unity.UCSExecutor.currentScope$(UCSExecutor.scala:66)\r\n\t\tat com.databricks.unity.UCSExecutor$.currentScope(UCSExecutor.scala:76)\r\n\t\tat com.databricks.unity.UnityCredentialScope$.currentScope(UnityCredentialScope.scala:96)\r\n\t\tat com.databricks.unity.UnityCredentialScope$.getSAMRegistry(UnityCredentialScope.scala:120)\r\n\t\tat com.databricks.unity.SAMRegistry$.getSAM(SAMRegistry.scala:343)\r\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.setDelegates(CredentialScopeFileSystem.scala:133)\r\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.initialize(CredentialScopeFileSystem.scala:178)\r\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\r\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\r\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:43)\r\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:481)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.$anonfun$readAndSimpleFilterFooter$1(GpuParquetScan.scala:616)\r\n\t\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\t\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.withResource(GpuParquetScan.scala:479)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.readAndSimpleFilterFooter(GpuParquetScan.scala:614)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.$anonfun$filterBlocks$1(GpuParquetScan.scala:663)\r\n\t\tat com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)\r\n\t\tat com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.withResource(GpuParquetScan.scala:479)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetFileFilterHandler.filterBlocks(GpuParquetScan.scala:627)\r\n\t\tat com.nvidia.spark.rapids.GpuParquetMultiFilePartitionReaderFactory.$anonfun$buildBaseColumnarReaderForCloud$1(GpuParquetScan.scala:995)\r\n\t\tat com.nvidia.spark.rapids.MultiFileCloudParquetPartitionReader$ReadBatchRunner.doRead(GpuParquetScan.scala:2131)\r\n\t\tat com.nvidia.spark.rapids.MultiFileCloudParquetPartitionReader$ReadBatchRunner.call(GpuParquetScan.scala:2106)\r\n\t\tat com.nvidia.spark.rapids.MultiFileCloudParquetPartitionReader$ReadBatchRunner.call(GpuParquetScan.scala:2087)\r\n\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\t\t... 3 more\r\n```",
    "closed_by": null,
    "reactions": {
        "url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210/timeline",
    "performed_via_github_app": null,
    "state_reason": null
}