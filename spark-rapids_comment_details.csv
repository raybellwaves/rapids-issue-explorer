,url,html_url,issue_url,id,node_id,comment_created_at,comment_updated_at,author_association,comment_text,performed_via_github_app,comment_user.login,user.id,user.node_id,user.avatar_url,user.gravatar_id,user.url,user.html_url,user.followers_url,user.following_url,user.gists_url,user.starred_url,user.subscriptions_url,user.organizations_url,user.repos_url,user.events_url,user.received_events_url,user.type,user.site_admin,reactions.url,comment_reactions.total_count,comment_reactions.+1,comment_reactions.-1,comment_reactions.laugh,comment_reactions.hooray,comment_reactions.confused,comment_reactions.heart,comment_reactions.rocket,comment_reactions.eyes,number
0,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/713542293,https://github.com/NVIDIA/spark-rapids/issues/6#issuecomment-713542293,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6,713542293,MDEyOklzc3VlQ29tbWVudDcxMzU0MjI5Mw==,2020-10-21T12:44:49Z,2020-10-21T12:44:49Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/6572 in cudf to try and support this.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/713542293/reactions,0,0,0,0,0,0,0,0,0,6
1,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959803463,https://github.com/NVIDIA/spark-rapids/issues/9#issuecomment-959803463,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9,959803463,IC_kwDOD7z77c45NXBH,2021-11-03T18:22:26Z,2021-11-03T18:22:26Z,COLLABORATOR,"We have had a customer ask about this, so we might bump up the priority on this.  I have been looking at the JSON parsing and how that relates to the existing CSV parsing.  Just like CSV parsing it is not great, but I think we could do with JSON what we want to do with CSV and parse all of the atomic types as Strings and then handle casting/parsing them ourselves. This will make the code a lot more robust in terms of Spark compatibility.  But there are still a number of issues that we have to look into and probably address.


 1. The current CUDF code does not support nested types for parsing JSON. https://github.com/rapidsai/cudf/issues/8827 should fix that, but it is not done yet.
 2. The current CUDF code parses types how they want to, and not how Spark wants them to be parsed.  Like I said above we might be able to ask CUDF to read all of the types as Strings and then parse them ourselves. This will not fix everything because looking at cast there are a number of types that we do not fully support when casting from a string still. But it will be a step in the right direction and should let us avoid most of the enable this type for JSON configs.  We could at a minimum reused the cast from string configs that already exist.
 3. ""allowSingleQuotes"" option. This is set to true by default so Spark allows parsing values with single quotes instead of just double quotes. The CUDF code does not. So we probably want to file an issue with CUDF to ask for this, and also in the short term document that we cannot support this.
 4. There are also a number of configs in Spark that we will need to play around with in CUDF, but I am fairly sure if they are enabled we will have to fall back to the CPU, or at least really document well the inconsistencies.  These include
   a. ""allowComments"" C/C++ style comments `//` and `/* */`
   b. ""allowUnquotesFieldNames""
   c. ""allowNumericLeadingZeros"" Here the parser strips the leading zeros for numeric JSON types instead of marking them as corrupt entries, so if I had something like `{""a"": 0001, ""b"": 01.1000000000}` both entries would cause the row to be marked as corrupt. Even if we provide a schema an ask for both a and b to be strings they show up as nulls because the values are technically corrupt. If I set the option to true they are parsed, but the leading and in the case of the float, trailing zeros are stripped from the resulting string.
   d. ""allowNonNumericNumbers"" The docs say that this mean INF, -INF, and NaN but I could not make INF work, just -INF. Not sure how well used this is.
   e. ""allowBackslashEscapingAnyCharacter"" typically only a few chars in the JSON standard are allowed. In CUDF they only support \"", \\, \t, \r, and \b.  Not sure if there are others in JSON or not. Also not sure what happens if CUDF if others are encountered vs in Spark.
   f. ""allowUnquotedControlChars"" Docs say ASCII characters with a value less than 32.  My guess is CUDF just allows these all the time.
5. The ""parseMode"" and ""columnNameOfCorruptRecord"" parse mode config probably needs to always be lenient and we need to fall back to the CPU like with CSV if we ever see the columnNameOfCorruptRecord in the schema. We just don't support that and I don't know what it would take for CUDF to be able to do it.
6. ""ignoreNullFields"" and ""dropFieldIfAllNull"" are things we might be able to do with post processing, but we need to play around with it a little.
7. IF we see multi-line we have to fall back to the CPU. We just cannot support it.
8. We need to look at encodings and see if we have to fall back to the CPU or not. I hope that we can normalize things as we read it in like we do with the line ending in CSV and will have to do with the line ending here, but I am not 100% sure on that.
9. We need to check the zoneId like with CSV and fallback if it is not UTC
10. Need to check the date/timestamp formats too and see if there is anything in there that we cannot support.
11. We also need to play around with JSON parsing in general and see what falls out.  It looks like Spark is fairly strict in parsing and there can be odd corner cases. For example it looks like unquoted number values, if they are going to be interpreted as a string, are parsed as numbers first and then cast to a string, So 3.00 becomes 3.0 but ""3.00"" remains unchanged. I don't see any way we can match this behavior and probably will just need to document it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959803463/reactions,0,0,0,0,0,0,0,0,0,9
2,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959809596,https://github.com/NVIDIA/spark-rapids/issues/9#issuecomment-959809596,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9,959809596,IC_kwDOD7z77c45NYg8,2021-11-03T18:30:12Z,2021-11-03T18:30:12Z,COLLABORATOR,"Oh, also a single line can contain multiple entries if the top level is an array.

```
[{""a"": 1, ""b"": 1}, {""a"": 2}]
[{""a"": 1, ""b"": 2.0}]
{""a"": 1, ""b"": ""inf""}
```
produces

```
+---+----+
|  a|   b|
+---+----+
|  1|   1|
|  2|null|
|  1| 2.0|
|  1| inf|
+---+----+
```
But if there is extra JSON like stuff at the end of the line, it is ignored.

```
{""a"": 1, ""b"": 1}, {""other"": 2}
{""a"": 1, ""b"": ""inf""} garbage
{""a"": 1, ""b"": 1} {""more"": 2}
```

produces the following with no errors.  Which feels really odd to me.

```
+---+---+
|  a|  b|
+---+---+
|  1|  1|
|  1|inf|
|  1|  1|
+---+---+
```

There may be a lot of other odd cases, that we need to look into.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959809596/reactions,0,0,0,0,0,0,0,0,0,9
3,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959811580,https://github.com/NVIDIA/spark-rapids/issues/9#issuecomment-959811580,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9,959811580,IC_kwDOD7z77c45NY_8,2021-11-03T18:32:59Z,2021-11-03T18:32:59Z,COLLABORATOR,"We might want to look at some of the Spark JSON tests, but they are not that complete.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959811580/reactions,0,0,0,0,0,0,0,0,0,9
4,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959822790,https://github.com/NVIDIA/spark-rapids/issues/9#issuecomment-959822790,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9,959822790,IC_kwDOD7z77c45NbvG,2021-11-03T18:48:17Z,2021-11-03T18:48:17Z,COLLABORATOR,"Oh that is interesting. The parsing of the JSON keys is case sensitive, but auto detection of the schema is not totally so you can get errors if you let Spark detect the schema and there are keys with different cases. i.e. A vs a. So we should test if we can select the keys in a case sensitive way.

Also if there are multiple keys in a record. For spark it looks like the last one wins. Not sure what CUDF does in those cases.

```
{""a"": 1, ""b"": 1}
{""a"": 1, ""a"": 2}
{""a"": 1, ""b"": 1}
```

produces

```
+---+----+
|  a|   b|
+---+----+
|  1|   1|
|  2|null|
|  1|   1|
+---+----+
```

with no errors",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959822790/reactions,0,0,0,0,0,0,0,0,0,9
5,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959828103,https://github.com/NVIDIA/spark-rapids/issues/9#issuecomment-959828103,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9,959828103,IC_kwDOD7z77c45NdCH,2021-11-03T18:54:49Z,2021-11-03T18:54:49Z,COLLABORATOR,Added Needs Triage back on so we can look at this again because I think most of the analysis of this is done.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/959828103/reactions,0,0,0,0,0,0,0,0,0,9
6,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/662699992,https://github.com/NVIDIA/spark-rapids/issues/12#issuecomment-662699992,https://api.github.com/repos/NVIDIA/spark-rapids/issues/12,662699992,MDEyOklzc3VlQ29tbWVudDY2MjY5OTk5Mg==,2020-07-22T21:14:00Z,2020-07-22T21:14:00Z,COLLABORATOR,Depends on https://github.com/rapidsai/cudf/issues/1352 ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/662699992/reactions,0,0,0,0,0,0,0,0,0,12
7,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708031575,https://github.com/NVIDIA/spark-rapids/issues/22#issuecomment-708031575,https://api.github.com/repos/NVIDIA/spark-rapids/issues/22,708031575,MDEyOklzc3VlQ29tbWVudDcwODAzMTU3NQ==,2020-10-13T21:57:22Z,2020-10-13T21:57:22Z,COLLABORATOR,This depends on #937,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708031575/reactions,0,0,0,0,0,0,0,0,0,22
8,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781542341,https://github.com/NVIDIA/spark-rapids/issues/22#issuecomment-781542341,https://api.github.com/repos/NVIDIA/spark-rapids/issues/22,781542341,MDEyOklzc3VlQ29tbWVudDc4MTU0MjM0MQ==,2021-02-18T18:19:45Z,2021-02-18T18:19:45Z,COLLABORATOR,We could partially implement this now.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781542341/reactions,0,0,0,0,0,0,0,0,0,22
9,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781542685,https://github.com/NVIDIA/spark-rapids/issues/22#issuecomment-781542685,https://api.github.com/repos/NVIDIA/spark-rapids/issues/22,781542685,MDEyOklzc3VlQ29tbWVudDc4MTU0MjY4NQ==,2021-02-18T18:20:18Z,2021-02-18T18:20:18Z,COLLABORATOR,To fully implement this we will need full support for bit for bit identical murmur3 hashing.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781542685/reactions,0,0,0,0,0,0,0,0,0,22
10,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/696976516,https://github.com/NVIDIA/spark-rapids/issues/43#issuecomment-696976516,https://api.github.com/repos/NVIDIA/spark-rapids/issues/43,696976516,MDEyOklzc3VlQ29tbWVudDY5Njk3NjUxNg==,2020-09-22T20:57:40Z,2020-09-22T20:57:40Z,COLLABORATOR,This is an umbrella issue for calendar interval type support. We will tag other calendar interval type related issues to this one.,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/696976516/reactions,0,0,0,0,0,0,0,0,0,43
11,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781539167,https://github.com/NVIDIA/spark-rapids/issues/43#issuecomment-781539167,https://api.github.com/repos/NVIDIA/spark-rapids/issues/43,781539167,MDEyOklzc3VlQ29tbWVudDc4MTUzOTE2Nw==,2021-02-18T18:14:50Z,2021-02-18T18:14:50Z,COLLABORATOR,"I am not 100% sure if we need cudf support for this or not any more.  We can create a struct to match what Spark has, most of which could be time interval types, but months gets to be a little bit harder to support. We would have to store it as an int

INT32: months, DURATION_DAYS: days, DURATION_MICROSECONDS microseconds.

After that it is mostly a question of how/where we can interpret these values, and months is not supported by a lot of cudf APIs because it is not the normal time interval. We would need some follow on work for various operators that could support this, and because most APIs use it for scalar operations we would need at least scalar support for structs.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781539167/reactions,0,0,0,0,0,0,0,0,0,43
12,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781531803,https://github.com/NVIDIA/spark-rapids/issues/65#issuecomment-781531803,https://api.github.com/repos/NVIDIA/spark-rapids/issues/65,781531803,MDEyOklzc3VlQ29tbWVudDc4MTUzMTgwMw==,2021-02-18T18:02:51Z,2021-02-18T18:02:51Z,COLLABORATOR,"This is likely to be a really low priority, because I don't know of any queries where the LIKE pattern is non-scalar.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781531803/reactions,0,0,0,0,0,0,0,0,0,65
13,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781513042,https://github.com/NVIDIA/spark-rapids/issues/69#issuecomment-781513042,https://api.github.com/repos/NVIDIA/spark-rapids/issues/69,781513042,MDEyOklzc3VlQ29tbWVudDc4MTUxMzA0Mg==,2021-02-18T17:34:03Z,2021-02-18T17:34:03Z,COLLABORATOR,"Added that this needs a cudf dependency, but no issue was filed against cudf because this is currently a low priority.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781513042/reactions,0,0,0,0,0,0,0,0,0,69
14,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637667406,https://github.com/NVIDIA/spark-rapids/issues/82#issuecomment-637667406,https://api.github.com/repos/NVIDIA/spark-rapids/issues/82,637667406,MDEyOklzc3VlQ29tbWVudDYzNzY2NzQwNg==,2020-06-02T16:32:18Z,2020-06-02T16:32:18Z,COLLABORATOR,This is a lower priority because the majority of the cases we see from users are covered already.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637667406/reactions,0,0,0,0,0,0,0,0,0,82
15,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133435212,https://github.com/NVIDIA/spark-rapids/issues/82#issuecomment-1133435212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/82,1133435212,IC_kwDOD7z77c5DjtlM,2022-05-20T22:30:35Z,2022-05-20T22:30:35Z,COLLABORATOR,"@revans2 @jlowe, does this similar to https://github.com/rapidsai/cudf/issues/3077 but with nested types support?",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133435212/reactions,0,0,0,0,0,0,0,0,0,82
16,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1135986617,https://github.com/NVIDIA/spark-rapids/issues/82#issuecomment-1135986617,https://api.github.com/repos/NVIDIA/spark-rapids/issues/82,1135986617,IC_kwDOD7z77c5Dtce5,2022-05-24T14:15:15Z,2022-05-24T14:15:15Z,MEMBER,"It does seem like something that could be leveraged for the case where `IN` is dealing with a series of dynamic expressions rather than literals.  However in my limited attempts at trying to get Spark to generate an `IN` expression that did not contain literals, Spark kept turning the query into a join rather than a project with an `IN` expression.

@revans2 @tgravescs do you know under what conditions Spark generates an `IN` with non-literal arguments?  The CPU code is prepped for it, but it was unclear to me what type of query would trigger it.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1135986617/reactions,0,0,0,0,0,0,0,0,0,82
17,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1135998058,https://github.com/NVIDIA/spark-rapids/issues/82#issuecomment-1135998058,https://api.github.com/repos/NVIDIA/spark-rapids/issues/82,1135998058,IC_kwDOD7z77c5DtfRq,2022-05-24T14:24:56Z,2022-05-24T14:24:56Z,COLLABORATOR,"> do you know under what conditions Spark generates an `IN` with non-literal arguments? The CPU code is prepped for it, but it was unclear to me what type of query would trigger it.

```
val df = Seq((1, Array(1, 2, 3)), (2, Array(1, 2, 3)), (3, Array(1, 2, 5))).toDF(""a"", ""ar"")
df.repartition(1).selectExpr(""*"", ""a in (1, 2, 3) as is_in_lit"", ""a in (ar[0], ar[1], ar[2]) as is_in"").show()
...
!Expression <In> a#84 IN (ar#85[0],ar#85[1],ar#85[2]) cannot run on GPU because list only supports IntegerType if it is a literal value
...
+---+---------+---------+-----+
|  a|       ar|is_in_lit|is_in|
+---+---------+---------+-----+
|  1|[1, 2, 3]|     true| true|
|  2|[1, 2, 3]|     true| true|
|  5|[1, 2, 3]|    false|false|
+---+---------+---------+-----+
```
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1135998058/reactions,0,0,0,0,0,0,0,0,0,82
18,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637698735,https://github.com/NVIDIA/spark-rapids/issues/83#issuecomment-637698735,https://api.github.com/repos/NVIDIA/spark-rapids/issues/83,637698735,MDEyOklzc3VlQ29tbWVudDYzNzY5ODczNQ==,2020-06-02T17:33:33Z,2020-06-02T17:33:33Z,COLLABORATOR,I think this is likely a limitation of how python deals with dates/timestamps and not an issue with spark.  We may need to just update how we do tests in those cases.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637698735/reactions,0,0,0,0,0,0,0,0,0,83
19,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944198516,https://github.com/NVIDIA/spark-rapids/issues/83#issuecomment-1944198516,https://api.github.com/repos/NVIDIA/spark-rapids/issues/83,1944198516,IC_kwDOD7z77c5z4h10,2024-02-14T16:38:26Z,2024-02-14T16:38:26Z,MEMBER,@revans2 is this still relevant to track or is this not a plugin issue?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944198516/reactions,0,0,0,0,0,0,0,0,0,83
20,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1946511486,https://github.com/NVIDIA/spark-rapids/issues/83#issuecomment-1946511486,https://api.github.com/repos/NVIDIA/spark-rapids/issues/83,1946511486,IC_kwDOD7z77c50BWh-,2024-02-15T16:32:38Z,2024-02-15T16:32:38Z,COLLABORATOR,"This is still a problem. I still think that this is a P2, but it is still an issue",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1946511486/reactions,0,0,0,0,0,0,0,0,0,83
21,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637669144,https://github.com/NVIDIA/spark-rapids/issues/88#issuecomment-637669144,https://api.github.com/repos/NVIDIA/spark-rapids/issues/88,637669144,MDEyOklzc3VlQ29tbWVudDYzNzY2OTE0NA==,2020-06-02T16:35:22Z,2020-06-02T16:35:22Z,COLLABORATOR,So it looks like the issue is really only in printing a > 4 digit year.  Do we have a cudf issue filed for this? Is spark actually inserting a `+` in front of the `10000` for the year?,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/637669144/reactions,0,0,0,0,0,0,0,0,0,88
22,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642144747,https://github.com/NVIDIA/spark-rapids/issues/88#issuecomment-642144747,https://api.github.com/repos/NVIDIA/spark-rapids/issues/88,642144747,MDEyOklzc3VlQ29tbWVudDY0MjE0NDc0Nw==,2020-06-10T17:13:49Z,2020-06-10T18:18:54Z,COLLABORATOR,"Yes, This is how it's working in Spark I think. 

0001-01-01 00:00:00 is the first day of year 1. 
9999-12-31 23:59:59 is the last day of year 9999 and millenia

after the millenia has ended spark adds a +1 to the year for the subsequent millenia. 

+10001-01-01 00:00:00 is the first day of this millenia
+19999-12-31 23:59:59 is the last day of this millenia

This goes up until +294247-01-09 20:00:54 after which the long will overflow 

So to do this in Spark we will have to know the boundaries of the milllenias in epoch and check the column for any values greater than the boundary and keep adding one until we don't find any more values that are greater than the value

here are the epochs for the millenias starting from -290308 upto +294247 




starting   epoch of millenia | ending epoch of millenia | prepended with
-- | -- | --
-9223372036854.00 | -9213651648423.00 | -29
-9213651648422.00 | -8898082128423.00 | -28
-8898082128422.00 | -8582512608423.00 | -27
-8582512608422.00 | -8266943088423.00 | -26
-8266943088422.00 | -7951373568423.00 | -25
-7951373568422.00 | -7635804048423.00 | -24
-7635804048422.00 | -7320234528423.00 | -23
-7320234528422.00 | -7004665008423.00 | -22
-7004665008422.00 | -6689095488423.00 | -21
-6689095488422.00 | -6373525968423.00 | -20
-6373525968422.00 | -6057956448423.00 | -19
-6057956448422.00 | -5742386928423.00 | -18
-5742386928422.00 | -5426817408423.00 | -17
-5426817408422.00 | -5111247888423.00 | -16
-5111247888422.00 | -4795678368423.00 | -15
-4795678368422.00 | -4480108848423.00 | -14
-4480108848422.00 | -4164539328423.00 | -13
-4164539328422.00 | -3848969808423.00 | -12
-3848969808422.00 | -3533400288423.00 | -11
-3533400288422.00 | -3217830768423.00 | -10
-3217830768422.00 | -2902261248423.00 | -9
-2902261248422.00 | -2586691728423.00 | -8
-2586691728422.00 | -2271122208423.00 | -7
-2271122208422.00 | -1955552688423.00 | -6
-1955552688422.00 | -1639983168423.00 | -5
-1639983168422.00 | -1324413648423.00 | -4
-1324413648422.00 | -1008844128423.00 | -3
-1008844128422.00 | -693274608423.00 | -2
-693274608422.00 | -377705088423.00 | -1
-377705088422.00 | -62167190823.00 | -
-62167190822.00 | 253402329599.00 | na
253402329600.00 | 568971849599.00 | +1
568971849600.00 | 884541369599.00 | +2
884541369600.00 | 1200110889599.00 | +3
1200110889600.00 | 1515680409599.00 | +4
1515680409600.00 | 1831249929599.00 | +5
1831249929600.00 | 2146819449599.00 | +6
2146819449600.00 | 2462388969599.00 | +7
2462388969600.00 | 2777958489599.00 | +8
2777958489600.00 | 3093528009599.00 | +9
3093528009600.00 | 3409097529599.00 | +10
3409097529600.00 | 3724667049599.00 | +11
3724667049600.00 | 4040236569599.00 | +12
4040236569600.00 | 4355806089599.00 | +13
4355806089600.00 | 4671375609599.00 | +14
4671375609600.00 | 4986945129599.00 | +15
4986945129600.00 | 5302514649599.00 | +16
5302514649600.00 | 5618084169599.00 | +17
5618084169600.00 | 5933653689599.00 | +18
5933653689600.00 | 6249223209599.00 | +19
6249223209600.00 | 6564792729599.00 | +20
6564792729600.00 | 6880362249599.00 | +21
6880362249600.00 | 7195931769599.00 | +22
7195931769600.00 | 7511501289599.00 | +23
7511501289600.00 | 7827070809599.00 | +24
7827070809600.00 | 8142640329599.00 | +25
8142640329600.00 | 8458209849599.00 | +26
8458209849600.00 | 8773779369599.00 | +27
8773779369600.00 | 9089348889599.00 | +28
9089348889600.00 | 9223372036854.00 | +29

",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642144747/reactions,0,0,0,0,0,0,0,0,0,88
23,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1195074563,https://github.com/NVIDIA/spark-rapids/issues/102#issuecomment-1195074563,https://api.github.com/repos/NVIDIA/spark-rapids/issues/102,1195074563,IC_kwDOD7z77c5HO2QD,2022-07-26T06:46:57Z,2022-07-26T06:48:08Z,CONTRIBUTOR,"It seems that this issue can be done with #6034 and [PR-5946](https://github.com/NVIDIA/spark-rapids/pull/5946).
We can add some tests case like:
```sql
# 'sort by' guarantee to return the result rows sorted within each partition
select * from table sort by c1 limit 10
# 'order by' guaranteed a global order in the data-frame.
# 'order by' + 'limit offset' will generate plan `TakeOrderedAndProject, GpuTopN`.
select * from table order by c1 limit 10 offset 10
```

With some tricks, e.g. `repartition(1)`, we can generate plan `CollectLimit, GlobalLimit`.",,sinkinben,31923950,MDQ6VXNlcjMxOTIzOTUw,https://avatars.githubusercontent.com/u/31923950?v=4,,https://api.github.com/users/sinkinben,https://github.com/sinkinben,https://api.github.com/users/sinkinben/followers,https://api.github.com/users/sinkinben/following{/other_user},https://api.github.com/users/sinkinben/gists{/gist_id},https://api.github.com/users/sinkinben/starred{/owner}{/repo},https://api.github.com/users/sinkinben/subscriptions,https://api.github.com/users/sinkinben/orgs,https://api.github.com/users/sinkinben/repos,https://api.github.com/users/sinkinben/events{/privacy},https://api.github.com/users/sinkinben/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1195074563/reactions,0,0,0,0,0,0,0,0,0,102
24,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/811998164,https://github.com/NVIDIA/spark-rapids/issues/125#issuecomment-811998164,https://api.github.com/repos/NVIDIA/spark-rapids/issues/125,811998164,MDEyOklzc3VlQ29tbWVudDgxMTk5ODE2NA==,2021-04-01T15:46:13Z,2021-04-01T15:46:21Z,COLLABORATOR,"Inf and -Inf are now partially supported, but they are not configurable.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/811998164/reactions,0,0,0,0,0,0,0,0,0,125
25,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1010592683,https://github.com/NVIDIA/spark-rapids/issues/125#issuecomment-1010592683,https://api.github.com/repos/NVIDIA/spark-rapids/issues/125,1010592683,IC_kwDOD7z77c48PGur,2022-01-12T03:20:37Z,2022-01-12T03:20:37Z,COLLABORATOR,JSON also can't parse NaN.,,wbo4958,1320706,MDQ6VXNlcjEzMjA3MDY=,https://avatars.githubusercontent.com/u/1320706?v=4,,https://api.github.com/users/wbo4958,https://github.com/wbo4958,https://api.github.com/users/wbo4958/followers,https://api.github.com/users/wbo4958/following{/other_user},https://api.github.com/users/wbo4958/gists{/gist_id},https://api.github.com/users/wbo4958/starred{/owner}{/repo},https://api.github.com/users/wbo4958/subscriptions,https://api.github.com/users/wbo4958/orgs,https://api.github.com/users/wbo4958/repos,https://api.github.com/users/wbo4958/events{/privacy},https://api.github.com/users/wbo4958/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1010592683/reactions,0,0,0,0,0,0,0,0,0,125
26,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1023481262,https://github.com/NVIDIA/spark-rapids/issues/125#issuecomment-1023481262,https://api.github.com/repos/NVIDIA/spark-rapids/issues/125,1023481262,IC_kwDOD7z77c49ARWu,2022-01-27T17:41:19Z,2022-01-27T17:41:19Z,CONTRIBUTOR,"I filed https://github.com/NVIDIA/spark-rapids/issues/4644 for supporting `positiveInf`, `negativeInf`, and `nanValue` CSV parser options to reduce the scope of this issue.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1023481262/reactions,0,0,0,0,0,0,0,0,0,125
27,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1533759841,https://github.com/NVIDIA/spark-rapids/issues/129#issuecomment-1533759841,https://api.github.com/repos/NVIDIA/spark-rapids/issues/129,1533759841,IC_kwDOD7z77c5ba1Fh,2023-05-03T21:15:25Z,2023-05-03T21:15:25Z,COLLABORATOR,"For clarity, this is triaged so that CSV reads fall back to the CPU for any non-`\` escape character",,rwlee,10645552,MDQ6VXNlcjEwNjQ1NTUy,https://avatars.githubusercontent.com/u/10645552?v=4,,https://api.github.com/users/rwlee,https://github.com/rwlee,https://api.github.com/users/rwlee/followers,https://api.github.com/users/rwlee/following{/other_user},https://api.github.com/users/rwlee/gists{/gist_id},https://api.github.com/users/rwlee/starred{/owner}{/repo},https://api.github.com/users/rwlee/subscriptions,https://api.github.com/users/rwlee/orgs,https://api.github.com/users/rwlee/repos,https://api.github.com/users/rwlee/events{/privacy},https://api.github.com/users/rwlee/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1533759841/reactions,0,0,0,0,0,0,0,0,0,129
28,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681046129,https://github.com/NVIDIA/spark-rapids/issues/129#issuecomment-1681046129,https://api.github.com/repos/NVIDIA/spark-rapids/issues/129,1681046129,IC_kwDOD7z77c5kMrpx,2023-08-16T17:54:58Z,2023-08-16T17:54:58Z,COLLABORATOR,"CUDF does not support escape characters in CSV yet.

https://github.com/rapidsai/cudf/issues/11984 is to add that in as an option.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681046129/reactions,0,0,0,0,0,0,0,0,0,129
29,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/812081119,https://github.com/NVIDIA/spark-rapids/issues/130#issuecomment-812081119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/130,812081119,MDEyOklzc3VlQ29tbWVudDgxMjA4MTExOQ==,2021-04-01T18:09:58Z,2021-04-01T18:09:58Z,COLLABORATOR,"The white space handling should probably be split off from multi-line support.

CUDF has inconsistent white space handling, but if we can clean it up and get it working properly for strings then we could use the CSV parser to pull back strings for everything, and then clean those stings up with similar code we use for casting.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/812081119/reactions,0,0,0,0,0,0,0,0,0,130
30,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/664412576,https://github.com/NVIDIA/spark-rapids/issues/132#issuecomment-664412576,https://api.github.com/repos/NVIDIA/spark-rapids/issues/132,664412576,MDEyOklzc3VlQ29tbWVudDY2NDQxMjU3Ng==,2020-07-27T13:57:26Z,2020-07-27T13:57:26Z,COLLABORATOR,"As of https://github.com/NVIDIA/spark-rapids/pull/435 reading legacy timestamps prior to 1900 is an error.  Corrected timestamps are not showing issues from 1900 to 1590, but we should retest everything.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/664412576/reactions,0,0,0,0,0,0,0,0,0,132
31,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/897112825,https://github.com/NVIDIA/spark-rapids/issues/132#issuecomment-897112825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/132,897112825,IC_kwDOD7z77c41eNr5,2021-08-11T20:02:06Z,2021-08-11T20:02:26Z,MEMBER,It looks like this works on Spark 3.1+.  Probably not high priority to allow it if nobody is asking for it.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/897112825/reactions,0,0,0,0,0,0,0,0,0,132
32,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/641535074,https://github.com/NVIDIA/spark-rapids/issues/136#issuecomment-641535074,https://api.github.com/repos/NVIDIA/spark-rapids/issues/136,641535074,MDEyOklzc3VlQ29tbWVudDY0MTUzNTA3NA==,2020-06-09T19:52:58Z,2020-06-09T19:52:58Z,COLLABORATOR,"It should be noted that for legacy mode date/timestamp parsing these values are not supported either, so it is likely a low priority to support them.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/641535074/reactions,0,0,0,0,0,0,0,0,0,136
33,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642077726,https://github.com/NVIDIA/spark-rapids/issues/140#issuecomment-642077726,https://api.github.com/repos/NVIDIA/spark-rapids/issues/140,642077726,MDEyOklzc3VlQ29tbWVudDY0MjA3NzcyNg==,2020-06-10T15:17:15Z,2020-06-10T15:17:15Z,MEMBER,"This seems likely to be a bug in libcudf's ORC reader.  Has that been confirmed and if so, is there a corresponding bug against libcudf?",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642077726/reactions,0,0,0,0,0,0,0,0,0,140
34,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642079951,https://github.com/NVIDIA/spark-rapids/issues/140#issuecomment-642079951,https://api.github.com/repos/NVIDIA/spark-rapids/issues/140,642079951,MDEyOklzc3VlQ29tbWVudDY0MjA3OTk1MQ==,2020-06-10T15:21:07Z,2020-06-10T15:21:07Z,COLLABORATOR,I have only filed issues against our project so far.  I have not had time to dig into any of them and confirm if there are cudf issues or not.  @jlowe if that is something you want to do feel free to dig into it. ,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/642079951/reactions,0,0,0,0,0,0,0,0,0,140
35,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1245584142,https://github.com/NVIDIA/spark-rapids/issues/140#issuecomment-1245584142,https://api.github.com/repos/NVIDIA/spark-rapids/issues/140,1245584142,IC_kwDOD7z77c5KPhsO,2022-09-13T15:32:08Z,2022-09-16T18:11:19Z,COLLABORATOR,"Is this still relevant? It seems we just had a fixed in https://github.com/rapidsai/cudf/issues/11525 but there is still https://github.com/rapidsai/cudf/issues/11691.

Edit: Found that the merged PR didn't fix this. It fixed orc reader but didn't fix orc writer.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1245584142/reactions,0,0,0,0,0,0,0,0,0,140
36,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/662712160,https://github.com/NVIDIA/spark-rapids/issues/143#issuecomment-662712160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/143,662712160,MDEyOklzc3VlQ29tbWVudDY2MjcxMjE2MA==,2020-07-22T21:41:47Z,2020-07-22T21:41:47Z,COLLABORATOR,CuDF only supports brotli at this time.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/662712160/reactions,0,0,0,0,0,0,0,0,0,143
37,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/644275338,https://github.com/NVIDIA/spark-rapids/issues/175#issuecomment-644275338,https://api.github.com/repos/NVIDIA/spark-rapids/issues/175,644275338,MDEyOklzc3VlQ29tbWVudDY0NDI3NTMzOA==,2020-06-15T17:43:41Z,2020-06-15T17:43:41Z,COLLABORATOR,"I think a part of this is going to be making our code work with stage level scheduling.  It is simple to say if the entire job needs GPUs or not, but on the GPU side we need to make sure that the operators can tell spark for each stage that we translated something to the GPU that it is needed.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/644275338/reactions,0,0,0,0,0,0,0,0,0,175
38,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/675706192,https://github.com/NVIDIA/spark-rapids/issues/175#issuecomment-675706192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/175,675706192,MDEyOklzc3VlQ29tbWVudDY3NTcwNjE5Mg==,2020-08-18T20:39:23Z,2020-08-18T20:39:23Z,COLLABORATOR,"After discussion, the right thing to do is create an example where we can demonstrate running an ETL job with the plugin and then in a subsequent stage running still on the GPU but without the plugin.  For example, an ETL job followed by an XGBoost job reading from Parquet files.  In between stages the configuration of the executors will change.    

This can then be turned into a test case.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/675706192/reactions,0,0,0,0,0,0,0,0,0,175
39,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/680262340,https://github.com/NVIDIA/spark-rapids/issues/268#issuecomment-680262340,https://api.github.com/repos/NVIDIA/spark-rapids/issues/268,680262340,MDEyOklzc3VlQ29tbWVudDY4MDI2MjM0MA==,2020-08-25T20:49:11Z,2020-08-25T20:49:11Z,MEMBER,"> We need a way to support patterns that are vectors

To clarify, the pattern vector is a column of patterns that has the same row count as the string column to be parsed, i.e.: each row can have a custom format string describing how the timestamp should be parsed.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/680262340/reactions,0,0,0,0,0,0,0,0,0,268
40,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/650373367,https://github.com/NVIDIA/spark-rapids/issues/294#issuecomment-650373367,https://api.github.com/repos/NVIDIA/spark-rapids/issues/294,650373367,MDEyOklzc3VlQ29tbWVudDY1MDM3MzM2Nw==,2020-06-26T20:02:46Z,2020-06-26T20:02:46Z,COLLABORATOR,I filed https://issues.apache.org/jira/browse/SPARK-32110 to document what I have found in spark.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/650373367/reactions,0,0,0,0,0,0,0,0,0,294
41,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/653216189,https://github.com/NVIDIA/spark-rapids/issues/294#issuecomment-653216189,https://api.github.com/repos/NVIDIA/spark-rapids/issues/294,653216189,MDEyOklzc3VlQ29tbWVudDY1MzIxNjE4OQ==,2020-07-02T20:55:04Z,2020-07-02T20:55:04Z,COLLABORATOR,"Some findings when compared against Apache Hive 3.x:

1. Literals: Both Hive CLI and SparkSQL treat the literals `0.0` and `-0.0` as equivalent. i.e. `0.0 = -0.0 `is `TRUE`. `SELECT 0.0 as a, -0.0 as b` selects `0.0` and `0.0`.
1. From data sources/files: The Spark REPL (and Scala, I’m guessing) treat the same literals as distinct. We can use this to write `-0.0` into a file. E.g. `Seq((-0.0, 0.0)).toDF.write.orc()` writes distinct values.
1. Equi-join: Hive 3 does not normalize float/double. Joining `0.0` and `-0.0` from ORC-file sources does not match rows. Spark normalizes, and thus matches.
1. Inequality joins: Both Hive 3 and SparkSQL 3 matches on `-0.0` < `0.0`. This is because neither normalizes on non-equijoins.

So in this regard, the only material difference between Hive and SparkSQL is that on equijoins, Hive does not normalize, and treats `-0.0` as distinct from `0.0`. It is consistent(ly wrong?) within itself. Spark normalizes, but only for equijoin.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/653216189/reactions,0,0,0,0,0,0,0,0,0,294
42,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732301313,https://github.com/NVIDIA/spark-rapids/issues/294#issuecomment-732301313,https://api.github.com/repos/NVIDIA/spark-rapids/issues/294,732301313,MDEyOklzc3VlQ29tbWVudDczMjMwMTMxMw==,2020-11-23T17:15:14Z,2020-11-23T17:15:14Z,COLLABORATOR,"I filed https://github.com/rapidsai/cudf/issues/6834 in cudf so we can work around things with bit-wise operations if possible. I believe that we should be able to make comparisons and sort match exactly with Spark.  On joins we are going to have a much harder time, but we still might be able to do it.  We need to be very careful with this though.  -0.0 and the various NaN values are rather rare in real life. I am not sure if it is worth the added performance cost for sort to do this, and the join I am especially concerned about what it would take to make it work.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732301313/reactions,0,0,0,0,0,0,0,0,0,294
43,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/677884999,https://github.com/NVIDIA/spark-rapids/issues/334#issuecomment-677884999,https://api.github.com/repos/NVIDIA/spark-rapids/issues/334,677884999,MDEyOklzc3VlQ29tbWVudDY3Nzg4NDk5OQ==,2020-08-20T20:24:46Z,2020-08-20T20:25:00Z,COLLABORATOR,"the CartesianProductExec is disabled by default in the plugin so this probably isn't super high priority, but it still is an issue, here is the exception:

   ```
                : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 23, ip-10-59-250-3.us-west-2.compute.internal, executor driver): java.lang.ArrayIndexOutOfBoundsException: 0
E                   	at ai.rapids.cudf.Table.<init>(Table.java:52)
E                   	at com.nvidia.spark.rapids.GpuColumnVector.from(GpuColumnVector.java:253)
E                   	at org.apache.spark.sql.rapids.execution.GpuBroadcastNestedLoopJoinExecBase$.$anonfun$innerLikeJoin$2(GpuBroadcastNestedLoopJoinExec.scala:107)
E                   	at com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
E                   	at com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
E                   	at org.apache.spark.sql.rapids.execution.GpuBroadcastNestedLoopJoinExecBase$.withResource(GpuBroadcastNestedLoopJoinExec.scala:92)
E                   	at org.apache.spark.sql.rapids.execution.GpuBroadcastNestedLoopJoinExecBase$.$anonfun$innerLikeJoin$1(GpuBroadcastNestedLoopJoinExec.scala:106)
```",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/677884999/reactions,0,0,0,0,0,0,0,0,0,334
44,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656849732,https://github.com/NVIDIA/spark-rapids/issues/343#issuecomment-656849732,https://api.github.com/repos/NVIDIA/spark-rapids/issues/343,656849732,MDEyOklzc3VlQ29tbWVudDY1Njg0OTczMg==,2020-07-10T19:22:19Z,2020-07-10T19:22:19Z,COLLABORATOR,"I am just curious how much effort we want to put into this.  If it is just a string, then we can include it in the description we already have implemented.  Or perhaps a separate field like we do for incompat.

https://github.com/NVIDIA/spark-rapids/blob/468abaa328d45e330ea4a7f7918201784b5a82e3/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuOverrides.scala#L1724",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656849732/reactions,0,0,0,0,0,0,0,0,0,343
45,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656851610,https://github.com/NVIDIA/spark-rapids/issues/343#issuecomment-656851610,https://api.github.com/repos/NVIDIA/spark-rapids/issues/343,656851610,MDEyOklzc3VlQ29tbWVudDY1Njg1MTYxMA==,2020-07-10T19:27:02Z,2020-07-10T19:27:02Z,COLLABORATOR,"Sorry one more thing.  The problem with all of this is trying to keep everything in sync, like any other documentation.  If we have to keep a text string somewhere in sync with the code it describes, we want to be sure that the code and the string are as close to each other as possible, and even then they will get out of sync.  The best would be to do something with the actual code.  If we could make some of the checks data driven, like input types, and scalar vs non-scalar parameters, then we could have both the documentation and the checks run off of the same code.  That to me is what we should be shooting for.

Annotations don't appear to solve any of that for me.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656851610/reactions,0,0,0,0,0,0,0,0,0,343
46,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656866674,https://github.com/NVIDIA/spark-rapids/issues/343#issuecomment-656866674,https://api.github.com/repos/NVIDIA/spark-rapids/issues/343,656866674,MDEyOklzc3VlQ29tbWVudDY1Njg2NjY3NA==,2020-07-10T20:05:53Z,2020-07-10T20:07:08Z,COLLABORATOR,"> The best would be to do something with the actual code. If we could make some of the checks data driven, like input types, and scalar vs non-scalar parameters, then we could have both the documentation and the checks run off of the same code. That to me is what we should be shooting for.

I agree, but this goes back to the level of effort. I think for types, the effort is worth it though, and I can see ways of tackling it. It seems to me that given a set of `DataType` we should be able to call a static function that returns some kind of _SupportLevel_ instance that can state that level: ""supported"", ""not supported"", ""supported IF condition"", and the condition expressed in a way that can be evaluated (hands waving).

> Annotations don't appear to solve any of that for me.

Well they do bring the info closer to the [code](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala#L26), but yeah no guarantee that this would be updated => it's not executable. That said I do not know what the alternative is, because there could be things not easily captured (or not possible to capture), and it seems like good code higene just like keeping function headers up to date, and adding a proper description (something in the checklist). I can think of ways to express that the gpu join only supports certain `JoinType` instances, something like:

```
  validIf(join.joinType)
      .matches(Inner, Cross)
      .otherwise(s""$join.joinType currently is not supported"")
```

But then you have the cases where we depend on something at runtime:

```
    val left = childPlans.head.convertIfNeeded()
    val right = childPlans(1).convertIfNeeded()
    // The broadcast part of this must be a BroadcastExchangeExec
    val buildSide = join.buildSide match {
      case BuildLeft => left
      case BuildRight => right
    }
    if (!buildSide.isInstanceOf[GpuBroadcastExchangeExec]) {
      throw new IllegalStateException(""the broadcast must be on the GPU too"")
    }
```

And yeah this one we can't really capture in a good way. Unless I am missing something.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/656866674/reactions,0,0,0,0,0,0,0,0,0,343
47,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/658449132,https://github.com/NVIDIA/spark-rapids/issues/357#issuecomment-658449132,https://api.github.com/repos/NVIDIA/spark-rapids/issues/357,658449132,MDEyOklzc3VlQ29tbWVudDY1ODQ0OTEzMg==,2020-07-14T22:43:39Z,2020-07-14T22:43:39Z,COLLABORATOR,"Hello there, @arunraman. Could I please request for more detail?

>  for Spark nested array 3 levels

Something like `List<Struct<int, string>>`?
What window functions did you have in mind? Like `collect_list()`?",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/658449132/reactions,0,0,0,0,0,0,0,0,0,357
48,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/673630467,https://github.com/NVIDIA/spark-rapids/issues/357#issuecomment-673630467,https://api.github.com/repos/NVIDIA/spark-rapids/issues/357,673630467,MDEyOklzc3VlQ29tbWVudDY3MzYzMDQ2Nw==,2020-08-13T18:09:18Z,2020-08-13T18:09:18Z,NONE,"@mythrocks I had requested this functionality in a conversation with @arunraman. Your intuition is correct on both counts.

The order of operations would be:
- Input dataframe has an index column and all other columns collapsed into a struct `Struct<datetime, int, string, float, boolean>`.
- We use `collect_list()` to collect the structs by the index which results in `List<Struct<datetime, int, string, float, boolean>>` per index
- We then sort each list based on the datetime column in the Struct.",,shazraz,28982321,MDQ6VXNlcjI4OTgyMzIx,https://avatars.githubusercontent.com/u/28982321?v=4,,https://api.github.com/users/shazraz,https://github.com/shazraz,https://api.github.com/users/shazraz/followers,https://api.github.com/users/shazraz/following{/other_user},https://api.github.com/users/shazraz/gists{/gist_id},https://api.github.com/users/shazraz/starred{/owner}{/repo},https://api.github.com/users/shazraz/subscriptions,https://api.github.com/users/shazraz/orgs,https://api.github.com/users/shazraz/repos,https://api.github.com/users/shazraz/events{/privacy},https://api.github.com/users/shazraz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/673630467/reactions,0,0,0,0,0,0,0,0,0,357
49,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781374884,https://github.com/NVIDIA/spark-rapids/issues/357#issuecomment-781374884,https://api.github.com/repos/NVIDIA/spark-rapids/issues/357,781374884,MDEyOklzc3VlQ29tbWVudDc4MTM3NDg4NA==,2021-02-18T14:18:05Z,2021-02-18T14:18:05Z,COLLABORATOR,"After the 0.4 release of the RAPIDS Accelerator we should have GPU accelerated support for everything except for sorting within the array `array_sort`.  CUDF has added some support for this in the 0.18 release, but Spark wants to use a higher order function for the ordering, which is not going to be simple to implement.

@shazraz  and @arunraman do you have some examples of the functions that you use for sorting the array?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/781374884/reactions,0,0,0,0,0,0,0,0,0,357
50,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/745390833,https://github.com/NVIDIA/spark-rapids/issues/363#issuecomment-745390833,https://api.github.com/repos/NVIDIA/spark-rapids/issues/363,745390833,MDEyOklzc3VlQ29tbWVudDc0NTM5MDgzMw==,2020-12-15T16:06:32Z,2020-12-15T16:06:32Z,NONE,"This is not ""oppresive language"", but technical language. This does not oppress anyone in their right mind, i.e. not pursuing nonsense revolutionary goals rather than doing something useful.",,pococito,16767388,MDQ6VXNlcjE2NzY3Mzg4,https://avatars.githubusercontent.com/u/16767388?v=4,,https://api.github.com/users/pococito,https://github.com/pococito,https://api.github.com/users/pococito/followers,https://api.github.com/users/pococito/following{/other_user},https://api.github.com/users/pococito/gists{/gist_id},https://api.github.com/users/pococito/starred{/owner}{/repo},https://api.github.com/users/pococito/subscriptions,https://api.github.com/users/pococito/orgs,https://api.github.com/users/pococito/repos,https://api.github.com/users/pococito/events{/privacy},https://api.github.com/users/pococito/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/745390833/reactions,1,1,0,0,0,0,0,0,0,363
51,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683821248,https://github.com/NVIDIA/spark-rapids/issues/424#issuecomment-683821248,https://api.github.com/repos/NVIDIA/spark-rapids/issues/424,683821248,MDEyOklzc3VlQ29tbWVudDY4MzgyMTI0OA==,2020-08-31T14:40:00Z,2020-08-31T14:40:00Z,COLLABORATOR,This should be in backlog as we dont need these right away,,kuhushukla,20541681,MDQ6VXNlcjIwNTQxNjgx,https://avatars.githubusercontent.com/u/20541681?v=4,,https://api.github.com/users/kuhushukla,https://github.com/kuhushukla,https://api.github.com/users/kuhushukla/followers,https://api.github.com/users/kuhushukla/following{/other_user},https://api.github.com/users/kuhushukla/gists{/gist_id},https://api.github.com/users/kuhushukla/starred{/owner}{/repo},https://api.github.com/users/kuhushukla/subscriptions,https://api.github.com/users/kuhushukla/orgs,https://api.github.com/users/kuhushukla/repos,https://api.github.com/users/kuhushukla/events{/privacy},https://api.github.com/users/kuhushukla/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683821248/reactions,0,0,0,0,0,0,0,0,0,424
52,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1025262630,https://github.com/NVIDIA/spark-rapids/issues/507#issuecomment-1025262630,https://api.github.com/repos/NVIDIA/spark-rapids/issues/507,1025262630,IC_kwDOD7z77c49HEQm,2022-01-30T23:47:15Z,2022-01-30T23:47:15Z,COLLABORATOR,Removing this from the sprint milestones as this is an overarching feature with sub-tasks for each sprint.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1025262630/reactions,0,0,0,0,0,0,0,0,0,507
53,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/671982734,https://github.com/NVIDIA/spark-rapids/issues/545#issuecomment-671982734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/545,671982734,MDEyOklzc3VlQ29tbWVudDY3MTk4MjczNA==,2020-08-11T14:32:08Z,2020-08-11T14:32:08Z,COLLABORATOR,"Note that I modified the premerge build to run the unit tests in series:
https://github.com/NVIDIA/spark-rapids/blob/branch-0.2/jenkins/spark-premerge-build.sh#L42

But it only runs integration against 3.0.0 right now.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/671982734/reactions,0,0,0,0,0,0,0,0,0,545
54,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/672278979,https://github.com/NVIDIA/spark-rapids/issues/545#issuecomment-672278979,https://api.github.com/repos/NVIDIA/spark-rapids/issues/545,672278979,MDEyOklzc3VlQ29tbWVudDY3MjI3ODk3OQ==,2020-08-11T21:05:06Z,2020-08-11T21:05:06Z,COLLABORATOR,"Let's get premerge tests running in parallel for 3.0 and 3.0.1 for now, as these are the Spark versions we are targeting for our 0.2 release.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/672278979/reactions,0,0,0,0,0,0,0,0,0,545
55,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/672551509,https://github.com/NVIDIA/spark-rapids/issues/545#issuecomment-672551509,https://api.github.com/repos/NVIDIA/spark-rapids/issues/545,672551509,MDEyOklzc3VlQ29tbWVudDY3MjU1MTUwOQ==,2020-08-12T03:33:50Z,2020-08-12T03:34:30Z,COLLABORATOR,"Currently we are working with blossom team to migrate `premerge` build to blossom jenkins, 
`parallel testing` definitely should be a must-have feature, stay tuned.",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/672551509/reactions,0,0,0,0,0,0,0,0,0,545
56,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687277136,https://github.com/NVIDIA/spark-rapids/issues/620#issuecomment-687277136,https://api.github.com/repos/NVIDIA/spark-rapids/issues/620,687277136,MDEyOklzc3VlQ29tbWVudDY4NzI3NzEzNg==,2020-09-04T17:14:32Z,2020-09-04T17:14:32Z,COLLABORATOR,"The plan is rewritten when there are multiple distincts along with non-distinct aggregations in the query by https://github.com/apache/spark/blob/1597d8fcd4c68e723eb3152335298c7d05155643/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDistinctAggregates.scala
 The computation is done where the regular aggregation expressions and every distinct clause is aggregated
 in a separate group.
The plan would not have `distinct` identifier anymore.

Currently, we are falling back to CPU for this case as follows.
In `partial` mode, if there is an aggregation within `If` of `First` expression -> fall back to CPU.
Non-distinct aggregations are computed by including `First` as mentioned in the bug. And in the plan, it includes an `If` expr for the aggs. 


",,nartal1,50492963,MDQ6VXNlcjUwNDkyOTYz,https://avatars.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687277136/reactions,0,0,0,0,0,0,0,0,0,620
57,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683171454,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683171454,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,683171454,MDEyOklzc3VlQ29tbWVudDY4MzE3MTQ1NA==,2020-08-28T22:22:19Z,2020-08-28T22:22:19Z,MEMBER,@gourav-sg can you create this issue in the spark-xgboost-examples repo?  That's where I think the new getting started guide notebook for xgboost would be expected.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683171454/reactions,0,0,0,0,0,0,0,0,0,629
58,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683252656,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683252656,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,683252656,MDEyOklzc3VlQ29tbWVudDY4MzI1MjY1Ng==,2020-08-29T07:48:18Z,2020-08-29T07:48:18Z,NONE,"This has nothing to do with xgboost, this request is for documentation on using SPARK SQL with GPU. What I was pointing out is that the authors should kindly note that while having documentation on xgboost looks great, one should also focus on creating documentation on 80% of data science, which largely includes running SPARK SQL for data preparation/ feature engineering.",,gourav-sg,1283523,MDQ6VXNlcjEyODM1MjM=,https://avatars.githubusercontent.com/u/1283523?v=4,,https://api.github.com/users/gourav-sg,https://github.com/gourav-sg,https://api.github.com/users/gourav-sg/followers,https://api.github.com/users/gourav-sg/following{/other_user},https://api.github.com/users/gourav-sg/gists{/gist_id},https://api.github.com/users/gourav-sg/starred{/owner}{/repo},https://api.github.com/users/gourav-sg/subscriptions,https://api.github.com/users/gourav-sg/orgs,https://api.github.com/users/gourav-sg/repos,https://api.github.com/users/gourav-sg/events{/privacy},https://api.github.com/users/gourav-sg/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683252656/reactions,0,0,0,0,0,0,0,0,0,629
59,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683297394,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683297394,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,683297394,MDEyOklzc3VlQ29tbWVudDY4MzI5NzM5NA==,2020-08-29T14:21:33Z,2020-08-29T14:21:33Z,MEMBER,"The headline of the request mentions updating latest examples, and the first line in the description of the issue is: `Create new documentation here https://github.com/NVIDIA/spark-xgboost-examples`.  If that is the intent of this request then it belongs in the spark-xgboost-examples repo, not here.

If it isn't that and does belong to this project, we need more clarity on exactly what is expected to be done in this repo to resolve this issue.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683297394/reactions,0,0,0,0,0,0,0,0,0,629
60,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683682302,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683682302,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,683682302,MDEyOklzc3VlQ29tbWVudDY4MzY4MjMwMg==,2020-08-31T09:53:26Z,2020-08-31T09:53:26Z,NONE,"Hi,

We need to have updated notebooks which tells us how to use SPARK SQL with
GPU, this has nothing to do with xgboost, as its applications comes after
the data/ features have been prepared for which most of the users will be
using SPARK SQL.

I am primarily focused on EMR, in AWS, and was working with the community
until wee hours of early morning to figure out the gaps which are not
documented, of presented in outdated and therefore misleading manner in
other places.

So we should have a simple notebook which should show how to start a EMR
cluster bootstrap it with proper CUDA packages, and then run SPARK in it.
Basically an end to end example where people will be able to follow the
instructions and start using SPARK over GPU in EMR.

Currently the fantastic community in NVIDIA has given those packages, but
due to lack of documentation or misleading documentation getting it to use
is a herculean task.


Thanks and Regards,
Gourav Sengupta


On Sat, Aug 29, 2020 at 3:21 PM Jason Lowe <notifications@github.com> wrote:

> The headline of the request mentions updating latest examples, and the
> first line in the description of the issue is: Create new documentation
> here https://github.com/NVIDIA/spark-xgboost-examples. If that is the
> intent of this request then it belongs in the spark-xgboost-examples repo,
> not here.
>
> If it isn't that and does belong to this project, we need more clarity on
> exactly what is expected to be done in this repo to resolve this issue.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683297394>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAJZLQ2UFXJ5Z7FGZ5DEG6LSDEFHRANCNFSM4QOSGERQ>
> .
>
",,gourav-sg,1283523,MDQ6VXNlcjEyODM1MjM=,https://avatars.githubusercontent.com/u/1283523?v=4,,https://api.github.com/users/gourav-sg,https://github.com/gourav-sg,https://api.github.com/users/gourav-sg/followers,https://api.github.com/users/gourav-sg/following{/other_user},https://api.github.com/users/gourav-sg/gists{/gist_id},https://api.github.com/users/gourav-sg/starred{/owner}{/repo},https://api.github.com/users/gourav-sg/subscriptions,https://api.github.com/users/gourav-sg/orgs,https://api.github.com/users/gourav-sg/repos,https://api.github.com/users/gourav-sg/events{/privacy},https://api.github.com/users/gourav-sg/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683682302/reactions,0,0,0,0,0,0,0,0,0,629
61,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683792472,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-683792472,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,683792472,MDEyOklzc3VlQ29tbWVudDY4Mzc5MjQ3Mg==,2020-08-31T13:52:39Z,2020-08-31T13:52:39Z,MEMBER,"Thanks @gourav-sg.  I'm assuming you want a new notebook for AWS EMR under `docs/demo/` then?

If so, I'd like to get the summary and description updated which refers to https://github.com/NVIDIA/spark-xgboost-examples and an Amazon blog post that is not about this repository, leading to a lot of confusion about what's proposed in this issue.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/683792472/reactions,0,0,0,0,0,0,0,0,0,629
62,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684974192,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-684974192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,684974192,MDEyOklzc3VlQ29tbWVudDY4NDk3NDE5Mg==,2020-09-01T16:17:55Z,2020-09-01T16:17:55Z,CONTRIBUTOR,"@gourav-sg, thanks for reporting the issue and review both the AWS blog and our github example code. Your feedback is critical to us.  Like you said, the AWS blog post is for XGBoost GPU accelerated training on Spark 2.x EMR Cluster.   The EMR team has a recent release 6.1.0 including Spark 3.0. We are still working with EMR team to enable GPU Spark 3.0 feature which is mainly the Spark SQL ETL you are interested.  We updated this github site to a new site: https://github.com/mgzhao/spark-xgboost-examples with two branches:  spark-3 and spark-2.  The spark-3 branch does have ETL example code on databrick/standalone.   I am working on publishing a detailed standalone EC2 cluster guide for GPU Spark 3.0.  Once we have GPU Spark 3.0 fully supported on EMR,  we will update you here and also add all the ETL examples/guide on Github.     ",,mgzhao,39813658,MDQ6VXNlcjM5ODEzNjU4,https://avatars.githubusercontent.com/u/39813658?v=4,,https://api.github.com/users/mgzhao,https://github.com/mgzhao,https://api.github.com/users/mgzhao/followers,https://api.github.com/users/mgzhao/following{/other_user},https://api.github.com/users/mgzhao/gists{/gist_id},https://api.github.com/users/mgzhao/starred{/owner}{/repo},https://api.github.com/users/mgzhao/subscriptions,https://api.github.com/users/mgzhao/orgs,https://api.github.com/users/mgzhao/repos,https://api.github.com/users/mgzhao/events{/privacy},https://api.github.com/users/mgzhao/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684974192/reactions,0,0,0,0,0,0,0,0,0,629
63,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685015404,https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-685015404,https://api.github.com/repos/NVIDIA/spark-rapids/issues/629,685015404,MDEyOklzc3VlQ29tbWVudDY4NTAxNTQwNA==,2020-09-01T17:25:00Z,2020-09-01T17:25:00Z,NONE,"Hi,
thanks a ton for responding back to me. To be honest enough what NVIDIA is
trying to do with SPARK and the rest of the concepts is truly re-building
the entire concept of data to value. You cannot imagine the number of half
minded, brain dead, bespoke, and technically redundant solutions in the
market which are trying to sell themselves off just trying to align
engineering and data science efforts into a single view.

NVIDIA did that with SPARK, and it will be great to start using that. It
should not take more than 8 to 10 hours of work to get an example up and
ready. And I will be eagerly looking ahead for the same.

SPARK 3.x.x was released by Amazon EMR last week, and we should have been
able to start using some of the most exciting and promising features of
SPARK 3 by NVIDIA by now :)

Regards,
Gourav Sengupta


On Tue, Sep 1, 2020 at 5:18 PM mgzhao <notifications@github.com> wrote:

> @gourav-sg <https://github.com/gourav-sg>, thanks for reporting the issue
> and review both the AWS blog and our github example code. Your feedback is
> critical to us. Like you said, the AWS blog post is for XGBoost GPU
> accelerated training on Spark 2.x EMR Cluster. The EMR team has a recent
> release 6.1.0 including Spark 3.0. We are still working with EMR team to
> enable GPU Spark 3.0 feature which is mainly the Spark SQL ETL you are
> interested. We updated this github site to a new site:
> https://github.com/mgzhao/spark-xgboost-examples with two branches:
> spark-3 and spark-2. The spark-3 branch does have ETL example code on
> databrick/standalone. I am working on publishing a detailed standalone EC2
> cluster guide for GPU Spark 3.0. Once we have GPU Spark 3.0 fully supported
> on EMR, we will update you here and also add all the ETL examples/guide on
> Github.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/NVIDIA/spark-rapids/issues/629#issuecomment-684974192>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAJZLQ4SPBTRXY5OBT4RG63SDUNEHANCNFSM4QOSGERQ>
> .
>
",,gourav-sg,1283523,MDQ6VXNlcjEyODM1MjM=,https://avatars.githubusercontent.com/u/1283523?v=4,,https://api.github.com/users/gourav-sg,https://github.com/gourav-sg,https://api.github.com/users/gourav-sg/followers,https://api.github.com/users/gourav-sg/following{/other_user},https://api.github.com/users/gourav-sg/gists{/gist_id},https://api.github.com/users/gourav-sg/starred{/owner}{/repo},https://api.github.com/users/gourav-sg/subscriptions,https://api.github.com/users/gourav-sg/orgs,https://api.github.com/users/gourav-sg/repos,https://api.github.com/users/gourav-sg/events{/privacy},https://api.github.com/users/gourav-sg/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685015404/reactions,0,0,0,0,0,0,0,0,0,629
64,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684662579,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-684662579,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,684662579,MDEyOklzc3VlQ29tbWVudDY4NDY2MjU3OQ==,2020-09-01T09:33:18Z,2020-09-01T09:41:08Z,NONE,"I had already started doing this work (just started, not much work yet), And I think it's better to listen to the experts from NV firstly",,JustPlay,5866501,MDQ6VXNlcjU4NjY1MDE=,https://avatars.githubusercontent.com/u/5866501?v=4,,https://api.github.com/users/JustPlay,https://github.com/JustPlay,https://api.github.com/users/JustPlay/followers,https://api.github.com/users/JustPlay/following{/other_user},https://api.github.com/users/JustPlay/gists{/gist_id},https://api.github.com/users/JustPlay/starred{/owner}{/repo},https://api.github.com/users/JustPlay/subscriptions,https://api.github.com/users/JustPlay/orgs,https://api.github.com/users/JustPlay/repos,https://api.github.com/users/JustPlay/events{/privacy},https://api.github.com/users/JustPlay/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684662579/reactions,0,0,0,0,0,0,0,0,0,635
65,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684939656,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-684939656,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,684939656,MDEyOklzc3VlQ29tbWVudDY4NDkzOTY1Ng==,2020-09-01T15:29:32Z,2020-09-01T15:29:32Z,COLLABORATOR,"I do like the idea of allowing more tasks through based off of the amount of free memory available in the system, but I do have a few concerns with the approach.  

The first is fragmentation, which we are working hard to find solutions to.  Having 5GB free does not mean that all 5GB are usable by another task.  There has been discussion about exposing a metric from RMM, something like largest possible allocation, to give us some kind of an idea about how bad fragmentation is.  That way we could take that into account too.

Second is that we use a BufferManager currently as a part of the UCX shuffle work, and we plan on using it more in the future for general processing, so we can hold more data on the GPU, but only spill it when we have to. You would have to find a way to take this into account in deciding what can go on the GPU and what cannot, what is more because we try to fill up the GPU it can also interfere with our measure of fragmentation, so we might need some statistics from the BufferManager about what is spillable and how big those buffers are. 

My final concern is that memory allocation is all or nothing in many cases and predicting how much operating memory is needed can be very hard.  For example there is a join in the plan.  In one case nothing matches and there is no output.  In another case everything matches badly and we get a combinitorial explosion.  If there is not enough memory free for the later case the task fails.  If we are constantly trying to push the GPU to the limits of what it can do we are going to hit these types of situations more and more.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/684939656/reactions,0,0,0,0,0,0,0,0,0,635
66,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685549733,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-685549733,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,685549733,MDEyOklzc3VlQ29tbWVudDY4NTU0OTczMw==,2020-09-02T10:08:59Z,2020-09-02T10:08:59Z,NONE,https://github.com/rapidsai/rmm/issues/536,,JustPlay,5866501,MDQ6VXNlcjU4NjY1MDE=,https://avatars.githubusercontent.com/u/5866501?v=4,,https://api.github.com/users/JustPlay,https://github.com/JustPlay,https://api.github.com/users/JustPlay/followers,https://api.github.com/users/JustPlay/following{/other_user},https://api.github.com/users/JustPlay/gists{/gist_id},https://api.github.com/users/JustPlay/starred{/owner}{/repo},https://api.github.com/users/JustPlay/subscriptions,https://api.github.com/users/JustPlay/orgs,https://api.github.com/users/JustPlay/repos,https://api.github.com/users/JustPlay/events{/privacy},https://api.github.com/users/JustPlay/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685549733/reactions,0,0,0,0,0,0,0,0,0,635
67,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685773926,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-685773926,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,685773926,MDEyOklzc3VlQ29tbWVudDY4NTc3MzkyNg==,2020-09-02T14:28:31Z,2020-09-02T14:28:31Z,MEMBER,"@JustPlay that issue alone does not solve fragmentation problems.  It's designed to target the particularly bad fragmentation that can occur when using multiple streams, as without it each stream is essentially fragmenting the space from the perspective of other streams.

We have seen significant fragmentation in some cases with only a single stream (e.g.: cannot allocate 1.8GB when only 1.7GB of total GPU memory is allocated).  Resolving that issue will not address those scenarios.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/685773926/reactions,0,0,0,0,0,0,0,0,0,635
68,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/686275415,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-686275415,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,686275415,MDEyOklzc3VlQ29tbWVudDY4NjI3NTQxNQ==,2020-09-03T06:06:41Z,2020-09-03T06:06:41Z,NONE,"> There has been discussion about exposing a metric from RMM, something like largest possible allocation, to give us some kind of an idea about how bad fragmentation is. That way we could take that into account too.

While I may add tracking of the largest available block to the pool memory resource, knowing the largest possible allocation requires syncing and merging all streams' free lists, which is not what you likely want to do until you fail to allocate.",,harrism,783069,MDQ6VXNlcjc4MzA2OQ==,https://avatars.githubusercontent.com/u/783069?v=4,,https://api.github.com/users/harrism,https://github.com/harrism,https://api.github.com/users/harrism/followers,https://api.github.com/users/harrism/following{/other_user},https://api.github.com/users/harrism/gists{/gist_id},https://api.github.com/users/harrism/starred{/owner}{/repo},https://api.github.com/users/harrism/subscriptions,https://api.github.com/users/harrism/orgs,https://api.github.com/users/harrism/repos,https://api.github.com/users/harrism/events{/privacy},https://api.github.com/users/harrism/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/686275415/reactions,0,0,0,0,0,0,0,0,0,635
69,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687169199,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-687169199,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,687169199,MDEyOklzc3VlQ29tbWVudDY4NzE2OTE5OQ==,2020-09-04T14:09:51Z,2020-09-04T14:09:51Z,NONE,"> > There has been discussion about exposing a metric from RMM, something like largest possible allocation, to give us some kind of an idea about how bad fragmentation is. That way we could take that into account too.
> 
> While I may add tracking of the largest available block to the pool memory resource, knowing the largest possible allocation requires syncing and merging all streams' free lists, which is not what you likely want to do until you fail to allocate.

No, i think you can ` tracking of the largest available block(s) to the pool memory resource` without doing stream sync (only give the user a info: there exists one or more block(s), but not guarantee usable immediately), let the sync to de done at the `actually allocating time`  (just let the task wait on the stream)

",,JustPlay,5866501,MDQ6VXNlcjU4NjY1MDE=,https://avatars.githubusercontent.com/u/5866501?v=4,,https://api.github.com/users/JustPlay,https://github.com/JustPlay,https://api.github.com/users/JustPlay/followers,https://api.github.com/users/JustPlay/following{/other_user},https://api.github.com/users/JustPlay/gists{/gist_id},https://api.github.com/users/JustPlay/starred{/owner}{/repo},https://api.github.com/users/JustPlay/subscriptions,https://api.github.com/users/JustPlay/orgs,https://api.github.com/users/JustPlay/repos,https://api.github.com/users/JustPlay/events{/privacy},https://api.github.com/users/JustPlay/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687169199/reactions,0,0,0,0,0,0,0,0,0,635
70,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687176045,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-687176045,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,687176045,MDEyOklzc3VlQ29tbWVudDY4NzE3NjA0NQ==,2020-09-04T14:21:26Z,2020-09-04T14:21:26Z,COLLABORATOR,"Also because the memory usage is changeing all the time this number will likely be out of date the instant it is reported.  I am a bit skeptical that a point in time measurement is going to give us what we want, but at least it would be a starting point.  We can iterate on rolling average memory usage and other things if we see some kind of improvement from the point in time metrics.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687176045/reactions,0,0,0,0,0,0,0,0,0,635
71,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687740032,https://github.com/NVIDIA/spark-rapids/issues/635#issuecomment-687740032,https://api.github.com/repos/NVIDIA/spark-rapids/issues/635,687740032,MDEyOklzc3VlQ29tbWVudDY4Nzc0MDAzMg==,2020-09-06T09:38:42Z,2020-09-06T09:38:42Z,NONE,I don't think you got my point. The largest current free block is not the same as largest possible allocation. Knowing the latter requires merging all free lists to coalesce blocks from multiple streams.,,harrism,783069,MDQ6VXNlcjc4MzA2OQ==,https://avatars.githubusercontent.com/u/783069?v=4,,https://api.github.com/users/harrism,https://github.com/harrism,https://api.github.com/users/harrism/followers,https://api.github.com/users/harrism/following{/other_user},https://api.github.com/users/harrism/gists{/gist_id},https://api.github.com/users/harrism/starred{/owner}{/repo},https://api.github.com/users/harrism/subscriptions,https://api.github.com/users/harrism/orgs,https://api.github.com/users/harrism/repos,https://api.github.com/users/harrism/events{/privacy},https://api.github.com/users/harrism/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/687740032/reactions,0,0,0,0,0,0,0,0,0,635
72,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732303067,https://github.com/NVIDIA/spark-rapids/issues/654#issuecomment-732303067,https://api.github.com/repos/NVIDIA/spark-rapids/issues/654,732303067,MDEyOklzc3VlQ29tbWVudDczMjMwMzA2Nw==,2020-11-23T17:18:07Z,2020-11-23T17:18:07Z,COLLABORATOR,@revans2 I believe this is done as part of your [PR](https://github.com/NVIDIA/spark-rapids/pull/1086/),,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732303067/reactions,0,0,0,0,0,0,0,0,0,654
73,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732312560,https://github.com/NVIDIA/spark-rapids/issues/654#issuecomment-732312560,https://api.github.com/repos/NVIDIA/spark-rapids/issues/654,732312560,MDEyOklzc3VlQ29tbWVudDczMjMxMjU2MA==,2020-11-23T17:33:44Z,2020-11-23T17:33:44Z,COLLABORATOR,"My PR took a different approach and ran the tests in parallel.  This does speed things up a lot, but I expect us to add more tests especially as we add in more data types and more operators.  I don't necessarily want to drop the idea of tagging tests too, and that could help speed up the tests even more.  I am going to drop the Release and put it back into need triage so we can look at it again.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/732312560/reactions,0,0,0,0,0,0,0,0,0,654
74,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/733253465,https://github.com/NVIDIA/spark-rapids/issues/654#issuecomment-733253465,https://api.github.com/repos/NVIDIA/spark-rapids/issues/654,733253465,MDEyOklzc3VlQ29tbWVudDczMzI1MzQ2NQ==,2020-11-24T21:50:42Z,2020-11-24T21:50:42Z,COLLABORATOR,"First step is to apply parallelism to all integration tests.  Second is to consider other alternatives like tagging tests for each environment, or dynamically selecting tests based on the changes.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/733253465/reactions,0,0,0,0,0,0,0,0,0,654
75,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944267176,https://github.com/NVIDIA/spark-rapids/issues/739#issuecomment-1944267176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/739,1944267176,IC_kwDOD7z77c5z4ymo,2024-02-14T17:16:56Z,2024-02-14T17:16:56Z,MEMBER,@abellina is this still relevant?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944267176/reactions,0,0,0,0,0,0,0,0,0,739
76,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944267929,https://github.com/NVIDIA/spark-rapids/issues/742#issuecomment-1944267929,https://api.github.com/repos/NVIDIA/spark-rapids/issues/742,1944267929,IC_kwDOD7z77c5z4yyZ,2024-02-14T17:17:23Z,2024-02-14T17:17:23Z,MEMBER,@abellina is this still relevant?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1944267929/reactions,0,0,0,0,0,0,0,0,0,742
77,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/697721117,https://github.com/NVIDIA/spark-rapids/issues/831#issuecomment-697721117,https://api.github.com/repos/NVIDIA/spark-rapids/issues/831,697721117,MDEyOklzc3VlQ29tbWVudDY5NzcyMTExNw==,2020-09-23T17:13:04Z,2020-09-23T17:13:04Z,CONTRIBUTOR,There is a good example of collecting stage and task level metrics via the Spark listeners in https://github.com/LucaCanali/sparkMeasure and this could easily be incorporated into `BenchUtils.runBench`. ,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/697721117/reactions,0,0,0,0,0,0,0,0,0,831
78,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/697955131,https://github.com/NVIDIA/spark-rapids/issues/831#issuecomment-697955131,https://api.github.com/repos/NVIDIA/spark-rapids/issues/831,697955131,MDEyOklzc3VlQ29tbWVudDY5Nzk1NTEzMQ==,2020-09-23T20:28:43Z,2020-09-23T20:28:43Z,COLLABORATOR,"actually I'm assigning myself to https://github.com/NVIDIA/spark-rapids/issues/773 to do an initial investigation if we have a problem, this one is to add it to benchmarking.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/697955131/reactions,0,0,0,0,0,0,0,0,0,831
79,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764794157,https://github.com/NVIDIA/spark-rapids/issues/932#issuecomment-1764794157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/932,1764794157,IC_kwDOD7z77c5pMJ8t,2023-10-16T15:56:23Z,2023-10-16T15:56:23Z,MEMBER,"Fixing this is relatively straightforward.  GPU write classes should *not* be columnar, but instead produce rows.  Most writes produce no output, and the ones that do only produce a single row, so this is not a performance problem.  That will eliminate the transition at the end of the plan, but then the transition moves above the write node since it is not columnar but the GPU exec child is.  To fix that, GPU write classes should also inherit from `ColumnarToRowTransition` so Spark knows not to inject an extra transition between a GPU write and its GPU exec child even though the GPU write does not produce columnar output.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764794157/reactions,0,0,0,0,0,0,0,0,0,932
80,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708506298,https://github.com/NVIDIA/spark-rapids/issues/946#issuecomment-708506298,https://api.github.com/repos/NVIDIA/spark-rapids/issues/946,708506298,MDEyOklzc3VlQ29tbWVudDcwODUwNjI5OA==,2020-10-14T16:12:07Z,2020-10-14T16:12:07Z,COLLABORATOR,"Sadly we don't have a great way to do it right now.  

Normally you could look at the GPU itself but because we use a pooling memory manager (RMM) the amount used is hidden because we have allocated a large block of memory up front.

For some operators we have a peak memory metric, but it just tells you about a single task not the actual state of the GPU and it is mostly looking at the inputs and output it does not tell how much memory is being used by the algorithm itself.

We could have visibility into exactly how much memory is being requested at any point in time by way of intercepting calls to the pooling memory manager, but have no easy way in Spark to export that data. We really just use it right now to see if we should try and free up memory to allow an allocation to succeed. Even then we do not know how fragmented the memory is.

@jlowe @tgravescs did I miss anything?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708506298/reactions,0,0,0,0,0,0,0,0,0,946
81,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708553678,https://github.com/NVIDIA/spark-rapids/issues/946#issuecomment-708553678,https://api.github.com/repos/NVIDIA/spark-rapids/issues/946,708553678,MDEyOklzc3VlQ29tbWVudDcwODU1MzY3OA==,2020-10-14T17:37:08Z,2020-10-14T17:37:08Z,MEMBER,"Agree with @revans2 there isn't a great way to do it today.

We're already hooked into most GPU allocations via a custom RMM memory handler used for JVM callbacks on OOM, so tracking total allocated RMM memory from the pool would be very straightforward (albeit with no information on pool fragmentation).  However reporting this via Spark metrics in a useful way is unclear.  This seems to be ideally an executor metric reporting peak GPU memory, but I believe Spark's executor metrics are a fixed set and not extensible to cover concepts like GPU memory.  We may want to propose an Apache Spark feature to make executor metrics extensible by plugins, but that is probably not a simple, straightforward task.

That leaves us with task/SQL metrics.  We could track the maximum RMM pool allocation level, getting and resetting this max level atomically when a task completes.  Then at least one task will show the true maximum RMM pool memory used. However this gets a bit less useful when using the RAPIDS shuffle manager which caches shuffle outputs in GPU memory, as some of the GPU memory used and ""charged"" to a particular task/stage could have been produced in an earlier stage or even by an unrelated job.  In that case it may be useful for a task SQL metric to report both peak total RMM pool usage and peak spillable GPU memory usage, although even with those two metrics one can't assume both peaks were achieved at the same time.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708553678/reactions,0,0,0,0,0,0,0,0,0,946
82,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708564889,https://github.com/NVIDIA/spark-rapids/issues/946#issuecomment-708564889,https://api.github.com/repos/NVIDIA/spark-rapids/issues/946,708564889,MDEyOklzc3VlQ29tbWVudDcwODU2NDg4OQ==,2020-10-14T17:57:05Z,2020-10-14T17:57:30Z,COLLABORATOR,"On executor metrics, we can add metrics via the metrics registry in the executor. The registry is available via the `SparkEnv`. These are reported using the standard codahale metrics system, so it's not something that shows up in the UI, but can be piped to CSV at least with a Console sink, Prometheus, or other sinks as well.

I am working on a change to add some metrics for shuffle, but not for GPU memory metrics. If @jlowe @revans2 think this would be a good idea perhaps we can add pool memory usage to these metrics.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708564889/reactions,0,0,0,0,0,0,0,0,0,946
83,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708568475,https://github.com/NVIDIA/spark-rapids/issues/946#issuecomment-708568475,https://api.github.com/repos/NVIDIA/spark-rapids/issues/946,708568475,MDEyOklzc3VlQ29tbWVudDcwODU2ODQ3NQ==,2020-10-14T18:03:08Z,2020-10-14T18:03:08Z,MEMBER,"> These are reported using the standard codahale metrics system, so it's not something that shows up in the UI

Yes, sorry, I was referring to the UI.  We can definitely report custom executor metrics, but if they are not available in the standard Spark web UI and eventlog then IMHO the utility in practice is significantly diminished.  I agree that having _some_ way to get the metrics beats nothing. :smile:",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/708568475/reactions,0,0,0,0,0,0,0,0,0,946
84,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/721178395,https://github.com/NVIDIA/spark-rapids/issues/958#issuecomment-721178395,https://api.github.com/repos/NVIDIA/spark-rapids/issues/958,721178395,MDEyOklzc3VlQ29tbWVudDcyMTE3ODM5NQ==,2020-11-03T14:58:58Z,2020-11-03T14:58:58Z,COLLABORATOR,@razajafri why do we need to audit this any further? COuld you elaborate here? Thanks!,,kuhushukla,20541681,MDQ6VXNlcjIwNTQxNjgx,https://avatars.githubusercontent.com/u/20541681?v=4,,https://api.github.com/users/kuhushukla,https://github.com/kuhushukla,https://api.github.com/users/kuhushukla/followers,https://api.github.com/users/kuhushukla/following{/other_user},https://api.github.com/users/kuhushukla/gists{/gist_id},https://api.github.com/users/kuhushukla/starred{/owner}{/repo},https://api.github.com/users/kuhushukla/subscriptions,https://api.github.com/users/kuhushukla/orgs,https://api.github.com/users/kuhushukla/repos,https://api.github.com/users/kuhushukla/events{/privacy},https://api.github.com/users/kuhushukla/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/721178395/reactions,0,0,0,0,0,0,0,0,0,958
85,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725268820,https://github.com/NVIDIA/spark-rapids/issues/958#issuecomment-725268820,https://api.github.com/repos/NVIDIA/spark-rapids/issues/958,725268820,MDEyOklzc3VlQ29tbWVudDcyNTI2ODgyMA==,2020-11-11T07:59:02Z,2020-11-11T07:59:02Z,COLLABORATOR,To support this we will need to make change in cudf to expose an API for the user to set/unset whether to ignore the lines starting with commentPrefix. ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725268820/reactions,0,0,0,0,0,0,0,0,0,958
86,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725443838,https://github.com/NVIDIA/spark-rapids/issues/958#issuecomment-725443838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/958,725443838,MDEyOklzc3VlQ29tbWVudDcyNTQ0MzgzOA==,2020-11-11T14:09:40Z,2020-11-11T14:09:40Z,COLLABORATOR,"@razajafri if I read through the cudf code '\0' appears to disable comments

https://github.com/rapidsai/cudf/blob/aaba25034c3b927fc1fb7a80ab4947eb06a6c6b5/cpp/src/io/csv/csv_gpu.cu#L1015
https://github.com/rapidsai/cudf/blob/aaba25034c3b927fc1fb7a80ab4947eb06a6c6b5/cpp/src/io/csv/csv_gpu.cu#L1115

So what we would need to do is to fall back to the CPU if someone actually set the comment character to '\0' instead of going off of the default.  We probably also should update the docs in cudf to explain this.


Because this is CSV and only for a very small corner case I don't think it is that critical to fix right now so dropping the priority.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725443838/reactions,0,0,0,0,0,0,0,0,0,958
87,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/713839558,https://github.com/NVIDIA/spark-rapids/issues/997#issuecomment-713839558,https://api.github.com/repos/NVIDIA/spark-rapids/issues/997,713839558,MDEyOklzc3VlQ29tbWVudDcxMzgzOTU1OA==,2020-10-21T19:59:04Z,2020-10-21T19:59:04Z,COLLABORATOR,"spark properties are for configuration of spark, it's not really for state information on the nodes you run on.  there is the runtime information, but that is easier because it should be global. The data you are asking for here could be different per executor. Executors can come and go with dynamic allocation and failures.  Really this is more of a spark generic feature request because you could want this for cpu type disk type, network type, etc.. There are some monitoring things spark has with metrics but none that cover this. 

This could also potentially be done in the executor plugins if you have somewhere to report it too and it wouldn't be the event log.

Unfortunately not a real easy way to do this right now and ideally would probably be part of the executor registration information",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/713839558/reactions,0,0,0,0,0,0,0,0,0,997
88,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/742247237,https://github.com/NVIDIA/spark-rapids/issues/1019#issuecomment-742247237,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1019,742247237,MDEyOklzc3VlQ29tbWVudDc0MjI0NzIzNw==,2020-12-10T05:20:52Z,2020-12-10T05:20:52Z,COLLABORATOR,"@razajafri should this be tracked in #1143?  

Please add a label to this.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/742247237/reactions,0,0,0,0,0,0,0,0,0,1019
89,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/742661265,https://github.com/NVIDIA/spark-rapids/issues/1019#issuecomment-742661265,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1019,742661265,MDEyOklzc3VlQ29tbWVudDc0MjY2MTI2NQ==,2020-12-10T17:12:49Z,2020-12-10T17:12:49Z,COLLABORATOR,I have added this to #1143 ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/742661265/reactions,0,0,0,0,0,0,0,0,0,1019
90,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/728695002,https://github.com/NVIDIA/spark-rapids/issues/1070#issuecomment-728695002,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1070,728695002,MDEyOklzc3VlQ29tbWVudDcyODY5NTAwMg==,2020-11-17T05:27:25Z,2020-11-17T05:27:56Z,COLLABORATOR,@mythrocks  Any updates on cuDF side ?,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/728695002/reactions,0,0,0,0,0,0,0,0,0,1070
91,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/728697358,https://github.com/NVIDIA/spark-rapids/issues/1070#issuecomment-728697358,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1070,728697358,MDEyOklzc3VlQ29tbWVudDcyODY5NzM1OA==,2020-11-17T05:35:13Z,2020-11-17T05:35:13Z,COLLABORATOR,"This is not blocking the functionality of `WindowInPandasUDF`, we can move it to 0.4 if it can not be done before 0.3 release.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/728697358/reactions,0,0,0,0,0,0,0,0,0,1070
92,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/746960393,https://github.com/NVIDIA/spark-rapids/issues/1070#issuecomment-746960393,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1070,746960393,MDEyOklzc3VlQ29tbWVudDc0Njk2MDM5Mw==,2020-12-16T20:26:40Z,2020-12-16T20:26:40Z,COLLABORATOR,"@firestarman, 

If I understand this correctly, the `GpuWindowInPandasUDF` now calculates the group bounds, using `groupby` + `count`. This calculation will be on GPU.

Should we use this issue to track the request for an API for exposing window bounds, not group bounds?",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/746960393/reactions,0,0,0,0,0,0,0,0,0,1070
93,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/747098504,https://github.com/NVIDIA/spark-rapids/issues/1070#issuecomment-747098504,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1070,747098504,MDEyOklzc3VlQ29tbWVudDc0NzA5ODUwNA==,2020-12-16T23:17:19Z,2020-12-16T23:17:19Z,COLLABORATOR,yes. updated,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/747098504/reactions,0,0,0,0,0,0,0,0,0,1070
94,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725169521,https://github.com/NVIDIA/spark-rapids/issues/1091#issuecomment-725169521,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1091,725169521,MDEyOklzc3VlQ29tbWVudDcyNTE2OTUyMQ==,2020-11-11T04:03:10Z,2020-11-11T04:03:10Z,COLLABORATOR,Please update the documentation at https://github.com/NVIDIA/spark-rapids/blob/branch-0.3/docs/compatibility.md#csv-dates when this is fixed.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/725169521/reactions,0,0,0,0,0,0,0,0,0,1091
95,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679498286,https://github.com/NVIDIA/spark-rapids/issues/1091#issuecomment-1679498286,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1091,1679498286,IC_kwDOD7z77c5kGxwu,2023-08-15T19:40:22Z,2023-08-15T19:40:22Z,COLLABORATOR,"I think that this is working now.

```
scala> spark.read.schema(StructType(Seq(StructField(""dates"", DateType, false), StructField(""ints"", IntegerType)))).csv(""./test.csv"").collect.foreach(System.out.println)
23/08/15 19:39:03 WARN GpuOverrides: 
*Exec <FileSourceScanExec> will run on GPU

[2019-01-03,1]                                                                  
[2019-01-03,1]
[2019-01-03,1]
[2019-01-05,2]
[2019-01-05,3]
[2019-01-06,6]

scala> spark.conf.set(""spark.rapids.sql.enabled"", false)

scala> spark.read.schema(StructType(Seq(StructField(""dates"", DateType, false), StructField(""ints"", IntegerType)))).csv(""./test.csv"").collect.foreach(System.out.println)
[2019-01-03,1]
[2019-01-03,1]
[2019-01-03,1]
[2019-01-05,2]
[2019-01-05,3]
[2019-01-06,6]

```

Could we get someone to add a test to verify that it continues to work.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679498286/reactions,0,0,0,0,0,0,0,0,0,1091
96,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/731377678,https://github.com/NVIDIA/spark-rapids/issues/1143#issuecomment-731377678,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1143,731377678,MDEyOklzc3VlQ29tbWVudDczMTM3NzY3OA==,2020-11-20T19:56:04Z,2020-11-20T19:56:04Z,COLLABORATOR,This is a follow on to our work on Spark Cache that was done as a part of #444 ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/731377678/reactions,0,0,0,0,0,0,0,0,0,1143
97,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/736827854,https://github.com/NVIDIA/spark-rapids/issues/1225#issuecomment-736827854,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1225,736827854,MDEyOklzc3VlQ29tbWVudDczNjgyNzg1NA==,2020-12-01T21:20:56Z,2020-12-01T21:20:56Z,COLLABORATOR,Need to investigate ParquetCachedBatchSerializer and other processing whether the new types similar to string types need to be updated in the plugin.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/736827854/reactions,0,0,0,0,0,0,0,0,0,1225
98,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758023540,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-758023540,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,758023540,MDEyOklzc3VlQ29tbWVudDc1ODAyMzU0MA==,2021-01-11T15:27:55Z,2021-01-11T15:27:55Z,COLLABORATOR,"How exactly would you want the computing to be parallelized?  Would you expect to have a single task run across multiple GPUs at the same time? or do you want a single executor to select the best free GPU to use when assigning a task? CUDF does not support either of these use cases yet. A single task using multiple GPUs would require CUDF to rewrite most of their kernels/algorithms to try and do this, which is not simple, but might be doable.  If you want multiple tasks to run on different GPUs we can sort of do that today, but you have to have multiple separate executors. Is there a reason you cannot ask Spark to launch the executor with only 1 GPU and proportionally less tasks?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758023540/reactions,0,0,0,0,0,0,0,0,0,1486
99,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758447177,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-758447177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,758447177,MDEyOklzc3VlQ29tbWVudDc1ODQ0NzE3Nw==,2021-01-12T06:52:29Z,2021-01-12T06:52:29Z,NONE,"> How exactly would you want the computing to be parallelized? Would you expect to have a single task run across multiple GPUs at the same time? or do you want a single executor to select the best free GPU to use when assigning a task? CUDF does not support either of these use cases yet. A single task using multiple GPUs would require CUDF to rewrite most of their kernels/algorithms to try and do this, which is not simple, but might be doable. If you want multiple tasks to run on different GPUs we can sort of do that today, but you have to have multiple separate executors. Is there a reason you cannot ask Spark to launch the executor with only 1 GPU and proportionally less tasks?
-----------------------------------
Thank you for your reply， In our project, we build an outer search engine wrapper for spark which bind multi gpu devices to search large scale vectors，each task call search method which is a member method of the search engine instance. so, in our case,  one executor bind multi gpu devices to new a search engine. and we don't know whether the spark-rapids would compete for gpu resources with our search engine",,coderyangyangyang,18379207,MDQ6VXNlcjE4Mzc5MjA3,https://avatars.githubusercontent.com/u/18379207?v=4,,https://api.github.com/users/coderyangyangyang,https://github.com/coderyangyangyang,https://api.github.com/users/coderyangyangyang/followers,https://api.github.com/users/coderyangyangyang/following{/other_user},https://api.github.com/users/coderyangyangyang/gists{/gist_id},https://api.github.com/users/coderyangyangyang/starred{/owner}{/repo},https://api.github.com/users/coderyangyangyang/subscriptions,https://api.github.com/users/coderyangyangyang/orgs,https://api.github.com/users/coderyangyangyang/repos,https://api.github.com/users/coderyangyangyang/events{/privacy},https://api.github.com/users/coderyangyangyang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758447177/reactions,0,0,0,0,0,0,0,0,0,1486
100,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758693565,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-758693565,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,758693565,MDEyOklzc3VlQ29tbWVudDc1ODY5MzU2NQ==,2021-01-12T14:29:53Z,2021-01-12T14:29:53Z,COLLABORATOR,"RAPIDS needs some kind of a GPU to run on.  Cuda does have the ability to share GPUs between multiple processes, but context switching becomes a performance problem and generally it should be avoided. Most resource managers like kubernetes and YARN will hand out GPU resources, but not partial GPUs.

https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/

> Each container can request one or more GPUs. It is not possible to request a fraction of a GPU.

https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html

Spark does the same thing and will request/schedule whole GPUs for an executor, but will allow you to split up the whole GPU between tasks in the executor.

> In our project, we build an outer search engine wrapper for spark which bind multi gpu devices to search large scale vectors

I am still a little confused about what you want. Are you asking to have a single query in it that will use both your search engine code and the RAPIDs plugin at the same time?  Or are you asking for RAPIDs queries to co-exist in the same multi-tenant cluster as search engine queries?

If you are asking for the first one (search engine and RAPIDS in the same query), then it is a really hard problem because of resource scheduling, like you mentioned.  This is very similar to doing ML/DL training at the same time as using the RAPIDS plugin, and sadly we don't have a good solution for this yet, because the two really need to be able to coordinate with one another so that they can share the resources effeciently.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/758693565/reactions,0,0,0,0,0,0,0,0,0,1486
101,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/759433952,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-759433952,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,759433952,MDEyOklzc3VlQ29tbWVudDc1OTQzMzk1Mg==,2021-01-13T13:01:39Z,2021-01-13T13:01:39Z,NONE,Thank you very much，I think we should modify our GPU lib to adapt spark-RAPIDS rules.,,coderyangyangyang,18379207,MDQ6VXNlcjE4Mzc5MjA3,https://avatars.githubusercontent.com/u/18379207?v=4,,https://api.github.com/users/coderyangyangyang,https://github.com/coderyangyangyang,https://api.github.com/users/coderyangyangyang/followers,https://api.github.com/users/coderyangyangyang/following{/other_user},https://api.github.com/users/coderyangyangyang/gists{/gist_id},https://api.github.com/users/coderyangyangyang/starred{/owner}{/repo},https://api.github.com/users/coderyangyangyang/subscriptions,https://api.github.com/users/coderyangyangyang/orgs,https://api.github.com/users/coderyangyangyang/repos,https://api.github.com/users/coderyangyangyang/events{/privacy},https://api.github.com/users/coderyangyangyang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/759433952/reactions,0,0,0,0,0,0,0,0,0,1486
102,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/759511598,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-759511598,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,759511598,MDEyOklzc3VlQ29tbWVudDc1OTUxMTU5OA==,2021-01-13T15:11:12Z,2021-01-13T15:11:12Z,COLLABORATOR,"@coderyangyangyang please let me know if you need some help with this. The main thing you would need to do is to use RMM for GPU memory allocation/deallocation. RMM is not really designed for multi-GPU in a single process, so I don't know how well it will work. Be aware that a lot of the java RAPIDS code also assumes that there will be a single GPU and tries to set the GPU automatically to avoid issues around new threads being created and auto-initializing to GPU-0. If you really have to have support for multiple GPUs in a single process we can work with you to try and overcome some of those issues in RMM and the RAPIDS java API.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/759511598/reactions,0,0,0,0,0,0,0,0,0,1486
103,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/760116829,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-760116829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,760116829,MDEyOklzc3VlQ29tbWVudDc2MDExNjgyOQ==,2021-01-14T10:49:13Z,2021-01-14T10:49:13Z,NONE,"we could sacrifice some worker memory to increase executors to the same num of gpu devices, and let the search engine  bind only one GPU on each executor.",,coderyangyangyang,18379207,MDQ6VXNlcjE4Mzc5MjA3,https://avatars.githubusercontent.com/u/18379207?v=4,,https://api.github.com/users/coderyangyangyang,https://github.com/coderyangyangyang,https://api.github.com/users/coderyangyangyang/followers,https://api.github.com/users/coderyangyangyang/following{/other_user},https://api.github.com/users/coderyangyangyang/gists{/gist_id},https://api.github.com/users/coderyangyangyang/starred{/owner}{/repo},https://api.github.com/users/coderyangyangyang/subscriptions,https://api.github.com/users/coderyangyangyang/orgs,https://api.github.com/users/coderyangyangyang/repos,https://api.github.com/users/coderyangyangyang/events{/privacy},https://api.github.com/users/coderyangyangyang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/760116829/reactions,0,0,0,0,0,0,0,0,0,1486
104,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/763166681,https://github.com/NVIDIA/spark-rapids/issues/1486#issuecomment-763166681,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1486,763166681,MDEyOklzc3VlQ29tbWVudDc2MzE2NjY4MQ==,2021-01-19T21:55:20Z,2021-01-19T21:55:20Z,COLLABORATOR,"We will leave this open as a feature request, but we do not have plans to address this soon.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/763166681/reactions,0,0,0,0,0,0,0,0,0,1486
105,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/760457471,https://github.com/NVIDIA/spark-rapids/issues/1516#issuecomment-760457471,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1516,760457471,MDEyOklzc3VlQ29tbWVudDc2MDQ1NzQ3MQ==,2021-01-14T20:29:28Z,2021-01-14T20:29:28Z,COLLABORATOR,Removed `audit` label.,,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/760457471/reactions,0,0,0,0,0,0,0,0,0,1516
106,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761075991,https://github.com/NVIDIA/spark-rapids/issues/1518#issuecomment-761075991,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1518,761075991,MDEyOklzc3VlQ29tbWVudDc2MTA3NTk5MQ==,2021-01-15T17:28:24Z,2021-01-15T17:28:24Z,COLLABORATOR,The main impact this will have on processing is setting the nullablitiy of columns after a filter properly. The main issue with this is that NullIntolerant is package private in spark so we will likely have to have some kind of a trampoline utility or something to accomplish this.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761075991/reactions,0,0,0,0,0,0,0,0,0,1518
107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761066402,https://github.com/NVIDIA/spark-rapids/issues/1524#issuecomment-761066402,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1524,761066402,MDEyOklzc3VlQ29tbWVudDc2MTA2NjQwMg==,2021-01-15T17:10:19Z,2021-01-15T17:10:19Z,COLLABORATOR,"Just for information, our CSV does not really match Spark's all that closely. We should test it, but we might just end up documenting an incompatibility.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761066402/reactions,0,0,0,0,0,0,0,0,0,1524
108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761055949,https://github.com/NVIDIA/spark-rapids/issues/1526#issuecomment-761055949,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1526,761055949,MDEyOklzc3VlQ29tbWVudDc2MTA1NTk0OQ==,2021-01-15T16:51:30Z,2021-01-15T16:51:30Z,COLLABORATOR,This really should just be testing. If you look at the code it modifies the plan to insert the nulls in a project before doing the union like normal.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/761055949/reactions,1,1,0,0,0,0,0,0,0,1526
109,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765560833,https://github.com/NVIDIA/spark-rapids/issues/1573#issuecomment-765560833,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1573,765560833,MDEyOklzc3VlQ29tbWVudDc2NTU2MDgzMw==,2021-01-22T17:11:39Z,2021-01-22T17:11:39Z,COLLABORATOR,"It is only ratio that is showing issues, but it is showing them in more than one place.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765560833/reactions,0,0,0,0,0,0,0,0,0,1573
110,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765570265,https://github.com/NVIDIA/spark-rapids/issues/1573#issuecomment-765570265,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1573,765570265,MDEyOklzc3VlQ29tbWVudDc2NTU3MDI2NQ==,2021-01-22T17:28:19Z,2021-01-22T17:28:19Z,COLLABORATOR,"It looks like round is doing the wrong thing for HALF_UP rounding. I expanded the input to it and it looks like `round(0.575, 2)` and `round(1.025, 2)` are doing the wrong thing on the GPU.  It is an explicit corner case that we should pay more attention to in the tests. I'll try and track this down to see what data types this impacts.

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765570265/reactions,0,0,0,0,0,0,0,0,0,1573
111,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765575887,https://github.com/NVIDIA/spark-rapids/issues/1573#issuecomment-765575887,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1573,765575887,MDEyOklzc3VlQ29tbWVudDc2NTU3NTg4Nw==,2021-01-22T17:38:56Z,2021-01-22T17:38:56Z,COLLABORATOR,"I am going to keep pushing, but it looks like cudf is not doing the correct thing for rounding on some floating point values. Both float and double are impacted, but it looks like they are not impacted the same by similar results.

```
scala> Seq(1.025, 0.575, 2.005, 2.015, 2.025).toDF(""f"").repartition(1).selectExpr(""round(CAST(f AS FLOAT), 2)"").collect
21/01/22 17:34:27 WARN GpuOverrides: 
*Exec <ProjectExec> could run on GPU
  *Expression <Alias> round(cast(f#80 as float), 2) AS round(CAST(f AS FLOAT), 2)#82 could run on GPU
    *Expression <Round> round(cast(f#80 as float), 2) could run on GPU
      *Expression <Cast> cast(f#80 as float) could run on GPU
  *Exec <ShuffleExchangeExec> could run on GPU
    *Partitioning <RoundRobinPartitioning> could run on GPU
    !NOT_FOUND <LocalTableScanExec> cannot run on GPU because no GPU enabled version of operator class org.apache.spark.sql.execution.LocalTableScanExec could be found

res11: Array[org.apache.spark.sql.Row] = Array([0.58], [1.02], [2.01], [2.02], [2.03])

scala> Seq(1.025, 0.575, 2.005, 2.015, 2.025).toDF(""f"").repartition(1).selectExpr(""round(CAST(f AS DOUBLE), 2)"").collect
21/01/22 17:34:49 WARN GpuOverrides: 
*Exec <ProjectExec> could run on GPU
  *Expression <Alias> round(f#90, 2) AS round(CAST(f AS DOUBLE), 2)#92 could run on GPU
    *Expression <Round> round(f#90, 2) could run on GPU
  *Exec <ShuffleExchangeExec> could run on GPU
    *Partitioning <RoundRobinPartitioning> could run on GPU
    !NOT_FOUND <LocalTableScanExec> cannot run on GPU because no GPU enabled version of operator class org.apache.spark.sql.execution.LocalTableScanExec could be found

res12: Array[org.apache.spark.sql.Row] = Array([0.57], [1.02], [2.0], [2.02], [2.02])

scala> Seq(1.025, 0.575, 2.005, 2.015, 2.025).toDF(""f"").repartition(1).selectExpr(""round(CAST(f AS DECIMAL(6, 3)), 2)"").collect
21/01/22 17:35:23 WARN GpuOverrides: 
*Exec <ProjectExec> could run on GPU
  *Expression <Alias> round(cast(f#100 as decimal(6,3)), 2) AS round(CAST(f AS DECIMAL(6,3)), 2)#102 could run on GPU
    *Expression <Round> round(cast(f#100 as decimal(6,3)), 2) could run on GPU
      *Expression <Cast> cast(f#100 as decimal(6,3)) could run on GPU
  *Exec <ShuffleExchangeExec> could run on GPU
    *Partitioning <RoundRobinPartitioning> could run on GPU
    !NOT_FOUND <LocalTableScanExec> cannot run on GPU because no GPU enabled version of operator class org.apache.spark.sql.execution.LocalTableScanExec could be found

res13: Array[org.apache.spark.sql.Row] = Array([0.58], [1.03], [2.01], [2.02], [2.03])
```",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765575887/reactions,0,0,0,0,0,0,0,0,0,1573
112,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765587505,https://github.com/NVIDIA/spark-rapids/issues/1573#issuecomment-765587505,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1573,765587505,MDEyOklzc3VlQ29tbWVudDc2NTU4NzUwNQ==,2021-01-22T17:59:42Z,2021-01-22T17:59:42Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/7195 for this in CUDF.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/765587505/reactions,0,0,0,0,0,0,0,0,0,1573
113,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/766889450,https://github.com/NVIDIA/spark-rapids/issues/1573#issuecomment-766889450,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1573,766889450,MDEyOklzc3VlQ29tbWVudDc2Njg4OTQ1MA==,2021-01-25T15:19:09Z,2021-01-25T15:19:09Z,COLLABORATOR,"It turns out that rapidsai/cudf#7195 is not a bug. I filed the issue without doing enough due diligence.  It looks like the problem is with how Spark actually deals with rounding double values.

```
BigDecimal(d).setScale(_scale, mode).toDouble
```

When converting from a double to a decimal value Spark converts the double to a string.  And java does some funky things to turn the value into a String that we cannot replicate in cudf yet.  This means that I think we are going to have to fall back to the CPU for round/bround on double values by default until we can figure out a better way to cast double values to decimal values that matches spark exactly.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/766889450/reactions,0,0,0,0,0,0,0,0,0,1573
114,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/771066629,https://github.com/NVIDIA/spark-rapids/issues/1632#issuecomment-771066629,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1632,771066629,MDEyOklzc3VlQ29tbWVudDc3MTA2NjYyOQ==,2021-02-01T18:34:01Z,2021-02-01T18:34:01Z,COLLABORATOR,looks like https://github.com/rapidsai/cudf/issues/7116 is filed to add decimal/fixed point to cudf for from_arrow and to_arrow,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/771066629/reactions,0,0,0,0,0,0,0,0,0,1632
115,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819512092,https://github.com/NVIDIA/spark-rapids/issues/1755#issuecomment-819512092,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1755,819512092,MDEyOklzc3VlQ29tbWVudDgxOTUxMjA5Mg==,2021-04-14T13:19:15Z,2021-04-14T13:19:15Z,NONE,Hi i would like to work on this issue!,,himanshu007-creator,65963997,MDQ6VXNlcjY1OTYzOTk3,https://avatars.githubusercontent.com/u/65963997?v=4,,https://api.github.com/users/himanshu007-creator,https://github.com/himanshu007-creator,https://api.github.com/users/himanshu007-creator/followers,https://api.github.com/users/himanshu007-creator/following{/other_user},https://api.github.com/users/himanshu007-creator/gists{/gist_id},https://api.github.com/users/himanshu007-creator/starred{/owner}{/repo},https://api.github.com/users/himanshu007-creator/subscriptions,https://api.github.com/users/himanshu007-creator/orgs,https://api.github.com/users/himanshu007-creator/repos,https://api.github.com/users/himanshu007-creator/events{/privacy},https://api.github.com/users/himanshu007-creator/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819512092/reactions,0,0,0,0,0,0,0,0,0,1755
116,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819533377,https://github.com/NVIDIA/spark-rapids/issues/1755#issuecomment-819533377,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1755,819533377,MDEyOklzc3VlQ29tbWVudDgxOTUzMzM3Nw==,2021-04-14T13:49:47Z,2021-04-14T13:49:47Z,COLLABORATOR,That would be great if you would like to contribute.  Please let us know what your ideas are for improving before starting coding so we can discuss.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819533377/reactions,0,0,0,0,0,0,0,0,0,1755
117,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819542838,https://github.com/NVIDIA/spark-rapids/issues/1755#issuecomment-819542838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1755,819542838,MDEyOklzc3VlQ29tbWVudDgxOTU0MjgzOA==,2021-04-14T14:02:23Z,2021-04-14T14:02:23Z,COLLABORATOR,"Ya it would be great. I started to work on adding the header periodically through the table, just because I am in the process of adding in new checks for partitioning, and probably do the code generation for scans as well.  But if there are better ideas it is simple to remove what I have been doing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819542838/reactions,0,0,0,0,0,0,0,0,0,1755
118,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/784528903,https://github.com/NVIDIA/spark-rapids/issues/1764#issuecomment-784528903,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1764,784528903,MDEyOklzc3VlQ29tbWVudDc4NDUyODkwMw==,2021-02-23T21:35:33Z,2021-02-23T21:35:33Z,COLLABORATOR,Will leave open until rapidsai/cudf#7410 issue is fixed so we can add the remaining test cases. ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/784528903/reactions,0,0,0,0,0,0,0,0,0,1764
119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/886688619,https://github.com/NVIDIA/spark-rapids/issues/1789#issuecomment-886688619,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1789,886688619,IC_kwDOD7z77c402ctr,2021-07-26T13:09:34Z,2021-07-26T13:09:34Z,COLLABORATOR,The rank/row number optimization ended up being the running window changes.  This ended up being a lot bigger than initially expected because for performance reasons this required us to switch to a scan/group by scan implementation instead.  The other memory optimizations are on hold until a customer ends up with issues because of them.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/886688619/reactions,0,0,0,0,0,0,0,0,0,1789
120,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124250971,https://github.com/NVIDIA/spark-rapids/issues/1815#issuecomment-1124250971,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1815,1124250971,IC_kwDOD7z77c5DArVb,2022-05-11T20:20:44Z,2022-05-11T20:20:44Z,COLLABORATOR,First step is prototyping identification of I/O condition during semaphore usage.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124250971/reactions,0,0,0,0,0,0,0,0,0,1815
121,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/791749511,https://github.com/NVIDIA/spark-rapids/issues/1879#issuecomment-791749511,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1879,791749511,MDEyOklzc3VlQ29tbWVudDc5MTc0OTUxMQ==,2021-03-05T22:21:39Z,2021-03-05T22:21:39Z,COLLABORATOR,we should add these rules but should just be optimizaiton,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/791749511/reactions,0,0,0,0,0,0,0,0,0,1879
122,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/793987886,https://github.com/NVIDIA/spark-rapids/issues/1901#issuecomment-793987886,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1901,793987886,MDEyOklzc3VlQ29tbWVudDc5Mzk4Nzg4Ng==,2021-03-09T14:43:10Z,2021-03-09T14:43:10Z,COLLABORATOR,"To clarify a little bit.  Generators like `explode` and `pos_explode` produce both multiple columns of output and multiple rows of output for a single row of input. Where as most expressions only produce a single value of output for each row of input.  GenerateExec gets around this by requiring the output of a Generator to be an Array of Structs, and then does the equivalent of a flatMap followed by removing the wrapped struct.  Our code will act differently, but the type checking will still need to match.

The current type checking for nested types is very limited. It is not a problem of correctness, we can still sufficiently filter out operators that need to fall back to the CPU, even if we have to do a few extra programmatic checks in some cases.  The issue is more about being able to express to the user what is and isn't supported when we generate the documentation.  It would be nice to clearly show that there is not just one output value/type for these expressions, but multiple, and also what those outputs look like.  This is not super important just something that would be nice to have.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/793987886/reactions,0,0,0,0,0,0,0,0,0,1901
123,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800488180,https://github.com/NVIDIA/spark-rapids/issues/1916#issuecomment-800488180,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1916,800488180,MDEyOklzc3VlQ29tbWVudDgwMDQ4ODE4MA==,2021-03-16T18:03:55Z,2021-03-16T18:04:26Z,COLLABORATOR,"I might have to do this for the GTC demo, so assigning myself, if I don't I can update priority.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800488180/reactions,0,0,0,0,0,0,0,0,0,1916
124,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/815022939,https://github.com/NVIDIA/spark-rapids/issues/1916#issuecomment-815022939,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1916,815022939,MDEyOklzc3VlQ29tbWVudDgxNTAyMjkzOQ==,2021-04-07T15:49:05Z,2021-04-07T15:49:05Z,COLLABORATOR,"I was able to do the demo without changes, so putting the needs triage back on so we can discuss and prioritize.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/815022939/reactions,0,0,0,0,0,0,0,0,0,1916
125,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800577692,https://github.com/NVIDIA/spark-rapids/issues/1929#issuecomment-800577692,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1929,800577692,MDEyOklzc3VlQ29tbWVudDgwMDU3NzY5Mg==,2021-03-16T20:22:52Z,2021-03-16T20:22:52Z,COLLABORATOR,"Preference is to put up a SPIP for Spark plugins to modify the executorEnv if so desired.  

@tgravescs is this something that is worth pursuing without going the SPIP route first? ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800577692/reactions,0,0,0,0,0,0,0,0,0,1929
126,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/808239861,https://github.com/NVIDIA/spark-rapids/issues/1929#issuecomment-808239861,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1929,808239861,MDEyOklzc3VlQ29tbWVudDgwODIzOTg2MQ==,2021-03-26T13:56:23Z,2021-03-26T13:56:23Z,COLLABORATOR,yes I think we should prototype and see how much changes. If its minor change to the plugin api which is developer api shouldn't need a spip,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/808239861/reactions,0,0,0,0,0,0,0,0,0,1929
127,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1012620466,https://github.com/NVIDIA/spark-rapids/issues/1931#issuecomment-1012620466,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1931,1012620466,IC_kwDOD7z77c48W1yy,2022-01-13T23:46:53Z,2022-01-13T23:46:53Z,MEMBER,@mythrocks Is this still relevant?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1012620466/reactions,0,0,0,0,0,0,0,0,0,1931
128,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1013285411,https://github.com/NVIDIA/spark-rapids/issues/1931#issuecomment-1013285411,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1931,1013285411,IC_kwDOD7z77c48ZYIj,2022-01-14T16:45:16Z,2022-01-14T16:45:16Z,COLLABORATOR,"> @mythrocks Is this still relevant?

We could save some time in building the full offsets that are passed to the underlying window operations. But we have not even measure how much of the time that is taking up.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1013285411/reactions,0,0,0,0,0,0,0,0,0,1931
129,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1023960934,https://github.com/NVIDIA/spark-rapids/issues/1931#issuecomment-1023960934,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1931,1023960934,IC_kwDOD7z77c49CGdm,2022-01-28T07:39:32Z,2022-01-28T07:39:32Z,COLLABORATOR,"Sorry I missed this. 
I didn't actually see a slowdown, just that there seemed to be multiple calls coming through. 

I can close this issue for now, and reopen if we find a slow path here. ",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1023960934/reactions,0,0,0,0,0,0,0,0,0,1931
130,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1024271632,https://github.com/NVIDIA/spark-rapids/issues/1931#issuecomment-1024271632,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1931,1024271632,IC_kwDOD7z77c49DSUQ,2022-01-28T14:23:07Z,2022-01-28T14:23:07Z,COLLABORATOR,"I want to keep it open because we know that there is duplicate code being called. It may be small, but I want to preserve this because at some point we are going to want to go through the backlog and start fixing things there.  Just unassigned yourself from this for now.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1024271632/reactions,0,0,0,0,0,0,0,0,0,1931
131,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800385160,https://github.com/NVIDIA/spark-rapids/issues/1949#issuecomment-800385160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1949,800385160,MDEyOklzc3VlQ29tbWVudDgwMDM4NTE2MA==,2021-03-16T15:55:19Z,2021-03-16T15:55:19Z,COLLABORATOR,Note only affects metrics showing on UI,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/800385160/reactions,0,0,0,0,0,0,0,0,0,1949
132,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/823588376,https://github.com/NVIDIA/spark-rapids/issues/1949#issuecomment-823588376,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1949,823588376,MDEyOklzc3VlQ29tbWVudDgyMzU4ODM3Ng==,2021-04-20T20:44:00Z,2021-04-20T20:44:00Z,COLLABORATOR,Removing from 0.5 since this only affects Spark 3.2. ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/823588376/reactions,0,0,0,0,0,0,0,0,0,1949
133,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/932543441,https://github.com/NVIDIA/spark-rapids/issues/1977#issuecomment-932543441,https://api.github.com/repos/NVIDIA/spark-rapids/issues/1977,932543441,IC_kwDOD7z77c43lXvR,2021-10-01T20:53:21Z,2021-10-01T20:53:21Z,COLLABORATOR,unassigning it because it's been deprioritized,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/932543441/reactions,0,0,0,0,0,0,0,0,0,1977
134,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/814420774,https://github.com/NVIDIA/spark-rapids/issues/2058#issuecomment-814420774,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2058,814420774,MDEyOklzc3VlQ29tbWVudDgxNDQyMDc3NA==,2021-04-06T20:33:55Z,2021-04-06T20:33:55Z,COLLABORATOR,"We need to review this in the context of https://issues.apache.org/jira/browse/SPARK-34706 

cc: @tgravescs ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/814420774/reactions,0,0,0,0,0,0,0,0,0,2058
135,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/815021157,https://github.com/NVIDIA/spark-rapids/issues/2058#issuecomment-815021157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2058,815021157,MDEyOklzc3VlQ29tbWVudDgxNTAyMTE1Nw==,2021-04-07T15:46:33Z,2021-04-07T15:46:33Z,COLLABORATOR,so this is something we can pull in as an improvement to the GpuBroadcastNestedLoopJoinExec and we should add a test if we can.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/815021157/reactions,0,0,0,0,0,0,0,0,0,2058
136,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681108647,https://github.com/NVIDIA/spark-rapids/issues/2069#issuecomment-1681108647,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2069,1681108647,IC_kwDOD7z77c5kM66n,2023-08-16T18:45:45Z,2023-08-16T18:45:45Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/13892 for this in CUDF,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681108647/reactions,0,0,0,0,0,0,0,0,0,2069
137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079164514,https://github.com/NVIDIA/spark-rapids/issues/2082#issuecomment-1079164514,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2082,1079164514,IC_kwDOD7z77c5AUr5i,2022-03-25T15:54:54Z,2022-03-25T15:54:54Z,COLLABORATOR,Please see: https://github.com/NVIDIA/spark-rapids/pull/5051.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079164514/reactions,0,0,0,0,0,0,0,0,0,2082
138,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/968979607,https://github.com/NVIDIA/spark-rapids/issues/2091#issuecomment-968979607,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2091,968979607,IC_kwDOD7z77c45wXSX,2021-11-15T14:43:59Z,2021-11-15T14:43:59Z,COLLABORATOR,fyi - the community is looking at supporting jdk 17 for Spark 3.3 -> https://issues.apache.org/jira/browse/SPARK-33772,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/968979607/reactions,1,1,0,0,0,0,0,0,0,2091
139,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124208179,https://github.com/NVIDIA/spark-rapids/issues/2091#issuecomment-1124208179,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2091,1124208179,IC_kwDOD7z77c5DAg4z,2022-05-11T19:30:22Z,2022-05-11T19:30:22Z,COLLABORATOR,looks related to https://github.com/netty/netty/issues/8708,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124208179/reactions,0,0,0,0,0,0,0,0,0,2091
140,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819063745,https://github.com/NVIDIA/spark-rapids/issues/2118#issuecomment-819063745,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2118,819063745,MDEyOklzc3VlQ29tbWVudDgxOTA2Mzc0NQ==,2021-04-13T21:22:36Z,2021-04-14T15:02:10Z,CONTRIBUTOR,"I plan on disabling `yy` on GPU for now and filing a follow-on issue to handle this consistently with Spark.

cuDF parsing is based on `strptime` and I found this in the `strptime` docs, so I am assuming for now that cuDF does the same but I need to confirm this as well as investigate the rules that Spark implements.

```
The year within century. When a century is not otherwise specified, 
values in the range [69,99] shall refer to years 1969 to 1999 inclusive, and 
values in the range [00,68] shall refer to years 2000 to 2068 inclusive;
leading zeros shall be permitted but shall not be required.
```

Spark uses [DateTimeFormatter](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html) which has different rules:

```
For parsing, this will parse using the base value of 2000, resulting in a year within the range 2000 to 2099 inclusive.
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/819063745/reactions,0,0,0,0,0,0,0,0,0,2118
141,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/822635880,https://github.com/NVIDIA/spark-rapids/issues/2118#issuecomment-822635880,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2118,822635880,MDEyOklzc3VlQ29tbWVudDgyMjYzNTg4MA==,2021-04-19T17:14:47Z,2021-04-19T17:14:47Z,COLLABORATOR,We will now fall back to the CPU but leaving this open so we can fix the correctness issue on the GPU in the longer run.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/822635880/reactions,0,0,0,0,0,0,0,0,0,2118
142,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/995534780,https://github.com/NVIDIA/spark-rapids/issues/2252#issuecomment-995534780,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2252,995534780,IC_kwDOD7z77c47Vqe8,2021-12-16T08:08:48Z,2021-12-16T08:08:48Z,COLLABORATOR,Reopen the issue [#8050](https://github.com/rapidsai/cudf/issues/8050) since cudf::merge is still missing the array and map type support.,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/995534780/reactions,0,0,0,0,0,0,0,0,0,2252
143,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/831944813,https://github.com/NVIDIA/spark-rapids/issues/2336#issuecomment-831944813,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2336,831944813,MDEyOklzc3VlQ29tbWVudDgzMTk0NDgxMw==,2021-05-04T13:33:01Z,2021-05-04T13:37:12Z,COLLABORATOR,"I worked on a prototype for speedup up spilling to pageable memory (which is a real issue for us) via both a GPU and a pinned bounce buffer. I need to run some verification on and can post numbers here after that.

The approach is kind of expensive and made some assumptions, so here is the high level idea:

- Focus on small buffers smaller than a threshold and those who are larger bypass this.
- Copy small buffers to device bounce buffer. We'd keep a few of these bounce buffers around. For example, one bounce buffer is working on the small copies D2D while the other is committed to a copy to host.
- Perform a big copy to pinned memory bounce buffer of same size, when the device bounce buffer is exhausted (or spill action is complete). This part could be avoided if we find copying to pageable memory is good enough at larger memcpy sizes.
- Perform a memcpy from pinned to pageable target. We need a thread at least for this one, since `memcpy` is synchronous.

So the pageable buffer could either be one large buffer, which means we have references to it and can't free it until all the host buffers that are referencing it get spilled to disk, which may be unacceptable, or individual pageable buffers. I had implemented the larger pageable buffer, but want to try the individual approach.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/831944813/reactions,0,0,0,0,0,0,0,0,0,2336
144,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925177762,https://github.com/NVIDIA/spark-rapids/issues/2382#issuecomment-925177762,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2382,925177762,IC_kwDOD7z77c43JRei,2021-09-22T18:24:59Z,2021-09-22T18:24:59Z,COLLABORATOR,"This may improve performance of writing/reading cache but to do any operation on a table with intervals, we will have to fallback on the CPU as cudf doesn't support intervals atm. I also don't think people generally cache tables with intervals, so unless we see a use case this is a much lower priority than we originally thought. ",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925177762/reactions,0,0,0,0,0,0,0,0,0,2382
145,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/841472704,https://github.com/NVIDIA/spark-rapids/issues/2420#issuecomment-841472704,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2420,841472704,MDEyOklzc3VlQ29tbWVudDg0MTQ3MjcwNA==,2021-05-14T20:03:33Z,2021-05-14T20:03:33Z,COLLABORATOR,OK as an aside `dependency-reduced-pom.xml` should never be checked in. It is generated by the shade plugin.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/841472704/reactions,1,1,0,0,0,0,0,0,0,2420
146,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/926849378,https://github.com/NVIDIA/spark-rapids/issues/2440#issuecomment-926849378,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2440,926849378,IC_kwDOD7z77c43Ppli,2021-09-24T18:49:35Z,2021-09-24T18:49:35Z,MEMBER,Reopened since this is being reverted in #3657.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/926849378/reactions,0,0,0,0,0,0,0,0,0,2440
147,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/926852005,https://github.com/NVIDIA/spark-rapids/issues/2440#issuecomment-926852005,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2440,926852005,IC_kwDOD7z77c43PqOl,2021-09-24T18:54:38Z,2021-09-24T18:54:38Z,MEMBER,"In order to do this properly, we minimally need a way to use the smaller table for the build-side table when performing inner joins to avoid the issue reported in #3288.  There may be a way to hack this in, but ideally we'd also want to avoid always fetching the entire, arbitrarily-chosen build-side data for inner joins which should be flexible on the build-side choice.  #2354 is related to a more general solution.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/926852005/reactions,0,0,0,0,0,0,0,0,0,2440
148,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332771044,https://github.com/NVIDIA/spark-rapids/issues/2470#issuecomment-1332771044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2470,1332771044,IC_kwDOD7z77c5PcHjk,2022-11-30T21:44:50Z,2022-11-30T21:44:50Z,COLLABORATOR,Sorting non-nested arrays is now supported in libcudf so I believe that this can be partially fixed if needed. Support for sorting nested arrays may need to wait further.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332771044/reactions,0,0,0,0,0,0,0,0,0,2470
149,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735609704,https://github.com/NVIDIA/spark-rapids/issues/2708#issuecomment-1735609704,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2708,1735609704,IC_kwDOD7z77c5nc01o,2023-09-26T14:03:48Z,2023-09-26T14:03:48Z,COLLABORATOR,"We need to be careful here because GpuRunningWindowFunction also has an optimization to do a scan aggregation instead of a normal window aggregation. This will have to be disabled for range based queries. The performance will not be great here, but at least it will work. We can have a follow on issue to try and understand if there is a good way to still have some of the performance wins from this.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735609704/reactions,0,0,0,0,0,0,0,0,0,2708
150,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/863467064,https://github.com/NVIDIA/spark-rapids/issues/2733#issuecomment-863467064,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2733,863467064,MDEyOklzc3VlQ29tbWVudDg2MzQ2NzA2NA==,2021-06-17T18:28:39Z,2021-06-17T18:28:50Z,COLLABORATOR,"We have not yet release 21.06 so its not available in maven yet.  It will be released once we are finished doing testing on it

You can check the download section of our docs for latest released version: https://nvidia.github.io/spark-rapids/docs/download.html",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/863467064/reactions,0,0,0,0,0,0,0,0,0,2733
151,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/863580025,https://github.com/NVIDIA/spark-rapids/issues/2733#issuecomment-863580025,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2733,863580025,MDEyOklzc3VlQ29tbWVudDg2MzU4MDAyNQ==,2021-06-17T21:35:26Z,2021-06-17T21:35:26Z,NONE,"thanks, @tgravescs . I promoted this question since I met problem when reading from delta table, the error message like ' covert struct type to GPU is not supported' under databricks 7.3ML LST
sudo wget -O /databricks/jars/rapids-4-spark_2.12-0.5.0.jar https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.5.0/rapids-4-spark_2.12-0.5.0.jar
sudo wget -O /databricks/jars/cudf-0.19.2-cuda10-1.jar https://repo1.maven.org/maven2/ai/rapids/cudf/0.19.2/cudf-0.19.2-cuda10-1.jar"""""", True)

but the parquet file is working fine. May I ask if the delta table would be supported in next release?",,neoaksa,34007729,MDQ6VXNlcjM0MDA3NzI5,https://avatars.githubusercontent.com/u/34007729?v=4,,https://api.github.com/users/neoaksa,https://github.com/neoaksa,https://api.github.com/users/neoaksa/followers,https://api.github.com/users/neoaksa/following{/other_user},https://api.github.com/users/neoaksa/gists{/gist_id},https://api.github.com/users/neoaksa/starred{/owner}{/repo},https://api.github.com/users/neoaksa/subscriptions,https://api.github.com/users/neoaksa/orgs,https://api.github.com/users/neoaksa/repos,https://api.github.com/users/neoaksa/events{/privacy},https://api.github.com/users/neoaksa/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/863580025/reactions,0,0,0,0,0,0,0,0,0,2733
152,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864030229,https://github.com/NVIDIA/spark-rapids/issues/2733#issuecomment-864030229,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2733,864030229,MDEyOklzc3VlQ29tbWVudDg2NDAzMDIyOQ==,2021-06-18T13:14:10Z,2021-06-18T13:14:10Z,COLLABORATOR,"You can look at the docs for 21.06 here: https://github.com/NVIDIA/spark-rapids/blob/branch-21.06/docs/supported_ops.md there is a table there that indicates which operations support structs for that release.  So it depends on what operator you were trying to use that it was complaining about.  We should support most structs when reading from parquet (which is what delta table was using underneath), but 0.5 should have supported it as well.   Delta table does other things though as well so perhaps something is going on there.

if you can share the actual output of the explain (--conf spark.rapids.sql.explain=NOT_ON_GPU  or ALL -> see https://nvidia.github.io/spark-rapids/docs/configs.html for more information on that) it may give me more context. I know we have read from delta tables in the past but don't know if we tried structs and perhaps we didn't hit your case.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864030229/reactions,0,0,0,0,0,0,0,0,0,2733
153,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864206947,https://github.com/NVIDIA/spark-rapids/issues/2733#issuecomment-864206947,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2733,864206947,MDEyOklzc3VlQ29tbWVudDg2NDIwNjk0Nw==,2021-06-18T18:26:34Z,2021-06-18T18:26:34Z,NONE,"@tgravescs ,  Thanks for your explain of details. I found the reason. It is caused by reading from a view which is combination of several delta tables. I persisted this view into a parquet file, then everything working fine including reading from delta table. 
```
create view data_pipeline_v as 
select js.id as pipeline_id,
      job.id as job_id,
      job.job_name as job_name,
      job.job_type as job_type,
      job.order as job_order,
      job.notebook_path as notebook_path,
      cust.cust_id as cust_id,
      cust.cust_name as cust_name,
      geo.country_time_zone as country_time_zone,
      geo.state_time_zone as state_time_zone,
      param.input_path as input_path,
      param.output_path as output_path,
      param.input_schema as input_schema,
      param.output_schema as output_schema,
      param.variables as variables
      from delta.`/mnt/eus-metadata/job_subscription` js
inner join delta.`/mnt/eus-metadata/job` job on js.job_fk=job.id
inner join delta.`/mnt/eus-metadata/customer` cust on cust.cust_id=js.customer_fk
inner join delta.`/mnt/eus-metadata/geography` geo on geo.id=cust.geography_fk
left join delta.`/mnt/eus-metadata/parameter` param on param.id=job.parameter_fk
where js.active =1
```",,neoaksa,34007729,MDQ6VXNlcjM0MDA3NzI5,https://avatars.githubusercontent.com/u/34007729?v=4,,https://api.github.com/users/neoaksa,https://github.com/neoaksa,https://api.github.com/users/neoaksa/followers,https://api.github.com/users/neoaksa/following{/other_user},https://api.github.com/users/neoaksa/gists{/gist_id},https://api.github.com/users/neoaksa/starred{/owner}{/repo},https://api.github.com/users/neoaksa/subscriptions,https://api.github.com/users/neoaksa/orgs,https://api.github.com/users/neoaksa/repos,https://api.github.com/users/neoaksa/events{/privacy},https://api.github.com/users/neoaksa/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864206947/reactions,0,0,0,0,0,0,0,0,0,2733
154,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864229759,https://github.com/NVIDIA/spark-rapids/issues/2733#issuecomment-864229759,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2733,864229759,MDEyOklzc3VlQ29tbWVudDg2NDIyOTc1OQ==,2021-06-18T19:18:16Z,2021-06-18T19:18:16Z,COLLABORATOR,"ok, thanks for the update.  If that unblocks you, I'm going to convert this into a task for us to test with a view to see what we don't support.  Please update if further questions.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/864229759/reactions,0,0,0,0,0,0,0,0,0,2733
155,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/916580182,https://github.com/NVIDIA/spark-rapids/issues/2765#issuecomment-916580182,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2765,916580182,IC_kwDOD7z77c42oedW,2021-09-10T02:20:32Z,2021-09-10T02:20:32Z,COLLABORATOR,"I just ran the integration test with LZ4, And cudf threw an exception with ""Unsupported compression type""

https://github.com/rapidsai/cudf/blob/branch-21.10/cpp/src/io/comp/uncomp.cpp#L570-L578",,wbo4958,1320706,MDQ6VXNlcjEzMjA3MDY=,https://avatars.githubusercontent.com/u/1320706?v=4,,https://api.github.com/users/wbo4958,https://github.com/wbo4958,https://api.github.com/users/wbo4958/followers,https://api.github.com/users/wbo4958/following{/other_user},https://api.github.com/users/wbo4958/gists{/gist_id},https://api.github.com/users/wbo4958/starred{/owner}{/repo},https://api.github.com/users/wbo4958/subscriptions,https://api.github.com/users/wbo4958/orgs,https://api.github.com/users/wbo4958/repos,https://api.github.com/users/wbo4958/events{/privacy},https://api.github.com/users/wbo4958/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/916580182/reactions,0,0,0,0,0,0,0,0,0,2765
156,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/916704750,https://github.com/NVIDIA/spark-rapids/issues/2765#issuecomment-916704750,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2765,916704750,IC_kwDOD7z77c42o83u,2021-09-10T07:48:53Z,2021-09-10T07:48:53Z,COLLABORATOR,"@Salonijain27 Hi Saloni, since cuDF hasn't supported LZ4, we should remove it from 21.10 target.",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/916704750/reactions,0,0,0,0,0,0,0,0,0,2765
157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/966857294,https://github.com/NVIDIA/spark-rapids/issues/2765#issuecomment-966857294,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2765,966857294,IC_kwDOD7z77c45oRJO,2021-11-12T06:25:33Z,2021-11-12T06:25:33Z,COLLABORATOR,"Putting this into the backlog, as far as I understand cudf does not support lz4 for ORC.  @revans2 please correct me if I am mistaken.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/966857294/reactions,0,0,0,0,0,0,0,0,0,2765
158,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/875055911,https://github.com/NVIDIA/spark-rapids/issues/2811#issuecomment-875055911,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2811,875055911,MDEyOklzc3VlQ29tbWVudDg3NTA1NTkxMQ==,2021-07-06T20:21:43Z,2021-07-06T20:22:05Z,CONTRIBUTOR,Here is a related PR with some proposed changes to Spark: https://github.com/apache/spark/pull/33140,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/875055911/reactions,0,0,0,0,0,0,0,0,0,2811
159,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/883683967,https://github.com/NVIDIA/spark-rapids/issues/2889#issuecomment-883683967,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2889,883683967,IC_kwDOD7z77c40q_J_,2021-07-20T20:31:52Z,2021-07-20T20:31:52Z,COLLABORATOR,This will be partially addressed in 21.08 and completed in 21.10. ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/883683967/reactions,0,0,0,0,0,0,0,0,0,2889
160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/884432921,https://github.com/NVIDIA/spark-rapids/issues/2889#issuecomment-884432921,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2889,884432921,IC_kwDOD7z77c40t2AZ,2021-07-21T19:17:54Z,2021-07-21T19:17:54Z,CONTRIBUTOR,Rather than try and support these edge cases I am now leaning towards having a regex to detect them instead and fail if we see them and tell the user to disable `spark.rapids.sql.castStringToTimestamp.enabled` if they run into this error.,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/884432921/reactions,0,0,0,0,0,0,0,0,0,2889
161,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/883529388,https://github.com/NVIDIA/spark-rapids/issues/2931#issuecomment-883529388,https://api.github.com/repos/NVIDIA/spark-rapids/issues/2931,883529388,IC_kwDOD7z77c40qZas,2021-07-20T16:28:49Z,2021-07-20T16:28:49Z,CONTRIBUTOR,"CSV parsing uses a different code path to our CAST logic and does not have special handling for things like ansi mode or timeParserPolicy. Also, parsing from all non-string types is disabled by default and the documentation points out the reasons for this.

If we did want to support ansi/legacy in CSV parsing than we could consider a different approach where we just read strings from the csv file and then wrap in a projection that casts columns from string to another type.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/883529388/reactions,0,0,0,0,0,0,0,0,0,2931
162,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/887807252,https://github.com/NVIDIA/spark-rapids/issues/3040#issuecomment-887807252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040,887807252,IC_kwDOD7z77c406t0U,2021-07-27T20:19:21Z,2021-07-27T20:19:21Z,COLLABORATOR,"On an issue like this, we are also leaving the other side waiting for the timeout. Adding the comment here, I may have to split that to a different issue, but we really ought to reply to the peer with an error message.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/887807252/reactions,0,0,0,0,0,0,0,0,0,3040
163,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/895613846,https://github.com/NVIDIA/spark-rapids/issues/3040#issuecomment-895613846,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3040,895613846,IC_kwDOD7z77c41YfuW,2021-08-09T23:17:10Z,2021-08-09T23:17:10Z,COLLABORATOR,Adding the needs triage label to see if this needs to be raised to a P1.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/895613846/reactions,0,0,0,0,0,0,0,0,0,3040
164,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/892161002,https://github.com/NVIDIA/spark-rapids/issues/3085#issuecomment-892161002,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3085,892161002,IC_kwDOD7z77c41LUvq,2021-08-03T20:57:48Z,2021-08-03T20:57:48Z,COLLABORATOR,this really doesn't matter until we move Parquet versions and these are really deprecated. Spark 3.2 moved up to parquet 1.12 but we shouldn't move until our least spark version is at that or there is a bug fix we really need.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/892161002/reactions,0,0,0,0,0,0,0,0,0,3085
165,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/892151531,https://github.com/NVIDIA/spark-rapids/issues/3092#issuecomment-892151531,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3092,892151531,IC_kwDOD7z77c41LSbr,2021-08-03T20:41:40Z,2021-08-03T20:41:40Z,COLLABORATOR,Not immediately required since this is a new type.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/892151531/reactions,0,0,0,0,0,0,0,0,0,3092
166,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1701568361,https://github.com/NVIDIA/spark-rapids/issues/3092#issuecomment-1701568361,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3092,1701568361,IC_kwDOD7z77c5la99p,2023-08-31T18:36:08Z,2023-08-31T18:36:08Z,CONTRIBUTOR,"`spark.sql.timestampType` is used to control timestamp behavior in many places, such as  SQL DDL, Cast clause, type literal, and the schema inference of data sources.

If the user specifies `TIMESTAMP_NTZ`, which we do not currently support, then we could have incorrect behavior in some cases. We should review and see if we need to fall back in any of these areas. ",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1701568361/reactions,0,0,0,0,0,0,0,0,0,3092
167,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724363262,https://github.com/NVIDIA/spark-rapids/issues/3092#issuecomment-1724363262,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3092,1724363262,IC_kwDOD7z77c5mx7H-,2023-09-18T20:51:29Z,2023-09-18T20:51:29Z,CONTRIBUTOR,It looks like the current behavior is correct because we fall back to CPU if a TimestampNTZType is involved. ,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724363262/reactions,0,0,0,0,0,0,0,0,0,3092
168,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/896202191,https://github.com/NVIDIA/spark-rapids/issues/3095#issuecomment-896202191,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3095,896202191,IC_kwDOD7z77c41avXP,2021-08-10T18:05:44Z,2021-08-10T18:05:44Z,CONTRIBUTOR,"Although no action is required, I would like to at least explore implementing our own cost evaluator that delegates to our existing CostBasedOptimizer and compares the original Spark plan with the plugin version of the plan, and chooses the cheapest plan. This would only take effect if the user specifies `spark.sql.adaptive.customCostEvaluatorClass` to use our custom cost evaluator. ",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/896202191/reactions,0,0,0,0,0,0,0,0,0,3095
169,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220958845,https://github.com/NVIDIA/spark-rapids/issues/3149#issuecomment-1220958845,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3149,1220958845,IC_kwDOD7z77c5Ixlp9,2022-08-19T18:01:37Z,2022-08-19T18:01:49Z,COLLABORATOR,"EqualNullSafe, EqualTo, GreaterThan, GreaterThanOrEqual, Greatest, LessThan, and LessThanOrEqual are all complete.

Least does not have support and not currently in progress.",,rwlee,10645552,MDQ6VXNlcjEwNjQ1NTUy,https://avatars.githubusercontent.com/u/10645552?v=4,,https://api.github.com/users/rwlee,https://github.com/rwlee,https://api.github.com/users/rwlee/followers,https://api.github.com/users/rwlee/following{/other_user},https://api.github.com/users/rwlee/gists{/gist_id},https://api.github.com/users/rwlee/starred{/owner}{/repo},https://api.github.com/users/rwlee/subscriptions,https://api.github.com/users/rwlee/orgs,https://api.github.com/users/rwlee/repos,https://api.github.com/users/rwlee/events{/privacy},https://api.github.com/users/rwlee/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220958845/reactions,0,0,0,0,0,0,0,0,0,3149
170,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332766163,https://github.com/NVIDIA/spark-rapids/issues/3150#issuecomment-1332766163,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3150,1332766163,IC_kwDOD7z77c5PcGXT,2022-11-30T21:39:53Z,2022-11-30T21:39:53Z,COLLABORATOR,Cudf dependecy is resolved. Now we can start working on this.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332766163/reactions,0,0,0,0,0,0,0,0,0,3150
171,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/896080915,https://github.com/NVIDIA/spark-rapids/issues/3180#issuecomment-896080915,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3180,896080915,IC_kwDOD7z77c41aRwT,2021-08-10T14:31:55Z,2021-08-10T14:31:55Z,COLLABORATOR,"I did some profiling on a simple sort and it looks to be a very small win at best.  The sampling itself for this simple case was only about 2 ms out of 5 ms to do all of the sub-sampling for the task. Which was a single batch. So there is a lot of overhead there that we should also probably look into.  That said, the entire query run we over 60 seconds, and the sub-sampling portion of it, including generating the data, was only 100ms.  So if we are looking at a very small improvement at best.  This is probably a low priority at this time.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/896080915/reactions,0,0,0,0,0,0,0,0,0,3180
172,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1339582176,https://github.com/NVIDIA/spark-rapids/issues/3186#issuecomment-1339582176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3186,1339582176,IC_kwDOD7z77c5P2Gbg,2022-12-06T15:49:05Z,2022-12-06T15:49:05Z,MEMBER,"Converting this to an epic, as there are many forms Delta Lake writes can take in a query plan.  We will be adding support for these write forms over multiple PRs, and the progress can be tracked in this epic.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1339582176/reactions,0,0,0,0,0,0,0,0,0,3186
173,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/904946492,https://github.com/NVIDIA/spark-rapids/issues/3269#issuecomment-904946492,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3269,904946492,IC_kwDOD7z77c418GM8,2021-08-24T20:19:47Z,2021-08-24T20:19:47Z,CONTRIBUTOR,@jlowe will be adding feature requests in cuDF for 21.12,,Salonijain27,25236093,MDQ6VXNlcjI1MjM2MDkz,https://avatars.githubusercontent.com/u/25236093?v=4,,https://api.github.com/users/Salonijain27,https://github.com/Salonijain27,https://api.github.com/users/Salonijain27/followers,https://api.github.com/users/Salonijain27/following{/other_user},https://api.github.com/users/Salonijain27/gists{/gist_id},https://api.github.com/users/Salonijain27/starred{/owner}{/repo},https://api.github.com/users/Salonijain27/subscriptions,https://api.github.com/users/Salonijain27/orgs,https://api.github.com/users/Salonijain27/repos,https://api.github.com/users/Salonijain27/events{/privacy},https://api.github.com/users/Salonijain27/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/904946492/reactions,0,0,0,0,0,0,0,0,0,3269
174,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/905525449,https://github.com/NVIDIA/spark-rapids/issues/3269#issuecomment-905525449,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3269,905525449,IC_kwDOD7z77c41-TjJ,2021-08-25T13:58:33Z,2022-01-18T14:54:23Z,MEMBER,"- ~~[ ] https://github.com/rapidsai/cudf/issues/9114 tracks the ability to generate a gather map complement~~
- [x] https://github.com/rapidsai/cudf/issues/9115 tracks Java bindings for drop_duplicates

See #3300 for a general discussion of the batched full join algorithm which can be applied with slight modifications to joins where the build side matches the join side.  For left outer with build left, we track the gather maps on the left side, emitting output batches as normal, but after the right stream side finishes, we track which rows were never generated in the left gather maps and emit null-join entries for those.  For left semi with build left, we track the gather maps for the left side, emitting _nothing_ as we stream the right side, and only at the end we concatenate all gather maps, remove duplicates, and emit those rows.  For left anti with build left, we do the same as left semi with build left but instead emit the complement of rows.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/905525449/reactions,0,0,0,0,0,0,0,0,0,3269
175,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/924360099,https://github.com/NVIDIA/spark-rapids/issues/3406#issuecomment-924360099,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3406,924360099,IC_kwDOD7z77c43GJ2j,2021-09-21T20:29:46Z,2021-09-21T20:29:46Z,COLLABORATOR,@revans2 is this resolved by PR #3531 ? ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/924360099/reactions,0,0,0,0,0,0,0,0,0,3406
176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/924524155,https://github.com/NVIDIA/spark-rapids/issues/3406#issuecomment-924524155,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3406,924524155,IC_kwDOD7z77c43Gx57,2021-09-22T02:10:08Z,2021-09-22T02:10:08Z,COLLABORATOR,"> @revans2 is this resolved by PR #3531 ?

Yes,  it is related to #3382 which has been ""fixed"" in temporary. To address this issue, we need a lot modifications on native layer, which can be hardly done in a short time.",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/924524155/reactions,0,0,0,0,0,0,0,0,0,3406
177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925015044,https://github.com/NVIDIA/spark-rapids/issues/3406#issuecomment-925015044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3406,925015044,IC_kwDOD7z77c43IpwE,2021-09-22T15:00:45Z,2021-09-22T15:00:45Z,COLLABORATOR,"We can close this and file a follow on issue to implement it fully, or we can just keep this open and re-prioritize it based off of wanting to support this properly.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925015044/reactions,0,0,0,0,0,0,0,0,0,3406
178,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925439886,https://github.com/NVIDIA/spark-rapids/issues/3406#issuecomment-925439886,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3406,925439886,IC_kwDOD7z77c43KReO,2021-09-23T01:11:33Z,2021-09-23T01:11:33Z,COLLABORATOR,Let's leave this open for fixing at a later date.  I will add it back the triage label so we discuss it before we decide how to disposition it.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/925439886/reactions,0,0,0,0,0,0,0,0,0,3406
179,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/934905814,https://github.com/NVIDIA/spark-rapids/issues/3406#issuecomment-934905814,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3406,934905814,IC_kwDOD7z77c43uYfW,2021-10-05T21:56:48Z,2021-10-05T21:56:48Z,COLLABORATOR,We do not have plans to support years beyond seven digits until a customer asks.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/934905814/reactions,0,0,0,0,0,0,0,0,0,3406
180,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/919499751,https://github.com/NVIDIA/spark-rapids/issues/3437#issuecomment-919499751,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3437,919499751,IC_kwDOD7z77c42znPn,2021-09-14T20:45:26Z,2021-09-14T20:45:26Z,COLLABORATOR,"This should be resolved when https://github.com/NVIDIA/spark-rapids/issues/3194 is resolved.  If 3914 does not get resolved in a timely fashion, we should come back to this and address it.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/919499751/reactions,0,0,0,0,0,0,0,0,0,3437
181,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/919508757,https://github.com/NVIDIA/spark-rapids/issues/3437#issuecomment-919508757,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3437,919508757,IC_kwDOD7z77c42zpcV,2021-09-14T21:00:04Z,2021-09-14T21:01:57Z,COLLABORATOR,"> This should be resolved when #3194 is resolved. If 3914 does not get resolved in a timely fashion, we should come back to this and address it.

This issue is orthogonal to #3194. The patterns that @sperlingxx is talking about here are patterns to denote what an aggregate exec will look like for the tests only. For example, databricks may use the `Complete` in some cases, where Apache Spark will treat the aggregate differently, and if we are trying to test that the GPU aggregate can take and produce compatible output with the CPU, we need to be able to address each flavor of the hash aggregate plans. For example (stolen from one the tests @sperlingxx had): 

```
_replace_modes_single_distinct = [
    # Spark: CPU -> CPU -> GPU(PartialMerge) -> GPU(Partial)
    # Databricks runtime: CPU(Final and Complete) -> GPU(PartialMerge)
    'partial|partialMerge',
    # Spark: GPU(Final) -> GPU(PartialMerge&Partial) -> CPU(PartialMerge) -> CPU(Partial)
    # Databricks runtime: GPU(Final&Complete) -> CPU(PartialMerge)
    'final|partialMerge&partial|final&complete',
]
```

So in this case, we want to keep on the GPU:

- First case: PartialMerge (databricks) and Partial (apache)
- Second case: Final or PartialMerge&Partial (for apache), and Final&Complete (databricks).

And the rest of the aggregate executes on the CPU, which is great as we can show in the tests we can be compatible if part of the plan needs to execute on the CPU due to some operation we don't support yet.

The patterns are a bit convoluted here, and you have to go through the comments to understand what's going on. The proposal is to at least try and associate each pattern with a flavor of Spark, but ideally we can find some common patterns that can be prebaked and documented so we don't have to read a bunch of comments each time.
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/919508757/reactions,0,0,0,0,0,0,0,0,0,3437
182,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/929643869,https://github.com/NVIDIA/spark-rapids/issues/3492#issuecomment-929643869,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3492,929643869,IC_kwDOD7z77c43aT1d,2021-09-28T21:36:56Z,2021-09-28T21:36:56Z,COLLABORATOR,"Instead of changing these to pyspark tests, unless there is a real reason to or hole that we aren't testing with existing tests,  we should just make the unit tests work on Databricks.   ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/929643869/reactions,0,0,0,0,0,0,0,0,0,3492
183,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/996500232,https://github.com/NVIDIA/spark-rapids/issues/3543#issuecomment-996500232,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3543,996500232,IC_kwDOD7z77c47ZWMI,2021-12-17T07:34:38Z,2021-12-17T07:34:50Z,COLLABORATOR,"As tagged, it requires the support from cudf.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/996500232/reactions,0,0,0,0,0,0,0,0,0,3543
184,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/996556990,https://github.com/NVIDIA/spark-rapids/issues/3543#issuecomment-996556990,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3543,996556990,IC_kwDOD7z77c47ZkC-,2021-12-17T09:14:34Z,2021-12-17T09:26:01Z,COLLABORATOR,"[The current implementation](https://github.com/rapidsai/cudf/blob/31f92d70bdf5b80434d24b6883e45496dbe85042/cpp/src/strings/json/json_path.cu#L1050) supports only a scalar (not a column) as the input path.
",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/996556990/reactions,0,0,0,0,0,0,0,0,0,3543
185,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1024711139,https://github.com/NVIDIA/spark-rapids/issues/3543#issuecomment-1024711139,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3543,1024711139,IC_kwDOD7z77c49E9nj,2022-01-28T22:37:15Z,2022-01-28T22:37:15Z,COLLABORATOR,Is this a request for a separate JSONPath string per row of the input column?,,nvdbaranec,56695930,MDQ6VXNlcjU2Njk1OTMw,https://avatars.githubusercontent.com/u/56695930?v=4,,https://api.github.com/users/nvdbaranec,https://github.com/nvdbaranec,https://api.github.com/users/nvdbaranec/followers,https://api.github.com/users/nvdbaranec/following{/other_user},https://api.github.com/users/nvdbaranec/gists{/gist_id},https://api.github.com/users/nvdbaranec/starred{/owner}{/repo},https://api.github.com/users/nvdbaranec/subscriptions,https://api.github.com/users/nvdbaranec/orgs,https://api.github.com/users/nvdbaranec/repos,https://api.github.com/users/nvdbaranec/events{/privacy},https://api.github.com/users/nvdbaranec/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1024711139/reactions,0,0,0,0,0,0,0,0,0,3543
186,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/977192232,https://github.com/NVIDIA/spark-rapids/issues/3554#issuecomment-977192232,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3554,977192232,IC_kwDOD7z77c46PsUo,2021-11-23T21:41:08Z,2021-11-23T21:41:08Z,COLLABORATOR,"Did not make progress on this in 21.12, putting into the backlog.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/977192232/reactions,0,0,0,0,0,0,0,0,0,3554
187,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/932446176,https://github.com/NVIDIA/spark-rapids/issues/3702#issuecomment-932446176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3702,932446176,IC_kwDOD7z77c43k__g,2021-10-01T18:06:08Z,2021-10-01T18:06:08Z,COLLABORATOR,deassinging myself as this needs to be triaged ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/932446176/reactions,0,0,0,0,0,0,0,0,0,3702
188,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1184971499,https://github.com/NVIDIA/spark-rapids/issues/3715#issuecomment-1184971499,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3715,1184971499,IC_kwDOD7z77c5GoTrr,2022-07-14T23:00:41Z,2022-07-14T23:00:41Z,COLLABORATOR,See cuDF issue: https://github.com/rapidsai/cudf/issues/11222 ,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1184971499/reactions,0,0,0,0,0,0,0,0,0,3715
189,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378003958,https://github.com/NVIDIA/spark-rapids/issues/3715#issuecomment-1378003958,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3715,1378003958,IC_kwDOD7z77c5SIqv2,2023-01-10T22:58:14Z,2023-01-10T22:58:14Z,COLLABORATOR,Depends on https://github.com/NVIDIA/spark-rapids/issues/7485 and https://github.com/NVIDIA/spark-rapids/issues/5430,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378003958/reactions,0,0,0,0,0,0,0,0,0,3715
190,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/950656647,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-950656647,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,950656647,IC_kwDOD7z77c44qd6H,2021-10-25T08:24:09Z,2021-10-25T08:24:09Z,CONTRIBUTOR,"tried to test parallel build within premerge CI test, however, two issues are encountered as following:
1. build failure (parallel=4)
```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.4:install (default-install) on project rapids-4-spark-udf_2.12: Failed to install metadata com.nvidia:rapids-4-spark-udf_2.12:21.12.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /root/.m2/repository/com/nvidia/rapids-4-spark-udf_2.12/21.12.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got p (position: END_TAG seen ...</metadata>\np... @20:2) -> [Help 1]
``` 
Note, this issue can be workarounded if build one single version first before parallel build other versions. Maybe it's related with `.m2` cache, that's always need setup during build for premerge job. 

2. test failure (parallel=4)
```
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:2.0.2:test (test) on project rapids-4-spark-tests_2.12: There are test failures -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.scalatest:scalatest-maven-plugin:2.0.2:test (test) on project rapids-4-spark-tests_2.12: There are test failures
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:956)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:288)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures
    at org.scalatest.tools.maven.TestMojo.execute (TestMojo.java:109)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:956)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:288)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
```
Please refer to testing PR https://github.com/pxLi/spark-rapids/pull/216/commits/fb0ba4c127bdea8fa10d491f4e75da8fdb6048a1 for more details. This issue can be reproduced with the script provided in the [PR](https://github.com/pxLi/spark-rapids/pull/216/commits/fb0ba4c127bdea8fa10d491f4e75da8fdb6048a1) as following steps:
```
export URM_URL=""<please_replace_with_real_urm_url>""
export WORKSPACE=`pwd`
export MVN_URM_MIRROR='-s jenkins/settings.xml -P mirror-apache-to-urm'
export CUDA_CLASSIFIER='cuda11'
export LIBCUDF_KERNEL_CACHE_PATH=`pwd`/.cudf
export CUDF_VER=21.12.0-SNAPSHOT
export PROJECT_VER=21.12.0-SNAPSHOT
export PROJECT_TEST_VER=21.12.0-SNAPSHOT
jenkins/spark-premerge-build.sh mvn_verify
```",,zhanga5,13271672,MDQ6VXNlcjEzMjcxNjcy,https://avatars.githubusercontent.com/u/13271672?v=4,,https://api.github.com/users/zhanga5,https://github.com/zhanga5,https://api.github.com/users/zhanga5/followers,https://api.github.com/users/zhanga5/following{/other_user},https://api.github.com/users/zhanga5/gists{/gist_id},https://api.github.com/users/zhanga5/starred{/owner}{/repo},https://api.github.com/users/zhanga5/subscriptions,https://api.github.com/users/zhanga5/orgs,https://api.github.com/users/zhanga5/repos,https://api.github.com/users/zhanga5/events{/privacy},https://api.github.com/users/zhanga5/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/950656647/reactions,0,0,0,0,0,0,0,0,0,3805
191,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/951773138,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-951773138,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,951773138,IC_kwDOD7z77c44uufS,2021-10-26T09:53:09Z,2021-10-26T09:53:09Z,COLLABORATOR,"I've seen this once and ended up deleting metadata-local.xml. I googled around and it looks like there is not concurrency control on the metadata file containing the information about classifiers.

1. https://www.mail-archive.com/geoserver-devel@lists.sourceforge.net/msg46922.html
2. http://mail-archives.apache.org/mod_mbox/hadoop-common-issues/202008.mbox/%3CJIRA.13320926.1596566600000.169602.1596568500110@Atlassian.JIRA%3E
3. https://www.mail-archive.com/users@maven.apache.org/msg141457.html

separate local m2 per executor of something like  https://github.com/takari/takari-local-repository could be solutions",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/951773138/reactions,0,0,0,0,0,0,0,0,0,3805
192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/952259487,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-952259487,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,952259487,IC_kwDOD7z77c44wlOf,2021-10-26T19:40:58Z,2021-10-26T19:40:58Z,COLLABORATOR,"Another idea : I don't think we need any artifacts installed but the aggregator jars. So if we could run just package in parallel, and then single-thread to install-file aggregator jars. ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/952259487/reactions,0,0,0,0,0,0,0,0,0,3805
193,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/953536687,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-953536687,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,953536687,IC_kwDOD7z77c441dCv,2021-10-28T06:20:04Z,2021-10-28T06:20:04Z,CONTRIBUTOR,"in fact, build one version took ~2 mins, while unit test took ~15mins for premerge ci job. Currently we build 6 versions and 2 versions (311, 320) that's enabled unit test. It means only ~10mins would be saved if introduce parallel build + serialized unit test. The more reasonable option would be run two versions unit test from different k8s pods for best parallelism, however, there's no much resource available currently. We already applied two k8s pods for premerge job for parallel integration test. Hopefully, more resource could be applied in the future then add parallel unit tests.",,zhanga5,13271672,MDQ6VXNlcjEzMjcxNjcy,https://avatars.githubusercontent.com/u/13271672?v=4,,https://api.github.com/users/zhanga5,https://github.com/zhanga5,https://api.github.com/users/zhanga5/followers,https://api.github.com/users/zhanga5/following{/other_user},https://api.github.com/users/zhanga5/gists{/gist_id},https://api.github.com/users/zhanga5/starred{/owner}{/repo},https://api.github.com/users/zhanga5/subscriptions,https://api.github.com/users/zhanga5/orgs,https://api.github.com/users/zhanga5/repos,https://api.github.com/users/zhanga5/events{/privacy},https://api.github.com/users/zhanga5/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/953536687/reactions,0,0,0,0,0,0,0,0,0,3805
194,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/953538501,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-953538501,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,953538501,IC_kwDOD7z77c441dfF,2021-10-28T06:23:46Z,2021-10-28T06:23:46Z,CONTRIBUTOR,"> 
> separate local m2 per executor of something like https://github.com/takari/takari-local-repository could be solutions

test this maven plugin indicated the first issue 1 (build failure) mentioned at above can be solved, however, still encountered issue 2 (test failure) with same error symptom.",,zhanga5,13271672,MDQ6VXNlcjEzMjcxNjcy,https://avatars.githubusercontent.com/u/13271672?v=4,,https://api.github.com/users/zhanga5,https://github.com/zhanga5,https://api.github.com/users/zhanga5/followers,https://api.github.com/users/zhanga5/following{/other_user},https://api.github.com/users/zhanga5/gists{/gist_id},https://api.github.com/users/zhanga5/starred{/owner}{/repo},https://api.github.com/users/zhanga5/subscriptions,https://api.github.com/users/zhanga5/orgs,https://api.github.com/users/zhanga5/repos,https://api.github.com/users/zhanga5/events{/privacy},https://api.github.com/users/zhanga5/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/953538501/reactions,0,0,0,0,0,0,0,0,0,3805
195,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/954982824,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-954982824,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,954982824,IC_kwDOD7z77c446-Go,2021-10-29T19:07:13Z,2021-10-29T19:08:00Z,COLLABORATOR,The fact that the tests are not enabled for all versions is a tradeoff between wanting to run it for all and the time it takes to get a premege check on  a PR. If all shims are in the parallel CI stages we can enable them for all versions,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/954982824/reactions,0,0,0,0,0,0,0,0,0,3805
196,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/955953991,https://github.com/NVIDIA/spark-rapids/issues/3805#issuecomment-955953991,https://api.github.com/repos/NVIDIA/spark-rapids/issues/3805,955953991,IC_kwDOD7z77c44-rNH,2021-11-01T05:56:04Z,2021-11-01T05:56:04Z,COLLABORATOR,"We can get back to this when we have enough GPU resources (planning resources for next year)

removed from Release 21.12 for now. This CI feature should not block specific plugin release",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/955953991/reactions,1,1,0,0,0,0,0,0,0,3805
197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332755669,https://github.com/NVIDIA/spark-rapids/issues/4000#issuecomment-1332755669,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4000,1332755669,IC_kwDOD7z77c5PcDzV,2022-11-30T21:28:21Z,2022-11-30T21:28:21Z,COLLABORATOR,libcudf currently supports only group by `corr`. Reduction and windowing `corr` needs to be requested unless we only need the group by support for query 11.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332755669/reactions,0,0,0,0,0,0,0,0,0,4000
198,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332767068,https://github.com/NVIDIA/spark-rapids/issues/4000#issuecomment-1332767068,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4000,1332767068,IC_kwDOD7z77c5PcGlc,2022-11-30T21:40:53Z,2022-11-30T21:40:53Z,COLLABORATOR,"`corr` in CUDF would not do what we want for group by. We need to be able to split it up between an initial pass and a merge pass. `corr` in CUDF does it all in one pass. This is also true for reduction. It is shuffling 6 different double columns of values to do the calculation.

```
Seq(n, xAvg, yAvg, ck, xMk, yMk)
```

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332767068/reactions,0,0,0,0,0,0,0,0,0,4000
199,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332772695,https://github.com/NVIDIA/spark-rapids/issues/4000#issuecomment-1332772695,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4000,1332772695,IC_kwDOD7z77c5PcH9X,2022-11-30T21:46:23Z,2022-11-30T21:46:23Z,COLLABORATOR,Oh then we also need to support `corr` with batched data similar to what we need for standard deviation before.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332772695/reactions,0,0,0,0,0,0,0,0,0,4000
200,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1969942451,https://github.com/NVIDIA/spark-rapids/issues/4032#issuecomment-1969942451,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4032,1969942451,IC_kwDOD7z77c51au-z,2024-02-28T21:22:08Z,2024-02-28T21:22:08Z,MEMBER,@tgravescs is this still relevant?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1969942451/reactions,0,0,0,0,0,0,0,0,0,4032
201,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/961300467,https://github.com/NVIDIA/spark-rapids/issues/4034#issuecomment-961300467,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4034,961300467,IC_kwDOD7z77c45TEfz,2021-11-04T18:17:40Z,2021-11-04T18:17:40Z,COLLABORATOR,I should add that this is a really stupid query and we and unlikely to see something like this in real life.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/961300467/reactions,0,0,0,0,0,0,0,0,0,4034
202,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097265126,https://github.com/NVIDIA/spark-rapids/issues/4034#issuecomment-1097265126,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4034,1097265126,IC_kwDOD7z77c5BZu_m,2022-04-12T21:57:20Z,2022-04-12T21:57:20Z,COLLABORATOR,This is kind of a special case of #1501 ,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097265126/reactions,0,0,0,0,0,0,0,0,0,4034
203,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049771167,https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1049771167,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146,1049771167,IC_kwDOD7z77c4-kjyf,2022-02-24T11:37:38Z,2022-02-24T11:37:38Z,COLLABORATOR,"@revans2  Please help review the solution:


Interval types can be found in https://spark.apache.org/docs/latest/sql-ref-datatypes.html

Currently plugin do not support write for csv, so let's talk about reading interval type from CSV.

Spark read interval code is in:
[IntervalUtils.fromDayTimeString](https://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala#L430)

There are legacy form and normal form switched by SQLConf.LEGACY_FROM_DAYTIME_STRING.

**legacy form**
By default, legacy from daytime string is disable, so we may not support this.

SQLConf.LEGACY_FROM_DAYTIME_STRING See: https://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L3042

parseDayTimeLegacy see:
https://github.com/apache/spark/blob/v3.2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala#L463toL475

**normal form**
By default, Spark use this form,  some examples see below table, we will support this form only.

|type|example|valid|comment|
|:-|:-|:-|:-|
|INTERVAL DAY | INTERVAL '100' DAY  |true    |    |
|INTERVAL DAY | 100 DAY  |true    |    |
|INTERVAL DAY TO HOUR | INTERVAL '100 10' DAY TO HOUR  |true    |    |
|INTERVAL DAY TO HOUR | 100 10   |true    |    |
|INTERVAL DAY TO SECOND | INTERVAL '100 10:30:40.999999' DAY TO SECOND  |true    |    |
|INTERVAL DAY TO SECOND | 100 10:30:40.999999  |true    |    |
|INTERVAL DAY TO SECOND | INTERVAL '100 10:30:40. &nbsp;  &nbsp;999999' DAY TO SECOND  |false    | has extra spaces   |
|INTERVAL DAY TO SECOND | INTERVAL '100  &nbsp;  &nbsp; 10:30:40.999999' DAY TO SECOND  |false    |  has extra spaces     |
|INTERVAL DAY TO SECOND | 100  &nbsp;  &nbsp;  10:30:40.999999  |false    | has extra spaces      |
|INTERVAL DAY TO SECOND | 100 10:30: &nbsp;  &nbsp; 40.999999  |false    | has extra spaces      |
|INTERVAL DAY TO SECOND | INTERVAL '-100 10:30:40.999999' DAY TO SECOND  |true    |       |
|INTERVAL DAY TO SECOND | INTERVAL -'-100 10:30:40.999999' DAY TO SECOND  |true    |    two negative signs is positive  |
|INTERVAL DAY TO SECOND | -100 10:30:40.999999  |true    |       |
|INTERVAL DAY TO SECOND | INTERVAL '100 26:30:40.999999' DAY TO SECOND  |false    |    hour is 26 > 23  |

The invalid value will be null when reading csv.

**proposed solution for the normal form**

Use Cudf ColumnView.extractRe to extract the day, hour, ... , second fields by specifing the groups in regexp, and then calculate the micros.

Gpu code is like:
```
    val intervalCV = builder.buildAndPutOnDevice()
    val start = System.nanoTime()
    val p = ""^INTERVAL\\s+([+|-])?'([+|-])?(\\d{1,9}) (\\d{1,2}):(\\d{1,2}):"" +
        ""(\\d{1,2})(\\.\\d{1,9})?'\\s+DAY\\s+TO\\s+SECOND$""

    // e.g.: INTERVAL -'-100 10:30:40.999999' DAY TO SECOND
    // group 0: sign is -
    // group 1: sign is -
    // group 2: day is 100
    // group 3: hour 10
    // group 4: minute 30
    // group 5: second 40
    // group 6: micro seconds 999999

    withResource(intervalCV.extractRe(p)) {
      table => {
        println(""row count: "" + table.getRowCount)

        val micros = table.getColumn(2).castTo(DType.INT64).mul(Scalar.fromLong(86400L * 1000000L))   // day to micros
            .add(table.getColumn(3).castTo(DType.INT64).mul(Scalar.fromLong(3600 * 1000000L))) // hour to micros
            .add(table.getColumn(4).castTo(DType.INT64).mul(Scalar.fromLong(60 * 1000000L))) // minute
            .add(table.getColumn(5).castTo(DType.INT64).mul(Scalar.fromLong(1000000L))) 
            .add(table.getColumn(6).castTo(DType.INT64))
      }
    }
    val timeS =(System.nanoTime() - start).toDouble / TimeUnit.SECONDS.toNanos(1)
    println(s""GPU used time $timeS S"")
```

Cpu code is like
```
    while (i < rowCount) {
      IntervalUtils.fromDayTimeString(intervals(i))
      i += 1
    }
```

row count: 10,000,000
GPU used time 0.86659265 S
CPU used time 16.818680952 S
GPU speedups about 19x
Is this acceptable or have more effient approach?
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049771167/reactions,0,0,0,0,0,0,0,0,0,4146
204,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049986672,https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1049986672,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146,1049986672,IC_kwDOD7z77c4-lYZw,2022-02-24T15:39:58Z,2022-02-24T15:39:58Z,COLLABORATOR,"I really dislike regular expression use in casts, but it is a good first step.  It would be nice to file a follow on issue to write a custom kernel to do this for us.

Also I assume you know that your code to do the conversion is leaking a lot of column views. I assume you did that just for readability of the code.

Second have you tested this with CSV?  The patch that added in support for writing/reading CSV https://issues.apache.org/jira/browse/SPARK-36831 did not add in anything that calls `fromDayTimeString` or similar. The Parquet code just stored these as an int or a long.  Please check to see if CSV is doing the same, because I suspect that they are, and then we don't need to do as much special processing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1049986672/reactions,0,0,0,0,0,0,0,0,0,4146
205,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1050499527,https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1050499527,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146,1050499527,IC_kwDOD7z77c4-nVnH,2022-02-25T04:01:48Z,2022-02-25T05:06:38Z,COLLABORATOR,"Filed an issue: https://github.com/rapidsai/cudf/issues/10356

CSV is text file, the day-time interval is stored in string form, e.g: 
```
INTERVAL '100 10:30:40.999999' DAY TO SECOND 
```
Spark used a similar method to parse interval string to day-time interval:  IntervalUtils.castStringToDTInterval
The IntervalUtils.fromDayTimeString in the example code also invoked IntervalUtils.castStringToDTInterval.

I know the leaking in the example code, thanks the kindly reminder.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1050499527/reactions,0,0,0,0,0,0,0,0,0,4146
206,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1092382787,https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1092382787,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146,1092382787,IC_kwDOD7z77c5BHHBD,2022-04-08T02:30:19Z,2022-04-08T02:30:19Z,COLLABORATOR,"Spark Accelerator already supported  reading `day-time` interval from CSV temporarily.
But is still waiting cuDF kernel to support.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1092382787/reactions,0,0,0,0,0,0,0,0,0,4146
207,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280243559,https://github.com/NVIDIA/spark-rapids/issues/4146#issuecomment-1280243559,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4146,1280243559,IC_kwDOD7z77c5MTvdn,2022-10-17T03:54:23Z,2022-10-17T03:54:23Z,COLLABORATOR,"The cuDF issue is closed but without a fix.
For details see https://github.com/rapidsai/cudf/issues/10356#issuecomment-1168208942.


",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280243559/reactions,0,0,0,0,0,0,0,0,0,4146
208,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/975671865,https://github.com/NVIDIA/spark-rapids/issues/4158#issuecomment-975671865,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4158,975671865,IC_kwDOD7z77c46J5I5,2021-11-22T15:59:57Z,2021-11-22T15:59:57Z,COLLABORATOR,"we are releasing docker images with both Apache Spark in it and the spark rapids plugin?     For what env?  k8s, yarn, etc?",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/975671865/reactions,0,0,0,0,0,0,0,0,0,4158
209,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/976062798,https://github.com/NVIDIA/spark-rapids/issues/4158#issuecomment-976062798,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4158,976062798,IC_kwDOD7z77c46LYlO,2021-11-23T00:58:40Z,2021-11-23T01:04:04Z,COLLABORATOR,"> we are releasing docker images with both Apache Spark in it and the spark rapids plugin? For what env? k8s, yarn, etc?

We are not going to release any images for now. The issue here is to track if we decide to provide out-of-box docker image in some public registry for quick starter, it could just contain specific spark bin and released plugin & cudfJNI artifacts",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/976062798/reactions,0,0,0,0,0,0,0,0,0,4158
210,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/974296972,https://github.com/NVIDIA/spark-rapids/issues/4164#issuecomment-974296972,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164,974296972,IC_kwDOD7z77c46EpeM,2021-11-19T18:13:09Z,2021-11-19T18:13:09Z,MEMBER,"> we look at a small amount of data (1 or 2 files at most) to try and determine the file schema.

The file schema is already known, fetched by Spark as part of planning and validating the query, and available via the `dataSchema` field of the `HadoopFsRelation`.   Do you mean something more sophisticated like parsing the footer of the files and looking at relative column data sizes or something more simplistic like a blind guess as to the relative data sizes of the columns based on their known Spark data types?",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/974296972/reactions,0,0,0,0,0,0,0,0,0,4164
211,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/974313918,https://github.com/NVIDIA/spark-rapids/issues/4164#issuecomment-974313918,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164,974313918,IC_kwDOD7z77c46Etm-,2021-11-19T18:38:21Z,2021-11-19T18:38:21Z,COLLABORATOR,"I would start off with the simplest possible approach and see how far that gets us.  I would only make it more complicated if we run into real world situations where it is too far off for us to get the benefit we want.

The simplest I can think of is to keep targeting `maxPartitionBytes`, but we look at the read schema vs the file schema and SWAG how much of each batch we are going to be able to skip and read the block size accordingly. Alternatively we could try and target the GPU target batch size instead. But then we have to try and understand the compression ratio of the columns that we are going to read in. That feels harder to do.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/974313918/reactions,0,0,0,0,0,0,0,0,0,4164
212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/997991687,https://github.com/NVIDIA/spark-rapids/issues/4164#issuecomment-997991687,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4164,997991687,IC_kwDOD7z77c47fCUH,2021-12-20T14:50:29Z,2021-12-20T14:50:29Z,COLLABORATOR,"I have been working on this for a while and there is no simple way to do what we want with only the information we have ahead of time. I tried a few heuristics based off of information that we currently have access to through Spark when creating the splits. I mainly looked at the number and types of columns to try and increase the maximum batch size config per-input. That way the batches would get larger and the sub-linear scaling of the GPU works better. This worked well for a few extreme cases, NDS queries 9, 7, and 88 but not as well in the general case.

There are a number of problems with the initial approach that I took.

1. Predicate push down, compression ratios, and row groups. I was able to get the size of the batches to increase in the general case, but it was far from consistent. This means that the maximum batch that was processed grew about 4 fold, to the point that it was larger than the ""maximum"" batch size. It also spread the sizes out a lot more, which made getting predictable computation much more difficult.
2. gpu decoding is volatile. For smaller batch sizes (up to about 200 MiB) there is a clear trend that more data is better.  After that point I don't think I have enough data to really come to any real conclusion. There is a lot of volatility from one bucket to another. There is huge volatility within buckets.  The size has a clear impact on computation time, but predicting what that computation time will be is not as clear.
3. Increased host memory pressure.  Before we send the data to the GPU we buffer it on the CPU.  If we increase the size of the batches, we now have more data held on the CPU at any point in time and that increases memory pressure, especially in relation to pinned memory.
4. Buffer time.  The time to decode parquet is not all about the GPU decoding time. We also have to read that data in and cache it in host memory. In general we want to overlap buffering data and computation on the GPU. But we still have to pay the initial cost of downloading the first batch of data before we can put anything on the GPU. If we increase the average size of a batch we are also increasing the amount of time it takes to download the data before we can put it on the GPU.
5. Side impacts.  I also saw a number of other impacts caused by changing the number of tasks and size of the batches. We saw increased contention for the GPU, which slowed down other processing too. We saw in many cases fewer tasks later in the processing too. I am not totally sure why AQE was changing things like this, but generally if there were fewer upstream tasks it resulted in fewer downstream tasks, even if the total amount of data stayed the same.  We also saw some impact to shuffle and compression on the GPU.

Despite all of this I have hope that this can be useful and we should look into this more. Even this imperfect code saved over 3,400 seconds of GPU compute time from decoding parquet. That is about 27.8% of the total compute time used to parse the parquet data on the GPU, and about 1.7% of the total run time of all of the queries, assuming that the computation could be spread evenly among all of the tasks.  So there is potential here to make a decent improvement. But it has not worked out perfectly.

We are looking at ways to improve the decoding performance of the parquet kernels themselves. In addition to this we might want to look at more of a control system approach instead. We lack information up front and it looks to be expensive to try and get that information early on. It might be better to try and dynamically adjust the sizes of the batches we buffer/send to the GPU based off of throughput rates that we are able to achieve while buffering and guesses about how much data the GPU can processes efficiently. But we first need to do a fair amount of benchmarking to understand what really impacts the performance of the buffering and the performance of decode. We also would need to look how AQE will impact downstream processing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/997991687/reactions,0,0,0,0,0,0,0,0,0,4164
213,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/983767223,https://github.com/NVIDIA/spark-rapids/issues/4252#issuecomment-983767223,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4252,983767223,IC_kwDOD7z77c46oxi3,2021-12-01T15:42:15Z,2021-12-01T15:42:15Z,MEMBER,"> We don't really have any API either so the javadoc doesn't matter to much at this point.

We do have a small API, consisting of at least these classes since they are advertised in the documentation and used in working examples:
- com.nvidia.spark.rapids.ColumnarRdd
- com.nvidia.spark.RapidsUDF",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/983767223/reactions,0,0,0,0,0,0,0,0,0,4252
214,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/988291621,https://github.com/NVIDIA/spark-rapids/issues/4252#issuecomment-988291621,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4252,988291621,IC_kwDOD7z77c466CIl,2021-12-07T21:58:56Z,2021-12-07T21:58:56Z,COLLABORATOR,We discussed and for now  it makes sense to just stop deploying the javadoc and source jars all together.  If it becomes an issue for other API we can figure out how to do it properly later.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/988291621/reactions,0,0,0,0,0,0,0,0,0,4252
215,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/986936714,https://github.com/NVIDIA/spark-rapids/issues/4301#issuecomment-986936714,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4301,986936714,IC_kwDOD7z77c4603WK,2021-12-06T16:26:14Z,2021-12-06T16:26:14Z,COLLABORATOR,"We cannot/should not remove the `NullIntolerant` in our code until we only support 3.3.0 or above.  `NullIntolerant` is used as metadata primarily in planning so it adds some null checks before the code. So this really just comes down to making sure that we have tests that cover nulls properly, which I think we do.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/986936714/reactions,0,0,0,0,0,0,0,0,0,4301
216,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/986931297,https://github.com/NVIDIA/spark-rapids/issues/4302#issuecomment-986931297,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4302,986931297,IC_kwDOD7z77c4602Bh,2021-12-06T16:20:29Z,2021-12-06T16:20:29Z,COLLABORATOR,I am not sure that we have any customers that will be impacted by this. It looks like we will do the right thing and fall back to the CPU if we do hit a situation in 3.3 where someone tried to pad a binary type. I am inclined to not worry about this until we have a customer that runs into the problem.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/986931297/reactions,0,0,0,0,0,0,0,0,0,4302
217,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1532183125,https://github.com/NVIDIA/spark-rapids/issues/4411#issuecomment-1532183125,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4411,1532183125,IC_kwDOD7z77c5bU0JV,2023-05-02T21:33:54Z,2023-05-02T21:33:54Z,MEMBER,libcudf is adding IS_NULL unary operator support in rapidsai/cudf#13145.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1532183125/reactions,0,0,0,0,0,0,0,0,0,4411
218,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155537014,https://github.com/NVIDIA/spark-rapids/issues/4415#issuecomment-1155537014,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4415,1155537014,IC_kwDOD7z77c5E4Bh2,2022-06-14T18:18:41Z,2022-06-14T18:18:41Z,CONTRIBUTOR,"I have been experimenting with transpiling these down to an equivalent character class and I don't think we can support these with our current approach. The equivalent character class for each of these are all extremely long. I would be concerned that complex patterns using these transpiled classes would run out of memory. 

Also, I discovered cases such as `\p{IsGreek}` vs `\p{InGreek}` which are both valid but match a different set of characters. Java doesn't have any documentation about this...",,anthony-chang,54450499,MDQ6VXNlcjU0NDUwNDk5,https://avatars.githubusercontent.com/u/54450499?v=4,,https://api.github.com/users/anthony-chang,https://github.com/anthony-chang,https://api.github.com/users/anthony-chang/followers,https://api.github.com/users/anthony-chang/following{/other_user},https://api.github.com/users/anthony-chang/gists{/gist_id},https://api.github.com/users/anthony-chang/starred{/owner}{/repo},https://api.github.com/users/anthony-chang/subscriptions,https://api.github.com/users/anthony-chang/orgs,https://api.github.com/users/anthony-chang/repos,https://api.github.com/users/anthony-chang/events{/privacy},https://api.github.com/users/anthony-chang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155537014/reactions,0,0,0,0,0,0,0,0,0,4415
219,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1042617064,https://github.com/NVIDIA/spark-rapids/issues/4496#issuecomment-1042617064,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4496,1042617064,IC_kwDOD7z77c4-JRLo,2022-02-17T06:28:39Z,2022-02-17T06:28:39Z,COLLABORATOR,I can work on it,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1042617064/reactions,0,0,0,0,0,0,0,0,0,4496
220,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1043916324,https://github.com/NVIDIA/spark-rapids/issues/4496#issuecomment-1043916324,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4496,1043916324,IC_kwDOD7z77c4-OOYk,2022-02-18T05:21:15Z,2022-02-18T05:21:15Z,COLLABORATOR,"Should we put the changes in Shim layers, or just modify the code in place?",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1043916324/reactions,0,0,0,0,0,0,0,0,0,4496
221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1044740976,https://github.com/NVIDIA/spark-rapids/issues/4496#issuecomment-1044740976,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4496,1044740976,IC_kwDOD7z77c4-RXtw,2022-02-18T15:54:20Z,2022-02-18T15:54:20Z,MEMBER,"If I understand this correctly, I don't think it makes sense to do this change unless we _really_ need any performance gain from it, as it would .  The original PR modified `Expression` and relies on a new method to be implemented by expressions.  Until Spark 3.3 is the minimum Spark spec we support, we cannot rely on `Expression` objects providing or having this method, because they will derive from Spark's `Expression` and old Spark versions don't have the required method definition.  We could override it in our `ShimExpression` wrappers, but this doesn't handle `Expression` objects coming from the original CPU plan.

I think we would need to see a demonstrated need for this (i.e.: some performance metrics showing how much can be gained on real-world queries) before prioritizing this work.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1044740976/reactions,1,0,0,0,0,0,1,0,0,4496
222,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1011314204,https://github.com/NVIDIA/spark-rapids/issues/4509#issuecomment-1011314204,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4509,1011314204,IC_kwDOD7z77c48R24c,2022-01-12T18:04:44Z,2022-01-12T18:04:44Z,MEMBER,"@andygrove FYI I added #4511 to the list, since I think we need to improve the current situation where regex kernels can fail with a confusing OOM error due to insufficient reserved memory rather than insufficient pool memory.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1011314204/reactions,1,1,0,0,0,0,0,0,0,4509
223,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1082791600,https://github.com/NVIDIA/spark-rapids/issues/4509#issuecomment-1082791600,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4509,1082791600,IC_kwDOD7z77c5Aihaw,2022-03-30T08:39:45Z,2022-04-02T08:18:35Z,COLLABORATOR,"~Hi @andygrove, I found another bug about `regexp_extract` #5088. Shall we put it in the list ?~",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1082791600/reactions,0,0,0,0,0,0,0,0,0,4509
224,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1086586254,https://github.com/NVIDIA/spark-rapids/issues/4509#issuecomment-1086586254,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4509,1086586254,IC_kwDOD7z77c5Aw_2O,2022-04-02T08:21:23Z,2022-04-02T08:21:23Z,COLLABORATOR,"Hi @andygrove, I added #5135 to the list as a high priority task, since I think it is a correctness issue which is not only triggered by corner cases.",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1086586254/reactions,1,1,0,0,0,0,0,0,0,4509
225,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1060274936,https://github.com/NVIDIA/spark-rapids/issues/4568#issuecomment-1060274936,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4568,1060274936,IC_kwDOD7z77c4_MoL4,2022-03-07T07:35:38Z,2022-03-07T07:35:38Z,COLLABORATOR,#4588 ,,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1060274936/reactions,0,0,0,0,0,0,0,0,0,4568
226,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1086156006,https://github.com/NVIDIA/spark-rapids/issues/4568#issuecomment-1086156006,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4568,1086156006,IC_kwDOD7z77c5AvWzm,2022-04-01T17:19:05Z,2022-04-01T17:19:41Z,COLLABORATOR,Part of this issue: https://github.com/NVIDIA/spark-rapids/issues/5058,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1086156006/reactions,0,0,0,0,0,0,0,0,0,4568
227,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332749829,https://github.com/NVIDIA/spark-rapids/issues/4608#issuecomment-1332749829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4608,1332749829,IC_kwDOD7z77c5PcCYF,2022-11-30T21:22:05Z,2022-11-30T21:22:05Z,COLLABORATOR,Cudf dependency was merged. This can be re-prioritized if needed.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332749829/reactions,0,0,0,0,0,0,0,0,0,4608
228,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995347833,https://github.com/NVIDIA/spark-rapids/issues/4608#issuecomment-1995347833,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4608,1995347833,IC_kwDOD7z77c527pd5,2024-03-13T18:43:36Z,2024-03-13T18:43:36Z,COLLABORATOR,"There are a number of follow on issues that are needed to really make this work

- [ ] https://github.com/rapidsai/cudf/issues/15278
- [ ] https://github.com/rapidsai/cudf/issues/15260
- [ ] https://github.com/rapidsai/cudf/issues/14239
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995347833/reactions,0,0,0,0,0,0,0,0,0,4608
229,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178353730,https://github.com/NVIDIA/spark-rapids/issues/4609#issuecomment-1178353730,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4609,1178353730,IC_kwDOD7z77c5GPEBC,2022-07-07T23:10:49Z,2022-07-07T23:10:49Z,COLLABORATOR,Removing from 22.08 until the cudf dependencies are satisfied.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178353730/reactions,0,0,0,0,0,0,0,0,0,4609
230,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1034659320,https://github.com/NVIDIA/spark-rapids/issues/4610#issuecomment-1034659320,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4610,1034659320,IC_kwDOD7z77c49q6X4,2022-02-10T09:00:02Z,2022-02-10T09:00:02Z,COLLABORATOR,"Filed a CUDF issue for the feature, https://github.com/rapidsai/cudf/issues/10265",,wbo4958,1320706,MDQ6VXNlcjEzMjA3MDY=,https://avatars.githubusercontent.com/u/1320706?v=4,,https://api.github.com/users/wbo4958,https://github.com/wbo4958,https://api.github.com/users/wbo4958/followers,https://api.github.com/users/wbo4958/following{/other_user},https://api.github.com/users/wbo4958/gists{/gist_id},https://api.github.com/users/wbo4958/starred{/owner}{/repo},https://api.github.com/users/wbo4958/subscriptions,https://api.github.com/users/wbo4958/orgs,https://api.github.com/users/wbo4958/repos,https://api.github.com/users/wbo4958/events{/privacy},https://api.github.com/users/wbo4958/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1034659320/reactions,0,0,0,0,0,0,0,0,0,4610
231,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1957909991,https://github.com/NVIDIA/spark-rapids/issues/4612#issuecomment-1957909991,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4612,1957909991,IC_kwDOD7z77c50s1Xn,2024-02-21T20:58:25Z,2024-02-21T20:58:25Z,COLLABORATOR,allowUnquotedControlChars is what CUDF supports by default already. We need a way to disable it because that is what Spark has by default.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1957909991/reactions,0,0,0,0,0,0,0,0,0,4612
232,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995380699,https://github.com/NVIDIA/spark-rapids/issues/4612#issuecomment-1995380699,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4612,1995380699,IC_kwDOD7z77c527xfb,2024-03-13T18:53:07Z,2024-03-13T18:53:07Z,COLLABORATOR,"This depends on https://github.com/rapidsai/cudf/issues/15222. It may be broken up into smaller pieces in the future, and there are a lot of things that could be validated.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995380699/reactions,0,0,0,0,0,0,0,0,0,4612
233,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1022733649,https://github.com/NVIDIA/spark-rapids/issues/4616#issuecomment-1022733649,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4616,1022733649,IC_kwDOD7z77c489a1R,2022-01-27T00:36:59Z,2022-02-03T20:40:47Z,COLLABORATOR,"Spark supports these escape characters :  \\"", \\, \\/, \b, \f, \n, \r, \t, \uXXXX. 
CUDF supports these for now: \\"", \\, \t, \r and \b. 
Other than these, when `allowBackslashEscapingAnyCharacter` option is set to true in Spark, any character can be escaped. For example: \\$10 results to $10. 

Currently CUDF doesn't throw error for the escape characters which are not supported. Output would be same as input.
Example: \nabc results to \nabc. 
  ",,nartal1,50492963,MDQ6VXNlcjUwNDkyOTYz,https://avatars.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1022733649/reactions,0,0,0,0,0,0,0,0,0,4616
234,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1029472609,https://github.com/NVIDIA/spark-rapids/issues/4616#issuecomment-1029472609,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4616,1029472609,IC_kwDOD7z77c49XIFh,2022-02-03T22:47:49Z,2022-02-04T00:14:46Z,COLLABORATOR,"I think falling back to CPU when this option is set is the right way for now. 
`allowBackslashEscapingAnyCharacter` is `boolean` so we cannot partially support only those escape characters which are supported by CUDF.  
So we can enable this only when other escape characters are supported(\n , \uXXXX etc) along with supporting escaping backslash for any other characters.",,nartal1,50492963,MDQ6VXNlcjUwNDkyOTYz,https://avatars.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1029472609/reactions,0,0,0,0,0,0,0,0,0,4616
235,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997801429,https://github.com/NVIDIA/spark-rapids/issues/4616#issuecomment-1997801429,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4616,1997801429,IC_kwDOD7z77c53FAfV,2024-03-14T16:03:16Z,2024-03-14T16:03:16Z,COLLABORATOR,Note that this is related to https://github.com/NVIDIA/spark-rapids/issues/10596,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997801429/reactions,0,0,0,0,0,0,0,0,0,4616
236,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997802471,https://github.com/NVIDIA/spark-rapids/issues/4616#issuecomment-1997802471,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4616,1997802471,IC_kwDOD7z77c53FAvn,2024-03-14T16:03:50Z,2024-03-14T16:03:50Z,COLLABORATOR,Also I just tested and \uXXXX appears to work properly out of the box with CUDF.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997802471/reactions,0,0,0,0,0,0,0,0,0,4616
237,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033307844,https://github.com/NVIDIA/spark-rapids/issues/4689#issuecomment-1033307844,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4689,1033307844,IC_kwDOD7z77c49lwbE,2022-02-09T03:29:13Z,2022-02-09T03:32:36Z,COLLABORATOR,I'd like to have a try. ,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033307844/reactions,0,0,0,0,0,0,0,0,0,4689
238,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033519104,https://github.com/NVIDIA/spark-rapids/issues/4689#issuecomment-1033519104,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4689,1033519104,IC_kwDOD7z77c49mkAA,2022-02-09T09:03:40Z,2022-02-09T09:03:40Z,COLLABORATOR,"Should we add 2 functions in `SparkShims` like
```scala
def createArrowColumnVector: ColumnVector
def getArrowColumnVector: ColumnVector
```
which return `AccessibleArrowColumnVector` for spark version < 3.3 and `ArrowColumnVector` for spark version >= 3.3?",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033519104/reactions,0,0,0,0,0,0,0,0,0,4689
239,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033999318,https://github.com/NVIDIA/spark-rapids/issues/4689#issuecomment-1033999318,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4689,1033999318,IC_kwDOD7z77c49oZPW,2022-02-09T17:13:53Z,2022-02-09T17:13:53Z,MEMBER,"Since this is not expected to result in a performance gain, I'm not sure this is worth addressing until Spark 3.3 is our minimum supported version.  At that point we can simply remove `AccessibleArrowColumnVector` and use the new capabilities of `ArrowColumnVector` in Spark 3.3+.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1033999318/reactions,1,0,0,1,0,0,0,0,0,4689
240,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189207362,https://github.com/NVIDIA/spark-rapids/issues/4711#issuecomment-1189207362,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4711,1189207362,IC_kwDOD7z77c5G4d1C,2022-07-19T15:36:14Z,2022-07-19T15:36:14Z,COLLABORATOR,"Note, we have added support for cgroups only for Dataproc via: https://github.com/NVIDIA/spark-rapids/issues/5261",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189207362/reactions,0,0,0,0,0,0,0,0,0,4711
241,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1034126056,https://github.com/NVIDIA/spark-rapids/issues/4729#issuecomment-1034126056,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4729,1034126056,IC_kwDOD7z77c49o4Lo,2022-02-09T19:40:10Z,2022-02-09T19:40:10Z,COLLABORATOR,How about binop comparison? Comparing all elements to the `int64_t::max` and do a reduction to see if all the numbers are fit within 64 bit rep.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1034126056/reactions,0,0,0,0,0,0,0,0,0,4729
242,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1045022258,https://github.com/NVIDIA/spark-rapids/issues/4820#issuecomment-1045022258,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4820,1045022258,IC_kwDOD7z77c4-ScYy,2022-02-18T19:00:31Z,2022-02-18T19:00:31Z,COLLABORATOR,"We can start to put in some support for UDTs, but a UDT is a java class that provides ways to translate to/from other standard SQL types. We can read in the standard SQL types but then it is going to take some work to understand exactly when/where the translations to/from the java type happen in Spark and making sure we can plumb all of that through.  That is the main reason we have not added any support for UDTs yet.

How is this used? typically someone will add in support for a UDT because they want to interact with this as a java class instead of as a SQL struct. Which means we are not likely going to be able to do much with this once it is read in except send it to the CPU for more processing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1045022258/reactions,0,0,0,0,0,0,0,0,0,4820
243,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046439702,https://github.com/NVIDIA/spark-rapids/issues/4820#issuecomment-1046439702,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4820,1046439702,IC_kwDOD7z77c4-X2cW,2022-02-21T03:33:22Z,2022-02-21T03:33:22Z,COLLABORATOR,"It should be a use case from ML side. Currently our 2 supported ML case are XGBoost and PCA, both of them are using VectorUDT in their CPU version. The entry point of this could be [VectorAssember(merge multiple columns into one)](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/ml/feature/VectorAssembler.html) or customized `UDF`(cast ArrayType to VectorUDT) like:
```
import org.apache.spark.ml.linalg.Vectors
val convertToVector = udf((array: Seq[Double]) => {
  Vectors.dense(array.map(_.toDouble).toArray)
})
```

For the first case, XGBoost added support for [multiple columns as input](https://github.com/dmlc/xgboost/blob/24e25802a7f2075cd111c5afab57dc2e32683544/jvm-packages/xgboost4j-spark/src/main/scala/ml/dmlc/xgboost4j/scala/spark/XGBoostClassifier.scala#L148-L152). For the second case, PCA also support ArrayType column directly. 
 ",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046439702/reactions,0,0,0,0,0,0,0,0,0,4820
244,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048001782,https://github.com/NVIDIA/spark-rapids/issues/4820#issuecomment-1048001782,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4820,1048001782,IC_kwDOD7z77c4-dzz2,2022-02-22T16:50:50Z,2022-02-22T16:50:50Z,COLLABORATOR,"Do you have an example full workflow/query that you want to have optimized? Adding in support for UDTs is possibly a lot of work and it would be nice to know what areas we should concentrate on first.  Looking at VectorUDT there are two implementations. One for sparse and another for dense vectors. Each line could be one or the other depending on the data in it. So reading out that data from parquet should not be too difficult, but how is it going to be used?  Are we going to have to support user defined functions that take user defined types? I am just concerned that this is the first layer of an onion and we can add in what you are asking, but I don't think it is going to help in terms of performance until we do a lot of follow on work too.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048001782/reactions,0,0,0,0,0,0,0,0,0,4820
245,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048390555,https://github.com/NVIDIA/spark-rapids/issues/4820#issuecomment-1048390555,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4820,1048390555,IC_kwDOD7z77c4-fSub,2022-02-23T02:13:39Z,2022-02-23T02:13:39Z,COLLABORATOR,"@revans2 I could share more details offline. 
Here are some of the needed operators on vector type based on logs:
1. input for CollectLimitExec
2. output for ProjectExec
3. AttributeReference produces vector
4. input&output for ScalaUDF UDF
5. reading/writing vector to/from parquet",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048390555/reactions,0,0,0,0,0,0,0,0,0,4820
246,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048798225,https://github.com/NVIDIA/spark-rapids/issues/4820#issuecomment-1048798225,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4820,1048798225,IC_kwDOD7z77c4-g2QR,2022-02-23T13:46:12Z,2022-02-23T13:46:12Z,COLLABORATOR,"All of those except CollectLimitExec look to be doable.  CollectLimitExec we do not currently support because of a performance optimization in Spark that we just have not felt the need to support. If you have a real use case that is not just show, then we should talk about an issue to add in support for that generally too.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1048798225/reactions,0,0,0,0,0,0,0,0,0,4820
247,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1044096280,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1044096280,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1044096280,IC_kwDOD7z77c4-O6UY,2022-02-18T07:59:00Z,2022-02-24T07:33:06Z,COLLABORATOR,"# Case sensitivity
## 1
```json
{""a"": 1, ""b"": 2}
{""A"": 3, ""B"": 4}
```
is parsed as 
```scala
+----+----+
|   a|   b|
+----+----+
|   1|   2|
|null|null|
+----+----+
```
in Spark, and
```python
      a     b     A     B
0   1.0   2.0  <NA>  <NA>
1  <NA>  <NA>   3.0   4.0
```
in CUDF
## 2
```json
{""a"": 1, ""B"": 2}
{""A"": 3, ""b"": 4}
```
is parsed as
```scala
org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema: `a`, `b`
```
in Spark, and
```python
      a     B     A     b
0   1.0   2.0  <NA>  <NA>
1  <NA>  <NA>   3.0   4.0
```
in CUDF

# Number
## 1
```json
{""a"": 1.0}
{""a"": 0.1}
{""a"": .1} 
```
is parsed as 
```scala
+---------------+----+
|_corrupt_record|   a|
+---------------+----+
|           null| 1.0|
|           null| 0.1|
|      {""a"": .1}|null|
+---------------+----+
```
in Spark (This will fall back to the CPU for parsing as it is a _corrupt_record), and
```python
     a
0  1.0
1  0.1
2  0.1
```
in CUDF


# Empty file

## 1
```json
{}
```
is parsed as 
```scala
++
||
++
||
++
```
in Spark, and
```python
RuntimeError: cuDF failure at: /workspace/.conda-bld/work/cpp/src/io/json/reader_impl.cu:609: Error determining column names.
```
in CUDF

# Comments
(although JSON does not support comments)

## 1
```json
// comment at first line
{""a"": 1}
```
is parsed as 
```scala
+--------------------+----+
|     _corrupt_record|   a|
+--------------------+----+
|// comment at fir...|null|
|                null|   1|
+--------------------+----+
```
in Spark (This will fall back to the CPU for parsing as it is a _corrupt_record), and
```python
RuntimeError: cuDF failure at: /workspace/.conda-bld/work/cpp/src/io/json/reader_impl.cu:371: Input data is not a valid JSON file.
```
in CUDF

## 2
```json
{""a"": 1}
// comment at last line
```
is parsed as 
```scala
+--------------------+----+
|     _corrupt_record|   a|
+--------------------+----+
|                null|   1|
|// comment at las...|null|
+--------------------+----+
```
in Spark (This will fall back to the CPU for parsing as it is a _corrupt_record), and
```python
      a
0   1.0
1  <NA>
```
in CUDF

## 3
```json
// {""a"": 0}
// {""a"": 1}
```
is parsed as 
```scala
org.apache.spark.sql.AnalysisException:
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default).
```
in Spark, and
```python
   a
0  0
1  1
```
in CUDF
",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1044096280/reactions,0,0,0,0,0,0,0,0,0,4821
248,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046414673,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1046414673,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1046414673,IC_kwDOD7z77c4-XwVR,2022-02-21T02:36:36Z,2022-02-24T08:29:39Z,COLLABORATOR,"# String

## 1: How many colons are there?
```json
{""a"":::::::::::}
```
is parsed as
```scala
org.apache.spark.sql.AnalysisException:
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default).
```
in Spark, and
```python
            a
0  ::::::::::
```
in CUDF

## 2: something beyond `0x10ffff`
(the code point of `𝞧` is 0x1d747 )
```json
{""a"": ""𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧""}
{""a"": ""𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧""}
{""a"": ""vvvvvvvvvvvvvvvvvvvv""}
{""a"": ""vvvvvvvvvvvvvvvvvvvvv""}
```
is parsed as
```scala
+--------------------+
|                   a|
+--------------------+
|𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧?...|
|𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧|
|vvvvvvvvvvvvvvvvvvvv|
|vvvvvvvvvvvvvvvvv...|
+--------------------+
```
in Spark (the weird thing is the `?`), and
```python
                       a
0      𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧
1         𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧𝞧
2   vvvvvvvvvvvvvvvvvvvv
3  vvvvvvvvvvvvvvvvvvvvv
```
in CUDF",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046414673/reactions,0,0,0,0,0,0,0,0,0,4821
249,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046415742,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1046415742,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1046415742,IC_kwDOD7z77c4-Xwl-,2022-02-21T02:39:49Z,2022-02-24T09:17:35Z,COLLABORATOR,"# How many lines are there?
test this rule:
```
element
    ws value ws
```
## 1
```json
{""a"": ""This is the first line""}






```
is parsed as 
```scala
+--------------------+
|                   a|
+--------------------+
|This is the first...|
+--------------------+
```
in Spark, and
```python
                        a
0  This is the first line
1                    <NA>
2                    <NA>
3                    <NA>
4                    <NA>
5                    <NA>
```
in CUDF

## 2
```json

{""a"": ""Is this the first line?""}
```
is parsed as 
```scala
+--------------------+
|                   a|
+--------------------+
|Is this the first...|
+--------------------+
```
in Spark, and
```python
RuntimeError: cuDF failure at: /workspace/.conda-bld/work/cpp/src/io/json/reader_impl.cu:371: Input data is not a valid JSON file.
```
in CUDF.",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046415742/reactions,0,0,0,0,0,0,0,0,0,4821
250,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046422670,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1046422670,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1046422670,IC_kwDOD7z77c4-XySO,2022-02-21T02:58:37Z,2022-02-21T02:58:37Z,COLLABORATOR,"# Array and Struct

## 1
```json
{""a"": [1,2]}
```
is parsed as 
```scala
+------+
|     a|
+------+
|[1, 2]|
+------+
```
in Spark, and 
```python
    a
0  [1
```
in CUDF.
",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046422670/reactions,0,0,0,0,0,0,0,0,0,4821
251,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046537940,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1046537940,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1046537940,IC_kwDOD7z77c4-YObU,2022-02-21T07:08:57Z,2022-02-21T07:08:57Z,COLLABORATOR,@revans2 Could you please help to review? Are these the `corner cases` in your mind?,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1046537940/reactions,0,0,0,0,0,0,0,0,0,4821
252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1047856982,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1047856982,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1047856982,IC_kwDOD7z77c4-dQdW,2022-02-22T14:32:44Z,2022-02-22T14:32:44Z,COLLABORATOR,@HaoYang670 If I know all of the corner cases I would have documented them on the original issue.  This is looking really good. But one thing you need to be careful of is that anything with `_corrupt_record` as a column in it will fall back to the CPU for parsing. So your analysis that looks at CUDF parsing this data is wrong because didn't do that. You can work around this by providing the schema to the json read command. Please make sure to check each query that it really was parsed on the GPU.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1047856982/reactions,1,1,0,0,0,0,0,0,0,4821
253,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132488657,https://github.com/NVIDIA/spark-rapids/issues/4821#issuecomment-1132488657,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4821,1132488657,IC_kwDOD7z77c5DgGfR,2022-05-20T05:28:16Z,2022-05-20T05:28:16Z,COLLABORATOR,"We'll need to create a CI job to run fuzzing test frequently to see if we can find more corner cases.
Move it to 22.08 since it's not in 22.06's target.",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132488657/reactions,0,0,0,0,0,0,0,0,0,4821
254,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1140971792,https://github.com/NVIDIA/spark-rapids/issues/4831#issuecomment-1140971792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4831,1140971792,IC_kwDOD7z77c5EAdkQ,2022-05-30T10:15:31Z,2022-05-30T10:15:31Z,COLLABORATOR,"Hi @firestarman @GaryShen2008, shall we deem #5691 as a sub-issue of this epic?",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1140971792/reactions,0,0,0,0,0,0,0,0,0,4831
255,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141581795,https://github.com/NVIDIA/spark-rapids/issues/4831#issuecomment-1141581795,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4831,1141581795,IC_kwDOD7z77c5ECyfj,2022-05-31T01:34:11Z,2022-05-31T01:34:11Z,COLLABORATOR,Yes please.,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141581795/reactions,0,0,0,0,0,0,0,0,0,4831
256,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1054356117,https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1054356117,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877,1054356117,IC_kwDOD7z77c4-2DKV,2022-02-28T15:12:57Z,2022-02-28T15:12:57Z,COLLABORATOR,"I don't know how much background you have in this so please excuse me if I explain some things that you already know. 

TLDR; It looks like your GPU is fully utilized and that is the bottleneck. Would need to look at GPU stats to know that for sure.

You have set your GPU concurrent task parallelism to 4.  This is the number of tasks that can share a GPU at any one point in time.  We limit the number of tasks that can be on the GPU so we don't use up all of the GPU's memory, but we have also seen some issues with compute on some GPUs with lots of memory and I should update the docs to reflect this. We keep it separate from the parallelism of the CPU, because it allows a user to configure the query to run with more CPU cores. This is especially helpful if there are portions of the query that require a lot of CPU. Adding more CPU cores beyond that parallelism number only impacts the parallelism of what can be run on the CPU. Looking at your screen shots it is clear that all of the query is running on the GPU. This means that once you get above 4 tasks the extra tasks may be able to do more I/O in parallel, but will not improve the computation speed.  In many cases we have seen this be a big win for performance, especially if the I/O is relatively slow. The reason why the average task time goes up for 5 tasks over 4 tasks is because Spark is measuring that from the perspective of the CPU, so if one of the tasks is blocked on a semaphore waiting to get on the GPU, then it looks like that task took longer to complete.

In your case there could be a number of different things happening and I probably would need more metrics to be 100% sure what it is. One of the first things that comes to mind is HDFS limitations. I don't think this is the case here, but you might want to check your network utilization and also the disk I/O on your HDFS nodes. You could also look at the buffer time metrics for reading the data. This should really only impact the first stage, and it looks like it is scaling much better than the second stage. So, like I said, it is not likely the problem.

The second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks. The GPU is great and is able to do sub-linear scaling so long as it has enough compute/memory bandwidth to be able to keep up. After that point it has to try to schedule what to run next. In our testing we have seen it struggle when there are a lot more things to do than resources to fulfill those requests. 

When running with just one task we tend to see a number of gaps in GPU utilization. This is because of I/O or just the time it takes for JNI to call down to the GPU to issue the next set of commands to run per task.  With 2 tasks we often see these gaps really fill in. One task will be on the GPU while the other is getting something else queued up to run. This is why the biggest gains often come with setting a parallelism of 2 (a 37% improvement in your case). After that there are diminishing returns, and it really depends on how good your hardware is, and the algorithms that you are going to run on it.

For your setup I would recommend trying to set your gpuParallelism to 3, and the number of tasks that you have to 6.  Ideally that would allow half of the tasks to be reading data while the other half are doing computation and you can overlap as much of that as possible.  In your setup that is not as likely to make as much of a difference because most of your I/O is local, and because of the amount of RAM you have on your device it is likely all coming from the page cache.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1054356117/reactions,0,0,0,0,0,0,0,0,0,4877
257,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1055071783,https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1055071783,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877,1055071783,IC_kwDOD7z77c4-4x4n,2022-03-01T06:28:04Z,2022-03-01T06:36:31Z,NONE,"> I don't know how much background you have in this so please excuse me if I explain some things that you already know.
> 
> TLDR; It looks like your GPU is fully utilized and that is the bottleneck. Would need to look at GPU stats to know that for sure.
> 
> You have set your GPU concurrent task parallelism to 4. This is the number of tasks that can share a GPU at any one point in time. We limit the number of tasks that can be on the GPU so we don't use up all of the GPU's memory, but we have also seen some issues with compute on some GPUs with lots of memory and I should update the docs to reflect this. We keep it separate from the parallelism of the CPU, because it allows a user to configure the query to run with more CPU cores. This is especially helpful if there are portions of the query that require a lot of CPU. Adding more CPU cores beyond that parallelism number only impacts the parallelism of what can be run on the CPU. Looking at your screen shots it is clear that all of the query is running on the GPU. This means that once you get above 4 tasks the extra tasks may be able to do more I/O in parallel, but will not improve the computation speed. In many cases we have seen this be a big win for performance, especially if the I/O is relatively slow. The reason why the average task time goes up for 5 tasks over 4 tasks is because Spark is measuring that from the perspective of the CPU, so if one of the tasks is blocked on a semaphore waiting to get on the GPU, then it looks like that task took longer to complete.
> 
> In your case there could be a number of different things happening and I probably would need more metrics to be 100% sure what it is. One of the first things that comes to mind is HDFS limitations. I don't think this is the case here, but you might want to check your network utilization and also the disk I/O on your HDFS nodes. You could also look at the buffer time metrics for reading the data. This should really only impact the first stage, and it looks like it is scaling much better than the second stage. So, like I said, it is not likely the problem.
> 
> The second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks. The GPU is great and is able to do sub-linear scaling so long as it has enough compute/memory bandwidth to be able to keep up. After that point it has to try to schedule what to run next. In our testing we have seen it struggle when there are a lot more things to do than resources to fulfill those requests.
> 
> When running with just one task we tend to see a number of gaps in GPU utilization. This is because of I/O or just the time it takes for JNI to call down to the GPU to issue the next set of commands to run per task. With 2 tasks we often see these gaps really fill in. One task will be on the GPU while the other is getting something else queued up to run. This is why the biggest gains often come with setting a parallelism of 2 (a 37% improvement in your case). After that there are diminishing returns, and it really depends on how good your hardware is, and the algorithms that you are going to run on it.
> 
> For your setup I would recommend trying to set your gpuParallelism to 3, and the number of tasks that you have to 6. Ideally that would allow half of the tasks to be reading data while the other half are doing computation and you can overlap as much of that as possible. In your setup that is not as likely to make as much of a difference because most of your I/O is local, and because of the amount of RAM you have on your device it is likely all coming from the page cache.

@revans2 
Thank you for your reply，
You mentioned above that you may need to get more metrics to to be 100% sure what it is, can you give some examples? I would like to show you. 

""The second stage appears to be where a lot of the issue is. The average task time appears to grow with the number of tasks put on it, and really jumps up with 4 tasks"" you mentioned above. Sorry, here I don't fully understand the explanation. Is the diminishing returns because of the GPU itself or the algorithm? During the execution process, does the data need to be frequently transferred from the host memory to the GPU memory, and will this also have a great impact on the execution performance? 

I have tried to set  gpuParallelism to 3, and the number of tasks to 6, The execution efficiency will drop further, which doesn't seem to be a good setup for my case. ",,YeahNew,33194373,MDQ6VXNlcjMzMTk0Mzcz,https://avatars.githubusercontent.com/u/33194373?v=4,,https://api.github.com/users/YeahNew,https://github.com/YeahNew,https://api.github.com/users/YeahNew/followers,https://api.github.com/users/YeahNew/following{/other_user},https://api.github.com/users/YeahNew/gists{/gist_id},https://api.github.com/users/YeahNew/starred{/owner}{/repo},https://api.github.com/users/YeahNew/subscriptions,https://api.github.com/users/YeahNew/orgs,https://api.github.com/users/YeahNew/repos,https://api.github.com/users/YeahNew/events{/privacy},https://api.github.com/users/YeahNew/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1055071783/reactions,0,0,0,0,0,0,0,0,0,4877
258,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056831964,https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1056831964,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877,1056831964,IC_kwDOD7z77c4-_fnc,2022-03-02T11:37:51Z,2022-03-02T11:37:51Z,COLLABORATOR,"My assumptions were apparently wrong. Could I get a little more information about your setup so I can try to reproduce it locally, or at least as close as I can come?  What is the file format that the data is stored in? Is it the original ""|"" separated values, ORC, parquet, or something else? What version of the RAPIDS Accelerator are you using? 

There are a number of SQL metrics that we have that can really help to debug some kinds of problems like this. If you could set the config `spark.rapids.sql.metrics.level` to DEBUG, it should enable all of the metrics. Then if you rerun the query and look at the sql tab it should show a lot of information about how long each part of the query is taking. This, ideally could help you understand how the scaling is happening for the join separate form reading parquet etc.  All of that information would be stored in the history file for each application. If you feel comfortable sending that to me I could take a look.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056831964/reactions,0,0,0,0,0,0,0,0,0,4877
259,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056859296,https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1056859296,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877,1056859296,IC_kwDOD7z77c4-_mSg,2022-03-02T12:10:52Z,2022-03-02T12:10:52Z,COLLABORATOR,"Also a few more questions. From the size it looks like you are reading from the original data format, ""|"" separated. I assume you are reading the data as floats and not decimal values?

I should also add that I would be interested in seeing the number of tasks run, simply because you are not setting spark.sql.files.maxPartitionBytes, which will default to 128MB, which is rather small for the GPU. But I first want to match your execution before I start making recommendations to change, because my first assumption was way off.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1056859296/reactions,0,0,0,0,0,0,0,0,0,4877
260,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1057060233,https://github.com/NVIDIA/spark-rapids/issues/4877#issuecomment-1057060233,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4877,1057060233,IC_kwDOD7z77c4_AXWJ,2022-03-02T15:29:56Z,2022-03-02T15:29:56Z,COLLABORATOR,"I was not able to reproduce the issue you saw locally. I have a lot more evidence now that you are using CSV because if I try to do the same query with parquet, and the same settings you have except with 12 tasks on a single a6000 GPU I can complete the query in under 20 seconds.  If I increase `spark.sql.files.maxPartitionByte` to `1g` I am able to complete the query in 15 seconds. This appears to indicate that the issue is with I/O or CSV parsing. But that is just speculation because I was not able to reproduce the problem.

Looking at GPU utilization, for me, the GPU is not fully utilized. I see my NVMe only doing around 2.2 GiB/sec in the best case when reading the CSV data. I manually verified that I can `cat` the files to `/dev/null` in parallel at 3 GiB/sec. I also saw 18,000 read operations per second, which feels rather high. I filed #6 a while ago to try and  improve it for CSV. But like I said before this is a performance problem with my setup, not with yours.

I don't see the time taken jump back up when using more cores per executor.
![time vs number of cores](https://user-images.githubusercontent.com/3441321/156392840-11f5551b-d27d-418b-b3b4-f71df1d3eb64.png)

It even goes down a little when going from 8 to 12 instead of up.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1057060233/reactions,0,0,0,0,0,0,0,0,0,4877
261,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1061807255,https://github.com/NVIDIA/spark-rapids/issues/4910#issuecomment-1061807255,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4910,1061807255,IC_kwDOD7z77c4_SeSX,2022-03-08T13:58:39Z,2022-03-08T13:58:39Z,COLLABORATOR,"It is unlikely that we are going to be able to support this fully without some changes to CUDF.

CUDF's round only supports HALF_UP and HALF_EVEN.

https://github.com/rapidsai/cudf/blob/57ff6f55b9fd44e8a8e10282d3f95d5f38e299ef/cpp/include/cudf/round.hpp#L36

Floor and Ceil are unary operators and do not support setting a scale.

So in the sort term we are probably going to have to fall back to the CPU if we see a floor or ceil with a scale that is not 0, and then later on put in support to CUDF for what we need.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1061807255/reactions,0,0,0,0,0,0,0,0,0,4910
262,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1069553291,https://github.com/NVIDIA/spark-rapids/issues/4937#issuecomment-1069553291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4937,1069553291,IC_kwDOD7z77c4_wBaL,2022-03-16T19:51:10Z,2022-03-16T19:51:10Z,COLLABORATOR,This should also cover https://github.com/NVIDIA/spark-rapids/issues/4788,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1069553291/reactions,0,0,0,0,0,0,0,0,0,4937
263,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132637238,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1132637238,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1132637238,IC_kwDOD7z77c5Dgqw2,2022-05-20T08:38:11Z,2022-05-20T08:38:11Z,COLLABORATOR,"@andygrove  @jlowe   @revans2 Help to check:

- Spark does not support reading CSV numeric UNIX timestamps as timestamps.
- Spark does support reading JSON numeric UNIX timestamps as timestamps. 
  But we have difficulty supporting this on GPU. 
  Spark behavoir:
       `string` ""1653014790""   =>  null 
       `int` 1653014790  =>  `2022-05-20 10:46:30`
  If one column has both above values, we can't know the original types if read this column as String by cuDF,  because we will get two string rows with the same value.

Details:

**Spark does not support reading CSV numeric UNIX timestamps as timestamps**
```
$SPARK_HOME/bin/pyspark 
from pyspark.sql.types import *
schema = StructType([StructField(""c1"", TimestampType())])

spark.read.csv(""/tmp/tmp.csv"").show()
+----------+
|       _c0|
+----------+
|1653012031|
|       197|
+----------+

spark.read.schema(schema).csv(""/tmp/tmp.csv"").show()
+----+
|  c1|
+----+
|null|
|null|
+----+
```

**Spark read JSON string int and int**

Spark code:
Spark uses `VALUE_STRING` and `VALUE_NUMBER_INT` tokens to distinguish `string` and `int`

https://github.com/apache/spark/blob/v3.3.0-rc2/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala#L251toL266

```
    case TimestampType =>
      (parser: JsonParser) => parseJsonToken[java.lang.Long](parser, dataType) {
        case VALUE_STRING if parser.getTextLength >= 1 =>
          try {
            timestampFormatter.parse(parser.getText)
          } catch {
            case NonFatal(e) =>
              // If fails to parse, then tries the way used in 2.0 and 1.x for backwards
              // compatibility.
              val str = DateTimeUtils.cleanLegacyTimestampStr(UTF8String.fromString(parser.getText))
              DateTimeUtils.stringToTimestamp(str, options.zoneId).getOrElse(throw e)
          }

        case VALUE_NUMBER_INT =>
          parser.getLongValue * 1000000L
      }
```

**Different behavior for `string int` and `int`**

```
cat tmp.json 
{ ""c1"": ""2020-01-01 00:00:00"" }
{ ""c1"": 1653014790 }  // Valid int

$SPARK_HOME/bin/pyspark 
from pyspark.sql.types import *
schema = StructType([StructField(""c1"", TimestampType())])

spark.read.schema(schema).json(""/tmp/tmp.json"").show()
+-------------------+
|                 c1|
+-------------------+
|2020-01-01 00:00:00|
|2022-05-20 10:46:30| // Valid timestamp
+-------------------+


cat tmp2.json 
{ ""c1"": ""2020-01-01 00:00:00"" }
{ ""c1"": ""1653014790"" }   // Note: it's a string, not int

spark.read.schema(schema).json(""/tmp/tmp2.json"").show()
+-------------------+
|                 c1|
+-------------------+
|2020-01-01 00:00:00|
|               null|    // Note
+-------------------+
```


**cuDF reads as `long`**
```
json
{ ""c1"": ""2020-01-13"" }
{ ""c1"": 1653014790 }
{ ""c1"": 0 }

result:
0                         isNull = false   // IMO, this should be false  
1653014790       isNull = false  
0                         isNull = false   // can't distinguish with the first row  
```
0 indicates the epoch `1970-01-01 00:00:00`
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132637238/reactions,0,0,0,0,0,0,0,0,0,4940
264,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132991533,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1132991533,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1132991533,IC_kwDOD7z77c5DiBQt,2022-05-20T14:44:55Z,2022-05-20T14:44:55Z,CONTRIBUTOR,@res-life I will take a look at this later today,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132991533/reactions,0,0,0,0,0,0,0,0,0,4940
265,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132995266,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1132995266,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1132995266,IC_kwDOD7z77c5DiCLC,2022-05-20T14:47:17Z,2022-05-20T14:47:41Z,COLLABORATOR,"@res-life 
Yes this is a known issue with JSON parsing. Spark distinguishes between quoted fields and non-quoted fields. For now we are just documenting this

https://github.com/NVIDIA/spark-rapids/blob/branch-22.06/docs/compatibility.md#json-floating-point

> Another limitation of the GPU JSON reader is that it will parse strings containing non-string boolean or numeric values where Spark will treat them as invalid inputs and will just return null.

Although it should be moved to its own section to make it more visible.

We hope that once CUDF finishes work on the new JSON parser that we can get some help from them to indicate if a field was quoted or not.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132995266/reactions,1,1,0,0,0,0,0,0,0,4940
266,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178352814,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1178352814,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1178352814,IC_kwDOD7z77c5GPDyu,2022-07-07T23:09:22Z,2022-07-07T23:09:22Z,COLLABORATOR,Removing from 22.08.  Will put it into a release once the cudf dependencies are resolved.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178352814/reactions,0,0,0,0,0,0,0,0,0,4940
267,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281752446,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1281752446,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1281752446,IC_kwDOD7z77c5MZf1-,2022-10-18T03:04:52Z,2022-10-18T03:04:52Z,COLLABORATOR,"It's not planned in 22.12, let me unassign myself.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281752446/reactions,0,0,0,0,0,0,0,0,0,4940
268,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1296771711,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1296771711,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1296771711,IC_kwDOD7z77c5NSyp_,2022-10-31T08:42:50Z,2022-10-31T08:57:43Z,COLLABORATOR,"I have failed to find a case that says Spark CSV can read numeric values as timestamps, or any related config to enable this behavior.
Maybe we can remove the `CSV ` word from the title,  since this issue appears to be specific to JSON reader.

@andygrove Do you have any concern for removing the 'CSV' word?",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1296771711/reactions,0,0,0,0,0,0,0,0,0,4940
269,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1297903272,https://github.com/NVIDIA/spark-rapids/issues/4940#issuecomment-1297903272,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4940,1297903272,IC_kwDOD7z77c5NXG6o,2022-11-01T01:43:26Z,2022-11-01T08:00:18Z,COLLABORATOR,"I am going to remove the `CSV` word from the title.
If any concern, free to add it back.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1297903272/reactions,0,0,0,0,0,0,0,0,0,4940
270,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098211593,https://github.com/NVIDIA/spark-rapids/issues/4962#issuecomment-1098211593,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4962,1098211593,IC_kwDOD7z77c5BdWEJ,2022-04-13T15:47:12Z,2022-04-13T15:47:12Z,COLLABORATOR,"This is an issue that might be related to it.

https://docs.nvidia.com/grid/latest/grid-licensing-user-guide/index.html#software-enforcement-grid-licensing

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098211593/reactions,0,0,0,0,0,0,0,0,0,4962
271,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1070336174,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1070336174,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1070336174,IC_kwDOD7z77c4_zAiu,2022-03-17T05:18:00Z,2022-03-17T05:18:00Z,COLLABORATOR,"This issue sounds interesting to me. Could I pick it up?

",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1070336174/reactions,0,0,0,0,0,0,0,0,0,4964
272,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1071216706,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1071216706,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1071216706,IC_kwDOD7z77c4_2XhC,2022-03-17T18:56:45Z,2022-03-17T18:56:45Z,COLLABORATOR,If you want to give it a try I am fine with it. I would start with a quick a dirty prototype and then measure performance so we can fail fast if it is not helping. I am a little skeptical that we are going to see too much performance improvement on properly configured queries. It is mainly going to be for queries with skewed data or if someone didn't configure the query to have enough shuffle partitions.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1071216706/reactions,1,1,0,0,0,0,0,0,0,4964
273,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077552857,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077552857,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1077552857,IC_kwDOD7z77c5AOibZ,2022-03-24T12:01:12Z,2022-03-24T12:45:49Z,COLLABORATOR,"Hi @revans2, I built a demo with `OneSizeBuffer` which acts as a blocking queue holding single element.  `OneSizeBuffer` is supposed to minimize the memory cost on caching host buffer while running the CPU workloads asychronizely. Based on the assumption that shuffle read and concat usually spends less time than subquent GPU processing, caching single host buffer on the deck is enough for the most of time.
 
The first host batch is computed in main thread. If there exists subsequent host batches, they are processed in a separate thread.

```scala
class GpuAsyncShuffleCoalesceIterator(child: Iterator[HostConcatResult],
                                      dataTypes: Array[DataType],
                                      metricsMap: Map[String, GpuMetric])
  extends Iterator[ColumnarBatch] with Arm {

  private[this] val semWaitTime = metricsMap(GpuMetric.SEMAPHORE_WAIT_TIME)
  private[this] val opTimeMetric = metricsMap(GpuMetric.OP_TIME)
  private[this] val outputBatchesMetric = metricsMap(GpuMetric.NUM_OUTPUT_BATCHES)
  private[this] val outputRowsMetric = metricsMap(GpuMetric.NUM_OUTPUT_ROWS)

  @transient private lazy val buffer = new OneSizeBuffer()

  private var hostConcatThread: Thread = _

  private class OneSizeBuffer {
    private val lock = new util.concurrent.locks.ReentrantLock()
    private val notFull = lock.newCondition()
    private val notEmpty = lock.newCondition()
    @volatile private var deck: HostConcatResult = _
    @volatile private var childIsOpen = true

    def nonEmpty: Boolean = deck != null || (if (childIsOpen) {
        if (!hostConcatThread.isAlive) {
          val stackTraceMsg = hostConcatThread.getStackTrace.mkString(""\n"")
          throw new IllegalStateException(
            ""The host concat thread crashed because: "" + stackTraceMsg)
        }
        true
      } else {
        false
      })

    def offer(): Unit = {
      println(""===== offer start ====="")
      lock.lock()
      try {
        while (deck != null) notFull.await()
        deck = child.next()
        notEmpty.signal()
        println(""===== offer end ====="")
      } finally {
        lock.unlock()
      }
    }

    def take(): HostConcatResult = {
      println(""===== take start ====="")
      lock.lock()
      try {
        while (deck == null) notEmpty.await()
        val ret = deck
        deck = null
        notFull.signal()
        println(""===== take end ====="")
        ret
      } finally {
        lock.unlock()
      }
    }

    def closeHostIterator(): Unit = {
      childIsOpen = false
    }
  }

  private var isFirstBatch: Boolean = true
  private var childIsEmpty: Boolean = _

  private def convertHostBatchToDevice(hostConcatResult: HostConcatResult) = {
    GpuSemaphore.acquireIfNecessary(TaskContext.get(), semWaitTime)
    withResource(new MetricRange(opTimeMetric)) { _ =>
      withResource(hostConcatResult) { hostConcatBatch =>
        val batch = HostConcatResultUtil.getColumnarBatch(hostConcatBatch, dataTypes)
        outputBatchesMetric += 1
        outputRowsMetric += batch.numRows()
        batch
      }
    }
  }

  override def hasNext: Boolean = {
    if (isFirstBatch) {
      isFirstBatch = false
      if (child.hasNext) {
        buffer.offer()
        hostConcatThread = new Thread(() => {
          while (child.hasNext) buffer.offer()
          buffer.closeHostIterator()
        })
        hostConcatThread.start()
        childIsEmpty = false
      } else {
        childIsEmpty = true
      }
      !childIsEmpty
    } else {
      !childIsEmpty && buffer.nonEmpty
    }
  }

  override def next(): ColumnarBatch = {
    if (!hasNext) {
      throw new NoSuchElementException(""No more columnar batches"")
    }
    convertHostBatchToDevice(buffer.take())
  }
}
```

Sadly,  I compared this approach with original `GpuShuffleCoalesceIterator`. I didn't find significant performance difference bewteen two approaches. 
I created the reduce partition of huge size with `collect_set` on a hot key:
```python
spark = SparkSession.builder.appName(""Async shuffle coalese"").enableHiveSupport().getOrCreate()

# generate a single column table which holds 1200000000 random ints ranging from 0 to 30000000
def gen_data(spark, data_path, n_part=10, rows_per_part=100000):
    rdd = spark.sparkContext.parallelize(list(range(n_part)), numSlices=n_part)
    def rand_gen(seed_iter):
        from random import Random
        from pyspark.sql import Row
        rd = Random(next(seed_iter))
        for _ in range(rows_per_part):
            rdVal = rd.randint(0, 1000 * 1000 * 30)
            yield Row(a=rdVal)

    rows = rdd.mapPartitions(rand_gen)
    schema = StructType([StructField('a', IntegerType(), False)])
    df = spark.createDataFrame(rows, schema)
    df.write.parquet(data_path)

gen_data(spark, path, n_part=400, rows_per_part=3000 * 1000) 

# set a moderate batch size
spark.conf.set(""spark.rapids.sql.batchSizeBytes"", str(1 << 20))

# run collect_set on a constant key to create a large shuffle read partition
df = spark.read.parquet(path) \
                            .groupby(lit(1)) \
                            .agg(collect_set(col(""a"")).alias(""arr"")) \
                            .selectExpr(""arr[0]"")
```

I conducted above tests with various batch size settings both on local machine and yarn. In no condition, there existed a significant performance boost by running shuffle read on a background thread.

Theoretically, the maximium performance boost which we can gain from the current improvement is the cost of shuffle read + host concat. However, I failed to create a scenairo which has a large I/O cost of shuffle read with a relatively small cost of subsequent GPU processing.
",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077552857/reactions,0,0,0,0,0,0,0,0,0,4964
274,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077818251,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077818251,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1077818251,IC_kwDOD7z77c5APjOL,2022-03-24T16:36:01Z,2022-03-24T16:36:01Z,COLLABORATOR,"I think this might be because we have good I/O.  Either there is enough memory that reads always hit the page cache or the disks + network are fast enough that it is not a problem.

@jlowe and @abellina do you think it is worth spending more time trying to find a use case for this, or should we let it go until we are playing around with releasing the GPU Semaphore #4970? Just because I can see it helping there even if the I/O is not slow.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077818251/reactions,0,0,0,0,0,0,0,0,0,4964
275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077855359,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077855359,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1077855359,IC_kwDOD7z77c5APsR_,2022-03-24T17:17:05Z,2022-03-24T17:17:05Z,COLLABORATOR,"We may need to trigger a lot more than 1 batch concurrently to make them fit under the concat time. I do not think that host concat by itself is the bottleneck, but instead it's the high processing needed upstream closer to the actual network fetch, and by the time this runs we've already waited for the fetch and block decompress that seems to be the highest CPU consumer.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077855359/reactions,0,0,0,0,0,0,0,0,0,4964
276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077878893,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077878893,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1077878893,IC_kwDOD7z77c5APyBt,2022-03-24T17:43:04Z,2022-03-24T17:43:04Z,COLLABORATOR,"@abellina am I reading the code wrong? We are fetching and concating in a background thread. That would make it so that if we take longer to process the current batch than we do to fetch and concatenate on the CPU the next batch, then the fetch/concat is effectively free.  But if we take longer to fetch/concat, then the GPU processing time is effectively free.

If we concatenated the data on the main thread, and not the background thread, then you would be correct.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077878893/reactions,0,0,0,0,0,0,0,0,0,4964
277,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077882589,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1077882589,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1077882589,IC_kwDOD7z77c5APy7d,2022-03-24T17:47:00Z,2022-03-24T17:47:00Z,COLLABORATOR,"Oh I think I misunderstood @revans2. Then yeah, agree. The next gives you whatever it could fetch/concat first, then keeps pulling on the shuffle to fetch more.

Then I agree, if the batch you returned on next() takes a while to process, the next time you call next() it should be free. ",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1077882589/reactions,0,0,0,0,0,0,0,0,0,4964
278,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096540050,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1096540050,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1096540050,IC_kwDOD7z77c5BW9-S,2022-04-12T10:30:07Z,2022-04-12T10:30:07Z,COLLABORATOR,"Hi @abellina @revans2, shall we move further on this issue ? If so, what should we do next ? ",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096540050/reactions,0,0,0,0,0,0,0,0,0,4964
279,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096721201,https://github.com/NVIDIA/spark-rapids/issues/4964#issuecomment-1096721201,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4964,1096721201,IC_kwDOD7z77c5BXqMx,2022-04-12T13:21:27Z,2022-04-12T13:21:54Z,COLLABORATOR,"I am taking a look at this issue: https://github.com/NVIDIA/spark-rapids/issues/5039, and I should have something  to post in the next couple of days, especially for writes. I think perhaps the threading strategy proposed there could be applied here as well, but that shouldn't stop us from getting a speed-of-light of this issue or adding NVTX ranges that would help us call out how often it would trigger as defined. 

The main thing that could override this is if we can parallelize the reads at a lower level than whole batch. I have not looked at the reads yet, as I've focused on writes. On the writes, there are two stages, one of which is easily parallelizable: each shuffle block per reduce partition is independently written to disk. The second stage is to take all these blocks and combine them in one file per map_id, so this second stage needs to read all the blocks and append to a single file (doable in parallel, just not as easily as the first stage). The first stage is where shuffle compression happens, so we can leverage more cores to compress. So far, this seems to yield some benefits (I estimate around 5% of execution of NDS at 3TB), to the point that now the second stage of the write to a single map_id file, is becoming the next bottleneck, as well as reads.

So I think this is the main part: could we decompress also in parallel at a lower level? If so, I don't think doing the whole batch in the background would make as much sense. If we can't decompress in parallel then yes, I think this is the only choice. I could be missing something here so would like to hear other's comments as well.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1096721201/reactions,1,1,0,0,0,0,0,0,0,4964
280,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1076289177,https://github.com/NVIDIA/spark-rapids/issues/4969#issuecomment-1076289177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4969,1076289177,IC_kwDOD7z77c5AJt6Z,2022-03-23T11:54:59Z,2022-03-23T11:55:40Z,COLLABORATOR,"I'm intrested in this, if nobody picks it, let me try.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1076289177/reactions,0,0,0,0,0,0,0,0,0,4969
281,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132490692,https://github.com/NVIDIA/spark-rapids/issues/4982#issuecomment-1132490692,https://api.github.com/repos/NVIDIA/spark-rapids/issues/4982,1132490692,IC_kwDOD7z77c5DgG_E,2022-05-20T05:31:40Z,2022-05-20T05:31:40Z,COLLABORATOR,Not planned in 22.06. Move to 22.08.,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132490692/reactions,0,0,0,0,0,0,0,0,0,4982
282,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079454830,https://github.com/NVIDIA/spark-rapids/issues/5043#issuecomment-1079454830,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5043,1079454830,IC_kwDOD7z77c5AVyxu,2022-03-25T21:39:09Z,2022-03-25T21:39:09Z,COLLABORATOR,"It looks like it depends on how you register the UDF and run the UDF.  We actually don't claim to catch all UDFs due to different ways of registering showing up differently in the plan.  It looks like this broke registering and using with SQL, like this:

```
val plusOne = udf((x: Int) => x + 1)
spark.udf.register(""plusOne"", plusOne)
spark.sql(""SELECT plusOne(5)"").collect()
```

In this case it the plan shows the udf name:
`Output [1]: [plusOne(5) AS plusOne(5)#37]
`

I'm not seeing anything in the plan that we can easily use in the qualification tool as an alternative so we might just have to document this for short term. I'll investigate further though too.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079454830/reactions,1,1,0,0,0,0,0,0,0,5043
283,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079455588,https://github.com/NVIDIA/spark-rapids/issues/5043#issuecomment-1079455588,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5043,1079455588,IC_kwDOD7z77c5AVy9k,2022-03-25T21:40:23Z,2022-03-25T21:40:23Z,COLLABORATOR,"Note it does still report UDF when its registered and used like:

```
val cleanCountryUdf = udf(cleanCountry)
val resDf = userDataRead.withColumn(""normalisedCountry"", cleanCountryUdf(col(""country"")))
```",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079455588/reactions,1,1,0,0,0,0,0,0,0,5043
284,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1082359205,https://github.com/NVIDIA/spark-rapids/issues/5043#issuecomment-1082359205,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5043,1082359205,IC_kwDOD7z77c5Ag32l,2022-03-29T20:48:30Z,2022-03-29T20:48:30Z,COLLABORATOR,We will look into modifying Spark 3.2.2+/3.3+ to see if we can prepend the UDF name with a UDF tag.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1082359205/reactions,1,1,0,0,0,0,0,0,0,5043
285,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1505811341,https://github.com/NVIDIA/spark-rapids/issues/5043#issuecomment-1505811341,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5043,1505811341,IC_kwDOD7z77c5ZwNuN,2023-04-12T19:28:39Z,2023-04-12T19:28:39Z,COLLABORATOR,"[SPARK-35440 SQL Add function type to ExpressionInfo for UDF](https://github.com/apache/spark/commit/af1dba7ca501fd9372b158793119163e3fcd1f24) adds the function type, such as ""scala_udf"", ""python_udf"", ""java_udf"", ""hive"", ""built-in"" to the `ExpressionInfo` for UDF.
",,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1505811341/reactions,0,0,0,0,0,0,0,0,0,5043
286,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1522373426,https://github.com/NVIDIA/spark-rapids/issues/5043#issuecomment-1522373426,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5043,1522373426,IC_kwDOD7z77c5avZMy,2023-04-25T20:27:21Z,2023-04-25T20:27:33Z,COLLABORATOR,"Created a jira ticket
https://issues.apache.org/jira/browse/SPARK-43131",,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1522373426/reactions,0,0,0,0,0,0,0,0,0,5043
287,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079275538,https://github.com/NVIDIA/spark-rapids/issues/5058#issuecomment-1079275538,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5058,1079275538,IC_kwDOD7z77c5AVHAS,2022-03-25T18:06:26Z,2022-03-25T18:06:26Z,COLLABORATOR,I'll take the add nvtx range task and report back task for now. It seems the issue as it stands isn't helpful enough to yield a change.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079275538/reactions,0,0,0,0,0,0,0,0,0,5058
288,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079298355,https://github.com/NVIDIA/spark-rapids/issues/5058#issuecomment-1079298355,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5058,1079298355,IC_kwDOD7z77c5AVMkz,2022-03-25T18:35:02Z,2022-03-25T18:35:15Z,COLLABORATOR,"See also #4078
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1079298355/reactions,0,0,0,0,0,0,0,0,0,5058
289,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080823401,https://github.com/NVIDIA/spark-rapids/issues/5058#issuecomment-1080823401,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5058,1080823401,IC_kwDOD7z77c5AbA5p,2022-03-28T15:52:47Z,2022-03-28T16:02:48Z,COLLABORATOR,"**For q78:** Ok in this case it is a join that is pulling on either side from an aggregate. The aggregate is the one doing pulling on a shuffle coalesce iterator to get blocks from the shuffle.

Because of this the SHJ optimization doesn't kick in. By the time we get the blocks it is too late. So this is a different case that can be optimized separately. It seems we need an iterator that knows to release the semaphore in a generic way for different types of execs (aggs, joins, others?)

**For q95:** it is a similar pattern, but it is not the aggregate. The SHJ is pulling the build side from another exec node, and so it is already on the GPU (it has no way to release the semaphore).

![Screenshot from 2022-03-28 10-50-41](https://user-images.githubusercontent.com/1901059/160437473-04584388-eac7-4b0b-b886-52a2a87a3f55.png)
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080823401/reactions,0,0,0,0,0,0,0,0,0,5058
290,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080969968,https://github.com/NVIDIA/spark-rapids/issues/5058#issuecomment-1080969968,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5058,1080969968,IC_kwDOD7z77c5Abkrw,2022-03-28T18:00:20Z,2022-03-28T18:00:20Z,COLLABORATOR,"Like I said in #4970 I think if we try to generalize all of this to be at the task/worker level it is going to make things a lot simpler. I really like what @jlowe said about #4964 in using CPU cores that are stuck waiting on the semaphore. If we had both in place we could do something like start to fetch shuffle data for an entire task, not just the one stream we are on right now. We could also release the semaphore when we know that I/O is required to start processing data. We could even look at starting to process a subset of the data if we have it ready to go.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080969968/reactions,1,1,0,0,0,0,0,0,0,5058
291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080812031,https://github.com/NVIDIA/spark-rapids/issues/5071#issuecomment-1080812031,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5071,1080812031,IC_kwDOD7z77c5Aa-H_,2022-03-28T15:41:25Z,2022-03-28T15:41:25Z,COLLABORATOR,"Okay a bit more information. It looks like the 130ms is JIT. In later calls the same code takes about 5ms to run. So this is a much lower priority, but it might be nice to have a way to start the JIT early in the life of the task so we can avoid this on the first run.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1080812031/reactions,0,0,0,0,0,0,0,0,0,5071
292,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332739147,https://github.com/NVIDIA/spark-rapids/issues/5109#issuecomment-1332739147,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5109,1332739147,IC_kwDOD7z77c5Pb_xL,2022-11-30T21:10:50Z,2023-03-06T04:51:51Z,COLLABORATOR,"A recently merged cudf PR enables GroupBy with array:
 * https://github.com/rapidsai/cudf/pull/11792

However, supporting arbitrary nested array needs to wait for:
 * https://github.com/rapidsai/cudf/issues/11222
 * https://github.com/rapidsai/cudf/issues/11672

While waiting for thes issues, if necessary then we can support non-nested array GroupBy first.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332739147/reactions,0,0,0,0,0,0,0,0,0,5109
293,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059899109,https://github.com/NVIDIA/spark-rapids/issues/5120#issuecomment-2059899109,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5120,2059899109,IC_kwDOD7z77c56x5Dl,2024-04-16T20:48:08Z,2024-04-16T20:48:08Z,COLLABORATOR,"Note: integrate test framework currently explicitly sets ANSI mode to off for each test, so we need to change this to test with ANSI mode on.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059899109/reactions,0,0,0,0,0,0,0,0,0,5120
294,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1093000936,https://github.com/NVIDIA/spark-rapids/issues/5177#issuecomment-1093000936,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5177,1093000936,IC_kwDOD7z77c5BJd7o,2022-04-08T15:24:07Z,2022-04-08T15:24:07Z,COLLABORATOR,I totally agree. I did a quick hack with this type of thing and it worked out okay. The biggest problem is that the plan when we get work on it has not gone through the exchange reuse optimization yet. That means that we might end up counting some paths multiple times in any roll-up metrics.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1093000936/reactions,0,0,0,0,0,0,0,0,0,5177
295,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097202027,https://github.com/NVIDIA/spark-rapids/issues/5187#issuecomment-1097202027,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5187,1097202027,IC_kwDOD7z77c5BZflr,2022-04-12T20:47:37Z,2022-04-12T20:47:37Z,COLLABORATOR,"At the moment, the team is exploring whether Parquet or another serializer format is preferable for caching.  Once that is known we can determine the priority of this issue.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097202027/reactions,0,0,0,0,0,0,0,0,0,5187
296,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098347251,https://github.com/NVIDIA/spark-rapids/issues/5199#issuecomment-1098347251,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5199,1098347251,IC_kwDOD7z77c5Bd3Lz,2022-04-13T18:17:25Z,2022-04-13T18:17:25Z,CONTRIBUTOR,cuDF issue: https://github.com/rapidsai/cudf/issues/10652,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098347251/reactions,1,1,0,0,0,0,0,0,0,5199
297,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178362271,https://github.com/NVIDIA/spark-rapids/issues/5199#issuecomment-1178362271,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5199,1178362271,IC_kwDOD7z77c5GPGGf,2022-07-07T23:21:43Z,2022-07-07T23:21:43Z,COLLABORATOR,Moving to 22.10 as the cudf work will be late in 22.08 and likely finish in 22.10.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178362271/reactions,0,0,0,0,0,0,0,0,0,5199
298,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1099210714,https://github.com/NVIDIA/spark-rapids/issues/5221#issuecomment-1099210714,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5221,1099210714,IC_kwDOD7z77c5BhJ_a,2022-04-14T13:53:06Z,2022-04-14T13:53:06Z,COLLABORATOR,"I think CUDF already supports this through dropListDuplicates

https://github.com/rapidsai/cudf/blob/ac27757092e9ba2bc0656b6a7dfbc79ce8b5e76a/java/src/main/java/ai/rapids/cudf/ColumnView.java#L2375-L2386

We should be able to implement this without any issues, so long at dropListDuplicates supports the types.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1099210714/reactions,0,0,0,0,0,0,0,0,0,5221
299,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2046320192,https://github.com/NVIDIA/spark-rapids/issues/5221#issuecomment-2046320192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5221,2046320192,IC_kwDOD7z77c55-F5A,2024-04-10T01:58:33Z,2024-04-10T02:14:35Z,NONE,"I am interested in taking this. Could anyone point me in the right direction for which file (`collectionOperations`?) this would live in and maybe a comparable Gpu* case class (`GpuArrayRemove`?)?

Edit: Okay I see `ArrayDistinct` in the CPU version of `collectionOperations` so I think I'm on the right path",,phish3y,44003290,MDQ6VXNlcjQ0MDAzMjkw,https://avatars.githubusercontent.com/u/44003290?v=4,,https://api.github.com/users/phish3y,https://github.com/phish3y,https://api.github.com/users/phish3y/followers,https://api.github.com/users/phish3y/following{/other_user},https://api.github.com/users/phish3y/gists{/gist_id},https://api.github.com/users/phish3y/starred{/owner}{/repo},https://api.github.com/users/phish3y/subscriptions,https://api.github.com/users/phish3y/orgs,https://api.github.com/users/phish3y/repos,https://api.github.com/users/phish3y/events{/privacy},https://api.github.com/users/phish3y/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2046320192/reactions,0,0,0,0,0,0,0,0,0,5221
300,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2047784979,https://github.com/NVIDIA/spark-rapids/issues/5221#issuecomment-2047784979,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5221,2047784979,IC_kwDOD7z77c56DrgT,2024-04-10T14:56:52Z,2024-04-10T14:56:52Z,COLLABORATOR,"@phish3y happy to have you start to work on this.

https://github.com/apache/spark/blob/0d7c07047a628bd42eb53eb49935f5e3f81ea1a1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L4036

is the CPU implementation that we want to try and target. It looks like they have special case equality for NaNs and Nulls, but I am not sure if it is going to work with -0.0 vs 0.0 properly. We probably need to do some explicit testing on different versions of Spark.

The other thing to be careful of is that it appears that Spark is purposely keeping the order of the values in the array the same and only removing duplicates that come later. I am not sure if we need to replicate this functionality or not. It would be ideal if we could, but I don't think this is critical because it started to happen after a bug fix. https://github.com/apache/spark/pull/33993

As for how you might be able to implement this I would suggest that you start with

https://github.com/rapidsai/cudf/blob/e727814c00ce0ae13febfeb44ca3d2db66f7f2e9/cpp/include/cudf/lists/stream_compaction.hpp#L87

using the java API for it 
https://github.com/rapidsai/cudf/blob/e727814c00ce0ae13febfeb44ca3d2db66f7f2e9/java/src/main/java/ai/rapids/cudf/ColumnView.java#L2513

Then we can see what data types work well out of the box and if we have to add in some special case processing to make it work.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2047784979/reactions,1,1,0,0,0,0,0,0,0,5221
301,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098187782,https://github.com/NVIDIA/spark-rapids/issues/5223#issuecomment-1098187782,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5223,1098187782,IC_kwDOD7z77c5BdQQG,2022-04-13T15:24:45Z,2022-04-13T15:24:45Z,COLLABORATOR,This is a string operation very similar to casting an array to a String which we already support. I think we should be able to do this without any help from CUDF.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098187782/reactions,0,0,0,0,0,0,0,0,0,5223
302,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332743108,https://github.com/NVIDIA/spark-rapids/issues/5223#issuecomment-1332743108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5223,1332743108,IC_kwDOD7z77c5PcAvE,2022-11-30T21:14:51Z,2022-11-30T21:14:51Z,COLLABORATOR,What is the difference between `array_join` and `concat_ws`?,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332743108/reactions,0,0,0,0,0,0,0,0,0,5223
303,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098185904,https://github.com/NVIDIA/spark-rapids/issues/5224#issuecomment-1098185904,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5224,1098185904,IC_kwDOD7z77c5BdPyw,2022-04-13T15:23:11Z,2022-04-13T15:23:11Z,COLLABORATOR,"We are going to need some help from CUDF for this, but @mythrocks probably implemented most of this as a part of the GpuGetMapValue implementation in CUDF JNI. so I would love to hear his opinion on this before we go much further with it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1098185904/reactions,0,0,0,0,0,0,0,0,0,5224
304,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1103511191,https://github.com/NVIDIA/spark-rapids/issues/5224#issuecomment-1103511191,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5224,1103511191,IC_kwDOD7z77c5Bxj6X,2022-04-20T06:16:27Z,2022-04-20T06:16:27Z,COLLABORATOR,"> We are going to need some help from CUDF for this, but @mythrocks probably implemented most of this as a part of the GpuGetMapValue implementation in CUDF JNI.

Sorry for the delayed response. I think the `cudf::lists::index_of()` call we added to support `GetMapValue` should do the trick. 

",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1103511191/reactions,0,0,0,0,0,0,0,0,0,5224
305,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612096282,https://github.com/NVIDIA/spark-rapids/issues/5224#issuecomment-1612096282,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5224,1612096282,IC_kwDOD7z77c5gFqMa,2023-06-28T20:58:31Z,2023-06-28T20:58:31Z,COLLABORATOR,Yes `index_of` with first occurrence should be enough for this.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612096282/reactions,0,0,0,0,0,0,0,0,0,5224
306,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097227623,https://github.com/NVIDIA/spark-rapids/issues/5227#issuecomment-1097227623,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5227,1097227623,IC_kwDOD7z77c5BZl1n,2022-04-12T21:17:04Z,2022-04-12T21:17:04Z,COLLABORATOR,"This is going to be difficult because of the lambda. We would really need to have CUDF support this probably using AST, or we are going to have to special case the default ordering lambda expression and use the built in sort that exists.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1097227623/reactions,0,0,0,0,0,0,0,0,0,5227
307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1168766894,https://github.com/NVIDIA/spark-rapids/issues/5227#issuecomment-1168766894,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5227,1168766894,IC_kwDOD7z77c5Fqfeu,2022-06-28T14:01:09Z,2022-06-28T14:01:09Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/11162 as the majority of the CUDF request.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1168766894/reactions,0,0,0,0,0,0,0,0,0,5227
308,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1168773531,https://github.com/NVIDIA/spark-rapids/issues/5227#issuecomment-1168773531,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5227,1168773531,IC_kwDOD7z77c5FqhGb,2022-06-28T14:06:40Z,2022-06-28T14:06:40Z,COLLABORATOR,"I filed https://github.com/rapidsai/cudf/issues/11163 to help us implement/run a number of these commands as written.

We really should be looking at doing some pattern matching as well. Especially for the default case, which I think is the most common.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1168773531/reactions,0,0,0,0,0,0,0,0,0,5227
309,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332746798,https://github.com/NVIDIA/spark-rapids/issues/5227#issuecomment-1332746798,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5227,1332746798,IC_kwDOD7z77c5PcBou,2022-11-30T21:18:48Z,2022-11-30T21:18:48Z,COLLABORATOR,"Can we just partially support `array_sort` without lambda (i.e., only use the default comparison behavior)? By doing so we can just call libcudf `sort_lists`.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1332746798/reactions,0,0,0,0,0,0,0,0,0,5227
310,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1797944181,https://github.com/NVIDIA/spark-rapids/issues/5227#issuecomment-1797944181,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5227,1797944181,IC_kwDOD7z77c5rKnN1,2023-11-07T07:22:36Z,2023-11-14T02:34:45Z,COLLABORATOR,"+1, this expression is also used in Scale Test Query40.

update: I'm going to do what ttnghia said, partially support it without lambda first.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1797944181/reactions,0,0,0,0,0,0,0,0,0,5227
311,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178552647,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1178552647,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1178552647,IC_kwDOD7z77c5GP0lH,2022-07-08T04:59:41Z,2022-07-08T04:59:41Z,COLLABORATOR,checking,,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178552647/reactions,0,0,0,0,0,0,0,0,0,5271
312,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185065011,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1185065011,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1185065011,IC_kwDOD7z77c5Goqgz,2022-07-15T00:59:35Z,2022-07-18T05:30:15Z,COLLABORATOR,"
To run NVIDIA A100 GPUs, you must use the [accelerator-optimized (A2)](https://cloud.google.com/compute/docs/accelerator-optimized-machines#a2_vms) machine type.

Each A2 machine type has a fixed GPU count, vCPU count, and memory size.
",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185065011/reactions,0,0,0,0,0,0,0,0,0,5271
313,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189206354,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1189206354,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1189206354,IC_kwDOD7z77c5G4dlS,2022-07-19T15:35:22Z,2022-07-19T15:35:22Z,COLLABORATOR,Note the PR in dataproc is merged,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189206354/reactions,0,0,0,0,0,0,0,0,0,5271
314,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189220829,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1189220829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1189220829,IC_kwDOD7z77c5G4hHd,2022-07-19T15:48:02Z,2022-07-19T15:48:02Z,COLLABORATOR,"we should decide what all we want to test... for instance os versions and perhaps configurations:

`gcloud dataproc clusters create tgravestestmig2 --region us-central1 --subnet default --zone us-central1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type a2-highgpu-1g --worker-boot-disk-size 500 --num-worker-local-ssds 4 --worker-accelerator type=nvidia-tesla-a100,count=1 --image-version 2.0-debian10 --initialization-actions 'gs://.../mig/rapids.sh,gs://.../mig/install_gpu_driver.sh' --project rapids-spark --metadata=startup-script-url=gs://.../mig/mig.sh --enable-component-gateway --optional-components=JUPYTER,ZEPPELIN --metadata=^:^MIG_CGI='14,14,14'`

There is the optional parameter:

```
There is an option added here as well to tell the script how to configure MIG, since its just a list I had to do the delimiter different then comma thing, if there is a better way let me know:

--metadata=^:^MIG_CGI='3g.20gb,9'
```

If that is left off it defaults 2 MIG instances with profile id 9.  

we could potentially test the scripts without the rapids plugin, but really the dataproc test suite should do that.

What do we test now for dataproc configurations?",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189220829/reactions,0,0,0,0,0,0,0,0,0,5271
315,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1191485582,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1191485582,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1191485582,IC_kwDOD7z77c5HBKCO,2022-07-21T13:27:49Z,2022-07-21T13:27:49Z,COLLABORATOR,"Can we test all 3 os's on a rotating basis.  Debian, ubuntu, and rocky probably just need test like once a week.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1191485582/reactions,0,0,0,0,0,0,0,0,0,5271
316,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1193450246,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1193450246,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1193450246,IC_kwDOD7z77c5HIpsG,2022-07-25T01:19:01Z,2022-07-25T01:25:40Z,COLLABORATOR,"Sorry for the delayed response, I was off for holiday last week.

We currently have a pipeline against the ubuntu, with the default `2 MIG instances with profile id 9`

Let me set up other jobs against Debian and rocky, plus diff `MIG_CGI`



",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1193450246/reactions,0,0,0,0,0,0,0,0,0,5271
317,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1193637517,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1193637517,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1193637517,IC_kwDOD7z77c5HJXaN,2022-07-25T06:30:57Z,2022-07-25T06:30:57Z,COLLABORATOR,"Tests on Ubuntu/Debian with diff `MIG_CGI` got PASS on Blossom CI job.

2.0-rockey8 image seems have some problem allocation A100 GPU and installing GPU driver as below:  driver install failed, and not GPU devices allocated in the Dataproc cluster.

https://console.cloud.google.com/storage/browser/rapids-test/google-cloud-dataproc-metainfo/99495401-1287-4193-9446-4db444aa0266?authuser=0&project=rapids-spark&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false

```
sa_116163337916449219958@exm-test-mig-w-0:~$ nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

sa_116163337916449219958@exm-test-mig-w-0:~$ ls /dev/nv*
nvram

```",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1193637517/reactions,0,0,0,0,0,0,0,0,0,5271
318,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1194434269,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1194434269,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1194434269,IC_kwDOD7z77c5HMZ7d,2022-07-25T18:12:14Z,2022-07-25T18:12:14Z,COLLABORATOR,"confirmed the mig script with rocky aren't working anymore, they were working before pr was merged so will have to investigate",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1194434269/reactions,1,1,0,0,0,0,0,0,0,5271
319,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1210164193,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1210164193,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1210164193,IC_kwDOD7z77c5IIaPh,2022-08-10T05:04:52Z,2022-08-10T05:04:52Z,COLLABORATOR,Move to 22.10 for MIG on Rocky8 because it's not ready for CI.,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1210164193/reactions,0,0,0,0,0,0,0,0,0,5271
320,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1236584863,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1236584863,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1236584863,IC_kwDOD7z77c5JtMmf,2022-09-05T06:26:52Z,2022-09-05T06:26:52Z,COLLABORATOR,Related GCP issue: https://github.com/GoogleCloudDataproc/initialization-actions/issues/1010,,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1236584863/reactions,0,0,0,0,0,0,0,0,0,5271
321,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1334806867,https://github.com/NVIDIA/spark-rapids/issues/5271#issuecomment-1334806867,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5271,1334806867,IC_kwDOD7z77c5Pj4lT,2022-12-02T06:12:35Z,2022-12-02T06:12:35Z,COLLABORATOR,Move out roadmap 22.12.,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1334806867/reactions,0,0,0,0,0,0,0,0,0,5271
322,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108707761,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108707761,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1108707761,IC_kwDOD7z77c5CFYmx,2022-04-25T15:14:55Z,2022-04-25T15:16:28Z,COLLABORATOR,"From what I see in the logs @wjxiz1992 the issue in the description is not the actual problem. The description is pointing at a brand new executor that is trying to start, but it can't, because the GPU is busy with a prior executor that is having issues. 

The earlier failure is an OOM with the async allocator that according to the logs looks just like fragmentation (which seems odd). 

@wjxiz1992 could you confirm that prior runs (without the mono jar) were actually using ASYNC allocator? Or where they using ARENA? My guess is that this is what actually changed.

1. We have 4 tasks with the semaphore (task 88, 89, 92, 94). Tasks 89, 94, and 92 are allocating ~50MB, and task 88 is trying to allocate 560MB. 
2. RMM was at 3.3 GB and we spill 300MB trying to let these allocations through. 
3. With RMM at 3.0 GB we still can't handle the allocations, note this is a 40GB GPU and our pool was sized correctly from the beginning (`dispatcher-Executor 22/04/24 05:24:25:995 INFO GpuDeviceManager: Initializing RMM ASYNC pool size = 39164.1875 MB on gpuId 0`)

So this seems like exactly the type of case where the async allocator should be helping. Alternatively we could have our stats wrong on what the RMM pool actually has allocated. It needs to be investigated further.

Pertinent logs:

```
Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:533 INFO DeviceMemoryEventHandler: Device allocation of 45350928 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
Executor task launch worker for task 94.0 in stage 1128.0 (TID 139368) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 47549192 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
Executor task launch worker for task 92.0 in stage 1128.0 (TID 139366) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 45360496 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
Executor task launch worker for task 88.0 in stage 1128.0 (TID 139362) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 560718784 bytes failed, device store has 0 bytes. Total RMM allocated is 3043957504 bytes.
Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:548 ERROR Executor: Exception in task 89.0 in stage 1128.0 (TID 139363)
java.lang.OutOfMemoryError: Could not allocate native memory: std::bad_alloc: out_of_memory: CUDA error at: /home/jenkins/agent/workspace/jenkins-cudf_nightly-dev-github-683-cuda11/java/target/cmake-build/_deps/rmm-src/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:120: cudaErrorMemoryAllocation out of memory
```",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108707761/reactions,0,0,0,0,0,0,0,0,0,5303
323,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108710445,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108710445,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1108710445,IC_kwDOD7z77c5CFZQt,2022-04-25T15:17:10Z,2022-04-25T15:17:10Z,COLLABORATOR,I believe this is a P1 because the logs are telling me this is a fragmentation issue and I was surprised by this behavior with the ASYNC allocator.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108710445/reactions,0,0,0,0,0,0,0,0,0,5303
324,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108858018,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1108858018,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1108858018,IC_kwDOD7z77c5CF9Si,2022-04-25T17:42:55Z,2022-04-25T17:42:55Z,COLLABORATOR,"I can reproduce this easily. I also computed the amount of memory allocated on the GPU manually using the RMM log and I ran OOM with 9GB allocated for example in a 40GB GPU while trying to allocate 500MB, so this seems like fragmentation still.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1108858018/reactions,0,0,0,0,0,0,0,0,0,5303
325,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1109210097,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1109210097,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1109210097,IC_kwDOD7z77c5CHTPx,2022-04-26T01:43:02Z,2022-04-26T01:43:02Z,COLLABORATOR,"> From what I see in the logs @wjxiz1992 the issue in the description is not the actual problem. The description is pointing at a brand new executor that is trying to start, but it can't, because the GPU is busy with a prior executor that is having issues.
> 
> The earlier failure is an OOM with the async allocator that according to the logs looks just like fragmentation (which seems odd).
> 
> @wjxiz1992 could you confirm that prior runs (without the mono jar) were actually using ASYNC allocator? Or where they using ARENA? My guess is that this is what actually changed.
> 
> 1. We have 4 tasks with the semaphore (task 88, 89, 92, 94). Tasks 89, 94, and 92 are allocating ~50MB, and task 88 is trying to allocate 560MB.
> 2. RMM was at 3.3 GB and we spill 300MB trying to let these allocations through.
> 3. With RMM at 3.0 GB we still can't handle the allocations, note this is a 40GB GPU and our pool was sized correctly from the beginning (`dispatcher-Executor 22/04/24 05:24:25:995 INFO GpuDeviceManager: Initializing RMM ASYNC pool size = 39164.1875 MB on gpuId 0`)
> 
> So this seems like exactly the type of case where the async allocator should be helping. Alternatively we could have our stats wrong on what the RMM pool actually has allocated. It needs to be investigated further.
> 
> Pertinent logs:
> 
> ```
> Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:533 INFO DeviceMemoryEventHandler: Device allocation of 45350928 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
> Executor task launch worker for task 94.0 in stage 1128.0 (TID 139368) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 47549192 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
> Executor task launch worker for task 92.0 in stage 1128.0 (TID 139366) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 45360496 bytes failed, device store has 0 bytes. Total RMM allocated is 3325171712 bytes.
> Executor task launch worker for task 88.0 in stage 1128.0 (TID 139362) 22/04/24 05:41:10:534 INFO DeviceMemoryEventHandler: Device allocation of 560718784 bytes failed, device store has 0 bytes. Total RMM allocated is 3043957504 bytes.
> Executor task launch worker for task 89.0 in stage 1128.0 (TID 139363) 22/04/24 05:41:10:548 ERROR Executor: Exception in task 89.0 in stage 1128.0 (TID 139363)
> java.lang.OutOfMemoryError: Could not allocate native memory: std::bad_alloc: out_of_memory: CUDA error at: /home/jenkins/agent/workspace/jenkins-cudf_nightly-dev-github-683-cuda11/java/target/cmake-build/_deps/rmm-src/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:120: cudaErrorMemoryAllocation out of memory
> ```

I didn't set this parameter in our template file, so it should always be the default one. I can switch to ARENA for another run.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1109210097/reactions,0,0,0,0,0,0,0,0,0,5303
326,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113503107,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1113503107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1113503107,IC_kwDOD7z77c5CXrWD,2022-04-29T16:27:05Z,2022-04-29T16:27:05Z,COLLABORATOR,"> I didn't set this parameter in our template file, so it should always be the default one. I can switch to ARENA for another run.

@wjxiz1992 can you please try with ARENA and let us know whether the same problem occurs? ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113503107/reactions,0,0,0,0,0,0,0,0,0,5303
327,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125560411,https://github.com/NVIDIA/spark-rapids/issues/5303#issuecomment-1125560411,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5303,1125560411,IC_kwDOD7z77c5DFrBb,2022-05-13T01:07:36Z,2022-05-13T13:36:01Z,COLLABORATOR,"We tested Decimal with UCX on and ARENA pool, but the job ran for a long time (more than 1 hour), then we aborted it.
From the driver log, it said some executor has no response as below.
We ran it with spark 3.2.1. The application id is app-20220511061958-0000.

```
[2022-05-11T06:27:19.946Z] block-manager-ask-thread-pool-202 22/05/11 06:27:05:372 WARN BlockManagerMaster: Failed to remove shuffle 122 - Cannot receive any reply from /****:49074 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
[2022-05-11T06:27:19.946Z] org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /****:49074 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2022-05-11T06:27:19.946Z] 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
[2022-05-11T06:27:19.946Z] 	at scala.util.Failure.recover(Try.scala:234)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Promise.complete(Promise.scala:53)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Promise.complete$(Promise.scala:52)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2022-05-11T06:27:19.946Z] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Promise.tryFailure(Promise.scala:112)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.Promise.tryFailure$(Promise.scala:112)
[2022-05-11T06:27:19.946Z] 	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2022-05-11T06:27:19.946Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2022-05-11T06:27:19.946Z] 	at java.lang.Thread.run(Thread.java:748)
[2022-05-11T06:27:19.946Z] Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /****:49074 in 120 seconds
[2022-05-11T06:27:19.946Z] 	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)
[2022-05-11T06:27:19.946Z] 	... 7 more
```",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125560411/reactions,0,0,0,0,0,0,0,0,0,5303
328,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306437351,https://github.com/NVIDIA/spark-rapids/issues/5321#issuecomment-1306437351,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5321,1306437351,IC_kwDOD7z77c5N3qbn,2022-11-08T00:42:18Z,2022-11-08T00:42:18Z,COLLABORATOR,"Looks like DB 11.3 backported changes related to this commit.

While working on DB3.3.0, I get the following error:

```
[ERROR] /home/ubuntu/spark-rapids/sql-plugin/src/main/330+/scala/com/nvidia/spark/rapids/shims/Spark330PlusShims.scala:49: not enough arguments for constructor FileScanRDD: (sparkSession: org.apache.spark.sql.SparkSession, readFunction: org.apache.spark.sql.execution.datasources.PartitionedFile => Iterator[org.apache.spark.sql.catalyst.InternalRow], filePartitions: Seq[org.apache.spark.sql.execution.datasources.FilePartition], debugWriter: Option[com.databricks.sql.catalyst.BadFilesWriter], asyncIOSafe: Boolean, fileNotFoundHint: String => String, repeatedReadsMetrics: Option[com.databricks.sql.execution.RepeatedReadsMetrics], fileSystemMetrics: Option[com.databricks.sql.execution.FileSystemMetrics], fileScanMetrics: com.databricks.sql.execution.FileScanMetrics, readSchema: org.apache.spark.sql.types.StructType, metadataColumns: Seq[org.apache.spark.sql.catalyst.expressions.AttributeReference], options: org.apache.spark.sql.catalyst.FileSourceOptions)org.apache.spark.sql.execution.datasources.FileScanRDD.
Unspecified value parameter readSchema.
[ERROR]     new FileScanRDD(sparkSession, readFunction, filePartitions, readDataSchema, metadataColumns)
[ERROR]     ^
[ERROR] /home/ubuntu/s
```",,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306437351/reactions,0,0,0,0,0,0,0,0,0,5321
329,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306450314,https://github.com/NVIDIA/spark-rapids/issues/5321#issuecomment-1306450314,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5321,1306450314,IC_kwDOD7z77c5N3tmK,2022-11-08T00:54:27Z,2022-11-08T00:54:27Z,COLLABORATOR,"```
val arr1 = classOf[org.apache.spark.sql.execution.datasources.FileScanRDD].getConstructors.head.getParameters.map(_.getName);

arr1: Array[String] = Array(sparkSession, readFunction, filePartitions, debugWriter, asyncIOSafe, fileNotFoundHint, repeatedReadsMetrics, fileSystemMetrics, fileScanMetrics, readSchema, metadataColumns, options)
```


```
classOf[org.apache.spark.sql.execution.datasources.FileScanRDD].getConstructors.head.getParameters;

res9: Array[java.lang.reflect.Parameter] = Array(final org.apache.spark.sql.SparkSession sparkSession, final scala.Function1<org.apache.spark.sql.execution.datasources.PartitionedFile, scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow>> readFunction, final scala.collection.Seq<org.apache.spark.sql.execution.datasources.FilePartition> filePartitions, final scala.Option<com.databricks.sql.catalyst.BadFilesWriter> debugWriter, final boolean asyncIOSafe, final scala.Function1<java.lang.String, java.lang.String> fileNotFoundHint, final scala.Option<com.databricks.sql.execution.RepeatedReadsMetrics> repeatedReadsMetrics, final scala.Option<com.databricks.sql.execution.FileSystemMetrics> fileSystemMetrics, final com.databricks.sql.execution.FileScanMetrics fileScanMetrics, final org.apache.spark.sql.types.StructType readSchema, final scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.AttributeReference> metadataColumns, final org.apache.spark.sql.catalyst.FileSourceOptions options)

```

",,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306450314/reactions,0,0,0,0,0,0,0,0,0,5321
330,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1320386673,https://github.com/NVIDIA/spark-rapids/issues/5321#issuecomment-1320386673,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5321,1320386673,IC_kwDOD7z77c5Os4Bx,2022-11-18T18:29:44Z,2022-11-18T18:29:44Z,COLLABORATOR,This isn't required for DB-11.3 build fix but good to have for Spark-3.4. ,,nartal1,50492963,MDQ6VXNlcjUwNDkyOTYz,https://avatars.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1320386673/reactions,0,0,0,0,0,0,0,0,0,5321
331,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1116584992,https://github.com/NVIDIA/spark-rapids/issues/5347#issuecomment-1116584992,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5347,1116584992,IC_kwDOD7z77c5Cjbwg,2022-05-03T20:36:37Z,2022-05-03T20:36:37Z,MEMBER,"Is the reordering going to be OK in practice?  Worried about a case where data was written out in a sorted order, and user query is assuming data read by a task will be read in a sorted order.  If we reorder the files then we'll come up with a different data order than the CPU would for the task, and I'm wondering if we could potentially break some expectations.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1116584992/reactions,0,0,0,0,0,0,0,0,0,5347
332,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118073197,https://github.com/NVIDIA/spark-rapids/issues/5347#issuecomment-1118073197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5347,1118073197,IC_kwDOD7z77c5CpHFt,2022-05-05T01:08:38Z,2022-05-05T01:08:38Z,COLLABORATOR,"Yeah, It indeed breaks this case as Jason said.",,wbo4958,1320706,MDQ6VXNlcjEzMjA3MDY=,https://avatars.githubusercontent.com/u/1320706?v=4,,https://api.github.com/users/wbo4958,https://github.com/wbo4958,https://api.github.com/users/wbo4958/followers,https://api.github.com/users/wbo4958/following{/other_user},https://api.github.com/users/wbo4958/gists{/gist_id},https://api.github.com/users/wbo4958/starred{/owner}{/repo},https://api.github.com/users/wbo4958/subscriptions,https://api.github.com/users/wbo4958/orgs,https://api.github.com/users/wbo4958/repos,https://api.github.com/users/wbo4958/events{/privacy},https://api.github.com/users/wbo4958/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118073197/reactions,0,0,0,0,0,0,0,0,0,5347
333,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118588319,https://github.com/NVIDIA/spark-rapids/issues/5347#issuecomment-1118588319,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5347,1118588319,IC_kwDOD7z77c5CrE2f,2022-05-05T13:57:21Z,2022-05-05T13:57:21Z,COLLABORATOR,"I reopened this because at least we can have people opt into this. Also if I remember correctly Spark was sorting the files in an odd way where it was grouping them by whole files as much as possible, but if they didn't fit then the bits and pieces were combined together in the end in a single task. So if a user is relying on this behavior they may be in for problems with Spark itself, if the files ever grow larger than a single split.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118588319/reactions,0,0,0,0,0,0,0,0,0,5347
334,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113565108,https://github.com/NVIDIA/spark-rapids/issues/5402#issuecomment-1113565108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5402,1113565108,IC_kwDOD7z77c5CX6e0,2022-04-29T17:42:04Z,2022-04-29T17:42:13Z,COLLABORATOR,"NOTE that this is probably just an odd corner case because we are throwing away the result after filtering on it. This is probably something that Spark didn't think about or test, and it may not be that much of a real world example.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1113565108/reactions,0,0,0,0,0,0,0,0,0,5402
335,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1116557045,https://github.com/NVIDIA/spark-rapids/issues/5402#issuecomment-1116557045,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5402,1116557045,IC_kwDOD7z77c5CjU71,2022-05-03T20:23:41Z,2022-05-03T20:23:41Z,COLLABORATOR,The follow up here is to file an issue with Spark.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1116557045/reactions,0,0,0,0,0,0,0,0,0,5402
336,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118717672,https://github.com/NVIDIA/spark-rapids/issues/5403#issuecomment-1118717672,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5403,1118717672,IC_kwDOD7z77c5Crkbo,2022-05-05T15:41:12Z,2022-05-05T15:41:12Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/10797 for this.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1118717672/reactions,0,0,0,0,0,0,0,0,0,5403
337,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123765617,https://github.com/NVIDIA/spark-rapids/issues/5458#issuecomment-1123765617,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5458,1123765617,IC_kwDOD7z77c5C-01x,2022-05-11T13:30:07Z,2022-05-11T13:30:07Z,COLLABORATOR,"I'm a bit confused as to how it would be worse.  The perfile one is doing the same thing and reading essentially reading the file twice correct?  once to scan through to get all block metadata and once to actually read it.  In the multi-file version isn't it supposed to change to only scan the block necessary to get the metadata by skipping to the location and thus read a lot less of the file?   I guess you can see in the cloud version that perfile got much much better at least.  Maybe if local and all in page cache its quicker but I would be curious to see what is slower.  Is the seek in the newer code slower for instance.

In your tests is it reading chunks of a file or does it end up reading whole files?  How big are the files?",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123765617/reactions,0,0,0,0,0,0,0,0,0,5458
338,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124489037,https://github.com/NVIDIA/spark-rapids/issues/5458#issuecomment-1124489037,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5458,1124489037,IC_kwDOD7z77c5DBldN,2022-05-12T03:18:57Z,2022-05-12T08:12:35Z,COLLABORATOR,">The perfile one is doing the same thing and reading essentially reading the file twice correct? ... In the multi-file version isn't it supposed to change to only scan the block necessary to get the metadata by skipping to the location and thus read a lot less of the file?

Yes, PERFILE reads only the necessary part of a file twice. But the newer reader used by multi-threaded reading changes more than essentially reading a file. It reads the data directly as stream, similar to how CPU does, without collecting the block metadata ahead. It is almost another thing comparing to the original collecting-meta-then-copying-data way. 

The key point that the newer code can improve perf a lot for cloud is, I think, it calls `seek` only once at the begining for each partition. But `seek` is called as many times as the number of the necessary blocks (more than 1000 in my tests) by collecting block metadata, so it took a lot of time to read the metadata in our tests, even only for the necessary part. I guess `seek` is probably slow for cloud cases. And the newer code reduces the calls to `seek` quite a lot.

The reasons why the orignal code is faster for local files in my tests are, I think,

1. `seek` is fast enough for local files, so collecting metadata should not take much time.
2. With the given block's metadata, most blocks were merged into one copy range, it reduced the data copying count a lot.
3. It copied the raw data chunk by chunk according to the merged copy ranges directly into the batch buffer, without parsing.

While the newer code needs to parse the meta and sync, rewirte the meta and sync, and copy the data to batch buffer per block, without merging. 

>In your tests is it reading chunks of a file or does it end up reading whole files? How big are the files?

In my tests, each file is about 5 MB, with about 1000 blocks inside (used for coalescing benchmarks before). So it should read the whole file.
Besides, the original code copied the data only once for each file, but the newer code ran data copying about 1000 times.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1124489037/reactions,0,0,0,0,0,0,0,0,0,5458
339,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1129320275,https://github.com/NVIDIA/spark-rapids/issues/5460#issuecomment-1129320275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5460,1129320275,IC_kwDOD7z77c5DUA9T,2022-05-17T21:09:51Z,2022-05-17T21:09:51Z,COLLABORATOR,At a minimum we should add this documentation to the tuning guide.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1129320275/reactions,0,0,0,0,0,0,0,0,0,5460
340,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123840772,https://github.com/NVIDIA/spark-rapids/issues/5461#issuecomment-1123840772,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5461,1123840772,IC_kwDOD7z77c5C_HME,2022-05-11T14:23:55Z,2022-05-11T14:23:55Z,MEMBER,"#548 is a subset of this, focused on compression codecs.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1123840772/reactions,0,0,0,0,0,0,0,0,0,5461
341,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125416663,https://github.com/NVIDIA/spark-rapids/issues/5475#issuecomment-1125416663,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5475,1125416663,IC_kwDOD7z77c5DFH7X,2022-05-12T20:59:56Z,2022-05-12T20:59:56Z,COLLABORATOR,"I tested this manually and there we are doing the right thing. It is all based off of column position, for good or bad. This means there is no real way to change the order of columns/etc without playing some games.  But it would be good to add some integration tests for this anyways.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1125416663/reactions,0,0,0,0,0,0,0,0,0,5475
342,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163557044,https://github.com/NVIDIA/spark-rapids/issues/5478#issuecomment-1163557044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5478,1163557044,IC_kwDOD7z77c5FWni0,2022-06-22T20:14:07Z,2022-06-22T20:14:07Z,CONTRIBUTOR,Depends on https://github.com/rapidsai/cudf/issues/11102,,anthony-chang,54450499,MDQ6VXNlcjU0NDUwNDk5,https://avatars.githubusercontent.com/u/54450499?v=4,,https://api.github.com/users/anthony-chang,https://github.com/anthony-chang,https://api.github.com/users/anthony-chang/followers,https://api.github.com/users/anthony-chang/following{/other_user},https://api.github.com/users/anthony-chang/gists{/gist_id},https://api.github.com/users/anthony-chang/starred{/owner}{/repo},https://api.github.com/users/anthony-chang/subscriptions,https://api.github.com/users/anthony-chang/orgs,https://api.github.com/users/anthony-chang/repos,https://api.github.com/users/anthony-chang/events{/privacy},https://api.github.com/users/anthony-chang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163557044/reactions,0,0,0,0,0,0,0,0,0,5478
343,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189626759,https://github.com/NVIDIA/spark-rapids/issues/5478#issuecomment-1189626759,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5478,1189626759,IC_kwDOD7z77c5G6EOH,2022-07-19T22:53:09Z,2022-07-19T22:53:09Z,CONTRIBUTOR,"There are still a number of inconsistencies, particularly when the pattern consists entirely of word boundaries. cuDF/python has an extra empty token in the array returned by string split which Spark does not. Examples:

```
- \b *** FAILED ***
  string_split pattern=\b isRegex=true data=a limit=-1 
  CPU [2]: a,  
  GPU [3]: , a, (RegularExpressionTranspilerSuite.scala:764)

- \b\b *** FAILED ***
  string_split pattern=\b\b isRegex=true data=a limit=-1 
  CPU [2]: a,  
  GPU [3]: , a, (RegularExpressionTranspilerSuite.scala:764)

- \B\B *** FAILED ***
  string_split pattern=\B\B isRegex=true data=-+ limit=-1 
  CPU [3]: -, +,  
  GPU [4]: , -, +, (RegularExpressionTranspilerSuite.scala:764)
```

It is possible to use a workaround for this though. We can check if the pattern consists of entirely word or non-word boundaries:
```scala
def isEntirely(pattern: RegexAST, component: RegexAST): Boolean = {
  pattern match {
    case RegexSequence(parts) => parts.forall(isEntirely(_, component))
    case RegexGroup(_, term) => isEntirely(term, component)
    case RegexChoice(l, r) => isEntirely(l, component) || isEntirely(r, component)
    case `component` => true
    case _ => false
  }
}

val isEntirelyWordBoundary = isEntirely(ast, RegexEscaped('b')) || isEntirely(ast, RegexEscaped('B'))
```
Let `arr` be the string array we get from cuDF's string split. If `isEntirelyWordBoundary && arr.length > 1 && arr.head = """"` is true, then we can remove the first element in `arr` and it will match the Spark string split. This method passed the Scala fuzz tests for `limit <= 1`.

For `limit > 1` this wouldn't work since removing an element would be like removing one of the splits. We could try to work around this by running the split with `limit = limit + 1` but the conditions for when to do this are even more complicated. 

Checking all these conditions on GPU and then doing the removal from a list column vector will be extremely complicated so I don't see a good way to implement this functionality as of now. 
",,anthony-chang,54450499,MDQ6VXNlcjU0NDUwNDk5,https://avatars.githubusercontent.com/u/54450499?v=4,,https://api.github.com/users/anthony-chang,https://github.com/anthony-chang,https://api.github.com/users/anthony-chang/followers,https://api.github.com/users/anthony-chang/following{/other_user},https://api.github.com/users/anthony-chang/gists{/gist_id},https://api.github.com/users/anthony-chang/starred{/owner}{/repo},https://api.github.com/users/anthony-chang/subscriptions,https://api.github.com/users/anthony-chang/orgs,https://api.github.com/users/anthony-chang/repos,https://api.github.com/users/anthony-chang/events{/privacy},https://api.github.com/users/anthony-chang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189626759/reactions,0,0,0,0,0,0,0,0,0,5478
344,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132978706,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1132978706,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1132978706,IC_kwDOD7z77c5Dh-IS,2022-05-20T14:38:53Z,2022-05-20T14:38:53Z,COLLABORATOR,"Yes, technically it is possible, but there are a lot of challenges with it. The biggest challenge is not moving the data, but in how do we partition the GPU properly so performance is not horrible. We support having python processes use the same GPU that the main Spark worker process is also using.

https://nvidia.github.io/spark-rapids/docs/additional-functionality/rapids-udfs.html#gpu-support-for-pandas-udf

But if all you want to do is a simple MapInPandas operation you have to statically partition the GPUs memory between the python processes and the Spark process. On a 16 GiB card like a v100 it can be the difference between running 2 tasks in parallel and only being able to run a single task at a time on the GPU. This has a big impact for overall performance. It can also impact how much we end up spilling to host memory or disk for out of core operations like sorting. This has a large impact on all GPUs, if they hit those cases. But if what you are primarily doing is ML/DL using python APIs it can make these use cases possible without needing a separate GPU for them in python.

> a full serialization using arrow is required

That is true, but happily CUDF operates on data that is in an arrow like format already so serializing to/from arrow is much cheaper than doing it on the CPU where it goes from Spark's UnsafeRow to arrow before being serialized with the java arrow API. We have seen some really great performance improvements by going from CUDF to Arrow on the CPU to the serialized arrow format using the native arrow library.

If you have a specific use case we are happy to work with you on it. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1132978706/reactions,0,0,0,0,0,0,0,0,0,5561
345,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133165691,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133165691,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1133165691,IC_kwDOD7z77c5Dirx7,2022-05-20T17:49:41Z,2022-05-20T18:07:50Z,NONE,"Thank you for the detailed information. I'm looking into the possibility of using Python interface to implement XGBoost pyspark interface, one of the concerns is that there might be overhead during data serialization, which is not just performance concern but also CPU and GPU memory usage. XGBoost has to load the data in full instead of batches, being memory hungry is one of its most significant blockers for users. As a result we hope that we can the reduce data copies as much as possible.",,trivialfis,16746409,MDQ6VXNlcjE2NzQ2NDA5,https://avatars.githubusercontent.com/u/16746409?v=4,,https://api.github.com/users/trivialfis,https://github.com/trivialfis,https://api.github.com/users/trivialfis/followers,https://api.github.com/users/trivialfis/following{/other_user},https://api.github.com/users/trivialfis/gists{/gist_id},https://api.github.com/users/trivialfis/starred{/owner}{/repo},https://api.github.com/users/trivialfis/subscriptions,https://api.github.com/users/trivialfis/orgs,https://api.github.com/users/trivialfis/repos,https://api.github.com/users/trivialfis/events{/privacy},https://api.github.com/users/trivialfis/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133165691/reactions,0,0,0,0,0,0,0,0,0,5561
346,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133179698,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133179698,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1133179698,IC_kwDOD7z77c5DivMy,2022-05-20T18:08:54Z,2022-05-20T18:08:54Z,COLLABORATOR,"There is a java/scala XGBoost interface that bypasses those issues. We have been testing it in integration with the RAPIDs Accelerator and there is no data movement. It stays on the GPU and plays nicely with RAPIDs.

https://github.com/NVIDIA/spark-rapids-examples/tree/branch-22.06/examples/XGBoost-Examples

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133179698/reactions,0,0,0,0,0,0,0,0,0,5561
347,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133186002,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133186002,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1133186002,IC_kwDOD7z77c5DiwvS,2022-05-20T18:16:56Z,2022-05-20T20:04:53Z,NONE,"Yes. It has been a heated debate between using the Python interface and the Java interface.  There are many concerns around the JAVA interface as well, like dependencies management and user defined objective/metric/callback that has to be run on Python process, synchronization between Python XGB package and JVM package, etc.

I'm thinking that the cost of serialization might be worth, especially during early development. It's out of scope for XGBoost to work with these non-ML issues (rather it's a data management issue that's best solved in spark). But we also want to understand better if there's a way to avoid the overhead in the future as this is not a XGB specific feature request and might benefit many other workflows as well.",,trivialfis,16746409,MDQ6VXNlcjE2NzQ2NDA5,https://avatars.githubusercontent.com/u/16746409?v=4,,https://api.github.com/users/trivialfis,https://github.com/trivialfis,https://api.github.com/users/trivialfis/followers,https://api.github.com/users/trivialfis/following{/other_user},https://api.github.com/users/trivialfis/gists{/gist_id},https://api.github.com/users/trivialfis/starred{/owner}{/repo},https://api.github.com/users/trivialfis/subscriptions,https://api.github.com/users/trivialfis/orgs,https://api.github.com/users/trivialfis/repos,https://api.github.com/users/trivialfis/events{/privacy},https://api.github.com/users/trivialfis/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133186002/reactions,0,0,0,0,0,0,0,0,0,5561
348,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133253204,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133253204,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1133253204,IC_kwDOD7z77c5DjBJU,2022-05-20T19:32:00Z,2022-05-20T19:32:00Z,COLLABORATOR,"You have a number of options. Sorry that this is so long and rambling, I don't have a ton of time to clean it up before sending it.

1. Disable RMM pooling and use python for XGBoost. The RAPIDs Accelerator will use a little bit of memory that XGBoost will not be able to get access to, but most of the memory should be available to it. The downside is that the RAPIDs Accelerator will be much slower because it will have to call cudaMalloc and cudfFree directly instead of using RMM pooling. The data transfer should be relatively fast, but if it is not we can work with you on doing a prototype with CUDA IPC (see option 4).
2. Statically split the memory between the Python process and the scala process. This will speed up the RAPIDs Accelerator, but a lot less memory will be available for XGBoost to use.The data transfer should still be relatively fast compared to what we have on the CPU. Again if it is not good enough we can look at doing an one off prototype.
3. Disable the RAPIDs Accelerator entirely. This will give all of the GPU memory to XGBoost, but the transfer will be much slower and there isn't much we can do about it. You also don't get any GPU acceleration for reading the data or feature engineering before training.
4. Roll your own solution as a prototype. We would read in the data/do feature engineering/etc and then hand that off to some custom code in scala that we could have python bindings for. It would get an `RDD[Table]` using [this](https://nvidia.github.io/spark-rapids/docs/additional-functionality/ml-integration.html). Then we can put in the barrier, and do a mapPartitions that would launch a custom python process, hopefully using most of the same code that Spark uses for python processes. Then it would handle doing CUDA IPC for each table over to the python process and release memory back as we go. At the end it would release any final remaining memory, tell the python process to do the training and wait for something to be sent back. That is what I have had in the back of my mind for a while on how we could do this, but I have been waiting on the async allocator and  CUDA IPC to be working properly together. From what I understand the async allocator has issues with cuda IPC that they are still working on, but it is not ready yet.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133253204/reactions,0,0,0,0,0,0,0,0,0,5561
349,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133283252,https://github.com/NVIDIA/spark-rapids/issues/5561#issuecomment-1133283252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5561,1133283252,IC_kwDOD7z77c5DjIe0,2022-05-20T19:57:28Z,2022-05-20T19:57:28Z,NONE,"Thank you for the suggestions! Really appreciate that.

XGBoost does support drawing memory from RMM so that's not an issue. I think the last point sufficiently shows that there's a way to improve in the future and I have high hope that it will get good performance once those blockers are sorted out.",,trivialfis,16746409,MDQ6VXNlcjE2NzQ2NDA5,https://avatars.githubusercontent.com/u/16746409?v=4,,https://api.github.com/users/trivialfis,https://github.com/trivialfis,https://api.github.com/users/trivialfis/followers,https://api.github.com/users/trivialfis/following{/other_user},https://api.github.com/users/trivialfis/gists{/gist_id},https://api.github.com/users/trivialfis/starred{/owner}{/repo},https://api.github.com/users/trivialfis/subscriptions,https://api.github.com/users/trivialfis/orgs,https://api.github.com/users/trivialfis/repos,https://api.github.com/users/trivialfis/events{/privacy},https://api.github.com/users/trivialfis/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1133283252/reactions,0,0,0,0,0,0,0,0,0,5561
350,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564732111,https://github.com/NVIDIA/spark-rapids/issues/5576#issuecomment-1564732111,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5576,1564732111,IC_kwDOD7z77c5dQ-rP,2023-05-26T17:56:05Z,2023-05-26T17:56:05Z,COLLABORATOR,"Notable failed upgrade in Spark 

[SPARK-43716][BUILD] Revert scala-maven-plugin to 4.8.0

https://github.com/apache/spark/pull/41261
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564732111/reactions,0,0,0,0,0,0,0,0,0,5576
351,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1134862417,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1134862417,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1134862417,IC_kwDOD7z77c5DpKBR,2022-05-23T16:03:49Z,2022-05-23T16:03:49Z,COLLABORATOR,"The documentation needs to be cleaned up to make this clear. We do not support running arbitrary java/scala code on the GPU. We have some experimental support that can [transpile a simple UDF into catalyst expressions](https://nvidia.github.io/spark-rapids/docs/additional-functionality/udf-to-catalyst-expressions.html), but we do not have a way to actually execute java or scala code on the GPU.

This means something like the following 
```
val df = Seq((""A"", 1), (""B"", 2), (""A"", 3), (""B"", 4)).toDF(""str"", ""num"")
df.groupByKey(r => r.getString(0)).flatMapGroups((k, it) => it.map(i => i.getInt(1))).collect()
```

will not be able to execute the `r => r.getString(0)` nor `(k, it) => it.map(i => i.getInt(1))` on the GPU.

- `StaticInvoke` calls into custom JVM static methods. We cannot generically support it on the GPU.
- `BoundReference` is simple to support, but generally only shows up after we have converted code to the GPU, not before, which is why we do not yet support it for translation. This appears to be related to how `StaticInvoke` is created.
- `AssertNotNull` would be simple to implement, but is only inserted into a plan when converting to a DataFrame from a DataSet, so we have not prioritized it.
- `ValidateExternalType` is there to validate that the value returned by a `DataSet` is compatible with the `DataFrame` it is being turned into. Because a `DataSet` is expected to hold things like a `java.math.BigDecimal` or a `java.lang.String` makes it very difficult to run on the GPU.
- `GetExternalRowField` pulls values out of a `Row` not an `InternalRow` or an `UnsafeRow` which is what we currently support.

In order to support these we would have to do a lot of JVM byte code analysis at the plan level to be able to understand what was happening with the plan, and find ways to execute it on the GPU. This is not trivial which is why we have not taken on the challenge yet. In the short term, if it is possible moving to a DataFrame API instead of data set would allow us to accelerate the processing on the GPU.

```
df.groupBy(""str"").agg(collect_list(col(""num"")).alias(""value"")).selectExpr(""explode(value) as value"").collect()
```
I know this is not possible in all cases, but the more complicated the JVM processing is, the less likely it is that we would be able to automatically do byte code analysis on it and create a corresponding GPU plan automatically.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1134862417/reactions,1,1,0,0,0,0,0,0,0,5587
352,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1136447221,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1136447221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1136447221,IC_kwDOD7z77c5DvM71,2022-05-24T21:26:00Z,2022-05-24T21:26:00Z,COLLABORATOR,@SidWeng is it okay if I hijack this and turn it into a research spike to see what it might take to add any dataset support at all?,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1136447221/reactions,0,0,0,0,0,0,0,0,0,5587
353,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1136703563,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1136703563,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1136703563,IC_kwDOD7z77c5DwLhL,2022-05-25T04:10:20Z,2022-05-25T04:10:20Z,NONE,"@revans2 Of course and thanks for  your explanation.

> In order to support these we would have to do a lot of JVM byte code analysis at the plan level to be able to understand what was happening with the plan, and find ways to execute it on the GPU.

Try to understand more detail, do you mean the byte code analysis should be done in `GpuOverrides.wrapPlan()`?
From my current knowledge, `GpuOverrides.wrapPlan()` maps `SparkPlan` into `SparkPlanMeta` which wraps `ExecRule`, then `GpuOverrides.doConvertPlan()` convert `SparkPlanMeta` to GPU version `SparkPlan` using `SparkPlanMeta.convertToGpu()`.",,SidWeng,40747777,MDQ6VXNlcjQwNzQ3Nzc3,https://avatars.githubusercontent.com/u/40747777?v=4,,https://api.github.com/users/SidWeng,https://github.com/SidWeng,https://api.github.com/users/SidWeng/followers,https://api.github.com/users/SidWeng/following{/other_user},https://api.github.com/users/SidWeng/gists{/gist_id},https://api.github.com/users/SidWeng/starred{/owner}{/repo},https://api.github.com/users/SidWeng/subscriptions,https://api.github.com/users/SidWeng/orgs,https://api.github.com/users/SidWeng/repos,https://api.github.com/users/SidWeng/events{/privacy},https://api.github.com/users/SidWeng/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1136703563/reactions,0,0,0,0,0,0,0,0,0,5587
354,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1137394986,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1137394986,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1137394986,IC_kwDOD7z77c5Dy0Uq,2022-05-25T15:01:20Z,2022-05-25T15:01:20Z,COLLABORATOR,"> Try to understand more detail, do you mean the byte code analysis should be done in GpuOverrides.wrapPlan()?
From my current knowledge, GpuOverrides.wrapPlan() maps SparkPlan into SparkPlanMeta which wraps ExecRule, then GpuOverrides.doConvertPlan() convert SparkPlanMeta to GPU version SparkPlan using SparkPlanMeta.convertToGpu().

Honestly, I do not know where the correct place to do the byte code analysis would be. Our UDF to Catalyst transpiler is a rule added to the logical plan optimizations. This is early on and ideally would let other optimizations take place, like predicate push down analysis that happens in the physical plan optimizations. If we wanted to really take advantage of everything possible we would want to have a rule early on in the logical plan that would do as much translation to catalyst as possible.

But that requires us to only use fully publicly available catalyst operations. it might be simpler to just add our own translation rules like you suggested for thinks like `InvokeStatic` that would do the byte code analysis and figure out that it is calling into `r.getString(0)` which we could then translate into whatever GPU code/operations are needed to support it, int he context it currently is in.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1137394986/reactions,0,0,0,0,0,0,0,0,0,5587
355,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1188157922,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1188157922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1188157922,IC_kwDOD7z77c5G0dni,2022-07-18T19:12:25Z,2022-07-18T19:12:25Z,COLLABORATOR,@SidWeng I have started to take a look at this and it is very complicated. It is likely to involve a lot of byte code analysis to have any hope of this working. But I really would like to get some examples of operations that we should look at first. When I search for dataset usage on github most of what I find are really bad examples where the dataframe API could be used just even more simply and cleanly than dataset could be. If you have some examples that you can share with us to help us focus on what is most important that would be great.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1188157922/reactions,0,0,0,0,0,0,0,0,0,5587
356,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1206246822,https://github.com/NVIDIA/spark-rapids/issues/5587#issuecomment-1206246822,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5587,1206246822,IC_kwDOD7z77c5H5d2m,2022-08-05T09:32:07Z,2022-08-05T09:32:22Z,NONE,"@revans2 Sorry for late reply and here's an example in our scenario:

We have a DataFrame of Record, what we want to do is group Record by region, then do some operation on each of group.
At first we use `repartitionAndSortWithinPartitions()` and `mapPartitions()` but we want to speed up with GPU and come up with the following code.
```
case class Record(id: String, name: String, region: String)

// df is a DataFrame of Record
df.groupByKey(i => i.getString(2))
   .flatMapGroups { case (_, iter) => func(iter) }
   .collect()
```
",,SidWeng,40747777,MDQ6VXNlcjQwNzQ3Nzc3,https://avatars.githubusercontent.com/u/40747777?v=4,,https://api.github.com/users/SidWeng,https://github.com/SidWeng,https://api.github.com/users/SidWeng/followers,https://api.github.com/users/SidWeng/following{/other_user},https://api.github.com/users/SidWeng/gists{/gist_id},https://api.github.com/users/SidWeng/starred{/owner}{/repo},https://api.github.com/users/SidWeng/subscriptions,https://api.github.com/users/SidWeng/orgs,https://api.github.com/users/SidWeng/repos,https://api.github.com/users/SidWeng/events{/privacy},https://api.github.com/users/SidWeng/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1206246822/reactions,0,0,0,0,0,0,0,0,0,5587
357,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700692,https://github.com/NVIDIA/spark-rapids/issues/5633#issuecomment-1220700692,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5633,1220700692,IC_kwDOD7z77c5IwmoU,2022-08-19T13:43:56Z,2022-08-19T13:43:56Z,COLLABORATOR,reopened by https://github.com/NVIDIA/spark-rapids/pull/6367,,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700692/reactions,0,0,0,0,0,0,0,0,0,5633
358,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700410,https://github.com/NVIDIA/spark-rapids/issues/5634#issuecomment-1220700410,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5634,1220700410,IC_kwDOD7z77c5Iwmj6,2022-08-19T13:43:40Z,2022-08-19T13:43:40Z,COLLABORATOR,reopened by https://github.com/NVIDIA/spark-rapids/pull/6367,,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700410/reactions,0,0,0,0,0,0,0,0,0,5634
359,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700036,https://github.com/NVIDIA/spark-rapids/issues/5635#issuecomment-1220700036,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5635,1220700036,IC_kwDOD7z77c5IwmeE,2022-08-19T13:43:16Z,2022-08-19T13:43:16Z,COLLABORATOR,reopened by https://github.com/NVIDIA/spark-rapids/pull/6367,,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220700036/reactions,0,0,0,0,0,0,0,0,0,5635
360,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220699737,https://github.com/NVIDIA/spark-rapids/issues/5636#issuecomment-1220699737,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5636,1220699737,IC_kwDOD7z77c5IwmZZ,2022-08-19T13:42:58Z,2022-08-19T13:42:58Z,COLLABORATOR,reopened by https://github.com/NVIDIA/spark-rapids/pull/6367,,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220699737/reactions,0,0,0,0,0,0,0,0,0,5636
361,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138555507,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1138555507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1138555507,IC_kwDOD7z77c5D3Ppz,2022-05-26T13:05:54Z,2022-05-26T13:09:42Z,NONE,"My SQL contains three tables and two joins, I used the SQL Tab of Spark WebUI, analyzed the details of the query, list the time cost of each node of the task within the last spark stage and compared them with CPU mode. I attached it here. It seems that GPU operator like Sort, HashAggregate are quitely faster than those correspoding CPU operator, howerver , GPU mode contains additional operators like GpuShuffleCoalesce and GpuCoaleseBatches, and these two operator cost too much times which slow down the total performance. 
I have not used nv nsight system yet, and from the past disscustion there, https://github.com/NVIDIA/spark-rapids/discussions/5394#discussioncomment-2658194, I guess that acquiring the GPU semaphore is the bottleneck. So now my question is, how could I resolve the performance problem and what's the update of RapidsShuffleManager ?
![time costs](https://user-images.githubusercontent.com/42287875/170493268-ec049e5c-2dce-4430-ac09-d02fb0d9378c.png)
",,cfangplus,42287875,MDQ6VXNlcjQyMjg3ODc1,https://avatars.githubusercontent.com/u/42287875?v=4,,https://api.github.com/users/cfangplus,https://github.com/cfangplus,https://api.github.com/users/cfangplus/followers,https://api.github.com/users/cfangplus/following{/other_user},https://api.github.com/users/cfangplus/gists{/gist_id},https://api.github.com/users/cfangplus/starred{/owner}{/repo},https://api.github.com/users/cfangplus/subscriptions,https://api.github.com/users/cfangplus/orgs,https://api.github.com/users/cfangplus/repos,https://api.github.com/users/cfangplus/events{/privacy},https://api.github.com/users/cfangplus/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138555507/reactions,0,0,0,0,0,0,0,0,0,5650
362,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138665010,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1138665010,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1138665010,IC_kwDOD7z77c5D3qYy,2022-05-26T14:54:22Z,2022-05-26T14:54:22Z,MEMBER,"The metrics above do not indicate GpuShuffleCoalesce and GpuCoalesceBatches are directly the issue.  For both operations, the `concat batch time total` is the time spent actually doing the operation that node requires (i.e.: concatenating multiple batches together into a larger batch), and in both cases, the time is pretty low (even less than a millisecond in many cases).  The `collect batch time total` metric is the time that node spent waiting for input -- in other words, waiting for the node above it in the plan to produce output.  With the concat metric being low and the collect time being high, that indicates the time was spent waiting for shuffle (or whatever node precedes it in the query plan).

Regarding the GPU semaphore, which version of the RAPIDS Accelerator are you using?  We have added an explicit metric for time spent waiting for the GPU semaphore in many SQL UI nodes which is available in release 21.12 and after.  It is disabled by default to keep the number of metrics manageable for the driver, but it can be enabled in those releases by setting `spark.rapids.sql.metrics.level=DEBUG`.

We have also made strides in recent releases to avoid holding the GPU semaphore while not actively processing data on the GPU (e.g.: #4588 and #4476).  There are still instances where this can happen, and the problem is quite complex.  For example, it would be relatively easy to always release the GPU semaphore when performing shuffle I/O or other host-based operations, but doing so while data remains in GPU memory allows new tasks to start adding data to GPU memory and it can easily lead to an OOM or heavy thrashing with excessive memory spill.  The thrashing can be so bad that it can be _faster_ to hold onto the semaphore and prevent too many tasks from trying to use the GPU simultaneously even though it seems wasteful at first glance.  It all depends on how fast the network is, how fast local disks are, how much memory new tasks will add to the GPU before the I/O completes, etc. etc.  It is a tricky problem with many variables that are difficult to predict.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1138665010/reactions,0,0,0,0,0,0,0,0,0,5650
363,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141742173,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1141742173,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1141742173,IC_kwDOD7z77c5EDZpd,2022-05-31T07:03:19Z,2022-05-31T07:03:19Z,NONE,"I updated the version of RAPIDS Accelerator to 22.04 and run the SQL again.
I found that the collect batch time in GPUShuffleCoalesce and GPUCoalesceBatches disappeared, and yes, the GPU semaphore wait time comes  in at 3.2s/5s = 60%. Here is some spark properties ,spark.rapids.sql.concorrentGpuTasks = 4 while spark.executor.cores = 12",,cfangplus,42287875,MDQ6VXNlcjQyMjg3ODc1,https://avatars.githubusercontent.com/u/42287875?v=4,,https://api.github.com/users/cfangplus,https://github.com/cfangplus,https://api.github.com/users/cfangplus/followers,https://api.github.com/users/cfangplus/following{/other_user},https://api.github.com/users/cfangplus/gists{/gist_id},https://api.github.com/users/cfangplus/starred{/owner}{/repo},https://api.github.com/users/cfangplus/subscriptions,https://api.github.com/users/cfangplus/orgs,https://api.github.com/users/cfangplus/repos,https://api.github.com/users/cfangplus/events{/privacy},https://api.github.com/users/cfangplus/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1141742173/reactions,0,0,0,0,0,0,0,0,0,5650
364,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1142607324,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1142607324,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1142607324,IC_kwDOD7z77c5EGs3c,2022-05-31T20:25:07Z,2022-05-31T20:25:07Z,COLLABORATOR,Work is planned in the 22.08 release cycle to look for improvement opportunities related to semaphore acquisition: https://github.com/NVIDIA/spark-rapids/issues/4568.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1142607324/reactions,0,0,0,0,0,0,0,0,0,5650
365,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149801212,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1149801212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1149801212,IC_kwDOD7z77c5EiJL8,2022-06-08T11:34:51Z,2022-06-08T11:46:45Z,NONE,"I have another question about the SQL, which mentioned above and contains three tables and two joins. I attched the physical plan here.
![sql physical plan](https://user-images.githubusercontent.com/42287875/172607324-a54da70c-6261-46ee-8469-e21147025db0.png)
The table grid which shows above provide the time cost for each plan node within the last stage, which contains the two join operations. And as you can see, we can estimate the total time of each task for this stage, 738ms + 827ms + 32ms + 22ms + 1.6s + 3.5s + 10ms + 3.5s + 48ms + 52ms + 5.1s + 1.7s + 2ms + 1.6s + 1.6s = 20.329s. However, the SPARK WEBUI shows that the time cost for task within the last stage is nearly 5s. WHY?
![media task time](https://user-images.githubusercontent.com/42287875/172608120-9402e1da-8745-44d1-9942-ba6a1e74c208.png)
  ",,cfangplus,42287875,MDQ6VXNlcjQyMjg3ODc1,https://avatars.githubusercontent.com/u/42287875?v=4,,https://api.github.com/users/cfangplus,https://github.com/cfangplus,https://api.github.com/users/cfangplus/followers,https://api.github.com/users/cfangplus/following{/other_user},https://api.github.com/users/cfangplus/gists{/gist_id},https://api.github.com/users/cfangplus/starred{/owner}{/repo},https://api.github.com/users/cfangplus/subscriptions,https://api.github.com/users/cfangplus/orgs,https://api.github.com/users/cfangplus/repos,https://api.github.com/users/cfangplus/events{/privacy},https://api.github.com/users/cfangplus/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149801212/reactions,0,0,0,0,0,0,0,0,0,5650
366,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150308446,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1150308446,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1150308446,IC_kwDOD7z77c5EkFBe,2022-06-08T19:24:26Z,2022-06-08T19:24:26Z,MEMBER,"> we can estimate the total time of each task for this stage, 738ms + 827ms + 32ms + 22ms + 1.6s + 3.5s + 10ms + 3.5s + 48ms + 52ms + 5.1s + 1.7s + 2ms + 1.6s + 1.6s = 20.329s

There's quite a bit of double-counting in this calculation.  For example, the `collect batch time total` metric for `GpuCoalesceBatches` is timing how long it took to collect all the input batches, which is effectively timing all the nodes _above_ this node in the stage rather than something specific to this node.  In this example, that includes the time for the filter, running window, sort, and shuffle coalesce.  In general, if you're interested in the time associated with just what one node in the plan is doing, you should ignore any ""collect time"" metrics as that is timing how long it took _other_ nodes to produce output rather than this node.  There are other metrics, like `build time total` and `stream time total`, that similarly can end up timing other nodes.  That's why the time spent in build time + stream time for the join towards the end of the stage ends up very close to the total task time.

The `op time total` metric will focus on just how long a single node is taking to perform its operation separate from time spent waiting for input iterators.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150308446/reactions,0,0,0,0,0,0,0,0,0,5650
367,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150842827,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1150842827,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1150842827,IC_kwDOD7z77c5EmHfL,2022-06-09T08:43:19Z,2022-06-09T08:43:19Z,NONE,"yea, nice comment, great thx @jlowe 
Another question, as you reply before:
> With the concat metric being low and the collect time being high, that indicates the time was spent waiting for shuffle (or whatever node precedes it in the query plan).
> 
Now as I have updated the version of RAPIDS Accelerator from 21.10 to 22.04. I found that the collect batch time in GPUShuffleCoalesce and GPUCoalesceBatches disappeared, and yes, the GPU semaphore wait time comes in at 3.5s, and it comes to be 3.2/5s, nearly 60% proportion in the task time cost. I list the detail here. So the question is, the bottleneck is GPU semaphore or shuffle read?  BTW, I didn't use Rapids shuffle manager here, as before we tested and its result would be worse, maybe after the 22.8 release, we could take a try ? 
![semaphore2](https://user-images.githubusercontent.com/42287875/172804665-f1e32dbc-c237-404c-a515-cee6c685ea43.png)
",,cfangplus,42287875,MDQ6VXNlcjQyMjg3ODc1,https://avatars.githubusercontent.com/u/42287875?v=4,,https://api.github.com/users/cfangplus,https://github.com/cfangplus,https://api.github.com/users/cfangplus/followers,https://api.github.com/users/cfangplus/following{/other_user},https://api.github.com/users/cfangplus/gists{/gist_id},https://api.github.com/users/cfangplus/starred{/owner}{/repo},https://api.github.com/users/cfangplus/subscriptions,https://api.github.com/users/cfangplus/orgs,https://api.github.com/users/cfangplus/repos,https://api.github.com/users/cfangplus/events{/privacy},https://api.github.com/users/cfangplus/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1150842827/reactions,0,0,0,0,0,0,0,0,0,5650
368,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1154012352,https://github.com/NVIDIA/spark-rapids/issues/5650#issuecomment-1154012352,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5650,1154012352,IC_kwDOD7z77c5EyNTA,2022-06-13T14:42:48Z,2022-06-13T14:42:48Z,MEMBER,"> So the question is, the bottleneck is GPU semaphore or shuffle read?

Clearly semaphore wait is the main bottleneck for this task, but it's difficult to say for sure whether there is shuffle being performed while the semaphore is being held (generally undesirable when this occurs but it does happen in some cases). 
 An Nsight trace would be able to show whether the semaphore is being held while shuffle is being performed.

Note that semaphore wait in itself is not necessarily something that needs to be eliminated at all costs, as the semaphore's intent is to prevent situations where adding more concurrent tasks to the GPU may lead to an out-of-memory error on the GPU.  For example, if you have very many concurrent tasks configured for an executor (e.g.: 256 cores) but a relatively small GPU (e.g.: T4 with only 16GB) then it is fully expected to see a relatively high semaphore wait time since most tasks will be waiting their turn to use the GPU.  If all 256 concurrent tasks tried to use the T4 at the same time, it's very likely the GPU will run out of memory.

There are a couple of ways to help reduce semaphore wait time.  First is seeing if you can run more concurrent tasks on the GPU by configuring `spark.rapids.sql.concurrentGpuTasks`.  The more concurrent tasks that are allowed on the GPU, the lower the average semaphore wait time per task.  Setting this too high can lead to GPU OOM errors.  Another way to reduce the time is by reducing the number of CPU cores per executor.  This will run fewer tasks concurrently per executor which can hurt query performance for portions that run only on the CPU, but it will also reduce CPU memory pressure from having so many tasks trying to run simultaneously which can occasionally run faster overall.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1154012352/reactions,0,0,0,0,0,0,0,0,0,5650
369,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155950197,https://github.com/NVIDIA/spark-rapids/issues/5703#issuecomment-1155950197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5703,1155950197,IC_kwDOD7z77c5E5mZ1,2022-06-15T03:45:10Z,2022-06-15T03:45:10Z,COLLABORATOR,@gerashegalov Does this issue request to run all the integration test cases with the config PYSP_TEST_spark_rapids_force_caller_classloader=false?,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155950197/reactions,0,0,0,0,0,0,0,0,0,5703
370,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155952820,https://github.com/NVIDIA/spark-rapids/issues/5703#issuecomment-1155952820,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5703,1155952820,IC_kwDOD7z77c5E5nC0,2022-06-15T03:52:21Z,2022-06-15T03:54:13Z,COLLABORATOR,"Sorry, I am confused. Is this a bug report or a feature request? 
If the latter one, can you elaborate more about the ENVs? like pytest-xdist + local cluster or vanilla IT against standalone cluster, can you share the expected commands to test in different scenarios? and do we need to run all IT cases, or just some specific cases for multiple spark shims?

Also we need some detailed ENV combinations to do the resource planning, thanks",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155952820/reactions,0,0,0,0,0,0,0,0,0,5703
371,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159159569,https://github.com/NVIDIA/spark-rapids/issues/5703#issuecomment-1159159569,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5703,1159159569,IC_kwDOD7z77c5FF18R,2022-06-17T19:10:04Z,2022-06-17T19:11:18Z,COLLABORATOR,"> @gerashegalov Does this issue request to run all the integration test cases with the config PYSP_TEST_spark_rapids_force_caller_classloader=false?

yes, it should be parametrized at Jenkinks level because we cannot do at the pytest level. We should try to make this support as generic as possible because there can be more settings like this.
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159159569/reactions,0,0,0,0,0,0,0,0,0,5703
372,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159165424,https://github.com/NVIDIA/spark-rapids/issues/5703#issuecomment-1159165424,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5703,1159165424,IC_kwDOD7z77c5FF3Xw,2022-06-17T19:20:10Z,2022-06-17T19:20:22Z,COLLABORATOR,"> Sorry, I am confused. Is this a bug report or a feature request? 

bug in a sense that we left this feature without continuous testing and it was broken by later PRs.
 
> If the latter one, can you elaborate more about the ENVs? like pytest-xdist + local cluster or vanilla IT against standalone cluster, can you share the expected commands to test in different scenarios? and do we need to run all IT cases, or just some specific cases for multiple spark shims?
> 
> Also we need some detailed ENV combinations to do the resource planning, thanks

Ideally I would like another instance of all tests we have with the ENV PYSP_TEST_spark_rapids_force_caller_classloader=false injected at whatever cadence the capacity allows but not less than weekly frequency. ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159165424/reactions,0,0,0,0,0,0,0,0,0,5703
373,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1182789021,https://github.com/NVIDIA/spark-rapids/issues/5704#issuecomment-1182789021,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5704,1182789021,IC_kwDOD7z77c5Gf-2d,2022-07-13T05:41:51Z,2022-07-13T05:42:04Z,COLLABORATOR,"We have some notebooks testing on Databricks almost every day.
If there's no requirement for a special test case, I think notebook should have been covered.
@gerashegalov  Which test cases do we want to run by pyspark, spark-shell?
",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1182789021/reactions,0,0,0,0,0,0,0,0,0,5704
374,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185280792,https://github.com/NVIDIA/spark-rapids/issues/5704#issuecomment-1185280792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5704,1185280792,IC_kwDOD7z77c5GpfMY,2022-07-15T07:55:42Z,2022-07-15T07:55:42Z,COLLABORATOR,Thanks @GaryShen2008 ! This is great! We need to do some forensics with @tgravescs to dig out the notebook that was responsible for #3760 to include this test case into our daily notebook testing.   ,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185280792/reactions,0,0,0,0,0,0,0,0,0,5704
375,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1191481989,https://github.com/NVIDIA/spark-rapids/issues/5704#issuecomment-1191481989,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5704,1191481989,IC_kwDOD7z77c5HBJKF,2022-07-21T13:24:37Z,2022-07-21T13:24:37Z,COLLABORATOR,I don't know what notebook showed this so we would have to go back to try to reproduce,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1191481989/reactions,0,0,0,0,0,0,0,0,0,5704
376,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149119307,https://github.com/NVIDIA/spark-rapids/issues/5750#issuecomment-1149119307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5750,1149119307,IC_kwDOD7z77c5EfitL,2022-06-07T20:13:29Z,2022-06-07T20:13:29Z,COLLABORATOR,Related to https://github.com/NVIDIA/spark-rapids/issues/4210,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1149119307/reactions,0,0,0,0,0,0,0,0,0,5750
377,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699796008,https://github.com/NVIDIA/spark-rapids/issues/5758#issuecomment-1699796008,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5758,1699796008,IC_kwDOD7z77c5lUNQo,2023-08-30T20:30:13Z,2023-08-30T20:30:13Z,COLLABORATOR,I am going to unassign myself until this is a higher priority,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699796008/reactions,0,0,0,0,0,0,0,0,0,5758
378,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155701479,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155701479,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1155701479,IC_kwDOD7z77c5E4prn,2022-06-14T20:52:28Z,2022-06-14T20:52:28Z,COLLABORATOR,"@gerashegalov, I am trying to follow these instructions and I can't replicate it so far. I tried local-cluster mode and standalone mode. Could you provide a bit more information:

1. What version of ucx do you have installed (`ucx_info -v`). I am running 1.12.0 locally.
2. I see 5.5 average constantly. Do you ever see that? Or is it 1 or 10 alternating. I am wondering if the issue that you are seeing is on the write, or on the read (or both). It would be nice to get it to happen, and try to inspect the parquet file on the CPU or with the shuffle manger ""off"" (e.g. a read + a show to get its contents).
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155701479/reactions,0,0,0,0,0,0,0,0,0,5818
379,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155736996,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155736996,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1155736996,IC_kwDOD7z77c5E4yWk,2022-06-14T21:39:00Z,2022-06-14T21:56:33Z,COLLABORATOR,"@abellina I downloaded the latest full revision https://github.com/openucx/ucx/tags yesterday 
```
$ ucx_info -v
# UCT version=1.12.1 revision dc92435
# configured with: --disable-logging --disable-debug --disable-assertions --disable-params-check --prefix=/usr --enable-examples --with-java=no
```
I'll try 1.12.0 too.

https://github.com/openucx/ucx/tags

UPDATE downgrading 1.12.0 yields the same buggy behavior
```
$ ucx_info -v
# UCT version=1.12.0 revision d367332
# configured with: --disable-logging --disable-debug --disable-assertions --disable-params-check --prefix=/usr --enable-examples --with-java=no
```

> I see 5.5 average constantly. Do you ever see that? Or is it 1 or 10 alternating. I am wondering if the issue that you are seeing is on the write, or on the read (or both). It would be nice to get it to happen, and try to inspect the parquet file on the CPU or with the shuffle manger ""off"" (e.g. a read + a show to get its contents).

I see 5.5 too occasionally (updated description)

",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155736996/reactions,0,0,0,0,0,0,0,0,0,5818
380,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155770339,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1155770339,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1155770339,IC_kwDOD7z77c5E46fj,2022-06-14T22:26:53Z,2022-06-14T22:26:53Z,COLLABORATOR,@abellina Interestingly this behavior has not reproduced for me yet on a bare-metal Ubuntu20.04 although it reproduces very easily on WSL2 VM Ubuntu20.04,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1155770339/reactions,0,0,0,0,0,0,0,0,0,5818
381,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974078,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1156974078,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1156974078,IC_kwDOD7z77c5E9gX-,2022-06-15T21:40:55Z,2022-06-15T21:40:55Z,COLLABORATOR,"Debugging with h/t @abellina it looks like the workaround is to set ` --conf spark.executorEnv.UCX_TLS=cuda_copy,tcp`

",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974078/reactions,0,0,0,0,0,0,0,0,0,5818
382,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974866,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1156974866,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1156974866,IC_kwDOD7z77c5E9gkS,2022-06-15T21:42:07Z,2022-06-15T21:42:07Z,COLLABORATOR,"```
Wed Jun 15 14:41:09 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.48.03    Driver Version: 516.25       CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
```",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156974866/reactions,0,0,0,0,0,0,0,0,0,5818
383,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159615192,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1159615192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1159615192,IC_kwDOD7z77c5FHlLY,2022-06-19T04:34:24Z,2022-06-19T04:36:01Z,COLLABORATOR,"<details>
<summary>ucx debug log files for the repro run</summary>
<pre>
$ $SPARK_HOME/bin/pyspark   --driver-class-path $PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar    --conf spark.executor.extraClassPath=$PWD/dist/target/rapids-4-spark_2.12-22.08.0-SNAPSHOT-cuda11.jar    --conf spark.plugins=com.nvidia.spark.SQLPlugin    --conf spark.rapids.sql.explain=ALL   --conf spark.shuffle.manager=com.nvidia.spark.rapids.spark321.RapidsShuffleManager   --conf spark.shuffle.service.enabled=false    --conf spark.dynamicAllocation.enabled=false  --conf spark.executorEnv.UCX_ERROR_SIGNALS= --conf spark.executorEnv.UCX_LOG_LEVEL=data --conf spark.executorEnv.UCX_LOG_FILE=/tmp/ucx_log_%p --conf spark.executorEnv.LD_LIBRARY_PATH=$HOME/dist/ucx_debug/lib --conf spark.executorEnv.UCX_MEMTYPE_CACHE=n   --conf spark.rapids.memory.gpu.minAllocFraction=0  --conf spark.rapids.memory.gpu.allocFraction=0.2 --master local-cluster[2,1,1200]
Python 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) 
[GCC 10.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
22/06/18 01:19:08 WARN Utils: Your hostname, NV-3L4YVG3 resolves to a loopback address: 127.0.1.1; using 172.22.19.221 instead (on interface eth0)
22/06/18 01:19:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/06/18 01:19:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/06/18 01:19:10 WARN RapidsPluginUtils: RAPIDS Accelerator 22.08.0-SNAPSHOT using cudf 22.08.0-SNAPSHOT.
22/06/18 01:19:10 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.
22/06/18 01:19:10 WARN RapidsShuffleInternalManager: Rapids Shuffle Plugin enabled. Transport enabled (remote fetches will use com.nvidia.spark.rapids.shuffle.ucx.UCXShuffleTransport. To disable the RAPIDS Shuffle Manager set `spark.rapids.shuffle.enabled` to false
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.8.13 (default, Mar 25 2022 06:04:10)
Spark context Web UI available at http://172.22.19.221:4040/
Spark context available as 'sc' (master = local-cluster[2,1,1200], app id = app-20220618011911-0000).
SparkSession available as 'spark'.
>>> spark.read.format('parquet').load('/tmp/df.parquet').selectExpr('avg(a)').collect()
22/06/18 01:19:23 WARN GpuOverrides:                                            
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#0) will run on GPU
    *Expression <Average> avg(a#0) will run on GPU
  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU
        *Expression <Average> avg(a#0) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:23 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#0) will run on GPU
    *Expression <Average> avg(a#0) will run on GPU
  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU
        *Expression <Average> avg(a#0) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:23 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#0) will run on GPU
    *Expression <Average> avg(a#0) will run on GPU
  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#0) will run on GPU
        *Expression <Average> avg(a#0) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:23 WARN GpuOverrides: 
*Exec <ShuffleExchangeExec> will run on GPU
  *Partitioning <SinglePartition$> will run on GPU
  *Exec <HashAggregateExec> will run on GPU
    *Expression <AggregateExpression> partial_avg(a#0) will run on GPU
      *Expression <Average> avg(a#0) will run on GPU
    *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:25 WARN GpuOverrides: =>                             (1 + 1) / 2]
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#0) will run on GPU
    *Expression <Average> avg(a#0) will run on GPU
  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU

22/06/18 01:19:25 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#0) will run on GPU
    *Expression <Average> avg(a#0) will run on GPU
  *Expression <Alias> avg(a#0)#2 AS avg(a)#3 will run on GPU

[Row(avg(a)=1.0)]                                                               
>>> spark.read.format('parquet').load('/tmp/df.parquet').selectExpr('avg(a)').collect()
22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#31) will run on GPU
    *Expression <Average> avg(a#31) will run on GPU
  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU
        *Expression <Average> avg(a#31) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#31) will run on GPU
    *Expression <Average> avg(a#31) will run on GPU
  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU
        *Expression <Average> avg(a#31) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#31) will run on GPU
    *Expression <Average> avg(a#31) will run on GPU
  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU
  *Exec <ShuffleExchangeExec> will run on GPU
    *Partitioning <SinglePartition$> will run on GPU
    *Exec <HashAggregateExec> will run on GPU
      *Expression <AggregateExpression> partial_avg(a#31) will run on GPU
        *Expression <Average> avg(a#31) will run on GPU
      *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <ShuffleExchangeExec> will run on GPU
  *Partitioning <SinglePartition$> will run on GPU
  *Exec <HashAggregateExec> will run on GPU
    *Expression <AggregateExpression> partial_avg(a#31) will run on GPU
      *Expression <Average> avg(a#31) will run on GPU
    *Exec <FileSourceScanExec> will run on GPU

22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#31) will run on GPU
    *Expression <Average> avg(a#31) will run on GPU
  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU

22/06/18 01:19:27 WARN GpuOverrides: 
*Exec <HashAggregateExec> will run on GPU
  *Expression <AggregateExpression> avg(a#31) will run on GPU
    *Expression <Average> avg(a#31) will run on GPU
  *Expression <Alias> avg(a#31)#33 AS avg(a)#34 will run on GPU

[Row(avg(a)=10.0)]
</pre>
</details>

[ucx_log_sanitized.zip](https://github.com/NVIDIA/spark-rapids/files/8934760/ucx_log_sanitized.zip)
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1159615192/reactions,0,0,0,0,0,0,0,0,0,5818
384,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162099203,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1162099203,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1162099203,IC_kwDOD7z77c5FRDoD,2022-06-21T17:52:34Z,2022-06-21T17:52:34Z,COLLABORATOR,"@gerashegalov the UCX debug logs are useful, but I would like to see corresponding executor logs as well (especially in standalone). If you can turn the log level to DEBUG in spark that would be really great.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162099203/reactions,0,0,0,0,0,0,0,0,0,5818
385,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162513861,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1162513861,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1162513861,IC_kwDOD7z77c5FSo3F,2022-06-22T01:11:41Z,2022-06-22T01:11:41Z,COLLABORATOR,"Repro logs with the DEBUG level Spark logs @abellina  [ucx_log_sanitized.zip](https://github.com/NVIDIA/spark-rapids/files/8953661/ucx_log_sanitized.zip)
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162513861/reactions,0,0,0,0,0,0,0,0,0,5818
386,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163113472,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1163113472,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1163113472,IC_kwDOD7z77c5FU7QA,2022-06-22T13:41:23Z,2022-06-22T13:41:23Z,COLLABORATOR,"OK, thanks for the logs! With @gerashegalov's help I was able to setup WSL2 and repro this in a windows machine.

The issue is specific to cuda_ipc with wakeup, which is using events to trigger when a send/recv is finished. What looks to be happening is that we are getting a callback from UCX even though the copy is not complete, which explains why it sometimes works. This is our default mode, rather than busy waiting, since we've found it to be better for performance and we know it works in other environments. Setting: `--conf spark.rapids.shuffle.ucx.useWakeup=false` (turning off wakeup and relying on busy waiting), is a potential workaround for WSL2, but it's not great. This looks to be a bug in UCX or CUDA (callback functions).

I added some logging to capture the batch being received and the corresponding sent batch. This is the batch at the receiving end:

```
DEBUG fetched cb Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 27 7f77914ebbb0)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 28 7f77914fffd0)}], cudfTable=140151513387040, rows=1}
GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000200, length=8, id=-1} VAL: null
COLUMN 0 - FLOAT64
0 0.0
GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000240, length=8, id=-1} VAL: null
COLUMN 1 - INT64
0 0
```

But the sender sent this (before UCX send):

```
DEBUG to send Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 25 7f160808ea00)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 26 7f160808eb70)}], cudfTable=139732600243792, rows=1}
GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000000, length=8, id=-1} VAL: null
COLUMN 0 - FLOAT64
0 10.0
GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000040, length=8, id=-1} VAL: null
COLUMN 1 - INT64
0 1
```

I believe the next step is to see if the above explanation makes sense to @Akshay-Venkatesh, then we may need to help repro at a lower level or help test a fix.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163113472/reactions,1,1,0,0,0,0,0,0,0,5818
387,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178400825,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1178400825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1178400825,IC_kwDOD7z77c5GPPg5,2022-07-08T00:28:03Z,2022-07-08T00:28:03Z,COLLABORATOR,Removing P1 and removing from 22.08 since the issue only occurs in WSL2 (which we do not support).  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178400825/reactions,0,0,0,0,0,0,0,0,0,5818
388,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180674173,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1180674173,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1180674173,IC_kwDOD7z77c5GX6h9,2022-07-11T17:26:23Z,2022-07-11T17:26:35Z,NONE,"> OK, thanks for the logs! With @gerashegalov's help I was able to setup WSL2 and repro this in a windows machine.
> 
> The issue is specific to cuda_ipc with wakeup, which is using events to trigger when a send/recv is finished. What looks to be happening is that we are getting a callback from UCX even though the copy is not complete, which explains why it sometimes works. This is our default mode, rather than busy waiting, since we've found it to be better for performance and we know it works in other environments. Setting: `--conf spark.rapids.shuffle.ucx.useWakeup=false` (turning off wakeup and relying on busy waiting), is a potential workaround for WSL2, but it's not great. This looks to be a bug in UCX or CUDA (callback functions).
> 
> I added some logging to capture the batch being received and the corresponding sent batch. This is the batch at the receiving end:
> 
> ```
> DEBUG fetched cb Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 27 7f77914ebbb0)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 28 7f77914fffd0)}], cudfTable=140151513387040, rows=1}
> GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000200, length=8, id=-1} VAL: null
> COLUMN 0 - FLOAT64
> 0 0.0
> GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000240, length=8, id=-1} VAL: null
> COLUMN 1 - INT64
> 0 0
> ```
> 
> But the sender sent this (before UCX send):
> 
> ```
> DEBUG to send Table{columns=[ColumnVector{rows=1, type=FLOAT64, nullCount=Optional.empty, offHeap=(ID: 25 7f160808ea00)}, ColumnVector{rows=1, type=INT64, nullCount=Optional.empty, offHeap=(ID: 26 7f160808eb70)}], cudfTable=139732600243792, rows=1}
> GPU COLUMN 0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000000, length=8, id=-1} VAL: null
> COLUMN 0 - FLOAT64
> 0 10.0
> GPU COLUMN 1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x800000040, length=8, id=-1} VAL: null
> COLUMN 1 - INT64
> 0 1
> ```
> 
> I believe the next step is to see if the above explanation makes sense to @Akshay-Venkatesh, then we may need to help repro at a lower level or help test a fix.

@abellina Sorry for the late response but do you happen to know if this issue is specific to WSL2? Want to know if there is an identical linux setup where the issue doesn't show up.",,Akshay-Venkatesh,3201794,MDQ6VXNlcjMyMDE3OTQ=,https://avatars.githubusercontent.com/u/3201794?v=4,,https://api.github.com/users/Akshay-Venkatesh,https://github.com/Akshay-Venkatesh,https://api.github.com/users/Akshay-Venkatesh/followers,https://api.github.com/users/Akshay-Venkatesh/following{/other_user},https://api.github.com/users/Akshay-Venkatesh/gists{/gist_id},https://api.github.com/users/Akshay-Venkatesh/starred{/owner}{/repo},https://api.github.com/users/Akshay-Venkatesh/subscriptions,https://api.github.com/users/Akshay-Venkatesh/orgs,https://api.github.com/users/Akshay-Venkatesh/repos,https://api.github.com/users/Akshay-Venkatesh/events{/privacy},https://api.github.com/users/Akshay-Venkatesh/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180674173/reactions,0,0,0,0,0,0,0,0,0,5818
389,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180774586,https://github.com/NVIDIA/spark-rapids/issues/5818#issuecomment-1180774586,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5818,1180774586,IC_kwDOD7z77c5GYTC6,2022-07-11T19:19:58Z,2022-07-11T19:19:58Z,COLLABORATOR,@Akshay-Venkatesh we haven't been able to reproduce the issue on available bare-metal Ubuntu environments. So it may be related to either virtualization aspect of WSL2 or to the Runtime/Driver combination aspect of WSL2. We statically link 11.5 CUDA Runtime.  ,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1180774586/reactions,0,0,0,0,0,0,0,0,0,5818
390,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156636288,https://github.com/NVIDIA/spark-rapids/issues/5839#issuecomment-1156636288,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5839,1156636288,IC_kwDOD7z77c5E8N6A,2022-06-15T15:44:22Z,2022-06-15T15:44:22Z,COLLABORATOR,"To be clear this only happens when you let Spark do schema discovery for CSV or JSON. For CSV it is at least sub-sampling the data, but for JSON it reads and parses all of the data to determine the schema. This is horribly slow in either case and is something that generally is avoided by anyone in production. If we are documenting this we should explain the situation that is happening and why we have not optimized this use case.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1156636288/reactions,0,0,0,0,0,0,0,0,0,5839
391,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162282867,https://github.com/NVIDIA/spark-rapids/issues/5881#issuecomment-1162282867,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5881,1162282867,IC_kwDOD7z77c5FRwdz,2022-06-21T20:07:39Z,2022-06-21T20:07:39Z,COLLABORATOR,"This might be useful as part of the profiling tool.  The tool could print the diff between what was expected vs. the actual configs used in the cluster. 

cc: @mattahrens ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1162282867/reactions,0,0,0,0,0,0,0,0,0,5881
392,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1167917631,https://github.com/NVIDIA/spark-rapids/issues/5884#issuecomment-1167917631,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5884,1167917631,IC_kwDOD7z77c5FnQI_,2022-06-27T21:23:53Z,2022-06-27T21:23:53Z,COLLABORATOR,Related cudf issue https://github.com/rapidsai/cudf/issues/9987,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1167917631/reactions,0,0,0,0,0,0,0,0,0,5884
393,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163567167,https://github.com/NVIDIA/spark-rapids/issues/5887#issuecomment-1163567167,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5887,1163567167,IC_kwDOD7z77c5FWqA_,2022-06-22T20:26:14Z,2022-06-22T20:26:14Z,MEMBER,"Note that `closeOnExcept`, `freeOnExcept`, and `withResourceIfAllowed` all have similar issues with respect to closures.  It may make sense to convert `Arm` into a standalone object rather than a trait since these functions do not need to be member functions.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163567167/reactions,0,0,0,0,0,0,0,0,0,5887
394,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163573401,https://github.com/NVIDIA/spark-rapids/issues/5888#issuecomment-1163573401,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5888,1163573401,IC_kwDOD7z77c5FWriZ,2022-06-22T20:33:25Z,2022-06-22T20:33:25Z,MEMBER,"Note that Apache Spark has the same issue with the `HashJoin` trait that provides the `join` method that is invoked on the executor side.

Moving the `doJoin` code to a standalone object (and providing the missing method values as additional arguments to the method) resolves the issue, although this may not be the cleanest solution.  There are already a ton of arguments to the method as it is.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163573401/reactions,0,0,0,0,0,0,0,0,0,5888
395,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163584994,https://github.com/NVIDIA/spark-rapids/issues/5889#issuecomment-1163584994,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5889,1163584994,IC_kwDOD7z77c5FWuXi,2022-06-22T20:47:00Z,2022-06-22T20:47:00Z,MEMBER,"Applying the following patch appears to fix it, but I'm not sure if there's any ramifications for this change:
```patch
diff --git a/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuScalarSubquery.scala b/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuScalarSubquery.scala
index be494bbbd..61098f410 100644
--- a/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuScalarSubquery.scala
+++ b/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuScalarSubquery.scala
@@ -31,11 +31,11 @@ import org.apache.spark.sql.vectorized.ColumnarBatch
  * other GPU overrides.
  */
 case class GpuScalarSubquery(
-    plan: BaseSubqueryExec,
+    @transient plan: BaseSubqueryExec,
     exprId: ExprId)
   extends ExecSubqueryExpression with GpuExpression with ShimExpression {
 
-  override def dataType: DataType = plan.schema.fields.head.dataType
+  override lazy val dataType: DataType = plan.schema.fields.head.dataType
   override def children: Seq[Expression] = Seq.empty
   override def nullable: Boolean = true
   override def toString: String = plan.simpleString(SQLConf.get.maxToStringFields)
```

Note that Apache Spark's `ScalarSubquery` has the same issue.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1163584994/reactions,0,0,0,0,0,0,0,0,0,5889
396,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169288920,https://github.com/NVIDIA/spark-rapids/issues/5899#issuecomment-1169288920,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5899,1169288920,IC_kwDOD7z77c5Fse7Y,2022-06-28T21:39:27Z,2022-06-28T21:39:27Z,COLLABORATOR,"Confirmed with @tgravescs that this would be a general enhancement (not specific to tools) to surface reader that was used.  Tools may need a separate enhancement to pick up update, but that could be determined after this implementation is complete.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169288920/reactions,0,0,0,0,0,0,0,0,0,5899
397,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1227578119,https://github.com/NVIDIA/spark-rapids/issues/5899#issuecomment-1227578119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5899,1227578119,IC_kwDOD7z77c5JK1sH,2022-08-25T17:41:43Z,2022-08-25T17:41:43Z,COLLABORATOR,"I believe we should try to normalize the metrics so that they do mean the same. We can probably add a note in our documentation on how we did it, but when I look at `bufferTime`, I really expect to see the time the task spent buffering, or in the case of a backgrounded process, I think a compromise may be the amount of time blocked on buffering (not unlike fetch wait time for shuffle).",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1227578119/reactions,0,0,0,0,0,0,0,0,0,5899
398,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1243981184,https://github.com/NVIDIA/spark-rapids/issues/5899#issuecomment-1243981184,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5899,1243981184,IC_kwDOD7z77c5KJaWA,2022-09-12T16:21:55Z,2022-09-12T16:22:45Z,COLLABORATOR,"We are not sure all the metrics are clear now, so lets reopen this.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1243981184/reactions,0,0,0,0,0,0,0,0,0,5899
399,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169205492,https://github.com/NVIDIA/spark-rapids/issues/5908#issuecomment-1169205492,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5908,1169205492,IC_kwDOD7z77c5FsKj0,2022-06-28T20:31:37Z,2022-06-28T20:31:37Z,COLLABORATOR,We probably need to fix issue #5152 to add an `origin` context to accurately throw this error. ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169205492/reactions,0,0,0,0,0,0,0,0,0,5908
400,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178433881,https://github.com/NVIDIA/spark-rapids/issues/5919#issuecomment-1178433881,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5919,1178433881,IC_kwDOD7z77c5GPXlZ,2022-07-08T01:26:01Z,2022-07-08T01:26:01Z,COLLABORATOR,Shouldn't this be a cudf issue if JNI support is needed?,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1178433881/reactions,0,0,0,0,0,0,0,0,0,5919
401,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169436145,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1169436145,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1169436145,IC_kwDOD7z77c5FtC3x,2022-06-29T01:22:36Z,2022-06-29T01:25:33Z,COLLABORATOR,"Yeah, we could add this feature.

how should we resolve conflicts between title suffix and label, e.g.
title contains [skip ci], but label have `databricks`, should we trigger databricks CI or simply skip ci?

Currently if title only, `skip ci` always win against other phrases
We may require some priority for title + label cases, any suggestion? @sameerz @razajafri
",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169436145/reactions,0,0,0,0,0,0,0,0,0,5928
402,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169448626,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1169448626,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1169448626,IC_kwDOD7z77c5FtF6y,2022-06-29T01:53:12Z,2022-06-29T01:53:12Z,COLLABORATOR,@pxLi can we replace the title trigger with labels triggers only? ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169448626/reactions,0,0,0,0,0,0,0,0,0,5928
403,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169449763,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1169449763,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1169449763,IC_kwDOD7z77c5FtGMj,2022-06-29T01:55:49Z,2022-06-29T01:56:20Z,COLLABORATOR,"> @pxLi can we replace the title trigger with labels triggers only?

yes, we could. But better to have an interim period. So we could keep both but label have higher priority during this period , and at last we remove the title support",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169449763/reactions,0,0,0,0,0,0,0,0,0,5928
404,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169526165,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1169526165,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1169526165,IC_kwDOD7z77c5FtY2V,2022-06-29T04:39:47Z,2022-06-29T04:40:31Z,COLLABORATOR,"IMO `skip ci` should take precedence over `databricks`. 

Yes, we should have a period of overlap
",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1169526165/reactions,0,0,0,0,0,0,0,0,0,5928
405,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1170018637,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1170018637,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1170018637,IC_kwDOD7z77c5FvRFN,2022-06-29T14:00:11Z,2022-06-29T14:00:11Z,MEMBER,"Here's my $0.02:
- There should be no ""priority"" between title tags and labels, they are effectively equivalent in every way.  Directives from title tags and labels should be acted on regardless of where they originated.
- The skip CI directive skips all CI regardless of other directives.  Bonus points if the skipping is as visible as possible in the Github PR checks summary.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1170018637/reactions,1,1,0,0,0,0,0,0,0,5928
406,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1176966128,https://github.com/NVIDIA/spark-rapids/issues/5928#issuecomment-1176966128,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5928,1176966128,IC_kwDOD7z77c5GJxPw,2022-07-07T02:16:56Z,2022-07-07T02:17:22Z,COLLABORATOR,"I will start working on this, we will have our own shared jenkins github lib shortly instead of blossom-provided one.

Also It might be a good time we move https://github.com/NVIDIA/spark-rapids/blob/branch-22.08/jenkins/Jenkinsfile-blossom.premerge back to internal gitlab repo for better development, maintenance and previous security concern",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1176966128/reactions,0,0,0,0,0,0,0,0,0,5928
407,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1223645987,https://github.com/NVIDIA/spark-rapids/issues/5944#issuecomment-1223645987,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5944,1223645987,IC_kwDOD7z77c5I71sj,2022-08-23T07:10:12Z,2022-08-23T07:10:12Z,COLLABORATOR,"@jlowe is this issue talking about the error when I try to run a  ""DELETE"" query in Iceberg:
```
: java.lang.IllegalStateException: Row-based execution should not occur for this class
        at com.nvidia.spark.rapids.GpuHashAggregateExec.doExecute(aggregate.scala:1532)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at com.nvidia.spark.rapids.GpuBringBackToHost.doExecute(GpuBringBackToHost.scala:38)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)
        at org.apache.spark.sql.execution.datasources.v2.DynamicFileFilterExec.doPrepare(DynamicFileFilterExec.scala:103)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:267)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$prepare$1$adapted(SparkPlan.scala:263)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:263)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:216)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)
        at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
        at org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.writeWithV2(ReplaceDataExec.scala:26)
        at org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(ReplaceDataExec.scala:34)
        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
        at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)
        at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)
        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
        at sun.reflect.GeneratedMethodAccessor120.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:750)

```",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1223645987/reactions,0,0,0,0,0,0,0,0,0,5944
408,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224079820,https://github.com/NVIDIA/spark-rapids/issues/5944#issuecomment-1224079820,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5944,1224079820,IC_kwDOD7z77c5I9fnM,2022-08-23T13:29:36Z,2022-08-23T14:15:16Z,MEMBER,"@wjxiz1992 that exception is a completely different issue.  This is about handling `merge-on-read` mode for Iceberg, where it can encode row deletes in metadata rather than rewriting data partitions to remove the rows.  That does not mean we cannot support DELETE operations, as v1 style deletes where it rewrites the data partitions is supported.

The exception you're getting above has to do with writes crashing and does not seem to have anything to do with reads. Please file a separate issue to track this.

From the stacktrace, @revans2 noticed there's an issue with `GpuBringBackToHost` not handling `doExecute` properly, so I filed #6397 to track.  Fixing that may resolve your issue, but we should track your issue separately in case it does not.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224079820/reactions,0,0,0,0,0,0,0,0,0,5944
409,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179293554,https://github.com/NVIDIA/spark-rapids/issues/5973#issuecomment-1179293554,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5973,1179293554,IC_kwDOD7z77c5GSpdy,2022-07-08T19:19:49Z,2022-07-08T19:19:49Z,COLLABORATOR,"This is for a runtime replaceable operator. So if we support RegExpExtract, then there is nothing more we need to do. We might want to add a test for RegExpSubString just to be sure that the replacements are happening as we expect them to. But it should just work out of the box.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179293554/reactions,0,0,0,0,0,0,0,0,0,5973
410,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185987593,https://github.com/NVIDIA/spark-rapids/issues/5973#issuecomment-1185987593,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5973,1185987593,IC_kwDOD7z77c5GsLwJ,2022-07-15T22:10:26Z,2022-07-15T22:10:26Z,CONTRIBUTOR,"Blocked until the build can compile, when https://github.com/NVIDIA/spark-rapids/issues/5806, https://github.com/NVIDIA/spark-rapids/issues/5807, and https://github.com/NVIDIA/spark-rapids/issues/5827 are resolved",,anthony-chang,54450499,MDQ6VXNlcjU0NDUwNDk5,https://avatars.githubusercontent.com/u/54450499?v=4,,https://api.github.com/users/anthony-chang,https://github.com/anthony-chang,https://api.github.com/users/anthony-chang/followers,https://api.github.com/users/anthony-chang/following{/other_user},https://api.github.com/users/anthony-chang/gists{/gist_id},https://api.github.com/users/anthony-chang/starred{/owner}{/repo},https://api.github.com/users/anthony-chang/subscriptions,https://api.github.com/users/anthony-chang/orgs,https://api.github.com/users/anthony-chang/repos,https://api.github.com/users/anthony-chang/events{/privacy},https://api.github.com/users/anthony-chang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1185987593/reactions,0,0,0,0,0,0,0,0,0,5973
411,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179452130,https://github.com/NVIDIA/spark-rapids/issues/5975#issuecomment-1179452130,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975,1179452130,IC_kwDOD7z77c5GTQLi,2022-07-09T01:15:29Z,2022-07-09T01:15:29Z,NONE,"I guess it probably be issue in `ParquetCachedBatchSerializer`?
Is it relates to xgboost pyspark integration code ?",,WeichenXu123,19235986,MDQ6VXNlcjE5MjM1OTg2,https://avatars.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179452130/reactions,0,0,0,0,0,0,0,0,0,5975
412,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179453501,https://github.com/NVIDIA/spark-rapids/issues/5975#issuecomment-1179453501,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975,1179453501,IC_kwDOD7z77c5GTQg9,2022-07-09T01:23:26Z,2022-07-09T01:23:26Z,CONTRIBUTOR,It is not specific to the xgboost pyspark code.    Just happened to encounter the issue when trying that.  ,,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179453501/reactions,0,0,0,0,0,0,0,0,0,5975
413,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179453949,https://github.com/NVIDIA/spark-rapids/issues/5975#issuecomment-1179453949,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5975,1179453949,IC_kwDOD7z77c5GTQn9,2022-07-09T01:26:33Z,2022-07-09T01:26:33Z,NONE,"> It is not specific to the xgboost pyspark code. Just happened to encounter the issue when trying that.

But happy to see you tried the xgboost pyspark code. 
If you found any performance issue pls reported to me.
Thanks!
",,WeichenXu123,19235986,MDQ6VXNlcjE5MjM1OTg2,https://avatars.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179453949/reactions,0,0,0,0,0,0,0,0,0,5975
414,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179988878,https://github.com/NVIDIA/spark-rapids/issues/5980#issuecomment-1179988878,https://api.github.com/repos/NVIDIA/spark-rapids/issues/5980,1179988878,IC_kwDOD7z77c5GVTOO,2022-07-11T05:41:26Z,2022-07-11T08:30:44Z,COLLABORATOR,"The fix should depend on https://github.com/NVIDIA/spark-rapids/pull/5960, and better to at least support type conversion between string and integral types for the example here.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1179988878/reactions,0,0,0,0,0,0,0,0,0,5980
415,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1184715726,https://github.com/NVIDIA/spark-rapids/issues/6002#issuecomment-1184715726,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6002,1184715726,IC_kwDOD7z77c5GnVPO,2022-07-14T17:28:36Z,2022-07-14T17:28:36Z,COLLABORATOR,Turning the feature on by default may negatively impact performance.  We would need to investigate.  Related issue https://github.com/NVIDIA/spark-rapids/issues/4580,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1184715726/reactions,0,0,0,0,0,0,0,0,0,6002
416,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189506123,https://github.com/NVIDIA/spark-rapids/issues/6023#issuecomment-1189506123,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6023,1189506123,IC_kwDOD7z77c5G5mxL,2022-07-19T20:11:12Z,2022-07-19T20:11:12Z,COLLABORATOR,"If we work on this, we should split `url_encode` and `url_decode` into one issue, and `parse_url` into another issue. ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1189506123/reactions,0,0,0,0,0,0,0,0,0,6023
417,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550519507,https://github.com/NVIDIA/spark-rapids/issues/6023#issuecomment-1550519507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6023,1550519507,IC_kwDOD7z77c5cawzT,2023-05-17T00:25:34Z,2023-05-17T00:25:34Z,COLLABORATOR,`parse_url` is handled by https://github.com/NVIDIA/spark-rapids/issues/6969,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550519507/reactions,0,0,0,0,0,0,0,0,0,6023
418,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984558516,https://github.com/NVIDIA/spark-rapids/issues/6095#issuecomment-1984558516,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6095,1984558516,IC_kwDOD7z77c52SfW0,2024-03-07T21:48:07Z,2024-03-07T21:48:07Z,MEMBER,@abellina is this still needed or does the datagen seed fuzzing add sufficient coverage over time?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984558516/reactions,0,0,0,0,0,0,0,0,0,6095
419,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298205909,https://github.com/NVIDIA/spark-rapids/issues/6105#issuecomment-1298205909,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6105,1298205909,IC_kwDOD7z77c5NYQzV,2022-11-01T08:26:35Z,2022-11-01T08:26:35Z,COLLABORATOR,"Hi @jlowe ,I have a task to do NDS benchmarks on Dataproc with its latest Spark3.3.0 preview image. Do we have any further plan to support Iceberg? Iceberg has been on it's 1.0.0 release.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298205909/reactions,0,0,0,0,0,0,0,0,0,6105
420,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298474399,https://github.com/NVIDIA/spark-rapids/issues/6105#issuecomment-1298474399,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6105,1298474399,IC_kwDOD7z77c5NZSWf,2022-11-01T13:02:45Z,2022-11-01T13:02:45Z,MEMBER,We've been busy recently focusing on adding support for other formats and other features.  I'll defer to @sameerz for prioritization of this feature.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298474399/reactions,0,0,0,0,0,0,0,0,0,6105
421,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299429161,https://github.com/NVIDIA/spark-rapids/issues/6105#issuecomment-1299429161,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6105,1299429161,IC_kwDOD7z77c5Nc7cp,2022-11-02T01:29:09Z,2022-11-02T01:29:09Z,COLLABORATOR,@wjxiz1992 you will need to build a jar with 0.14.x just for the test.  We do not have a near term plan to add support for 0.14.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299429161/reactions,1,0,0,0,0,0,0,0,1,6105
422,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1202922081,https://github.com/NVIDIA/spark-rapids/issues/6149#issuecomment-1202922081,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6149,1202922081,IC_kwDOD7z77c5HsyJh,2022-08-02T16:08:14Z,2022-08-02T16:08:14Z,MEMBER,"Note that for the CHAR type, casting to a string requires stripping the trailing whitespaces from the value to match the CPU behavior.  See https://github.com/NVIDIA/spark-rapids/pull/6188#discussion_r935683053.  It would be nice if we could ask libcudf to load the CHAR column by stripping trailing whitespace instead of adding it, so we don't have to perform a post-processing step on the CHAR columns.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1202922081/reactions,1,0,0,0,0,0,0,0,1,6149
423,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1210457354,https://github.com/NVIDIA/spark-rapids/issues/6149#issuecomment-1210457354,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6149,1210457354,IC_kwDOD7z77c5IJh0K,2022-08-10T10:09:39Z,2022-09-06T07:12:36Z,CONTRIBUTOR,"I divide these castings into these subcategories, according to the source type.

- [X] `integer -> integer` (It was done in #5960 .)
- [x] `bool/int8/int16/int32/int64 -> {string, float, double(float64), timestamp}`. 
  + Issue #6272 
  + PR #6273 
----
- [x] `float32 -> {bool, integer types, double, string, timestamp}`
- [x] `double  -> {bool, integer types, float32, string, timestamp}`
  + Issue #6291
  + PR #6319
----
- [ ] [Pending] `date -> {string, timestamp}`
  + Issue #6357 
  + [branch: orc-cast-date](https://github.com/sinkinben/spark-rapids/commit/c293a1173905670ecdcd85b8ca82a54b199f7109)
----
- [ ] [Pending] `string  -> {bool, integer types, float32, double, timestamp, date}`
  + Issue #6394 
  + Draft PR #6411 

----

- [ ] `timestamp -> {integer types, float32, double, string, date}`


Two special case:
- [ ] `decimal <-> {bool, integer types, float, double, string, timestamp}`
- [ ] `binary <-> string`

Whitespaces of char/varchar/string should be paied attention to, which is mentioned above.
",,sinkinben,31923950,MDQ6VXNlcjMxOTIzOTUw,https://avatars.githubusercontent.com/u/31923950?v=4,,https://api.github.com/users/sinkinben,https://github.com/sinkinben,https://api.github.com/users/sinkinben/followers,https://api.github.com/users/sinkinben/following{/other_user},https://api.github.com/users/sinkinben/gists{/gist_id},https://api.github.com/users/sinkinben/starred{/owner}{/repo},https://api.github.com/users/sinkinben/subscriptions,https://api.github.com/users/sinkinben/orgs,https://api.github.com/users/sinkinben/repos,https://api.github.com/users/sinkinben/events{/privacy},https://api.github.com/users/sinkinben/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1210457354/reactions,0,0,0,0,0,0,0,0,0,6149
424,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1217733527,https://github.com/NVIDIA/spark-rapids/issues/6149#issuecomment-1217733527,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6149,1217733527,IC_kwDOD7z77c5IlSOX,2022-08-17T09:19:05Z,2022-08-31T09:21:24Z,CONTRIBUTOR,"## A Summary of Implementation Details

**Casting from Integer Types**

| Casting                                | Implementation Description                                   |
| :------------------------------------- | :----------------------------------------------------------- |
| `bool -> float/double`                 | Based on `ColumnVector.castTo` in cuDF.                      |
| `bool -> string`                       | Call `castTo`, and convert them into upper cases `TRUE/FALSE` (as CPU code did). |
| `int8/16/32/64 -> float/double/string` | Call `castTo`                                                |
| `bool/int8/16/32 -> timestamp`         | The original value is in seconds, and convert them into micro-seconds. Since `timestamp` is stored in `int64`, there is no integer-overflow. |
| `int64 -> timestamp`                   | 1. From spark311 until spark320 (inluding 311, 312, 313, 314), they consider the integers as milliseconds when casting integers to timestamp. <br />2. For spark320+ (including spark320), they consider the integers as seconds.<br />3. For both cases, convert them to microseconds. |





**Casting from Float types**

| Casting                                 | Implementation Description                                   |
| --------------------------------------- | ------------------------------------------------------------ |
| `float/double -> {bool, int8/16/32/64}` | 1. First replace rows that cannot fit in long with nulls.<br />2. Convert the ColumnVector to Long type<br />3. Down cast long to the target integral type. |
| `float <-> double`                      | 1. Call `ColumnView.castTo`. <br />2. When casting `double -> float`, if `double` value is greater than `FLOAT_MAX`, then mark this value with `Infinite`. |
| `float/double -> string`                | 1. cuDF keep 9 decimal numbers after the decimal point, and CPU keeps more than 10.<br />2. Added a config item `spark.rapids.sql.format.orc.floatTypesToString.enable` (default value is true) to control whether if we can cast `float/double -> string` while reading ORC. |
| `float/double -> timestamp`             | 1. ORC assumes the original `float/double` values are in seconds.<br />2. If `ROUND(val * 1000) > LONG_MAX` , replace it with null, e.g. `val = 1e20`. Otherwise, keep these values, and convert them into milli-seonds vector.<br />3. Multiply 1000, convert them into micro-seconds vector. Pay attention to long(INT64) overflow here, since timestamp is stored in `INT64`. |


**Casting from string**

| Casting                        | Implementation Description                                   |
| ------------------------------ | ------------------------------------------------------------ |
| `string -> bool/int8/16/32/64` | 1. Check the pattern of input strings by regular expression, replace invalid strings with `null`.<br />2. Follow the CPU ORC conversion. Firstly convert string to long, then down cast long to target integral type.<br />3. For `string -> bool`, cases `""true"",""false` are invalid, they should be `""0"", ""1""`. |
| `string -> float/double`       | 1. Check the pattern of input strings by regex. The leading/trailing spaces should be ignored. Replace the invalid strings with `null`.<br />2. Call `castTo(FLOAT)` or `castTo(DOUBLE)` in cuDF. |
| `string -> date/timestamp`     | Working on it.                                               |


**Casting from Date types (TODO)**

| Casting             | Implementation Description                                   |
| ------------------- | ------------------------------------------------------------ |
| `date -> string`    | Call `ColumnView.asString()`, and it will call `asStrings(""%Y-%m-%d"")` inside. |
| `date -> timestamp` | 1. Convert the `date` columnar vector into `INT64` type.<br />2. Multiply it with `24 * 60 * 60 * 1e6`, and then convert it into `TIMESTAMP_MICROSECONDS`. |


However, there are still some issues. For more details, see the comments in #6357 .

Here is the [Code branch](https://github.com/sinkinben/spark-rapids/commit/c293a1173905670ecdcd85b8ca82a54b199f7109).
",,sinkinben,31923950,MDQ6VXNlcjMxOTIzOTUw,https://avatars.githubusercontent.com/u/31923950?v=4,,https://api.github.com/users/sinkinben,https://github.com/sinkinben,https://api.github.com/users/sinkinben/followers,https://api.github.com/users/sinkinben/following{/other_user},https://api.github.com/users/sinkinben/gists{/gist_id},https://api.github.com/users/sinkinben/starred{/owner}{/repo},https://api.github.com/users/sinkinben/subscriptions,https://api.github.com/users/sinkinben/orgs,https://api.github.com/users/sinkinben/repos,https://api.github.com/users/sinkinben/events{/privacy},https://api.github.com/users/sinkinben/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1217733527/reactions,0,0,0,0,0,0,0,0,0,6149
425,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1231220260,https://github.com/NVIDIA/spark-rapids/issues/6149#issuecomment-1231220260,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6149,1231220260,IC_kwDOD7z77c5JYu4k,2022-08-30T06:45:32Z,2022-08-30T06:46:06Z,CONTRIBUTOR,"As the discussion mentioned in https://github.com/apache/orc/issues/1237, 
> Both Apache Spark and ORC community recommend to use explicit SQL CAST method instead of depending on data source's Schema Evolution.

That is we can replace Schema evolution with `CAST` in SQL. 

For example, if we have an ORC file, it contains one column `date_str`, and `date_str` is some strings with a pattern of `YYYY-mm-dd`.
```shell
# Read `date_str` in type of string, do not use schema evolution
scala> var df = spark.read.schema(""date_str string"").orc(""/tmp/orc/data.orc"");
scala> df.show()
+----------+
|  date_str|
+----------+
|2002-01-01|
|2022-08-29|
|2022-08-31|
|2022-01-32|
|9808-02-30|
|2022-06-31|
+----------+
# Cast `date_str` to type of `date`, using SQL-CAST
scala> df.registerTempTable(""table"")
scala> df.sqlContext.sql(""select CAST(date_str as date) from table"").show()
+----------+
|  date_str|
+----------+
|2002-01-01|
|2022-08-29|
|2022-08-31|
|      null|
|      null|
|      null|
+----------+

```",,sinkinben,31923950,MDQ6VXNlcjMxOTIzOTUw,https://avatars.githubusercontent.com/u/31923950?v=4,,https://api.github.com/users/sinkinben,https://github.com/sinkinben,https://api.github.com/users/sinkinben/followers,https://api.github.com/users/sinkinben/following{/other_user},https://api.github.com/users/sinkinben/gists{/gist_id},https://api.github.com/users/sinkinben/starred{/owner}{/repo},https://api.github.com/users/sinkinben/subscriptions,https://api.github.com/users/sinkinben/orgs,https://api.github.com/users/sinkinben/repos,https://api.github.com/users/sinkinben/events{/privacy},https://api.github.com/users/sinkinben/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1231220260/reactions,0,0,0,0,0,0,0,0,0,6149
426,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1207752588,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1207752588,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1207752588,IC_kwDOD7z77c5H_NeM,2022-08-08T07:12:17Z,2022-08-08T07:12:17Z,COLLABORATOR,"@NvTimLiu to help investigate if any good solution, thanks~",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1207752588/reactions,0,0,0,0,0,0,0,0,0,6252
427,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1212627623,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1212627623,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1212627623,IC_kwDOD7z77c5IRzqn,2022-08-12T00:51:16Z,2022-08-12T00:51:16Z,COLLABORATOR,"There are some variables need to be initialized, checking",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1212627623/reactions,0,0,0,0,0,0,0,0,0,6252
428,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1216590170,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1216590170,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1216590170,IC_kwDOD7z77c5Ig7Fa,2022-08-16T12:47:30Z,2022-08-16T12:47:30Z,COLLABORATOR,"To run `spark-nightly-build.sh` , you need to install build environment on your host as described in: 
 https://github.com/NVIDIA/spark-rapids/blob/branch-22.10/jenkins/Dockerfile-blossom.ubuntu

To run `spark-tests.sh` ,  you need to install build environment on your host as described in (similar for centos/rocky): 
https://github.com/NVIDIA/spark-rapids/blob/branch-22.10/jenkins/Dockerfile-blossom.integration.ubuntu
I mean not building jars head with `spark-nightly-build.sh` 

The best practice is to build out the docker images with the docker files, the run build/test inside the docker images.


I could SUCCESSFULLY run both the nightly scripts locally.

https://github.com/NVIDIA/spark-rapids/blob/branch-22.10/jenkins/spark-nightly-build.sh
https://github.com/NVIDIA/spark-rapids/blob/branch-22.10/jenkins/spark-tests.sh

To run `spark-tests.sh`, you need to build jars with `spark-nightly-build.sh` ahead, then you will get the test SNAPSHOT jars locally. The best practice is to run `spark-nightly-build.sh` scripts in the test docker container (e.g. building from `jenkins/Dockerfile-blossom.integration.ubuntu`), the run the `spark-tests.sh` in the same container.",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1216590170/reactions,0,0,0,0,0,0,0,0,0,6252
429,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1221853549,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1221853549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1221853549,IC_kwDOD7z77c5I1AFt,2022-08-22T05:33:17Z,2022-08-22T05:33:17Z,COLLABORATOR,"For `spark-nightly-build.sh`, Need to set `SKIP_DEPLOY=true` to avoid pushing jars maven repo.

For `spark-tests.sh`, need to change to dowload spark binary from spark public archive webpage.

https://github.com/NVIDIA/spark-rapids/blob/branch-22.10/jenkins/spark-tests.sh#L99-L100",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1221853549/reactions,0,0,0,0,0,0,0,0,0,6252
430,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222182063,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1222182063,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1222182063,IC_kwDOD7z77c5I2QSv,2022-08-22T10:49:24Z,2022-08-22T10:49:24Z,COLLABORATOR,"The build/test scripts takes long to to finish.

The build scripts `spark-nightly-build.sh` take 6 hours.

The test scripts `spark-tests`  takes 2 hours X one spark shims, all the tests with 8 shims will take a whole day if we run it one my local host with one GPU card.

@gerashegalov I was told this issue/requirement was originated from you, could you please help to check if the above conclusions  what we expected?",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222182063/reactions,0,0,0,0,0,0,0,0,0,6252
431,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222496986,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1222496986,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1222496986,IC_kwDOD7z77c5I3dLa,2022-08-22T15:11:42Z,2022-08-23T13:10:12Z,COLLABORATOR,"Hi @NvTimLiu ,

I have recently had two scenarios. 

1. An invasive PR I knew will pass premerge but would fail an unidentified set of integration tests in nightly tests.
2. A PR that fails a single premerge test on CI and when I rerun it with the same command line locally it will pass. I could repro only after editing a bunch of lines in the script to run it in a local container and it finally reproduced the issue reliably

For Scenario 1: Knowing that running all nightly tests *locally* takes a long time I'd like to be able to  submit a CI job to run all nightly tests before I merge the PR 

Now suppose some test failed in that CI job which is like Scenario 2. Then I want to run this single test locally the same way as CI to reproduce the failure 
```bash
docker run --gpus all -it <ci-image> <repro-command-for-a-failing-test> 
```

Bonus points if the above command would mount my local repo for quick iterations without requiring local commits.
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222496986/reactions,0,0,0,0,0,0,0,0,0,6252
432,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224048732,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1224048732,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1224048732,IC_kwDOD7z77c5I9YBc,2022-08-23T13:06:02Z,2022-08-23T13:06:02Z,COLLABORATOR,"Hi @gerashegalov 
I got your scenarios,
For scenario 1, the nightly BUILD+TESTs CI process is pretty complicated, and takes a lot of time and CPU/GPU resources,  due to the ""dependencies and downstream triggers of `spark-rapids-JNI(1.5h)`/`DB build(2h)`/`nightly build(6h)`/`nightly tests [x7 GPUs parallelly] (2h)`"".   
In a word, we do not have enough resources to trigger extra nightly CI jobs besides the regular one. 

For scenario 2, it is practical, we can load the FAILURE workspace into the premerge docker container to quick restore the test environment, let me try this first.
",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224048732/reactions,0,0,0,0,0,0,0,0,0,6252
433,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1225273823,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1225273823,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1225273823,IC_kwDOD7z77c5JCDHf,2022-08-24T06:50:08Z,2022-08-24T06:50:08Z,COLLABORATOR,"It's not ideal for but the resource problem can be mitigated by batching multiple PRs. 
Let us say we have a GitHub PR label `ready-for-nightly` or something like that. And there are a few scheduled daily/nightly CI jobs trying to merge as many of such PRs into a test branch as possible with a merge conflict. Then it executes a single nightly test run in CI against that cumulative test branch. ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1225273823/reactions,0,0,0,0,0,0,0,0,0,6252
434,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1229623030,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1229623030,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1229623030,IC_kwDOD7z77c5JSo72,2022-08-29T01:18:36Z,2022-08-29T01:18:36Z,COLLABORATOR,"This is a practical way if we aggregate/merge several  `ready-for-nightly` PRs together, then run the nightly CI, a single nightly test run in CI against that cumulative test branch.

BYW,  does `a single nightly test run`  mean we and run tests against one spark-version, or all the spark-versions? @gerashegalov ",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1229623030/reactions,0,0,0,0,0,0,0,0,0,6252
435,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1229925922,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1229925922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1229925922,IC_kwDOD7z77c5JTy4i,2022-08-29T08:07:30Z,2022-08-29T08:07:30Z,COLLABORATOR,"> It's not ideal for but the resource problem can be mitigated by batching multiple PRs. Let us say we have a GitHub PR label `ready-for-nightly` or something like that. And there are a few scheduled daily/nightly CI jobs trying to merge as many of such PRs into a test branch as possible with a merge conflict. Then it executes a single nightly test run in CI against that cumulative test branch.

We have the resource problem that we don't run full testing in the pre-merge pipeline.
And the full test pipeline(like nightly build to nightly IT) takes a long time.
I don't think we should spend time to add another pipeline to support 'ready-for-nightly'. If we think a frequent full testing is required, we can simply change the nightly build & test more frequently like every 12 hours or less to detect the problem. 

I'd like to encourage developers to run the testing locally before merging the PR. For example, we know the pre-merge will run IT with which version of spark. And the developers should run the others if it's not covered by pre-merge and we know it should run.
Should the problem be how easy a developer can run IT locally against some versions of spark?",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1229925922/reactions,0,0,0,0,0,0,0,0,0,6252
436,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1230640580,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1230640580,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1230640580,IC_kwDOD7z77c5JWhXE,2022-08-29T17:41:17Z,2022-08-29T17:41:17Z,COLLABORATOR,"@GaryShen2008 Running locally is a practice with major deficiencies 
1. Practice shows there are always non-trivial discrepancies in the local environment and the CI in terms of hardware, OS, NVIDIA driver versions, a bunch of environment variables that affect the test execution. In my experience, there is always a non-negligible chance that the test suite passing locally will fail on CI

2. Running locally is also much more expensive considering that it takes away from developer's productivity.  
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1230640580/reactions,0,0,0,0,0,0,0,0,0,6252
437,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1232471380,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1232471380,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1232471380,IC_kwDOD7z77c5JdgVU,2022-08-31T05:12:20Z,2022-08-31T05:20:28Z,COLLABORATOR,"I think we may just wait for more resources to get ready. 
Temp workarounds can not completely fulfill the requirements here, and would spend us much time developing and mainly maintenance efforts

> Let us say we have a GitHub PR label ready-for-nightly or something like that. And there are a few scheduled daily/nightly CI jobs trying to merge as many of such PRs into a test branch as possible with a merge conflict. Then it executes a single nightly test run in CI against that cumulative test branch.

this sounds like it may introduce more chaos to pre-merge phase. I am not sure if any other projects have some successful cases w/ it",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1232471380/reactions,0,0,0,0,0,0,0,0,0,6252
438,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234633227,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1234633227,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1234633227,IC_kwDOD7z77c5JlwIL,2022-09-01T18:24:41Z,2022-09-01T18:24:41Z,COLLABORATOR,"I am pretty sure it's not going to be a temporary workaround but a reasonable feature because the struggle for resources is a constant you can bet on in my view. So this is a future proof generic feature that covers a range of situations with n=1 for some rare PRs where you want/must to test in isolation or > 1 for the normal case where PRs can be easily pooled together for testing.

> this sounds like it may introduce more chaos to pre-merge phase. 

I don't see how you can make this conclusion.  How is premerge going to be affected by a single/few (based on the acceptable and approved cadence) runs a day? That's a whole point of batching PRs.

> I am not sure if any other projects have some successful cases w/ it

Unique problems require unique solutions. But I don't think it's that unique.
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234633227/reactions,0,0,0,0,0,0,0,0,0,6252
439,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234949652,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1234949652,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1234949652,IC_kwDOD7z77c5Jm9YU,2022-09-02T00:54:27Z,2022-09-02T00:54:44Z,COLLABORATOR,"> I don't see how you can make this conclusion. How is premerge going to be affected by a single/few (based on the acceptable and approved cadence) runs a day? That's a whole point of batching PRs.

1. It the test passed, everything is fine. But if failures, people would spend much more time investigating errors as the increased complexity of debugging in REF of multiple merged commits. Even there is actually no fault in their own PRs, but the merged REF CI run would block their merge
2. Current nightly CIs do the same job IMO, I do not see any huge benefit to put it as pre-merge
",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234949652/reactions,0,0,0,0,0,0,0,0,0,6252
440,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1235010959,https://github.com/NVIDIA/spark-rapids/issues/6252#issuecomment-1235010959,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6252,1235010959,IC_kwDOD7z77c5JnMWP,2022-09-02T02:51:33Z,2022-09-02T02:51:33Z,COLLABORATOR,"1. yes that's a trade off. You can control by limiting the number of PRs. 
2. We are not talking about making it a mandatory premerge. My proposal is to provide to developers as an option.

Our test code and script is full of places making the local test environment different from CI. 
Recent examples are
- https://github.com/NVIDIA/spark-rapids/pull/6358#issuecomment-1220970470
- https://github.com/NVIDIA/spark-rapids/issues/5714
- https://github.com/NVIDIA/spark-rapids/issues/6444

After running a test locally with success. and you merged the PR, moved on to new tasks, chances are high that nightly tests will come with a bunch of problems incurring high mental context switch cost.

There are two solution avenues to this problem:
1. Make the the tests more deterministic and close the gap between CI and local. I think this is very difficult to achieve based on the vast differences in hardware and the time it will take to remove nondeterminism from the test code, and preventing it from being reintroduce.
2.  Accept the nondeterminism, allow developers to run tests on the CI before checking in to minimize incurred tech debt . For complex PRs I'd rather wait for a few days before merging them but have much higher confidence that the project is actually DONE.",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1235010959/reactions,0,0,0,0,0,0,0,0,0,6252
441,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1209695630,https://github.com/NVIDIA/spark-rapids/issues/6258#issuecomment-1209695630,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6258,1209695630,IC_kwDOD7z77c5IGn2O,2022-08-09T17:55:12Z,2022-08-09T17:55:12Z,COLLABORATOR,Is this just a type check issue or more complicated than that?,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1209695630/reactions,0,0,0,0,0,0,0,0,0,6258
442,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1209825994,https://github.com/NVIDIA/spark-rapids/issues/6265#issuecomment-1209825994,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6265,1209825994,IC_kwDOD7z77c5IHHrK,2022-08-09T20:11:22Z,2022-08-09T20:11:22Z,COLLABORATOR,"This likely does not impact us, but we should improve our predicate pushdown testing to ensure we are not impacted by this. ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1209825994/reactions,0,0,0,0,0,0,0,0,0,6265
443,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1211444721,https://github.com/NVIDIA/spark-rapids/issues/6276#issuecomment-1211444721,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6276,1211444721,IC_kwDOD7z77c5INS3x,2022-08-11T00:48:20Z,2022-08-11T00:48:20Z,COLLABORATOR,"If we want to add JNIs for them, should we put them in cudf or spark-rapids-jni?",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1211444721/reactions,0,0,0,0,0,0,0,0,0,6276
444,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1212227987,https://github.com/NVIDIA/spark-rapids/issues/6276#issuecomment-1212227987,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6276,1212227987,IC_kwDOD7z77c5IQSGT,2022-08-11T16:42:30Z,2022-08-11T16:42:30Z,COLLABORATOR,"> If we want to add JNIs for them, should we put them in cudf or spark-rapids-jni?
cudf. These are cudf APIs and we are not putting anything in that is super Spark specific.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1212227987/reactions,0,0,0,0,0,0,0,0,0,6276
445,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1216955656,https://github.com/NVIDIA/spark-rapids/issues/6336#issuecomment-1216955656,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6336,1216955656,IC_kwDOD7z77c5IiUUI,2022-08-16T17:45:15Z,2022-08-16T17:45:15Z,COLLABORATOR,"Oh I forgot to add that for some reason our DecimalGen does not currently generate negative numbers. I tried to fix this, but ran into this error.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1216955656/reactions,0,0,0,0,0,0,0,0,0,6336
446,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1217114652,https://github.com/NVIDIA/spark-rapids/issues/6336#issuecomment-1217114652,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6336,1217114652,IC_kwDOD7z77c5Ii7Ic,2022-08-16T20:15:26Z,2022-08-16T20:15:26Z,COLLABORATOR,Fall back to the CPU for the short term.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1217114652/reactions,0,0,0,0,0,0,0,0,0,6336
447,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1235805200,https://github.com/NVIDIA/spark-rapids/issues/6336#issuecomment-1235805200,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6336,1235805200,IC_kwDOD7z77c5JqOQQ,2022-09-02T18:45:08Z,2022-09-02T18:45:08Z,MEMBER,"We fallback to the CPU for max width decimals in #6398.  Lowering priority as the plugin now produces correct values for pmod, albeit via falling back for this corner case.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1235805200/reactions,0,0,0,0,0,0,0,0,0,6336
448,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252758012,https://github.com/NVIDIA/spark-rapids/issues/6336#issuecomment-1252758012,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6336,1252758012,IC_kwDOD7z77c5Kq5H8,2022-09-20T18:34:14Z,2022-09-20T18:34:14Z,COLLABORATOR,Removing from 22.10,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252758012/reactions,0,0,0,0,0,0,0,0,0,6336
449,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220451175,https://github.com/NVIDIA/spark-rapids/issues/6357#issuecomment-1220451175,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6357,1220451175,IC_kwDOD7z77c5Ivptn,2022-08-19T09:20:55Z,2022-08-22T06:02:10Z,CONTRIBUTOR,"We need to pay attention the difference between Julian calendar and Gregorian calendar.

It seems that this is controlled by a config `orc.proleptic.gregorian` in CPU-spark.

In CPU spark, setting it to true/false, will bring different results, if the time is before `1582/10/04`.

```shell
scala> spark.conf.set(""orc.proleptic.gregorian"", ""false"")

scala> spark.read.schema(""date timestamp"").orc(""/tmp/orc/data.orc"").show(false)
+-------------------+
|date               |
+-------------------+
|1582-01-01 00:00:00|
|1582-01-01 00:00:00|
+-------------------+


scala> spark.conf.set(""orc.proleptic.gregorian"", ""true"")

scala> spark.read.schema(""date timestamp"").orc(""/tmp/orc/data.orc"").show(false)
+-------------------+
|date               |
+-------------------+
|1581-12-22 00:00:00|
|1581-12-22 00:00:00|
+-------------------+

```

In above example, the first result is actually what we want.

The solution of conversion between between Julian calendar and Gregorian calendar: https://github.com/sinkinben/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala",,sinkinben,31923950,MDQ6VXNlcjMxOTIzOTUw,https://avatars.githubusercontent.com/u/31923950?v=4,,https://api.github.com/users/sinkinben,https://github.com/sinkinben,https://api.github.com/users/sinkinben/followers,https://api.github.com/users/sinkinben/following{/other_user},https://api.github.com/users/sinkinben/gists{/gist_id},https://api.github.com/users/sinkinben/starred{/owner}{/repo},https://api.github.com/users/sinkinben/subscriptions,https://api.github.com/users/sinkinben/orgs,https://api.github.com/users/sinkinben/repos,https://api.github.com/users/sinkinben/events{/privacy},https://api.github.com/users/sinkinben/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1220451175/reactions,0,0,0,0,0,0,0,0,0,6357
450,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1254557371,https://github.com/NVIDIA/spark-rapids/issues/6357#issuecomment-1254557371,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6357,1254557371,IC_kwDOD7z77c5Kxwa7,2022-09-22T05:48:08Z,2022-09-22T05:48:08Z,COLLABORATOR,Move it from 22.10.,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1254557371/reactions,0,0,0,0,0,0,0,0,0,6357
451,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224740653,https://github.com/NVIDIA/spark-rapids/issues/6385#issuecomment-1224740653,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6385,1224740653,IC_kwDOD7z77c5JAA8t,2022-08-23T19:47:37Z,2022-08-23T19:47:37Z,COLLABORATOR,We should add a test to make sure we fall back to the CPU in the case of a `row_index` column being requested.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1224740653/reactions,0,0,0,0,0,0,0,0,0,6385
452,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222974040,https://github.com/NVIDIA/spark-rapids/issues/6391#issuecomment-1222974040,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6391,1222974040,IC_kwDOD7z77c5I5RpY,2022-08-22T20:37:19Z,2022-08-22T20:37:33Z,NONE,"Similar to https://github.com/NVIDIA/spark-rapids/issues/5561 , I can help work on the feature if it's approved by maintainers here.  Although, we need some additional help from ops for maintaining a new Python package.",,trivialfis,16746409,MDQ6VXNlcjE2NzQ2NDA5,https://avatars.githubusercontent.com/u/16746409?v=4,,https://api.github.com/users/trivialfis,https://github.com/trivialfis,https://api.github.com/users/trivialfis/followers,https://api.github.com/users/trivialfis/following{/other_user},https://api.github.com/users/trivialfis/gists{/gist_id},https://api.github.com/users/trivialfis/starred{/owner}{/repo},https://api.github.com/users/trivialfis/subscriptions,https://api.github.com/users/trivialfis/orgs,https://api.github.com/users/trivialfis/repos,https://api.github.com/users/trivialfis/events{/privacy},https://api.github.com/users/trivialfis/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1222974040/reactions,0,0,0,0,0,0,0,0,0,6391
453,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1232142085,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1232142085,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1232142085,IC_kwDOD7z77c5JcP8F,2022-08-30T20:37:41Z,2022-08-30T20:37:41Z,COLLABORATOR,"@ilaychen Could you send the details to spark-rapids-support <spark-rapids-support@nvidia.com>?
Such as the Spark RAPIDS version you are using and the sample dataset which can reproduce this issue?
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1232142085/reactions,0,0,0,0,0,0,0,0,0,6435
454,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1241054956,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1241054956,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1241054956,IC_kwDOD7z77c5J-P7s,2022-09-08T18:04:46Z,2022-09-08T18:08:18Z,NONE,"Hi, I found the case where this issue happens.
It seems like spark-rapids returns wrong results even for a simple df.count() after processing the sample of data that is mentioned below.
**This issue doesn't appear for very small files (less than 1k rows)**

You can generate a csv file of ~1k rows with this kind of data:
(try to use these rows, and generate ~1k similar rows)
```
3453433564704482365,7622 S Handy Street,"""",Silva,85468,NY,United States,US
1641493545665433335,241 Inlusive Road,Flat 30Trafalgar Court,Manchester,M16 8JW,"""",United Kingdom,GB
1297115076542454494,2557 Latte Blvd,#402,Kansas City,64108,MO,United States,US
2119952246048784100,42 North Street,"""",Sand-on-Sea,ZZ2 5HU,"""",United Kingdom,GB
2186678475639058807,329 Johannah Way,"""",Bridger,07507,NJ,United States,US
1730422379578793088,25 Yore Rd,"""",MIAM,M9M 1W5,ON,Canada,CA
```

Read it within a spark-rapids session with:
```
customSchema = StructType([
  StructField(""id"", StringType(), True),
  StructField(""addr_1"", StringType(), True),
  StructField(""addr_2"", StringType(), True),
  StructField(""city"", StringType(), True),
  StructField(""zip"", StringType(), True),
  StructField(""state"", StringType(), True),
  StructField(""cntry"", StringType(), True),
  StructField(""cntry_code"", StringType(), True)]
)
df = spark.read.csv(path, schema=customSchema)
```

and count the dataframe.. you'll see that the number of rows in the actual file is different than the number of rows in the dataframe.
Another way to see that the processing has a bug - is to try to read id 1730422379578793088.. spark-rapids can't read it
`spark.sql(""SELECT * from df_tmpView where cust_id = '1730422379578793088'"").count()`",,ilaychen,45047397,MDQ6VXNlcjQ1MDQ3Mzk3,https://avatars.githubusercontent.com/u/45047397?v=4,,https://api.github.com/users/ilaychen,https://github.com/ilaychen,https://api.github.com/users/ilaychen/followers,https://api.github.com/users/ilaychen/following{/other_user},https://api.github.com/users/ilaychen/gists{/gist_id},https://api.github.com/users/ilaychen/starred{/owner}{/repo},https://api.github.com/users/ilaychen/subscriptions,https://api.github.com/users/ilaychen/orgs,https://api.github.com/users/ilaychen/repos,https://api.github.com/users/ilaychen/events{/privacy},https://api.github.com/users/ilaychen/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1241054956/reactions,0,0,0,0,0,0,0,0,0,6435
455,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1241257676,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1241257676,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1241257676,IC_kwDOD7z77c5J_BbM,2022-09-08T21:35:11Z,2022-09-08T21:35:11Z,COLLABORATOR,"@ilaychen I duplicated your sample data to 2000+ CSV rows(without header) and used latest 22.10 snapshot jar to test it.
And it worked fine for me:
```
from pyspark.sql.types import *

customSchema = StructType([
  StructField(""id"", StringType(), True),
  StructField(""addr_1"", StringType(), True),
  StructField(""addr_2"", StringType(), True),
  StructField(""city"", StringType(), True),
  StructField(""zip"", StringType(), True),
  StructField(""state"", StringType(), True),
  StructField(""cntry"", StringType(), True),
  StructField(""cntry_code"", StringType(), True)]
)

path = ""/home/xxx/data/xxx/samplecsv/""
df = spark.read.csv(path, schema=customSchema)
df.count()

2394
```

It matches the sample CSV file:
```
$  wc -l a.csv
2394 a.csv
```

Could you share below by email to us([spark-rapids-support@nvidia.com](mailto:spark-rapids-support@nvidia.com))?
1. Sample data(maybe 1k+ rows) which can reproduce this issue.
2. What is the exact version of your Spark RAPIDS jar?
3. What are the detailed spark and spark rapids configs you are using? Maybe the whole spark-defaults.conf used",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1241257676/reactions,0,0,0,0,0,0,0,0,0,6435
456,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284425227,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1284425227,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1284425227,IC_kwDOD7z77c5MjsYL,2022-10-19T18:40:09Z,2022-10-19T18:40:09Z,COLLABORATOR,"Thanks @ilaychen  for sharing the sample data and I can reproduce the issue now.
The key to reproduce is if there is a value with `""""`, then it will stop there.
For example, if one column is:
```
abc""""
```

GPU run:
```
>>> df.count()
12
```

GPU run:
```
>>> spark.conf.set(""spark.rapids.sql.enabled"",""false"")
>>> df.count()
14
```
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284425227/reactions,0,0,0,0,0,0,0,0,0,6435
457,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284503586,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1284503586,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1284503586,IC_kwDOD7z77c5Mj_gi,2022-10-19T19:55:58Z,2022-10-19T19:56:09Z,NONE,"My pleasure! @viadea 
Adding the example csv file that produces this error:
```
134324937434,#1991 N Grayhawk,"""",Menlo Park,89025,AB,United States,US
208564744937,""63,trevion Way"","""",st Lothian,h7f4h8,"""",United Kingdom,GB
132709376823,16 Oakland PARK RD,"""",ring,l1w1e4,South,Canada,CA
224867848652,7 kingwell Court,"""",United,s7jd9,South United,United Kingdom,GB
169636884295,30 cartuja Road,"""",Halifax,L0R 9p2,ON,Canada,CA
859473321609,Street,"""",Manchester,92220,OR,United States,US
141096112545,99 rue des,"""",Australia,jsd9je,"""",France,FR
160397658930,5 Rise,"""",walligshngton,RY6 8LT,FORT,United Kingdom,GB
726367494002,1852 Townsend st,666,Wallsend,90382,CA,United States,US
187644735867,Bärbel-HAMPDEN-Ping 37,"""",Miami,13355,"""",Kingdom,ZZ
948475348324,155 sw City ct,Rochdale,Germany,30864,FL,Australia,QQ
164083193213,abc"""","""",Jerez Fra.,11401,Cadiz,Spain,ES
198732413077,3p Grove Rochdale road,BAW,Fulifax,HX4 trW,"""",Israel,GB
227433927227,95 novem blvd,"""",RAW VILLAGE,3173,XYZ,Australia,IL
```
The schema that is mentioned above still applies 😃 ",,ilaychen,45047397,MDQ6VXNlcjQ1MDQ3Mzk3,https://avatars.githubusercontent.com/u/45047397?v=4,,https://api.github.com/users/ilaychen,https://github.com/ilaychen,https://api.github.com/users/ilaychen/followers,https://api.github.com/users/ilaychen/following{/other_user},https://api.github.com/users/ilaychen/gists{/gist_id},https://api.github.com/users/ilaychen/starred{/owner}{/repo},https://api.github.com/users/ilaychen/subscriptions,https://api.github.com/users/ilaychen/orgs,https://api.github.com/users/ilaychen/repos,https://api.github.com/users/ilaychen/events{/privacy},https://api.github.com/users/ilaychen/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284503586/reactions,0,0,0,0,0,0,0,0,0,6435
458,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284556173,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1284556173,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1284556173,IC_kwDOD7z77c5MkMWN,2022-10-19T20:48:01Z,2022-10-19T20:48:01Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/11948 in CUDF for the issue as it is their bug. I'll also try to take a look at their code to see what I can come up with.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284556173/reactions,0,0,0,0,0,0,0,0,0,6435
459,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1285842837,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1285842837,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1285842837,IC_kwDOD7z77c5MpGeV,2022-10-20T16:30:27Z,2022-10-20T16:30:27Z,COLLABORATOR,"It looks like there are two issues happening here.  One of them is that CUDF is returning the wrong number of rows. It gets confused when it sees the """" in the row separator logic.  The second one is that CUDF does not support escape characters.  https://github.com/NVIDIA/spark-rapids/issues/129 instead it only supports the pandas default of double quotes to escape a single quote. i.e. `abc""""` => `abc""` because the `""""` is used to escape a single quote.  I think the first one CUDF might be able to fix. The second one appears to be working as designed, at least until we can get them to add in a new feature to the CSV parser.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1285842837/reactions,0,0,0,0,0,0,0,0,0,6435
460,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681112908,https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1681112908,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6435,1681112908,IC_kwDOD7z77c5kM79M,2023-08-16T18:49:27Z,2023-08-16T18:59:13Z,COLLABORATOR,the CUDF issue https://github.com/rapidsai/cudf/issues/11948 is to fix it.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681112908/reactions,0,0,0,0,0,0,0,0,0,6435
461,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1233344145,https://github.com/NVIDIA/spark-rapids/issues/6469#issuecomment-1233344145,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6469,1233344145,IC_kwDOD7z77c5Jg1aR,2022-08-31T19:37:02Z,2022-08-31T19:37:02Z,COLLABORATOR,"I found one workaround though
```
individual_match_regex = r""((([0-9]{2}p)([0-9]{2}p)([0-9]{2}p)([0-9]{2}p)([0-9]{2}p)))\\.abc""

```",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1233344145/reactions,0,0,0,0,0,0,0,0,0,6469
462,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1244119953,https://github.com/NVIDIA/spark-rapids/issues/6470#issuecomment-1244119953,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6470,1244119953,IC_kwDOD7z77c5KJ8OR,2022-09-12T18:15:39Z,2022-09-12T18:15:39Z,COLLABORATOR,Related cudf issue https://github.com/rapidsai/cudf/issues/11663,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1244119953/reactions,0,0,0,0,0,0,0,0,0,6470
463,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234601209,https://github.com/NVIDIA/spark-rapids/issues/6477#issuecomment-1234601209,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6477,1234601209,IC_kwDOD7z77c5JloT5,2022-09-01T17:51:51Z,2022-09-01T17:51:51Z,COLLABORATOR,"So we actually run CDH integration tests with kryo on by default and it was failing there, but it was hidden by another failure which was happening.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234601209/reactions,0,0,0,0,0,0,0,0,0,6477
464,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234618181,https://github.com/NVIDIA/spark-rapids/issues/6477#issuecomment-1234618181,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6477,1234618181,IC_kwDOD7z77c5JlsdF,2022-09-01T18:09:15Z,2022-09-01T18:09:15Z,COLLABORATOR,"I argue that we can benefit from at least one smoke pytest in this repo, ideally in a premerge suite.",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234618181/reactions,0,0,0,0,0,0,0,0,0,6477
465,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234694241,https://github.com/NVIDIA/spark-rapids/issues/6477#issuecomment-1234694241,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6477,1234694241,IC_kwDOD7z77c5Jl_Bh,2022-09-01T19:29:38Z,2022-09-01T19:29:38Z,COLLABORATOR,"I'm fine with us adding another one if we think its useful, but to set the serializer it has to be done at startup so we would need a completely different run of the tests - like shuffle or cache tests are done.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1234694241/reactions,1,1,0,0,0,0,0,0,0,6477
466,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1236987838,https://github.com/NVIDIA/spark-rapids/issues/6477#issuecomment-1236987838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6477,1236987838,IC_kwDOD7z77c5Juu--,2022-09-05T12:59:50Z,2022-09-05T12:59:50Z,COLLABORATOR,"> So we actually run CDH integration tests with kryo on by default and it was failing there, but it was hidden by another failure which was happening.

Hi @tgravescs, I didn't find CDH IT with GpuKryoRegistrator config. Where's the test?
",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1236987838/reactions,0,0,0,0,0,0,0,0,0,6477
467,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1238286941,https://github.com/NVIDIA/spark-rapids/issues/6477#issuecomment-1238286941,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6477,1238286941,IC_kwDOD7z77c5JzsJd,2022-09-06T15:10:17Z,2022-09-06T15:10:17Z,COLLABORATOR,"CDH defaults to have kryo enabled, if you run with --conf ""spark.rapids.sql.enabled=true"" on CDH, they automatically enable the config. ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1238286941/reactions,0,0,0,0,0,0,0,0,0,6477
468,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1243844261,https://github.com/NVIDIA/spark-rapids/issues/6543#issuecomment-1243844261,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6543,1243844261,IC_kwDOD7z77c5KI46l,2022-09-12T14:40:03Z,2022-09-12T14:40:03Z,COLLABORATOR,This looks like a new feature in Spark. I think we just need to make sure that we fall back to the CPU in these cases and make sure that we update the docs accordingly. At least until a customer asks for this functionality.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1243844261/reactions,0,0,0,0,0,0,0,0,0,6543
469,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252869034,https://github.com/NVIDIA/spark-rapids/issues/6556#issuecomment-1252869034,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6556,1252869034,IC_kwDOD7z77c5KrUOq,2022-09-20T20:20:43Z,2022-09-20T20:20:43Z,COLLABORATOR,Could be resolved by https://github.com/NVIDIA/spark-rapids/issues/6565,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252869034/reactions,0,0,0,0,0,0,0,0,0,6556
470,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252989092,https://github.com/NVIDIA/spark-rapids/issues/6556#issuecomment-1252989092,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6556,1252989092,IC_kwDOD7z77c5Krxik,2022-09-20T22:41:03Z,2022-09-20T22:41:03Z,COLLABORATOR,"> Could be resolved by #6565

I see it the other way around. #6565 is the goal. And this issue is the means. Right now moving the public classes to a standalone submodule without having a dependency on the sql-plugin means they will fail to compile.",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1252989092/reactions,0,0,0,0,0,0,0,0,0,6556
471,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821728568,https://github.com/NVIDIA/spark-rapids/issues/6556#issuecomment-1821728568,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6556,1821728568,IC_kwDOD7z77c5slV84,2023-11-21T21:36:05Z,2023-11-21T21:36:05Z,COLLABORATOR,"Current state is that the most simple classes are already in sql-plugin-api. Others require extensive jdep analysis to refactor,",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821728568/reactions,0,0,0,0,0,0,0,0,0,6556
472,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984684572,https://github.com/NVIDIA/spark-rapids/issues/6556#issuecomment-1984684572,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6556,1984684572,IC_kwDOD7z77c52S-Ic,2024-03-07T22:41:44Z,2024-03-07T22:41:44Z,MEMBER,@gerashegalov is this issue still relevant or is it resolved by the sql-plugin-api module?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984684572/reactions,0,0,0,0,0,0,0,0,0,6556
473,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984754775,https://github.com/NVIDIA/spark-rapids/issues/6556#issuecomment-1984754775,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6556,1984754775,IC_kwDOD7z77c52TPRX,2024-03-07T23:08:13Z,2024-03-07T23:08:13Z,COLLABORATOR,@jlowe It is not resolved per the comment above. I estimate it is a complicated refactor and we do not seem to have any pressing reason to do it a the moment.,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1984754775/reactions,1,1,0,0,0,0,0,0,0,6556
474,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1683926827,https://github.com/NVIDIA/spark-rapids/issues/6560#issuecomment-1683926827,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6560,1683926827,IC_kwDOD7z77c5kXq8r,2023-08-18T13:31:31Z,2023-08-18T13:31:31Z,COLLABORATOR,Reopening because #9072 reverted the PR for this,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1683926827/reactions,0,0,0,0,0,0,0,0,0,6560
475,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1683933048,https://github.com/NVIDIA/spark-rapids/issues/6560#issuecomment-1683933048,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6560,1683933048,IC_kwDOD7z77c5kXsd4,2023-08-18T13:36:10Z,2023-08-18T13:36:10Z,COLLABORATOR,#9071 was the reason it was reverted and there is a comment there and in #9072 that I think is the cause of the bug. But we want to make sure that we have good tests that cover the case.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1683933048/reactions,0,0,0,0,0,0,0,0,0,6560
476,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1693743007,https://github.com/NVIDIA/spark-rapids/issues/6565#issuecomment-1693743007,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6565,1693743007,IC_kwDOD7z77c5k9Hef,2023-08-25T18:01:30Z,2023-08-25T18:01:30Z,COLLABORATOR,"As part of this issue, would we do away with `unshimmed-common-from-spark311.txt`? If so, I am wondering if this thread will be resolved https://github.com/NVIDIA/spark-rapids/pull/9113#discussion_r1305857766",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1693743007/reactions,1,1,0,0,0,0,0,0,0,6565
477,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1271723010,https://github.com/NVIDIA/spark-rapids/issues/6603#issuecomment-1271723010,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6603,1271723010,IC_kwDOD7z77c5LzPQC,2022-10-07T15:11:14Z,2022-10-07T15:11:14Z,COLLABORATOR,"We had an initial discussion with a developer in CUDF on how we might be able to support time zone transitions.  He has a proof of concept that is not fully done up at https://github.com/rapidsai/cudf/pull/11872

Generally the patch needs a small database that holds the time zone transitions and can apply them to timestamps to convert to/from UTC.

The patch asks for the following columns.

* `zone_name` name of the time zone (how it will be looked up)
* `country_code` the country this time zone is a part of
* `abbreviation` an short version of the name that is being transitioned to.
* `time_start` seconds since the epoch when this operation takes effect
* `gmt_offset` the new offset in seconds from UTC/GMT after the transition
* `dst`appears to be a ""1"" if going to daylight savings time, else a 0

The patch has a database that they dumped themselves, and long term have plans to read the database directly from the standard Linux location. But we are concerned about the database being different compared to the one in java, both in terms of when/how it is updated and also in terms of localization of the names.

The plan right now is to dump as much of the database from java as possible using the java.time.ZoneId` class. We can get a list of all zones from this class if needed `getAvailableZoneIds` or for a single `ZoneId` as provided by Spark for many expressions or just as the current timezone.

Here we can get `zone_name` by calling `getId`. But we might need to think more about localization and what not when we parse strings timestamps, like with CSV and/or JSON.  We also need to think about how the timezone is stored in ORC so we can make sure that we can support that as well.

I could not find any API to get `country_code`. Even though a timezone is often associated with a country java just does not expose this information. I also could not find any way to get `abbreviation` out.  I don't think that these are strictly needed, but we will need to see. Some of it may be related to the local and once we know that we might be able to use it to get out what we need/want.

The rest of the columns come from specific rules to get to the rules for a `ZoneId` you call `getRules`.  Under that there are two types of transition rules. One that is for set known transitions `getTransitions` and the other is for a set of rules that can be used to generate other transitions `getTransitionRules`.  The rules can be used to generate specific transitions for a given year, so we can concentrate on just the `ZoneOffsetTransition` class that is returned as a part of a list from `getTransitions`, but we can run into some problems here, because we have to guess how many years to compute future transitions for.  The example database in CUDF goes to the year 2499. For java we can go much further. Technically for TIMESTAMP_MICROSECONDS, which is what Spark uses for timestamps, we could go out to the year 586,912. It could be doable to go that far when generating the database, but I am not 100% sure we want to. But if we don't we would need to play games with looking for the maximum timestamp/etc and then add in the rules to that point.

* `time_start` can come from `.toEpochSecond`
* `gmt_offset` can come from `.getOffsetAfter.getTotalSeconds`
* `dst` can come from `.isGap`

One thing to note is that we might need to also use `.getOffsetBefore` on the first transition to get the starting point for the first transition in the CUDF database.  This is not an explicit transition, it is just what happened before we started to have rules (LMT).  Not sure where the start time for LMT in their database comes from though, so not sure how to come up with something there.

This is just a first pass at looking at the APIs we will need to work with them to understand exactly what they need and why so we can work on a final solution.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1271723010/reactions,0,0,0,0,0,0,0,0,0,6603
478,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1275193275,https://github.com/NVIDIA/spark-rapids/issues/6603#issuecomment-1275193275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6603,1275193275,IC_kwDOD7z77c5MAee7,2022-10-11T19:47:05Z,2022-10-11T19:47:05Z,COLLABORATOR,"Reading the prototype code it is actually very simple.  A time zone table that is specific to a given time zone is sorted by the time when a transition occurs.  When translating to UTC from a given timezone the occurrence time  a lower bound (or upper bound I need to think about it more) is used against the occurrence time + utc_offset to get an index into the time period when this rule would apply.  Then the utc_offset is subtracted from the current timestamp to put in in UTC.

The opposite is done when going from UTC to a specific time zone.  We could do all of this today, or we could write out own implementation that knows how to use a transition rule, so we don't have to worry about not having generated enough lines. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1275193275/reactions,0,0,0,0,0,0,0,0,0,6603
479,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1276543898,https://github.com/NVIDIA/spark-rapids/issues/6603#issuecomment-1276543898,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6603,1276543898,IC_kwDOD7z77c5MFoOa,2022-10-12T18:00:31Z,2022-10-12T18:00:31Z,MEMBER,"Note that with timezones using daylight savings or other similar discontinuities where time can ""roll back"" there can be ambiguous mappings from a timestamp in those timezones to UTC during the rollback windows.  For example, the timestamp `2022-11-06 01:30:00` in the US/Central timezone has an offset to UTC of either 5 or 6, as the time occurs twice in that timezone exactly an hour apart because of the daylight savings rollback of an hour that occurs at 2AM on that day.

A simple experiment with `to_utc_timestamp` shows that Spark is picking the earlier time in this ambiguous case, e.g.:
```
scala> val df = Seq(""2022-11-06T00:30:00"", ""2022-11-06T01:00:00"", ""2022-11-06T01:30:00"", ""2022-11-06T02:00:00"", ""2022-11-06T02:30:00"").toDF(""ts"")
df: org.apache.spark.sql.DataFrame = [ts: string]

scala> df.selectExpr(""to_utc_timestamp(ts, 'US/Central') as tu"").show
+-------------------+
|                 tu|
+-------------------+
|2022-11-06 05:30:00|
|2022-11-06 06:00:00|
|2022-11-06 06:30:00|
|2022-11-06 08:00:00|
|2022-11-06 08:30:00|
+-------------------+
```",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1276543898/reactions,0,0,0,0,0,0,0,0,0,6603
480,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1259988172,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1259988172,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1259988172,IC_kwDOD7z77c5LGeTM,2022-09-27T20:03:41Z,2022-09-27T20:03:41Z,COLLABORATOR,"@ttnghia are there specific areas where `min` and `max` are being used, or are there test cases that are missing this? ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1259988172/reactions,0,0,0,0,0,0,0,0,0,6635
481,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1259990957,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1259990957,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1259990957,IC_kwDOD7z77c5LGe-t,2022-09-27T20:06:41Z,2022-09-27T20:07:53Z,COLLABORATOR,"Not now, but we just hit it in a test for https://github.com/NVIDIA/spark-rapids/issues/6130.

I file this issue so we can be more aware in the future of this case.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1259990957/reactions,0,0,0,0,0,0,0,0,0,6635
482,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1260000607,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1260000607,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1260000607,IC_kwDOD7z77c5LGhVf,2022-09-27T20:17:04Z,2022-09-27T20:17:04Z,COLLABORATOR,"So do you want us to document this somewhere? I am just trying to understand what the result of this is.  If it is more awareness, then okay we can close this and tell everyone not to do it. If you think there might be bugs in the existing code, then we can go through the code and verify that they are all covered.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1260000607/reactions,0,0,0,0,0,0,0,0,0,6635
483,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1260044175,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1260044175,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1260044175,IC_kwDOD7z77c5LGr-P,2022-09-27T20:56:08Z,2022-09-27T20:57:19Z,COLLABORATOR,"> So do you want us to document this somewhere? 

Yes, I think so. We can call this a bug since we can get wrong results (compared to Spark's results) with `min` and `max` on floating point columns, but I don't know how to fix it because these `min` and `max` are from cudf JNI, not from our plugin code. If we are allowed to fix it there then that's great. Otherwise, we should document this clearly.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1260044175/reactions,0,0,0,0,0,0,0,0,0,6635
484,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262118815,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1262118815,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1262118815,IC_kwDOD7z77c5LOmef,2022-09-29T11:01:51Z,2022-09-29T11:01:51Z,COLLABORATOR,Is the bug related to `GpuMax` or `GpuMin`? I guess we have added a workaround way to match the Spark's behavior: https://github.com/NVIDIA/spark-rapids/pull/5989,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262118815/reactions,0,0,0,0,0,0,0,0,0,6635
485,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262257817,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1262257817,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1262257817,IC_kwDOD7z77c5LPIaZ,2022-09-29T13:10:33Z,2022-09-29T13:11:45Z,COLLABORATOR,"These are `ColumnVector.min()` and `ColumnVector.max()` which are Java functions from cudf, not the plugin code. These functions are only called internally, not the ones that SQL users execute.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262257817/reactions,1,1,0,0,0,0,0,0,0,6635
486,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262263077,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1262263077,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1262263077,IC_kwDOD7z77c5LPJsl,2022-09-29T13:13:53Z,2022-09-29T13:13:53Z,COLLABORATOR,Should we move the NaN handling (what we have done in `GpuMax` and `GpuMin`) into cuDF or rapids-JNI ?,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262263077/reactions,0,0,0,0,0,0,0,0,0,6635
487,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262266543,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1262266543,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1262266543,IC_kwDOD7z77c5LPKiv,2022-09-29T13:16:43Z,2022-09-29T13:16:43Z,COLLABORATOR,That is also part of my question above. Moving the implementation to cudf JNI will make `GpuMin/Max` and `min/max` consistent but cudf JNI may be used somewhere else not just our plugin thus I'm not sure.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262266543/reactions,0,0,0,0,0,0,0,0,0,6635
488,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262271184,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1262271184,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1262271184,IC_kwDOD7z77c5LPLrQ,2022-09-29T13:19:53Z,2022-09-29T13:19:53Z,COLLABORATOR,"Not familiar with JNI, but I wonder could we move the implementation to https://github.com/NVIDIA/spark-rapids-jni ?",,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1262271184/reactions,0,0,0,0,0,0,0,0,0,6635
489,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1263856493,https://github.com/NVIDIA/spark-rapids/issues/6635#issuecomment-1263856493,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6635,1263856493,IC_kwDOD7z77c5LVOtt,2022-09-30T17:55:01Z,2022-09-30T17:55:01Z,COLLABORATOR,"> Not familiar with JNI, but I wonder could we move the implementation to https://github.com/NVIDIA/spark-rapids-jni ?

Min/Max for a float/double reduction is something we could look into, but a group by aggregation or a window operation is more difficult.  There is a lot more of a framework around them, and for group by we need the results to appear on the correct lines afterwards.  If one operation is doing a hash based aggregation and the other is doing a sort based one it gets to be really complicated to make them all play nicely together.  If CUDF could provide a way to plug in our own aggregations to group by or window afterwards, then it would be very interesting.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1263856493/reactions,0,0,0,0,0,0,0,0,0,6635
490,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1267520891,https://github.com/NVIDIA/spark-rapids/issues/6678#issuecomment-1267520891,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6678,1267520891,IC_kwDOD7z77c5LjNV7,2022-10-04T20:08:39Z,2022-10-04T20:08:39Z,COLLABORATOR,We need to validate that we are not impacted by doing testing.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1267520891/reactions,0,0,0,0,0,0,0,0,0,6678
491,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1267561865,https://github.com/NVIDIA/spark-rapids/issues/6678#issuecomment-1267561865,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6678,1267561865,IC_kwDOD7z77c5LjXWJ,2022-10-04T20:49:58Z,2022-10-04T20:49:58Z,COLLABORATOR,"Sort of related, this issue was filed for Spark 3.1 a while ago: https://github.com/NVIDIA/spark-rapids/issues/1527",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1267561865/reactions,0,0,0,0,0,0,0,0,0,6678
492,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1277875523,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1277875523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1277875523,IC_kwDOD7z77c5MKtVD,2022-10-13T16:22:30Z,2022-10-13T18:11:12Z,COLLABORATOR,"Thought about this issue a bit more, what I think we want is a version of the [tracking_resource_adaptor](https://github.com/rapidsai/rmm/blob/branch-22.12/include/rmm/mr/device/tracking_resource_adaptor.hpp) but, rather than have a single map for all threads, I think that we want to keep track of the maximum outstanding GPU footprint _per thread_. Also to note, the main motivation here would be to figure out if our estimation on memory usage for some GPU code is higher than anticipated, to help us debug waste or inform heuristics to control what tasks we allow on the GPU.

This should allow us to do the following:

```
val maxOutstandingUsage = withMemoryTracking { 
  val gpuData = materialize data on gpu
  val result = withResource(gpuData) { _.callCudfFunction }
  result.close() 
    // at this point our maximum outstanding should be:
    // gpuData + max(allocated) inside of `callCudfFunction`
}
```

In this scenario when we enter the `withMemoryTracking` block, we would ask a per-thread tracking resource to start tracking _this_ thread before we materialize data. The materialization of `gpuData` incurs calls to rmm to get memory, so that adds to the outstanding amount, and then the call to the cuDF code could be allocations that are kept around (outstanding) for a while, allocations and frees that happen within the C++ code before the kernel, or results from this code. So we can keep track of how much is outstanding at any given time by adding to a thread-local variable how many bytes have been requested, and subtracting when we call free.

If one of our allocations failed and we handled them via a spill it shouldn't matter. That is because the spill code should be careful to disable the tracking for those spills (e.g. a `withoutMemoryTracking` call). This means we wouldn't discount frees in `this` thread for some other thread's allocations that are irrelevant to the code being tracked.

I hope/believe this could be a pretty low overhead system. Note this doesn't, I don't think, help tracking when an expensive kernel is loaded, as far as I understand that can be a one-time-penalty when we open the shared library. I know we have seen this with some of the regular expression kernels in the past. Pinging @jlowe on this overall for comments.
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1277875523/reactions,0,0,0,0,0,0,0,0,0,6745
493,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280866635,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1280866635,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1280866635,IC_kwDOD7z77c5MWHlL,2022-10-17T13:31:36Z,2022-10-17T13:35:43Z,COLLABORATOR,"I think one approach here is to have a stack of simple memory tracking info in RmmJni. When a `withMemoryTracking` block is issued we push to the stack one of these objects. The `tracking_resource_adaptor` could then check this stack for the current thread, and if it has something in it, it uses the top tracker to track allocations for now.

When `withMemoryTracking` is finishing, it calls a function in the RMM jni bits to pop this element from the stack. If it is the last element, we have turned the feature off. If it is not the last element we get the amount tracked in this scope and add the maximum outstanding we just popped to the next element in the stack (the calling scope also saw that maximum outstanding), and we continue to track with the remaining tracker in the stack.

We also need to keep a set of addresses we allocated in _this_ thread, unfortunately. Given spill, the current thread may need to spill to satisfy an allocation. It seems we could ignore frees that we didn't allocate while tracking. The hope is that these `withMemoryTracking` blocks are as close as possible to a cuDF call.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1280866635/reactions,0,0,0,0,0,0,0,0,0,6745
494,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1426098896,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1426098896,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1426098896,IC_kwDOD7z77c5VAIrQ,2023-02-10T17:08:24Z,2023-02-10T17:08:24Z,COLLABORATOR,"Nsys has added memory tracking capabilities as of late, and we believe we can use the correlationId + NVTX ranges  to accomplish this as a post processing step given an NVTX range. We should investigate if this solution does what we need.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1426098896/reactions,0,0,0,0,0,0,0,0,0,6745
495,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778907276,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1778907276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1778907276,IC_kwDOD7z77c5qB_iM,2023-10-25T09:51:19Z,2023-10-26T07:21:08Z,COLLABORATOR,"Hi @abellina I am trying to profile the GPU memory usage during a query run. I used nsys to profile, but didn't find metrics like `peak memory usage`
![image](https://github.com/NVIDIA/spark-rapids/assets/20476954/da1a442c-c2a1-438a-a03f-d928e3573946)
 
I was using `NVIDIA Nsight Systems version 2022.2.1.31-5fe97ab` installed in our internal cluster. 
I saw a post about it: https://forums.developer.nvidia.com/t/nsys-measure-memory/118394 which is posted on 2021, but it contains the `memory usage` part in the graph...


Update:
The memory usage metrics are disabled by default, it can be turned on by an extra nsys argument `--cuda-memory-usage=true`
Then we can see the memory utilization part in the graph:
![image](https://github.com/NVIDIA/spark-rapids/assets/20476954/fc8621f3-8339-4414-a1e6-50e8f4cfbab9)
",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778907276/reactions,0,0,0,0,0,0,0,0,0,6745
496,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783076931,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1783076931,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1783076931,IC_kwDOD7z77c5qR5hD,2023-10-27T15:07:56Z,2023-10-27T15:07:56Z,COLLABORATOR,"I haven't used this feature, the main question I'd have is whether it works with a pool, especially the async pools. It most definitely does not work with ARENA because that's all CPU managed, but cudaAsync I'd hope shows it. ",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783076931/reactions,0,0,0,0,0,0,0,0,0,6745
497,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1784527518,https://github.com/NVIDIA/spark-rapids/issues/6745#issuecomment-1784527518,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6745,1784527518,IC_kwDOD7z77c5qXbqe,2023-10-30T05:52:48Z,2023-10-30T05:52:48Z,COLLABORATOR,The profile result above is from a run with ASYNC pool.,,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1784527518/reactions,0,0,0,0,0,0,0,0,0,6745
498,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1275008356,https://github.com/NVIDIA/spark-rapids/issues/6749#issuecomment-1275008356,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6749,1275008356,IC_kwDOD7z77c5L_xVk,2022-10-11T17:05:14Z,2022-10-11T17:05:14Z,COLLABORATOR,"From looking at the patch and the code for GPuCratedNamedStruct I don't think we will be impacted. The name of each field is passed into the constructor. But this is just to add a test to verify it, and that should be relatively simple to do.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1275008356/reactions,0,0,0,0,0,0,0,0,0,6749
499,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1279186838,https://github.com/NVIDIA/spark-rapids/issues/6800#issuecomment-1279186838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6800,1279186838,IC_kwDOD7z77c5MPteW,2022-10-14T15:58:29Z,2022-10-14T15:58:29Z,COLLABORATOR,What specific types of applications were benchmarked?  The estimates that we have done are the NDS benchmark @ SF3K and the accuracy ranges by query application but it generally within a bounds of 20-30% error.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1279186838/reactions,0,0,0,0,0,0,0,0,0,6800
500,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281742821,https://github.com/NVIDIA/spark-rapids/issues/6800#issuecomment-1281742821,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6800,1281742821,IC_kwDOD7z77c5MZdfl,2022-10-18T02:46:42Z,2022-10-18T02:46:42Z,COLLABORATOR,"the Apps are some customers' pipelines with dozens of queries, most of the queries are as simple as ""create table TableA as select c1,c2.. from TableB where cn=XXX"", and the total time cost of these queries is about 20%-30% of the app. ",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281742821/reactions,0,0,0,0,0,0,0,0,0,6800
501,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281748872,https://github.com/NVIDIA/spark-rapids/issues/6800#issuecomment-1281748872,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6800,1281748872,IC_kwDOD7z77c5MZe-I,2022-10-18T02:58:28Z,2022-10-18T02:58:28Z,COLLABORATOR,"it is hard to reproduce because we cannot get the event logs, it also makes sense that the estimated values may not be that accurate due to current implements, but I would like to keep this issue open so that if we hit the same issue with other customers which could share logs with us, we can keep tracking.",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1281748872/reactions,0,0,0,0,0,0,0,0,0,6800
502,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1282443792,https://github.com/NVIDIA/spark-rapids/issues/6800#issuecomment-1282443792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6800,1282443792,IC_kwDOD7z77c5McIoQ,2022-10-18T13:59:21Z,2022-10-18T13:59:21Z,COLLABORATOR, its documented that the tool is an estimate. If there is nothing actionable here then we should close it and reopen when you have logs that we can look at and possibly take action on.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1282443792/reactions,0,0,0,0,0,0,0,0,0,6800
503,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298207903,https://github.com/NVIDIA/spark-rapids/issues/6814#issuecomment-1298207903,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6814,1298207903,IC_kwDOD7z77c5NYRSf,2022-11-01T08:28:37Z,2022-11-02T01:44:43Z,COLLABORATOR,"Hi @viadea , does ""2 nodes cluster"" mean two executors or two workers ?
And could you share the spark version being used?",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1298207903/reactions,0,0,0,0,0,0,0,0,0,6814
504,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303022722,https://github.com/NVIDIA/spark-rapids/issues/6814#issuecomment-1303022722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6814,1303022722,IC_kwDOD7z77c5NqoyC,2022-11-04T06:12:56Z,2022-11-04T06:12:56Z,COLLABORATOR,"Actually this is not a CSV issue, but an issue of the GPU version of `CollectLlimitExec`.

The GPU version of `CollectLimitExec` will not always return the head line as the first row due to a shuffle inside it, but Spark treats the returned single row as the CSV header line when inferring the CSV schema.

Details can be found in this early issue https://github.com/NVIDIA/spark-rapids/issues/882, especially the comment https://github.com/NVIDIA/spark-rapids/issues/882#issuecomment-702343763 .",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303022722/reactions,0,0,0,0,0,0,0,0,0,6814
505,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303052553,https://github.com/NVIDIA/spark-rapids/issues/6814#issuecomment-1303052553,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6814,1303052553,IC_kwDOD7z77c5NqwEJ,2022-11-04T07:07:06Z,2022-11-04T07:07:06Z,COLLABORATOR,Filed a tracking issue https://github.com/NVIDIA/spark-rapids/issues/7005. And I am going to mark it as done in the CSV epic issue.,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303052553/reactions,0,0,0,0,0,0,0,0,0,6814
506,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309106825,https://github.com/NVIDIA/spark-rapids/issues/6814#issuecomment-1309106825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6814,1309106825,IC_kwDOD7z77c5OB2KJ,2022-11-09T17:37:18Z,2022-11-09T17:38:01Z,COLLABORATOR,"@firestarman ""2 nodes cluster"" means 2 worker servers ",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309106825/reactions,1,0,0,0,0,0,0,0,1,6814
507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1324440103,https://github.com/NVIDIA/spark-rapids/issues/6814#issuecomment-1324440103,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6814,1324440103,IC_kwDOD7z77c5O8Von,2022-11-23T01:34:28Z,2022-11-23T01:34:28Z,COLLABORATOR,Move out of 22.12 target.,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1324440103/reactions,0,0,0,0,0,0,0,0,0,6814
508,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284213252,https://github.com/NVIDIA/spark-rapids/issues/6829#issuecomment-1284213252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6829,1284213252,IC_kwDOD7z77c5Mi4oE,2022-10-19T15:38:55Z,2022-10-19T15:38:55Z,COLLABORATOR,"Spark followps:

- https://github.com/apache/spark/commit/9bc8c06bc4 [[SPARK-39876](https://issues.apache.org/jira/browse/SPARK-39876)][FOLLOW-UP][SQL] Add parser and Dataset tests for SQL UNPIVOT",,amahussein,50450311,MDQ6VXNlcjUwNDUwMzEx,https://avatars.githubusercontent.com/u/50450311?v=4,,https://api.github.com/users/amahussein,https://github.com/amahussein,https://api.github.com/users/amahussein/followers,https://api.github.com/users/amahussein/following{/other_user},https://api.github.com/users/amahussein/gists{/gist_id},https://api.github.com/users/amahussein/starred{/owner}{/repo},https://api.github.com/users/amahussein/subscriptions,https://api.github.com/users/amahussein/orgs,https://api.github.com/users/amahussein/repos,https://api.github.com/users/amahussein/events{/privacy},https://api.github.com/users/amahussein/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1284213252/reactions,0,0,0,0,0,0,0,0,0,6829
509,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1653033328,https://github.com/NVIDIA/spark-rapids/issues/6839#issuecomment-1653033328,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6839,1653033328,IC_kwDOD7z77c5ih0lw,2023-07-27T07:11:37Z,2023-07-27T07:11:50Z,COLLABORATOR,"Not sure if `FromUTCTimestamp support PST` should be a sub task.
CC @viadea ",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1653033328/reactions,0,0,0,0,0,0,0,0,0,6839
510,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1784409365,https://github.com/NVIDIA/spark-rapids/issues/6839#issuecomment-1784409365,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6839,1784409365,IC_kwDOD7z77c5qW-0V,2023-10-30T03:14:44Z,2023-10-30T03:14:44Z,COLLABORATOR,"Follow on issue: https://github.com/NVIDIA/spark-rapids/issues/9570
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1784409365/reactions,0,0,0,0,0,0,0,0,0,6839
511,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1793151546,https://github.com/NVIDIA/spark-rapids/issues/6839#issuecomment-1793151546,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6839,1793151546,IC_kwDOD7z77c5q4VI6,2023-11-03T22:00:14Z,2023-11-03T22:00:14Z,COLLABORATOR,"Adding in some issues for testing too 

https://github.com/NVIDIA/spark-rapids/issues/9627

https://github.com/NVIDIA/spark-rapids/issues/9633

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1793151546/reactions,0,0,0,0,0,0,0,0,0,6839
512,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828857907,https://github.com/NVIDIA/spark-rapids/issues/6839#issuecomment-1828857907,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6839,1828857907,IC_kwDOD7z77c5tAigz,2023-11-28T00:23:06Z,2023-11-30T23:57:20Z,COLLABORATOR,"Test cases statistics:
|code snapshot|TZ|passed|skipped|xfailed|xpassed|total|
|:-|:-|:-|:-|:-|:-|:-|
|Before xfail all the impacted cases #9773|UTC|19361|2210|373|283|22227|
|After xfail all the impacted cases #9773|non-UTC|7589|2214|3719|8613|22135|
|Introduced fine grained checker from #9719|non-UTC|20173|1411|358|382|22324|",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828857907/reactions,0,0,0,0,0,0,0,0,0,6839
513,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998779638,https://github.com/NVIDIA/spark-rapids/issues/6839#issuecomment-1998779638,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6839,1998779638,IC_kwDOD7z77c53IvT2,2024-03-15T01:51:15Z,2024-03-15T01:51:15Z,COLLABORATOR,"Not planing work on this for release 24.04
@sameerz can we move it to next release?",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998779638/reactions,0,0,0,0,0,0,0,0,0,6839
514,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1847774868,https://github.com/NVIDIA/spark-rapids/issues/6840#issuecomment-1847774868,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6840,1847774868,IC_kwDOD7z77c5uIs6U,2023-12-08T20:02:12Z,2023-12-08T20:02:12Z,COLLABORATOR,"After doing some research into the JDK's API, here are some interesting findings:

1) Java stores the transitions for the timezones with daylight savings after they have already happened. So for past dates, the transition list has all the previous daylight savings transitions in the transition list as fixed transitions (which makes sense). This means that the transition list will actually grow each year for these timezones. Also, the implication is that for dates in the past, we would adopt the same approach we take with non-repeating rules. 

2) Transition rules exist for dates in the future. The idea is for each current and future year, you can compute the ZoneOffsetTransition for that future year. 

So we have 2 approaches we can try here:

1) We can use `ZoneOffsetTransitionRule.createTransitition(...)` to create transitions for a set of future years and append these to fixed transition list for that timezone. This would incur a longer initialization time and more memory required to store the extra transitions, but computing the conversion to and from UTC would simply use the existing kernel as if the transitions were fixed and non-repeating. This is definitely the simplest to implement and is parallel to the implementation that cuDF currently uses in parsing non-UTC timestamps from ORC files. 

2) We would store the rules in another column vector, and then implement an addition to the kernel that would use rules if we would fall out of the existing transition list (ie use a transition that doesn't exist yet). We would have to process the timestamp to figure out which rule would apply and then apply a created transition from that rule.  The logic is more complicated in this case so it might not perform as well as the simpler fixed transition kernel since we would need some additional conditional logic in this case.  But initialization time would only increase a nominal amount to store the rules, and the storage cost would only increase on the yearly basis that Java uses and for the additional columns to store rules.

For now, I'm inclined to try out (2) first since I think (1) feels more like too much additional cost for what looks like premature optimization. However, we could keep in mind to implement both approaches (or a combination) since it could be something that is tuned to the data involved. For example if we are working with a lot of future timestamps, (1) could be better since that algorithm is a lot cleaner than doing a lot of processing and extraction from each timestamp to determine if it falls under a specific rule. Also, if we can partially tune an implementation of (1), maybe we can cover more than enough with less overhead.
",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1847774868/reactions,0,0,0,0,0,0,0,0,0,6840
515,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1847784228,https://github.com/NVIDIA/spark-rapids/issues/6840#issuecomment-1847784228,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6840,1847784228,IC_kwDOD7z77c5uIvMk,2023-12-08T20:10:27Z,2023-12-08T20:10:27Z,COLLABORATOR,"Should also add, at minimum for approach (2), we should ensure the transitions for the current year are already computed since that logic is potentially more complicated given the overlap between fixed transitions and choosing rules for a daylight savings cycle that hasn't yet completed. It might be much easier to start the following year with fresh rule choice at that point.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1847784228/reactions,0,0,0,0,0,0,0,0,0,6840
516,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870447166,https://github.com/NVIDIA/spark-rapids/issues/6840#issuecomment-1870447166,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6840,1870447166,IC_kwDOD7z77c5vfMI-,2023-12-27T16:18:07Z,2023-12-27T16:18:07Z,COLLABORATOR,"I did some initial work looking at approach 1 when I filed the issue. The size of the database is not likely to be too horrible on a per time-zone basis. It would probably be about 11 MiB per time zone because it would need to go out to the year 294247 or so to fully support everything that Spark can also support. 

```
294247 years * 2 transitions per year * (8 + 8 + 4 bytes) => 11.22 MiB
```

There are about 206 time zones that have transition rules

```
scala> java.time.ZoneId.getAvailableZoneIds.asScala.map(f => java.time.ZoneId.of(f).normalized).filter(f => !f.getRules().getTransitionRules.isEmpty()).size
res18: Int = 206
```

Which means we would need about 2.25 GiB to store all of the rules for those timezones.  That is small enough That I don't think we would need to worry about the Column size limits in Spark, but it is large enough that I don't think we want to try and keep the entire thing cached in either CPU or GPU memory. So option 2 is the ideal solution if we can make it work. If we need to we could try and play some games where we load timezones on demand as they are needed, as we probably need only one or two timezone ever. We could also look for the maximum timestamp and load up to that point, as it should be very very uncommon to need all of the data. But that would open up a number of corner cases which I would not be super happy about. So if we can make the dynamic version work, then it would be awesome.

Also the dynamic version would set us up to be able to support things like fixed offset rules that don't actually have a name.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870447166/reactions,0,0,0,0,0,0,0,0,0,6840
517,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998774655,https://github.com/NVIDIA/spark-rapids/issues/6840#issuecomment-1998774655,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6840,1998774655,IC_kwDOD7z77c53IuF_,2024-03-15T01:49:14Z,2024-03-15T01:49:14Z,COLLABORATOR,"Not planing work on this for release 24.04
@sameerz  can we move it to next release?",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998774655/reactions,0,0,0,0,0,0,0,0,0,6840
518,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863691527,https://github.com/NVIDIA/spark-rapids/issues/6846#issuecomment-1863691527,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6846,1863691527,IC_kwDOD7z77c5vFa0H,2023-12-20T01:15:09Z,2023-12-20T01:15:09Z,COLLABORATOR,"
Alfred is co-working on this.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863691527/reactions,0,0,0,0,0,0,0,0,0,6846
519,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874700561,https://github.com/NVIDIA/spark-rapids/issues/6846#issuecomment-1874700561,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6846,1874700561,IC_kwDOD7z77c5vvakR,2024-01-02T23:58:46Z,2024-01-02T23:58:46Z,COLLABORATOR,"To note: It's probably more relevant to support offsets in the string as opposed to full timezone IDs. Offsets are part of the toString output of OffsetDateTime in Java, and the ISO 8601 format (using the 'Z'). The ISO 8601 format is common enough that it should be the priority of support in this feature.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874700561/reactions,0,0,0,0,0,0,0,0,0,6846
520,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998780694,https://github.com/NVIDIA/spark-rapids/issues/6846#issuecomment-1998780694,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6846,1998780694,IC_kwDOD7z77c53IvkW,2024-03-15T01:51:41Z,2024-03-15T01:51:41Z,COLLABORATOR,"I'm not planing work on this for release 24.04
@sameerz can we move it to next release?",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998780694/reactions,0,0,0,0,0,0,0,0,0,6846
521,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1412639705,https://github.com/NVIDIA/spark-rapids/issues/6875#issuecomment-1412639705,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6875,1412639705,IC_kwDOD7z77c5UMyvZ,2023-02-01T19:54:58Z,2023-02-01T19:54:58Z,MEMBER,"This isn't a complete solution, but the allow list that's in delta_lake_write_test seems to be working pretty well in practice.  It has the drawback that it's allowing some nodes to be on the CPU that might also appear in the ""real"" query being executed that we would not want on the CPU.  Seems like we would want a way to identify and isolate one or more specific sub-queries we're checking for Delta Lake support rather than every query that happens to fly by.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1412639705/reactions,0,0,0,0,0,0,0,0,0,6875
522,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1293187000,https://github.com/NVIDIA/spark-rapids/issues/6917#issuecomment-1293187000,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6917,1293187000,IC_kwDOD7z77c5NFHe4,2022-10-27T08:39:17Z,2022-10-27T08:52:49Z,COLLABORATOR,"The root cause is probably the same with https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1285842837.

Some rows are changed in the writen CSV file, comparing to the original file. And some of the changed rows now contain strings like `x""""`, which the GPU reading can not handle correctly, as mentioned in https://github.com/NVIDIA/spark-rapids/issues/6435#issuecomment-1285842837.

Here is a minimal repro case.
The 4 rows are picked from the writen file, and the first row ends with `\""""`, leading to the following two rows being skipped in GPU reading.
```
category,description
Sci/Tech,The U.S. Forest Service on Wednesday rejected environmentalists' appeal of a plan to poison a stream south of Lake Tahoe to aid what wildlife officials call \""the rarest trout in America.\""""
Sci/Tech,One of the pleasures of    stargazing is noticing and enjoying the various colors that stars display in    dark skies. These hues offer direct visual evidence of how stellar temperatures    vary.
Sci/Tech,""Britain granted its first license for human cloning Wednesday, joining South Korea on the leading edge of stem cell research, which is restricted by the Bush administration and which many scientists believe may lead to new treatments for a range of diseases.""
Sci/Tech,Meteorologists at North Carolina State University are working on a way to more accurately measure rainfall in small areas.
```

CPU
```
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")

scala> spark.read.option(""header"", ""true"").csv(""/data/tmp/6917/test.csv"").show()
+--------+--------------------+
|category|         description|
+--------+--------------------+
|Sci/Tech|The U.S. Forest S...|
|Sci/Tech|One of the pleasu...|
|Sci/Tech|Britain granted i...|
|Sci/Tech|Meteorologists at...|
+--------+--------------------+
```
GPU
```
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""true"")

scala> spark.read.option(""header"", ""true"").csv(""/data/tmp/6917/test.csv"").show()
+--------+--------------------+
|category|         description|
+--------+--------------------+
|Sci/Tech|The U.S. Forest S...|
|Sci/Tech|Meteorologists at...|
+--------+--------------------+
```",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1293187000/reactions,0,0,0,0,0,0,0,0,0,6917
523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681202001,https://github.com/NVIDIA/spark-rapids/issues/6930#issuecomment-1681202001,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6930,1681202001,IC_kwDOD7z77c5kNRtR,2023-08-16T20:05:07Z,2023-08-16T20:05:07Z,COLLABORATOR,"This should now be unblocked, or at least more of it should be unblocked now that https://github.com/rapidsai/cudf/issues/12013 has been finished.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681202001/reactions,0,0,0,0,0,0,0,0,0,6930
524,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1431879643,https://github.com/NVIDIA/spark-rapids/issues/6947#issuecomment-1431879643,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6947,1431879643,IC_kwDOD7z77c5VWL_b,2023-02-15T19:11:56Z,2023-02-15T19:11:56Z,COLLABORATOR,Comment to subscribe to the issue.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1431879643/reactions,0,0,0,0,0,0,0,0,0,6947
525,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1297703579,https://github.com/NVIDIA/spark-rapids/issues/6955#issuecomment-1297703579,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6955,1297703579,IC_kwDOD7z77c5NWWKb,2022-10-31T21:24:07Z,2022-10-31T21:24:07Z,COLLABORATOR,@GregoryKimball had another great idea for an optimization if the fallback is a part of a filter. As a part of the fallback for a filter. If the filter is in the form of `A and B and C and D` then we could process some of the expressions before we fall back to the CPU. So for example if `C` has an expression that would only work on the CPU we could first do a filter on `A and B and D` on the GPU. Then we could do the special project to help produce the CPU parts of `C` and then finally do a filter for `C` at the end. We should add part of this into the issue we file to look at `FilterExec`,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1297703579/reactions,1,1,0,0,0,0,0,0,0,6955
526,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299178149,https://github.com/NVIDIA/spark-rapids/issues/6968#issuecomment-1299178149,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6968,1299178149,IC_kwDOD7z77c5Nb-Kl,2022-11-01T21:12:01Z,2022-11-01T21:12:01Z,COLLABORATOR,The CUDF dependency is https://github.com/rapidsai/cudf/issues/12043,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299178149/reactions,0,0,0,0,0,0,0,0,0,6968
527,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299253927,https://github.com/NVIDIA/spark-rapids/issues/6970#issuecomment-1299253927,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6970,1299253927,IC_kwDOD7z77c5NcQqn,2022-11-01T21:47:07Z,2022-11-01T21:47:07Z,COLLABORATOR,The CUDF issue is https://github.com/rapidsai/cudf/issues/12044,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1299253927/reactions,0,0,0,0,0,0,0,0,0,6970
528,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1300836797,https://github.com/NVIDIA/spark-rapids/issues/6981#issuecomment-1300836797,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6981,1300836797,IC_kwDOD7z77c5NiTG9,2022-11-02T16:28:54Z,2022-11-02T16:28:54Z,COLLABORATOR,"As a follow on to this we might be able to play some performance games even when there is an order by column.  The performance of sorting a single simple columns is much less than sorting multiple columns.  Here is a quick test from my desktop.

```
spark.time(spark.range(10000000000L).selectExpr(""id"", ""500000000 - id as other"", ""id + 1000000 as even_other"")
   .orderBy(""other"").show())
5.2 seconds

spark.time(spark.range(10000000000L).selectExpr(""id"", ""500000000 - id as other"", ""id + 1000000 as even_other"")
   .orderBy(""other"", ""id"").show())
11.1 seconds

spark.time(spark.range(10000000000L).selectExpr(""id"", ""500000000 - id as other"", ""id + 1000000 as even_other"")
   .orderBy(""other"", ""id"", ""even_other"").show())
11.1 seconds
```

When going from 1 to 2 columns in the order by the time more than doubled, but when going from 2 to 3 columns the time stayed the same.

The point is that even if we have an order-by clause in the output and the order by is for a single simple column, then we could sort by the single simple column instead of sorting by all of the columns and still do the ""dynamic partition writer"" assuming that the order of the output rows is preserved when we do the partition call. Which I think it would have to for the proposed code above to do the right thing anyways.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1300836797/reactions,0,0,0,0,0,0,0,0,0,6981
529,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1300863422,https://github.com/NVIDIA/spark-rapids/issues/6981#issuecomment-1300863422,https://api.github.com/repos/NVIDIA/spark-rapids/issues/6981,1300863422,IC_kwDOD7z77c5NiZm-,2022-11-02T16:39:44Z,2022-11-02T16:39:44Z,MEMBER,"cudf does not guarantee the order within partitions after `partition` is called, so we could not rely on that.  Therefore if we're using cudf's `partition` we could only perform this optimization if the query does not require any order before the write.  However it may be difficult to distinguish in the physical CPU plan whether a required ordering on the write came from normal partitioning logic on the CPU or an explicit orderBy requested by the user as part of the write.

Alternatively we could work with cudf to implement a partition that does preserve order (or write our own).",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1300863422/reactions,0,0,0,0,0,0,0,0,0,6981
530,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1338389643,https://github.com/NVIDIA/spark-rapids/issues/7001#issuecomment-1338389643,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7001,1338389643,IC_kwDOD7z77c5PxjSL,2022-12-06T00:02:45Z,2022-12-06T00:02:45Z,COLLABORATOR,"Given that the approach in #7127 doesn't seem to improve performance at all (and generally performs quite poorly), I believe we should try another approach. What if we focus on the ProjectExec case, and split the ProjectExec into a GpuProjectExec (which runs expressions that can on GPU) and a regular ProjectExec (that runs expressions that only run on CPU)? We still put this approach behind a configuration option at first, because I also think there will be edge cases/scenarios where this approach doesn't work as well.  If this works, then we can pursue a similar approach with FilterExec.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1338389643/reactions,0,0,0,0,0,0,0,0,0,7001
531,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1339548928,https://github.com/NVIDIA/spark-rapids/issues/7001#issuecomment-1339548928,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7001,1339548928,IC_kwDOD7z77c5P1-UA,2022-12-06T15:25:01Z,2022-12-06T15:25:01Z,COLLABORATOR,"I would like to see if there is some way for us to use code gen for the execution.  Splitting the Project Exec off feels good, but is not going to work in every situation, so I would like to try a little bit harder here if we can.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1339548928/reactions,0,0,0,0,0,0,0,0,0,7001
532,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303761579,https://github.com/NVIDIA/spark-rapids/issues/7005#issuecomment-1303761579,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7005,1303761579,IC_kwDOD7z77c5NtdKr,2022-11-04T15:25:58Z,2022-11-04T15:25:58Z,COLLABORATOR,"Those are the reasons that we don't replace `CollectLimitExec` by default. It is difficult to actually make this work given our current rules (but not impossible).  From a performance standpoint we might just want to insert a GpuLocalLimitExec ahead of the CollectLimitExec. We are going to have to pull back N rows to the CPU anyways for the  `executeCollect` to work. In the common case only a single task is going to run with a `CollectLimitExec` and a reasonable number of rows, so why not just limit those rows before we copy it to the CPU?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1303761579/reactions,0,0,0,0,0,0,0,0,0,7005
533,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306509576,https://github.com/NVIDIA/spark-rapids/issues/7005#issuecomment-1306509576,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7005,1306509576,IC_kwDOD7z77c5N38EI,2022-11-08T02:03:21Z,2022-11-09T07:18:38Z,COLLABORATOR,"> In the common case only a single task is going to run with a CollectLimitExec and a reasonable number of rows, so why not just limit those rows before we copy it to the CPU?

Hi, not fully get what you mean for the last suggestion.

If `CollectLimitExec ` is always  or commonly used as the last operation, how about marking it not support columnar processing, and implement the `execute` and `executeCollect`, where it calls the child's `executeColumnar` and fetch only the limit rows, similar to what `GpuColumnarToRowExec` does.

Besides, add a rule to make sure a `ColumnarToRowExec ` will not be inserted before the `CollectLimitExec `. Then cases like show, take, head will run into `executeCollect ` path, where it avoids the shuffle in `execute`.

Of course there should be many details I miss here, but this is an overall idea.
",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1306509576/reactions,0,0,0,0,0,0,0,0,0,7005
534,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1307513698,https://github.com/NVIDIA/spark-rapids/issues/7005#issuecomment-1307513698,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7005,1307513698,IC_kwDOD7z77c5N7xNi,2022-11-08T16:45:20Z,2022-11-08T16:45:20Z,COLLABORATOR,"> If `CollectLimitExec ` is always or commonly used as the last operation, how about mark it not support columnar processing, and implement the `execute` and `executeCollect`, where it calls the child's `executeColumnar` and fetch only the limit rows, similar to what `GpuColumnarToRowExec` does.

That is essentially what I am suggesting, but hopefully with less code changes.

What we have today.

```
ColumnarProcessing ... -> ColumnarToRow -> CollectLimitExec
```

This works, but `ColumnarToRow` is going to move at least one full batch of data from the GPU to the CPU so that `CollectLimitExec` can throw away everything but the first N rows.

You are proposing to fix this by having a `GpuCollectLimitExec` that would do three steps.
1. Limit the input data using the GPU
2. Copy the columnar data to the CPU and convert it into rows
3. Return the row based data using the `executeCollect` API

```
ColumnarProcessing ... -> |            GpuCollectLimitExec            |
                          | Limit -> ColumnarToRow -> executeCollect  |
```

This is fine and works, but it means that we have to mark `GpuCollectLimitExec` as doing the columnar to row transition so the planning rules handle it properly and don't try to insert in extra transitions.

What I am proposing is to use existing Execs to do the same thing.

```
ColumnarProcessing ... -> GpuLocalLimitExec -> GpuColumnarToRowExec -> CollectLimitExec
```

@firestarman your solution would end up being cleaner on the UI, which is a big advantage for end users. So if you are willing to put in the effort to make that happen I would love to see it. I was just thinking of how we could do it with as little work as possible.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1307513698/reactions,0,0,0,0,0,0,0,0,0,7005
535,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1308941962,https://github.com/NVIDIA/spark-rapids/issues/7030#issuecomment-1308941962,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7030,1308941962,IC_kwDOD7z77c5OBN6K,2022-11-09T15:33:03Z,2022-11-09T15:33:03Z,MEMBER,This request is more about asking cast-as-date and date_sub to be supported as AST expressions than the join itself.  If those were supported by AST then the join would have been on the GPU.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1308941962/reactions,1,1,0,0,0,0,0,0,0,7030
536,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1322765267,https://github.com/NVIDIA/spark-rapids/issues/7030#issuecomment-1322765267,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7030,1322765267,IC_kwDOD7z77c5O18vT,2022-11-21T22:55:34Z,2022-11-21T22:55:34Z,COLLABORATOR,"Note if this issue is resolved, then the integration test used in #7037 will need to be updated as it currently relies on this running on the CPU (an unfortunate limitation of the testing scenario involved).",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1322765267/reactions,0,0,0,0,0,0,0,0,0,7030
537,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309226047,https://github.com/NVIDIA/spark-rapids/issues/7035#issuecomment-1309226047,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7035,1309226047,IC_kwDOD7z77c5OCTQ_,2022-11-09T19:02:47Z,2022-11-09T19:02:47Z,COLLABORATOR,Related cudf issue https://github.com/rapidsai/cudf/issues/8641,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309226047/reactions,0,0,0,0,0,0,0,0,0,7035
538,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309246807,https://github.com/NVIDIA/spark-rapids/issues/7035#issuecomment-1309246807,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7035,1309246807,IC_kwDOD7z77c5OCYVX,2022-11-09T19:20:32Z,2022-11-09T19:20:32Z,COLLABORATOR,"We need to be a bit careful here to be sure that our requirements match that of pandas/python. In Spark the output of all sha hashes is a string that is a lowercase HEX encoded representation of the binary hash result. This corresponds to the `hexdigest()` method. Also https://github.com/rapidsai/cudf/issues/8641 calls out sha256 and sha512, not sha1. Spark supports sha1 and sha2. sha2 supports bit lengths of 224, 256, 384, and 512. sha2 matches sha256 and sha512.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309246807/reactions,0,0,0,0,0,0,0,0,0,7035
539,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309567722,https://github.com/NVIDIA/spark-rapids/issues/7035#issuecomment-1309567722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7035,1309567722,IC_kwDOD7z77c5ODmrq,2022-11-10T00:08:18Z,2022-11-10T00:08:18Z,COLLABORATOR,"The PR https://github.com/rapidsai/cudf/pull/9215 attached to https://github.com/rapidsai/cudf/issues/8641 appears to be adding support for SHA-1.  

",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1309567722/reactions,0,0,0,0,0,0,0,0,0,7035
540,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1889769810,https://github.com/NVIDIA/spark-rapids/issues/7035#issuecomment-1889769810,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7035,1889769810,IC_kwDOD7z77c5wo5lS,2024-01-12T18:30:47Z,2024-01-12T18:30:47Z,COLLABORATOR,Depends on https://github.com/rapidsai/cudf/pull/14391,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1889769810/reactions,0,0,0,0,0,0,0,0,0,7035
541,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1310985607,https://github.com/NVIDIA/spark-rapids/issues/7047#issuecomment-1310985607,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7047,1310985607,IC_kwDOD7z77c5OJA2H,2022-11-10T22:31:24Z,2022-11-10T22:31:24Z,COLLABORATOR,note https://github.com/NVIDIA/spark-rapids/pull/7049/files is a fix to help with this.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1310985607/reactions,0,0,0,0,0,0,0,0,0,7047
542,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1324754757,https://github.com/NVIDIA/spark-rapids/issues/7062#issuecomment-1324754757,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7062,1324754757,IC_kwDOD7z77c5O9idF,2022-11-23T09:18:21Z,2022-11-23T09:18:21Z,COLLABORATOR,"The Alluxio can evict the not used caches.
Some delta parquet files are useless if users query the latest data, and Alluxio can only cache the needed files.
I think Alluxio can manage the caches according to the actual access times.
Do we need to do this?



",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1324754757/reactions,0,0,0,0,0,0,0,0,0,7062
543,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1329352507,https://github.com/NVIDIA/spark-rapids/issues/7062#issuecomment-1329352507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7062,1329352507,IC_kwDOD7z77c5PPE87,2022-11-28T16:05:56Z,2022-11-28T16:05:56Z,COLLABORATOR,"I'm not sure what you mean by this?  This request says to not cache them regular parquet files that are not delta files, it has nothing to do with eviction because it wouldn't be cached.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1329352507/reactions,0,0,0,0,0,0,0,0,0,7062
544,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1329354331,https://github.com/NVIDIA/spark-rapids/issues/7062#issuecomment-1329354331,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7062,1329354331,IC_kwDOD7z77c5PPFZb,2022-11-28T16:07:08Z,2022-11-28T16:07:08Z,COLLABORATOR,Now we may only be able to estimate this because a user probably could access a delta parquet file directly but we don't expect that.  If they use the delta file index we should consider a delta file and if not then we don't cache it.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1329354331/reactions,0,0,0,0,0,0,0,0,0,7062
545,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998738509,https://github.com/NVIDIA/spark-rapids/issues/7062#issuecomment-1998738509,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7062,1998738509,IC_kwDOD7z77c53IlRN,2024-03-15T01:30:19Z,2024-03-15T01:31:22Z,COLLABORATOR,"We are using filecache instead of Alluxio, so unassign from me.
Suggest close it.

",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998738509/reactions,0,0,0,0,0,0,0,0,0,7062
546,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1321309572,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1321309572,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1321309572,IC_kwDOD7z77c5OwZWE,2022-11-21T01:09:44Z,2022-11-21T01:09:44Z,COLLABORATOR,"cc @abellina to help, thanks",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1321309572/reactions,0,0,0,0,0,0,0,0,0,7116
547,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1322562436,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1322562436,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1322562436,IC_kwDOD7z77c5O1LOE,2022-11-21T19:45:15Z,2022-11-21T19:45:50Z,COLLABORATOR,"Note this is not going to happen in 22.12 very likely. It will have to wait for a UCX release, I am not sure what the state of that is yet, but I can update here when I learn more.

FYI @sameerz @mattahrens @GaryShen2008 ",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1322562436/reactions,0,0,0,0,0,0,0,0,0,7116
548,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378013902,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1378013902,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1378013902,IC_kwDOD7z77c5SItLO,2023-01-10T23:09:19Z,2023-01-10T23:09:19Z,COLLABORATOR,"PR open to support this for jucx: to support this: https://github.com/openucx/ucx/pull/8796

Overall issue: https://github.com/openucx/ucx/issues/8762",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378013902/reactions,0,0,0,0,0,0,0,0,0,7116
549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833235334,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1833235334,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1833235334,IC_kwDOD7z77c5tRPOG,2023-11-30T07:23:48Z,2023-11-30T07:24:09Z,COLLABORATOR,"bump up...

https://github.com/openucx/ucx/issues/8762 @abellina I saw ucx repo has merged several updates for arm releases, but at least in jucx 1.15.0 there is still only `libjucx_amd64.so` but no aarch64 content. is there any update?",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833235334/reactions,0,0,0,0,0,0,0,0,0,7116
550,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836458805,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1836458805,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1836458805,IC_kwDOD7z77c5tdiM1,2023-12-01T16:56:08Z,2023-12-01T16:56:08Z,COLLABORATOR,"@pxLi you are right it's not there: https://github.com/openucx/ucx/issues/8762#issuecomment-1836456087. Asking the UCX team for help.

",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836458805/reactions,0,0,0,0,0,0,0,0,0,7116
551,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836560913,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1836560913,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1836560913,IC_kwDOD7z77c5td7IR,2023-12-01T18:10:24Z,2023-12-01T18:10:24Z,COLLABORATOR,@pxLi how do we turn on the shuffle smoke tests for the ARM build? I feel we could have caught this if the CI for x86/arm were similar (not to say that I shouldn't have manually tested it),,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836560913/reactions,0,0,0,0,0,0,0,0,0,7116
552,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1837696758,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1837696758,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1837696758,IC_kwDOD7z77c5tiQb2,2023-12-04T01:26:40Z,2023-12-04T01:31:15Z,COLLABORATOR,"> @pxLi how do we turn on the shuffle smoke tests for the ARM build? I feel we could have caught this if the CI for x86/arm were similar (not to say that I shouldn't have manually tested it)

We are trying to add 24.02 arm sanity test (due to lack of resource we would only test IT against spark341 for arm), feel free to update https://github.com/NVIDIA/spark-rapids/blob/branch-24.02/jenkins/spark-tests.sh#L250C1-L250C26 if need some extra stuff for arm shuffle one (like provide ENV to trigger special cases or configs in pipeline)",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1837696758/reactions,1,0,0,0,0,0,1,0,0,7116
553,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1838638552,https://github.com/NVIDIA/spark-rapids/issues/7116#issuecomment-1838638552,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7116,1838638552,IC_kwDOD7z77c5tl2XY,2023-12-04T13:23:51Z,2023-12-04T13:23:51Z,COLLABORATOR,"Thanks @pxLi. Nothing special should happen for the arm UCX case (once we get a jucx fix). It should automatically pick up the right shared library, so the same smoke tests we have for x86 should work.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1838638552/reactions,1,1,0,0,0,0,0,0,0,7116
554,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340053157,https://github.com/NVIDIA/spark-rapids/issues/7230#issuecomment-1340053157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7230,1340053157,IC_kwDOD7z77c5P35al,2022-12-06T21:47:18Z,2022-12-06T21:47:18Z,COLLABORATOR,Depends on https://github.com/rapidsai/cudf/issues/12298,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340053157/reactions,0,0,0,0,0,0,0,0,0,7230
555,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499307935,https://github.com/NVIDIA/spark-rapids/issues/7230#issuecomment-1499307935,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7230,1499307935,IC_kwDOD7z77c5ZXZ-f,2023-04-06T16:11:11Z,2023-04-06T16:11:11Z,COLLABORATOR,"The example above is only array-of-array. For complete support (remaining array-of-struct), this also depends on:
 * https://github.com/rapidsai/cudf/issues/11222",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499307935/reactions,0,0,0,0,0,0,0,0,0,7230
556,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998739075,https://github.com/NVIDIA/spark-rapids/issues/7240#issuecomment-1998739075,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7240,1998739075,IC_kwDOD7z77c53IlaD,2024-03-15T01:31:08Z,2024-03-15T01:31:08Z,COLLABORATOR,"We are using filecache instead of Alluxio, so unassign from me.
Suggest close it.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998739075/reactions,0,0,0,0,0,0,0,0,0,7240
557,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1337432333,https://github.com/NVIDIA/spark-rapids/issues/7241#issuecomment-1337432333,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7241,1337432333,IC_kwDOD7z77c5Pt5kN,2022-12-05T14:08:40Z,2022-12-05T14:08:40Z,COLLABORATOR,this is the same as https://github.com/NVIDIA/spark-rapids/issues/6463,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1337432333/reactions,0,0,0,0,0,0,0,0,0,7241
558,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998739638,https://github.com/NVIDIA/spark-rapids/issues/7241#issuecomment-1998739638,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7241,1998739638,IC_kwDOD7z77c53Ili2,2024-03-15T01:31:50Z,2024-03-15T01:31:50Z,COLLABORATOR,"
We are using filecache instead of Alluxio, so unassign from me.
Suggest close it.

",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998739638/reactions,0,0,0,0,0,0,0,0,0,7241
559,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340245009,https://github.com/NVIDIA/spark-rapids/issues/7289#issuecomment-1340245009,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7289,1340245009,IC_kwDOD7z77c5P4oQR,2022-12-07T01:21:21Z,2022-12-07T01:21:21Z,COLLABORATOR,"There are some challenges to ascertain the behaviour of Hive's `LazyTimestamp` parsing, and mimicking it on GPU. For instance, consider the following input strings:
```
2020-09-16 22:32:01
2020-09-16 22:32:01.1
2020-09-16 22:32:01.12
2020-09-16 22:32:01.123
2020-09-16 22:32:01.1234
2020-09-16 22:32:01.12345
2020-09-16 22:32:01.123456
2020-09-16 22:32:01.123456
```
When parsed with `TABLEPROPERTIES( 'timestamp.formats'='yyyy-MM-dd HH:mm:ss.SSS' )`, Apache Spark yields:
```
2020-09-16 22:32:01
2020-01-16 22:32:01.1
2020-01-16 22:32:01.12
2020-01-16 22:32:01.123
2020-09-16 22:32:01.1234
2020-09-16 22:32:01.12345
2020-09-16 22:32:01.123456
2020-09-16 22:32:01.123456
```
The same with `'timestamp.formats'='yyyy-MM-dd HH:mm:ss.SSSS'` yields:
```
2020-09-16 22:32:01
2020-09-16 22:32:01.1
2020-09-16 22:32:01.12
2020-09-16 22:32:01.123
2020-09-16 22:32:01.123
2020-09-16 22:32:01.12345
2020-09-16 22:32:01.123456
2020-09-16 22:32:01.123456
```
And with `'timestamp.formats'='yyyy-MM-dd HH:mm:ss.SSSSS'`, we get:
```
2020-09-16 22:32:01
2020-09-16 22:32:01.1
2020-09-16 22:32:01.12
2020-09-16 22:32:01.123
2020-09-16 22:32:01.123
2020-09-16 22:32:01.123
2020-09-16 22:32:01.123456
2020-09-16 22:32:01.123456
```
Et cetera. I'm not sure if this is intentional behaviour.
",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340245009/reactions,0,0,0,0,0,0,0,0,0,7289
560,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340285707,https://github.com/NVIDIA/spark-rapids/issues/7289#issuecomment-1340285707,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7289,1340285707,IC_kwDOD7z77c5P4yML,2022-12-07T02:23:16Z,2022-12-07T02:23:16Z,COLLABORATOR,"It isn't clear what timestamp formats are supported in `LazySimpleSerDe`. For instance, setting the format to `yyyy-MM-ddTHH:mm:ss.SSS` causes an `IllegalArgumentException` *at the time the `TBLPROPERTIES` are altered/set*.

On the other hand, setting it to something clearly borked, like `yyyy-MM-dd HH:mm:ss.SSS[SSS}` seems to fail silently, falling back to defaults.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1340285707/reactions,0,0,0,0,0,0,0,0,0,7289
561,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1341958948,https://github.com/NVIDIA/spark-rapids/issues/7289#issuecomment-1341958948,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7289,1341958948,IC_kwDOD7z77c5P_Ksk,2022-12-08T04:00:46Z,2022-12-08T04:00:46Z,COLLABORATOR,"Here is another interesting corner case: `timestamp.formats` may be specified in either of `TBLPROPERTIES` or `SERDEPROPERTIES` or, critically, _both_.

When specified in both, `TBLPROPERTIES` takes preference. E.g. Consider this table:
```
# Detailed Table Information                                                
Database                default                                     
Table                   mytable                                     
Owner                   mithunr                                     
Created Time            Wed Dec 07 19:53:31 PST 2022                        
Last Access             UNKNOWN                                     
Created By              Spark 3.2.1                                 
Type                    EXTERNAL                                    
Provider                hive                                        
Table Properties        [timestamp.formats=dd/MM/yyyy HH:mm:ss.SSS, transient_lastDdlTime=1670471651]                       
Statistics              437 bytes                                   
Location                file:/tmp/mytable                           
Serde Library           org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                          
InputFormat             org.apache.hadoop.mapred.TextInputFormat                            
OutputFormat            org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                          
Storage Properties      [serialization.format=1, timestamp.formats=yyyy/MM/dd HH:mm:ss.SSS]                         
Partition Provider      Catalog                                     
```
Table properties: dd/MM/yyyy
SerDe properties: yyyy/MM/dd

Only `dd/MM/yyyy` is honoured:
```
""2020/09/16 22:32:01.1"" => NULL
""16/09/2020 22:32:01.2"" => 2020-09-16 22:32:01.2
```

(I haven't yet tested what happens with different specifications per partition.)",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1341958948/reactions,0,0,0,0,0,0,0,0,0,7289
562,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551266739,https://github.com/NVIDIA/spark-rapids/issues/7332#issuecomment-1551266739,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7332,1551266739,IC_kwDOD7z77c5cdnOz,2023-05-17T12:01:32Z,2023-05-17T12:01:32Z,COLLABORATOR,Is there any way to test OCI after supporting it?,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551266739/reactions,0,0,0,0,0,0,0,0,0,7332
563,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1552593825,https://github.com/NVIDIA/spark-rapids/issues/7332#issuecomment-1552593825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7332,1552593825,IC_kwDOD7z77c5cirOh,2023-05-18T07:04:35Z,2023-05-18T07:04:35Z,COLLABORATOR,"`spark.rapids.cloudSchemes` is a Spark config so you can run the job with an additional config as

```
--conf spark.rapids.cloudSchemes=""oci""
```
, then the RAPIDS Accelerator will treat it as a cloud scheme and use the MULTITHREADED reader.

So nothing is required unless we want to add in `oci` to the default list.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1552593825/reactions,0,0,0,0,0,0,0,0,0,7332
564,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000333068,https://github.com/NVIDIA/spark-rapids/issues/7332#issuecomment-2000333068,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7332,2000333068,IC_kwDOD7z77c53OqkM,2024-03-15T19:36:57Z,2024-03-15T19:36:57Z,COLLABORATOR,"Note, in my initial testing I wasn't seeing much difference in making this cloud but need to investigate read performance more, removing from sprint for now",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000333068/reactions,0,0,0,0,0,0,0,0,0,7332
565,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1897871659,https://github.com/NVIDIA/spark-rapids/issues/7353#issuecomment-1897871659,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7353,1897871659,IC_kwDOD7z77c5xHzkr,2024-01-18T06:19:14Z,2024-01-18T06:19:14Z,COLLABORATOR,"Hi Teams, do we still have this issue?",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1897871659/reactions,0,0,0,0,0,0,0,0,0,7353
566,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1369947616,https://github.com/NVIDIA/spark-rapids/issues/7423#issuecomment-1369947616,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7423,1369947616,IC_kwDOD7z77c5Rp73g,2023-01-03T16:07:46Z,2023-01-03T16:07:46Z,COLLABORATOR,"@sameerz we need to mark this as a blocker for the next release, unless we are going to drop support for CDH. Hive text reads are enabled by default.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1369947616/reactions,1,1,0,0,0,0,0,0,0,7423
567,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1370060253,https://github.com/NVIDIA/spark-rapids/issues/7423#issuecomment-1370060253,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7423,1370060253,IC_kwDOD7z77c5RqXXd,2023-01-03T17:51:44Z,2023-01-03T17:51:44Z,COLLABORATOR,Never mind I didn't see how it without being logged in.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1370060253/reactions,0,0,0,0,0,0,0,0,0,7423
568,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1374314533,https://github.com/NVIDIA/spark-rapids/issues/7423#issuecomment-1374314533,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7423,1374314533,IC_kwDOD7z77c5R6mAl,2023-01-07T00:45:28Z,2023-01-07T13:56:10Z,COLLABORATOR,"I have explored this a little bit on CDH. It appears that the Hive text reader on CDH behaves differently from Apache Spark, Databricks, etc.

1. `simple_floats`/`decimal(10,3)`: On CDH, 3 rows are read differently from other distros, because CDH appears to be trimming the input strings. (Like `spark-rapids` used to.)
2. `simple_floats`/`decimal(38,10)`: Same as above.
3. `simple_ints`/`decimal(10,2-3)`: Again, input trimming.
4. `timestamps`: Several discrepancies from other distros:
   - Spaces are trimmed.
   - The timestamp regex accepts incomplete timestamps, e.g. 
     - `2020-09-16`
     - `2020-09-16 22:32`
   - The timestamp regex accepts timestamps with `T` separating the date and timestamp. E.g.
     - `2020-09-16T22:32`
5. `dates`: 2 major differences:
   - Spaces are trimmed.
   - Timestamps with and without `T` in them are honoured.

This does appear to be a secret-sauce situation. We need a CDH-specific shim for this reader.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1374314533/reactions,0,0,0,0,0,0,0,0,0,7423
569,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1379339004,https://github.com/NVIDIA/spark-rapids/issues/7423#issuecomment-1379339004,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7423,1379339004,IC_kwDOD7z77c5SNwr8,2023-01-11T18:51:10Z,2023-01-11T18:57:52Z,COLLABORATOR,"FWIW, I have ~filed~ merged https://github.com/NVIDIA/spark-rapids/pull/7486 to provide a way for the CDH shim to program an out.
In the short term, CDH can override `HiveTableScanExec` to always run on CPU. An example is provided in https://github.com/NVIDIA/spark-rapids/commit/2d2943b59acedbc8dfbf8f8e1e15757be5ed8c70.
In the longer term, CDH can customize the regexes for dates/timestamps, and override the whitespace stripping behaviour.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1379339004/reactions,0,0,0,0,0,0,0,0,0,7423
570,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377955734,https://github.com/NVIDIA/spark-rapids/issues/7450#issuecomment-1377955734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7450,1377955734,IC_kwDOD7z77c5SIe-W,2023-01-10T22:06:25Z,2023-01-10T22:06:25Z,COLLABORATOR,"As long as the physical type is not exposed in an operator we are replacing, we do not need to take action on this.  However it would be a good optimization for the long term.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377955734/reactions,0,0,0,0,0,0,0,0,0,7450
571,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1370534585,https://github.com/NVIDIA/spark-rapids/issues/7452#issuecomment-1370534585,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7452,1370534585,IC_kwDOD7z77c5RsLK5,2023-01-04T06:31:07Z,2023-01-04T06:31:07Z,COLLABORATOR,This affects both Spark 3.4 and Spark 3.3.2,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1370534585/reactions,0,0,0,0,0,0,0,0,0,7452
572,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371160023,https://github.com/NVIDIA/spark-rapids/issues/7452#issuecomment-1371160023,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7452,1371160023,IC_kwDOD7z77c5Ruj3X,2023-01-04T16:38:44Z,2023-01-04T16:38:44Z,COLLABORATOR,"We currently do not support ""_metadata"" columns and fall back to the CPU if we see it (this is for Spark 3.3.0).

```
scala> spark.read.parquet(""./target/DF"").selectExpr(""*"", ""_metadata"").show(truncate=false)
23/01/04 16:31:07 WARN GpuOverrides: 
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> cast(a#64 as string) AS a#80 will run on GPU
      *Expression <Cast> cast(a#64 as string) will run on GPU
    *Expression <Alias> cast(_metadata#70 as string) AS _metadata#83 will run on GPU
      *Expression <Cast> cast(_metadata#70 as string) will run on GPU
    *Exec <ProjectExec> will run on GPU
      *Expression <Alias> named_struct(file_path, file_path#88, file_name, file_name#89, file_size, file_size#90L, file_modification_time, file_modification_time#91) AS _metadata#70 will run on GPU
        *Expression <CreateNamedStruct> named_struct(file_path, file_path#88, file_name, file_name#89, file_size, file_size#90L, file_modification_time, file_modification_time#91) will run on GPU
      !Exec <FileSourceScanExec> cannot run on GPU because hidden metadata columns are not supported on GPU
```

I am not sure about `row_index`. I don't see that in any of the PRs for the spark issue. It looks like we actually do all of the work for this on the GPU already, so it might be worth not falling back to the CPU and adding in some tests to cover it, but that is a separate issue.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371160023/reactions,0,0,0,0,0,0,0,0,0,7452
573,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371774374,https://github.com/NVIDIA/spark-rapids/issues/7452#issuecomment-1371774374,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7452,1371774374,IC_kwDOD7z77c5Rw52m,2023-01-05T04:40:43Z,2023-01-05T04:40:43Z,COLLABORATOR,Thank you for the explanation @revans2. ,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371774374/reactions,0,0,0,0,0,0,0,0,0,7452
574,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371776665,https://github.com/NVIDIA/spark-rapids/issues/7452#issuecomment-1371776665,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7452,1371776665,IC_kwDOD7z77c5Rw6aZ,2023-01-05T04:45:55Z,2023-01-05T04:48:51Z,COLLABORATOR,File #7458 to track the idea in your comment.,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1371776665/reactions,0,0,0,0,0,0,0,0,0,7452
575,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1372439472,https://github.com/NVIDIA/spark-rapids/issues/7458#issuecomment-1372439472,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7458,1372439472,IC_kwDOD7z77c5RzcOw,2023-01-05T16:25:47Z,2023-01-05T16:25:47Z,COLLABORATOR,"When we first wrote the FileSourceScanExec code we fell back to the CPU if we saw a ""_metadata"" column in the output.  But looking at how that ""_metadata"" column is produced it looks like we should be able to support it. We just need to understand a bit better exactly what is happening here.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1372439472/reactions,0,0,0,0,0,0,0,0,0,7458
576,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1375131619,https://github.com/NVIDIA/spark-rapids/issues/7468#issuecomment-1375131619,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7468,1375131619,IC_kwDOD7z77c5R9tfj,2023-01-09T05:46:40Z,2023-01-09T05:46:40Z,COLLABORATOR,This is a follow-up of #6822 ,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1375131619/reactions,0,0,0,0,0,0,0,0,0,7468
577,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377981926,https://github.com/NVIDIA/spark-rapids/issues/7478#issuecomment-1377981926,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7478,1377981926,IC_kwDOD7z77c5SIlXm,2023-01-10T22:31:59Z,2023-01-10T22:32:37Z,COLLABORATOR,"Need to test whether casting from a date or timestamp to timestampNTZ falls back to the CPU.  If so, we should add support for casting from a UTC date or a timestamp to timestampNTZ if it is easy enough to do.  If it is not easy to add support, file a follow on issue.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377981926/reactions,0,0,0,0,0,0,0,0,0,7478
578,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377991734,https://github.com/NVIDIA/spark-rapids/issues/7482#issuecomment-1377991734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7482,1377991734,IC_kwDOD7z77c5SInw2,2023-01-10T22:43:36Z,2023-01-10T22:43:36Z,COLLABORATOR,We should test the behavior of the JSON parser in RAPIDS Spark and see whether it matches the behavior described in [SPARK-40646](https://issues.apache.org/jira/browse/SPARK-40646).  Then we can determine next steps.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1377991734/reactions,0,0,0,0,0,0,0,0,0,7482
579,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378009448,https://github.com/NVIDIA/spark-rapids/issues/7490#issuecomment-1378009448,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7490,1378009448,IC_kwDOD7z77c5SIsFo,2023-01-10T23:04:18Z,2023-01-10T23:04:18Z,COLLABORATOR,This fix also went into Spark 3.3.2.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1378009448/reactions,0,0,0,0,0,0,0,0,0,7490
580,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1410608348,https://github.com/NVIDIA/spark-rapids/issues/7496#issuecomment-1410608348,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496,1410608348,IC_kwDOD7z77c5UFCzc,2023-01-31T15:38:59Z,2023-01-31T15:38:59Z,COLLABORATOR,"just a note a few things of interest:
1) CPU job fails with executors lost
2) Seeing a job failure that is really a job cancelled due to replanning on the GPU runs.  This isn't actually a failure though.  
3) with gpu run and g4dn.xlarge I do see some executors lost but doesn't fail job, just a couple task failures.  Likely possible to fail entire job like CPU did.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1410608348/reactions,1,1,0,0,0,0,0,0,0,7496
581,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577221506,https://github.com/NVIDIA/spark-rapids/issues/7496#issuecomment-1577221506,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7496,1577221506,IC_kwDOD7z77c5eAn2C,2023-06-05T17:50:06Z,2023-06-05T17:50:06Z,COLLABORATOR,"So this is still an issue where @tgravescs has seen executors killed, but it is unclear what the reason is. It requires further investigation.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577221506/reactions,0,0,0,0,0,0,0,0,0,7496
582,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1382169057,https://github.com/NVIDIA/spark-rapids/issues/7511#issuecomment-1382169057,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7511,1382169057,IC_kwDOD7z77c5SYjnh,2023-01-13T17:29:04Z,2023-01-13T17:29:04Z,COLLABORATOR,"We do. We had to copy/port over the push helper and the block fetcher iterator, both classes were updated in this PR. It would make sense to ""sync up"" these classes.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1382169057/reactions,0,0,0,0,0,0,0,0,0,7511
583,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1386215998,https://github.com/NVIDIA/spark-rapids/issues/7520#issuecomment-1386215998,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7520,1386215998,IC_kwDOD7z77c5Sn_o-,2023-01-17T23:19:55Z,2023-01-17T23:19:55Z,COLLABORATOR,We need to either fallback to the CPU in ANSI mode or implement the new rounding behavior.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1386215998/reactions,0,0,0,0,0,0,0,0,0,7520
584,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387467147,https://github.com/NVIDIA/spark-rapids/issues/7520#issuecomment-1387467147,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7520,1387467147,IC_kwDOD7z77c5SsxGL,2023-01-18T17:42:17Z,2023-01-18T17:42:17Z,COLLABORATOR,Another commit for this:  [apache/spark@42721120f3](https://github.com/apache/spark/commit/42721120f3),,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387467147/reactions,0,0,0,0,0,0,0,0,0,7520
585,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387224502,https://github.com/NVIDIA/spark-rapids/issues/7525#issuecomment-1387224502,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7525,1387224502,IC_kwDOD7z77c5Sr122,2023-01-18T15:08:16Z,2023-01-18T15:08:16Z,COLLABORATOR,"This change is just a few lines that operate on the logical plan. This should have no impact to our ability to run the resulting command. 

It would be nice to see how the configs that they set operate in practice for the GPU, and if we can implement something to improve the NDS performance using them.  But they are both off by default, so we would need to understand why. One of them uses a BloomFilter aggregation and check with an xxhash64. We don't currently support either of them, and we are not likely to support them any time soon.

The other one uses a sub-query to try and do something similar, so it might be interesting to see what it does by itself.

```
spark.sql.optimizer.runtime.bloomFilter.enabled=false
spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled=true
```",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387224502/reactions,0,0,0,0,0,0,0,0,0,7525
586,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387704224,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1387704224,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1387704224,IC_kwDOD7z77c5Stq-g,2023-01-18T20:03:42Z,2023-01-18T20:03:42Z,MEMBER,"@revans2 have you taken an Nsight Systems trace of this case?  Curious if it's primarily a problem with building the hash table vs. probing it via the stream table (or both).  My guess is we're getting killed by the thread collisions on the same key when trying to build the hash table, and it's not so much an issue when we probe it later.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387704224/reactions,0,0,0,0,0,0,0,0,0,7529
587,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387713922,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1387713922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1387713922,IC_kwDOD7z77c5SttWC,2023-01-18T20:11:02Z,2023-01-18T20:11:02Z,COLLABORATOR,I stopped at finding the issue. I had already spent enough time on this that I wanted to stop and check in on the priority for next steps.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1387713922/reactions,0,0,0,0,0,0,0,0,0,7529
588,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397424442,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1397424442,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1397424442,IC_kwDOD7z77c5TSwE6,2023-01-19T18:25:38Z,2023-01-19T18:25:38Z,COLLABORATOR,"Okay, I think this is really just us doing a very bad job a join output estimation. The join output estimation code looks at the build side of the table and guesses at a row multiplication factor based off of the average key count. In this case we have a build side with a relatively low cardinality and lots fo duplication. The estimate ends up being about 20,000 for anything that has a build table.  The stream side has a lot of values that are not in the build table. So the average size increase is actually 210, not 20,000. 

So the more we end up partitioning the input the more likely it is that we end up with an accurate estimate. In many cases the estimation is 0 because there is no build table. In the other cases we have less keys on the stream side that don't match anything.

I don't think that this is likely to happen in reality, but it could. The only real way to fix this is to have better join estimation that takes into account the stream table too.  We can probably do that if we had some kind of bloom filter in place that would be really nice to look into, and hopefully not too difficult to implement. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397424442/reactions,0,0,0,0,0,0,0,0,0,7529
589,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397430915,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1397430915,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1397430915,IC_kwDOD7z77c5TSxqD,2023-01-19T18:31:37Z,2023-01-19T18:31:37Z,MEMBER,"I assume another alternative is to finally solve #2440 where we can get the join output size ""for free"" in most cases but find a way to mitigate the performance pitfalls we ran into there.  IIRC the biggest issue with the last attempt was needing to throw away the built hash table when we decided to split the table we used to build that hash table (e.g.: inner join where we can freely pick).  Join code doesn't currently support splitting what was declared the build side, but the libcudf join algorithm is slower if we don't always pick the smaller table as the build table.  Thus, if the stream batch is smaller than the build batch, we should split the build batch but currently the code isn't prepped to handle that.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397430915/reactions,0,0,0,0,0,0,0,0,0,7529
590,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397434937,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1397434937,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1397434937,IC_kwDOD7z77c5TSyo5,2023-01-19T18:35:30Z,2023-01-19T18:35:30Z,COLLABORATOR,"Yes, but how much slower is it to always have the build side be the build side in CUDF? And can we use AQE or something to dynamically switch the build side so we know which one is truly smaller? I think there are ways to fix those issues too.  We might want to have a JOIN optimization epic of some sort where we come up with a plan on how to avoid some of these pitfalls.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397434937/reactions,0,0,0,0,0,0,0,0,0,7529
591,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397438542,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1397438542,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1397438542,IC_kwDOD7z77c5TSzhO,2023-01-19T18:39:00Z,2023-01-19T18:39:00Z,MEMBER,"> Yes, but how much slower is it to always have the build side be the build side in CUDF? 

Slow enough that we didn't check in the first PR attempt because benchmarks showed the guesstimate was faster than the ""for free"" approach.  The biggest issue there is that we don't have a way to make the built hash table spillable, so we can't save it across batches.  If we could not have to rebuild the hash table for every batch, that could be a big win.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397438542/reactions,0,0,0,0,0,0,0,0,0,7529
592,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397453409,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-1397453409,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,1397453409,IC_kwDOD7z77c5TS3Jh,2023-01-19T18:53:23Z,2023-01-19T18:53:23Z,COLLABORATOR,"> If we could not have to rebuild the hash table for every batch, that could be a big win.

Exactly that is what I thought the slowness was. That picking the wrong side was slightly slower in most cases, but in a few it was rather bad. But we were not able to offset the slowness because we had to rebuild the has table each time. I would love to see us resurrect that code and see what happens if we just keep the hash table around.  Just for an experiment to see if it is worth spending the time to make the hash table spillable.  If it is a huge win, then it might be worth it. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1397453409/reactions,0,0,0,0,0,0,0,0,0,7529
593,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138135160,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-2138135160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,2138135160,IC_kwDOD7z77c5_cVp4,2024-05-29T19:39:05Z,2024-05-29T19:39:05Z,MEMBER,"We are about to close the related cudf issue (https://github.com/rapidsai/cudf/issues/14948) and want to ensure we understand the performance regression described here properly.

Going through the above discussions, it seems that #2440 can help solve the problem and no actions are needed on the libcudf side, did I miss something?",,PointKernel,12716979,MDQ6VXNlcjEyNzE2OTc5,https://avatars.githubusercontent.com/u/12716979?v=4,,https://api.github.com/users/PointKernel,https://github.com/PointKernel,https://api.github.com/users/PointKernel/followers,https://api.github.com/users/PointKernel/following{/other_user},https://api.github.com/users/PointKernel/gists{/gist_id},https://api.github.com/users/PointKernel/starred{/owner}{/repo},https://api.github.com/users/PointKernel/subscriptions,https://api.github.com/users/PointKernel/orgs,https://api.github.com/users/PointKernel/repos,https://api.github.com/users/PointKernel/events{/privacy},https://api.github.com/users/PointKernel/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138135160/reactions,0,0,0,0,0,0,0,0,0,7529
594,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138241322,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-2138241322,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,2138241322,IC_kwDOD7z77c5_cvkq,2024-05-29T20:50:43Z,2024-05-29T20:50:43Z,MEMBER,"@PointKernel rapidsai/cudf#14948 doesn't seem to be related?  I'm assuming this is in reference to rapidsai/cudf#15262.  If so, curious why the latter is going to be closed?  It's not clear to me how optimizations around distinct joins apply to this problem where the issue is about low cardinality, highly duplicated build-side keys that have many collisions and potentially long chain walking when building the hash table.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138241322/reactions,0,0,0,0,0,0,0,0,0,7529
595,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138286065,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-2138286065,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,2138286065,IC_kwDOD7z77c5_c6fx,2024-05-29T21:22:41Z,2024-05-29T21:22:41Z,MEMBER,"I see, https://github.com/rapidsai/cudf/issues/14948 is indeed unrelated then. https://github.com/rapidsai/cudf/issues/15262 probably describes another problem as well since it's for low cardinality groupby IIUC.

>  the issue is about low cardinality, highly duplicated build-side keys that have many collisions and potentially long chain walking when building the hash table

Tuning CG size can help in this case https://github.com/rapidsai/cudf/blob/9192d259633c382c6f98f956dc7f43d754ebbf44/cpp/include/cudf/detail/join.hpp#L46. Using a larger CG size like 4, 8 or 16 instead of 2 can help with high-multiplicity cases. Is it easy for you to test the performance impact of larger CG sizes? If it's proved to be effective, libcudf can expose CG size in public APIs to make your tuning work easier.",,PointKernel,12716979,MDQ6VXNlcjEyNzE2OTc5,https://avatars.githubusercontent.com/u/12716979?v=4,,https://api.github.com/users/PointKernel,https://github.com/PointKernel,https://api.github.com/users/PointKernel/followers,https://api.github.com/users/PointKernel/following{/other_user},https://api.github.com/users/PointKernel/gists{/gist_id},https://api.github.com/users/PointKernel/starred{/owner}{/repo},https://api.github.com/users/PointKernel/subscriptions,https://api.github.com/users/PointKernel/orgs,https://api.github.com/users/PointKernel/repos,https://api.github.com/users/PointKernel/events{/privacy},https://api.github.com/users/PointKernel/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138286065/reactions,0,0,0,0,0,0,0,0,0,7529
596,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138306017,https://github.com/NVIDIA/spark-rapids/issues/7529#issuecomment-2138306017,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7529,2138306017,IC_kwDOD7z77c5_c_Xh,2024-05-29T21:38:36Z,2024-05-29T21:38:36Z,MEMBER,"@PointKernel thanks for the pointers.  Can you comment on rapidsai/cudf#15262?  That's the issue tracking the poor performance of highly duplicated build-side keys when building hash tables.  I know it's talking about aggregations, but I believe the problem is common between that and joins -- both start with building a hash table, and that build is particularly slow when there are many key collisions and long chaining.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138306017/reactions,1,1,0,0,0,0,0,0,0,7529
597,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1422036491,https://github.com/NVIDIA/spark-rapids/issues/7531#issuecomment-1422036491,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7531,1422036491,IC_kwDOD7z77c5Uwo4L,2023-02-08T05:27:01Z,2023-02-08T05:27:01Z,COLLABORATOR,I'd like to have a try.,,HaoYang670,59198230,MDQ6VXNlcjU5MTk4MjMw,https://avatars.githubusercontent.com/u/59198230?v=4,,https://api.github.com/users/HaoYang670,https://github.com/HaoYang670,https://api.github.com/users/HaoYang670/followers,https://api.github.com/users/HaoYang670/following{/other_user},https://api.github.com/users/HaoYang670/gists{/gist_id},https://api.github.com/users/HaoYang670/starred{/owner}{/repo},https://api.github.com/users/HaoYang670/subscriptions,https://api.github.com/users/HaoYang670/orgs,https://api.github.com/users/HaoYang670/repos,https://api.github.com/users/HaoYang670/events{/privacy},https://api.github.com/users/HaoYang670/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1422036491/reactions,0,0,0,0,0,0,0,0,0,7531
598,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398615568,https://github.com/NVIDIA/spark-rapids/issues/7553#issuecomment-1398615568,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7553,1398615568,IC_kwDOD7z77c5TXS4Q,2023-01-20T16:14:34Z,2023-01-20T16:14:34Z,COLLABORATOR,@rwlee do you have the inputs that caused the CPU to overflow? It would be very helpful. ,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398615568/reactions,0,0,0,0,0,0,0,0,0,7553
599,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398638486,https://github.com/NVIDIA/spark-rapids/issues/7553#issuecomment-1398638486,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7553,1398638486,IC_kwDOD7z77c5TXYeW,2023-01-20T16:34:43Z,2023-01-20T16:34:43Z,COLLABORATOR,"I am seeing a lot more failures that just these. I see failures for `arithmetic_ops_test.py::test_pmod_mixed_decimal[Integer-Decimal(6,5)]`

With differences like
```diff
$ diff CPU.txt GPU.txt
2c2
< Row(a=Decimal('-5.24982'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-5.24982'), b=2147483647, pmod(a, b)=Decimal('-6.24982'))
7c7
< Row(a=Decimal('-3.17710'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-3.17710'), b=2147483647, pmod(a, b)=Decimal('-4.17710'))
786c786
< Row(a=Decimal('-5.00890'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-5.00890'), b=2147483647, pmod(a, b)=Decimal('-6.00890'))
846c846
< Row(a=Decimal('-7.46971'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-7.46971'), b=2147483647, pmod(a, b)=Decimal('-8.46971'))
956c956
< Row(a=Decimal('-0.76877'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-0.76877'), b=2147483647, pmod(a, b)=Decimal('-1.76877'))
1163c1163
< Row(a=Decimal('-5.95426'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-5.95426'), b=2147483647, pmod(a, b)=Decimal('-6.95426'))
1369c1369
< Row(a=Decimal('-6.36876'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-6.36876'), b=2147483647, pmod(a, b)=Decimal('-7.36876'))
1588c1588
< Row(a=Decimal('-3.41718'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-3.41718'), b=2147483647, pmod(a, b)=Decimal('-4.41718'))
1792c1792
< Row(a=Decimal('-7.57246'), b=2147483647, pmod(a, b)=None)
---
> Row(a=Decimal('-7.57246'), b=2147483647, pmod(a, b)=Decimal('-8.57246'))
```

In all of these cases we are returning a negative number. But with the way that pmod is defined in spark.

```
  private def pmod(a: Decimal, n: Decimal): Decimal = {
    val r = a % n
    if (r != null && r.compare(Decimal.ZERO) < 0) {(r + n) % n} else r
  }
```

We should never return a negative number. It looks very much like a bug in our code and we might want to disable pmod support for decimal until we can figure out how to fix it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398638486/reactions,0,0,0,0,0,0,0,0,0,7553
600,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398809112,https://github.com/NVIDIA/spark-rapids/issues/7553#issuecomment-1398809112,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7553,1398809112,IC_kwDOD7z77c5TYCIY,2023-01-20T18:59:52Z,2023-01-20T18:59:52Z,COLLABORATOR,"Yes, there are many different failures -- those were the only 2 errors in that specific type pair. There are 29 type pairs that fail, but I didn't find the number of rows that don't match for the other 28 pairs. Sorry for not making that clear.",,rwlee,10645552,MDQ6VXNlcjEwNjQ1NTUy,https://avatars.githubusercontent.com/u/10645552?v=4,,https://api.github.com/users/rwlee,https://github.com/rwlee,https://api.github.com/users/rwlee/followers,https://api.github.com/users/rwlee/following{/other_user},https://api.github.com/users/rwlee/gists{/gist_id},https://api.github.com/users/rwlee/starred{/owner}{/repo},https://api.github.com/users/rwlee/subscriptions,https://api.github.com/users/rwlee/orgs,https://api.github.com/users/rwlee/repos,https://api.github.com/users/rwlee/events{/privacy},https://api.github.com/users/rwlee/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1398809112/reactions,0,0,0,0,0,0,0,0,0,7553
601,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1399015815,https://github.com/NVIDIA/spark-rapids/issues/7553#issuecomment-1399015815,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7553,1399015815,IC_kwDOD7z77c5TY0mH,2023-01-20T22:15:42Z,2023-01-20T22:15:42Z,COLLABORATOR,"```
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Integer-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [1, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Integer-Decimal(6,4)] - AssertionError: GPU and CPU are not both null at [1, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Integer-Decimal(5,3)] - AssertionError: GPU and CPU are not both null at [34, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Integer-Decimal(4,2)] - AssertionError: GPU and CPU are not both null at [809, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Integer-Decimal(5,4)] - AssertionError: GPU and CPU are not both null at [34, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(4,2)] - AssertionError: GPU and CPU are not both null at [30, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(3,-2)] - AssertionError: GPU and CPU are not both null at [4, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(16,7)] - AssertionError: GPU and CPU are not both null at [10, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [1, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(6,4)] - AssertionError: GPU and CPU are not both null at [1, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(5,4)] - AssertionError: GPU and CPU are not both null at [29, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Long-Decimal(5,3)] - AssertionError: GPU and CPU are not both null at [29, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(15,3)-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [1006, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(15,3)-Short] - AssertionError: GPU and CPU are not both null at [77, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(30,12)-Short] - AssertionError: GPU and CPU are not both null at [605, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(15,3)-Decimal(3,-2)] - AssertionError: GPU and CPU are not both null at [79, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(3,-3)-Decimal(6,4)] - AssertionError: GPU and CPU are not both null at [137, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(3,-3)-Decimal(5,4)] - AssertionError: GPU and CPU are not both null at [147, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(30,12)-Decimal(3,-2)] - AssertionError: GPU and CPU are not both null at [606, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(3,-3)-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [137, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Integer] - AssertionError: GPU and CPU are not both null at [52, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [8, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Decimal(6,4)] - AssertionError: GPU and CPU are not both null at [0, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Decimal(5,4)] - AssertionError: GPU and CPU are not both null at [94, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Decimal(5,3)] - AssertionError: GPU and CPU are not both null at [0, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(27,7)-Decimal(4,2)] - AssertionError: GPU and CPU are not both null at [0, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(20,-3)-Decimal(6,5)] - AssertionError: GPU and CPU are not both null at [1186, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(20,-3)-Decimal(6,4)] - AssertionError: GPU and CPU are not both null at [1841, 'pmod(a, b)']
FAILED ../../src/main/python/arithmetic_ops_test.py::test_pmod_mixed[Decimal(20,-3)-Decimal(16,7)] - AssertionError: GPU and CPU are not both null at [748, 'pmod(a, b)']
```

These are the full list of pairs that cause issues",,rwlee,10645552,MDQ6VXNlcjEwNjQ1NTUy,https://avatars.githubusercontent.com/u/10645552?v=4,,https://api.github.com/users/rwlee,https://github.com/rwlee,https://api.github.com/users/rwlee/followers,https://api.github.com/users/rwlee/following{/other_user},https://api.github.com/users/rwlee/gists{/gist_id},https://api.github.com/users/rwlee/starred{/owner}{/repo},https://api.github.com/users/rwlee/subscriptions,https://api.github.com/users/rwlee/orgs,https://api.github.com/users/rwlee/repos,https://api.github.com/users/rwlee/events{/privacy},https://api.github.com/users/rwlee/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1399015815/reactions,0,0,0,0,0,0,0,0,0,7553
602,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411306130,https://github.com/NVIDIA/spark-rapids/issues/7553#issuecomment-1411306130,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7553,1411306130,IC_kwDOD7z77c5UHtKS,2023-02-01T01:16:57Z,2023-02-01T01:16:57Z,COLLABORATOR,Moving to 23.04 and lowering the priority since we will fall back to the cpu for `pmod` with decimal type.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411306130/reactions,0,0,0,0,0,0,0,0,0,7553
603,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1404283100,https://github.com/NVIDIA/spark-rapids/issues/7583#issuecomment-1404283100,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7583,1404283100,IC_kwDOD7z77c5Ts6jc,2023-01-25T22:01:22Z,2023-01-27T15:38:25Z,COLLABORATOR,"The `makeSplitIterator` method in this class, returning `Iterator[ColumnarBatch]` is calling `contiguousSplit` and holding on to all the split tables while the caller calls `.next()`. Additionally, this iterator does not make an attempt to close these tables if there is an exception or the task finishes, it just leaks them. The code does make an attempt to use `closeOnExcept` during a next call (inside of `splitInput.zipWithIndex.iterator.map`) but that is not sufficient, as it only handles exceptions that can occur when converting the cuDF Table to a ColumnarBatch which I don't think is the intention.

We do need to split, so we should split and make all the contiguous tables spillable, then we can `getColumnarBatch` from these spillables. We also need to register a shutdown hook against the `TaskContext` so that we can remove these spillables if the task exits prematurely.

Here's the detected leak when tasks failed:

```
23/01/27 15:35:55 ERROR MemoryCleaner: Leaked device buffer (ID: 1439): 2023-01-27 15:35:54.0969 UTC: INC
java.lang.Thread.getStackTrace(Thread.java:1564)
ai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:333)
ai.rapids.cudf.MemoryCleaner$Cleaner.addRef(MemoryCleaner.java:91)
ai.rapids.cudf.MemoryBuffer.incRefCount(MemoryBuffer.java:275)
ai.rapids.cudf.MemoryBuffer.<init>(MemoryBuffer.java:117)
ai.rapids.cudf.BaseDeviceMemoryBuffer.<init>(BaseDeviceMemoryBuffer.java:30)
ai.rapids.cudf.DeviceMemoryBuffer.<init>(DeviceMemoryBuffer.java:116)
ai.rapids.cudf.DeviceMemoryBuffer.fromRmm(DeviceMemoryBuffer.java:112)
ai.rapids.cudf.ContiguousTable.fromPackedTable(ContiguousTable.java:40)
ai.rapids.cudf.Table.contiguousSplit(Native Method)
ai.rapids.cudf.Table.contiguousSplit(Table.java:2170)
com.nvidia.spark.rapids.GpuGenerateExec.$anonfun$makeSplitIterator$2(GpuGenerateExec.scala:769)
com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
com.nvidia.spark.rapids.GpuGenerateExec.withResource(GpuGenerateExec.scala:660)
com.nvidia.spark.rapids.GpuGenerateExec.makeSplitIterator(GpuGenerateExec.scala:768)
com.nvidia.spark.rapids.GpuGenerateExec.$anonfun$doGenerate$2(GpuGenerateExec.scala:746)
com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
com.nvidia.spark.rapids.GpuGenerateExec.withResource(GpuGenerateExec.scala:660)
com.nvidia.spark.rapids.GpuGenerateExec.$anonfun$doGenerate$1(GpuGenerateExec.scala:738)
com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
com.nvidia.spark.rapids.GpuGenerateExec.withResource(GpuGenerateExec.scala:660)
com.nvidia.spark.rapids.GpuGenerateExec.doGenerate(GpuGenerateExec.scala:733)
com.nvidia.spark.rapids.GpuGenerateExec.$anonfun$doExecuteColumnar$4(GpuGenerateExec.scala:721)
com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
com.nvidia.spark.rapids.GpuGenerateExec.withResource(GpuGenerateExec.scala:660)
com.nvidia.spark.rapids.GpuGenerateExec.$anonfun$doExecuteColumnar$3(GpuGenerateExec.scala:719)
scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1(GpuExec.scala:193)
com.nvidia.spark.rapids.CollectTimeIterator.$anonfun$hasNext$1$adapted(GpuExec.scala:192)
com.nvidia.spark.rapids.Arm.withResource(Arm.scala:28)
com.nvidia.spark.rapids.Arm.withResource$(Arm.scala:26)
com.nvidia.spark.RebaseHelper$.withResource(RebaseHelper.scala:26)
com.nvidia.spark.rapids.CollectTimeIterator.hasNext(GpuExec.scala:192)
com.nvidia.spark.rapids.AbstractGpuCoalesceIterator.hasNext(GpuCoalesceBatches.scala:292)
scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
com.nvidia.spark.rapids.GpuHashAggregateIterator.$anonfun$hasNext$2(aggregate.scala:232)
scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
scala.Option.getOrElse(Option.scala:189)
com.nvidia.spark.rapids.GpuHashAggregateIterator.hasNext(aggregate.scala:232)
org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.partNextBatch(GpuShuffleExchangeExecBase.scala:317)
org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase$$anon$1.hasNext(GpuShuffleExchangeExecBase.scala:340)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
org.apache.spark.scheduler.Task.run(Task.scala:131)
org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:750)
```",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1404283100/reactions,0,0,0,0,0,0,0,0,0,7583
604,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411297553,https://github.com/NVIDIA/spark-rapids/issues/7585#issuecomment-1411297553,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7585,1411297553,IC_kwDOD7z77c5UHrER,2023-02-01T01:08:25Z,2023-02-01T01:08:25Z,COLLABORATOR,The handling of line terminators was documented in the compatibility guide in PR #7211 ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411297553/reactions,0,0,0,0,0,0,0,0,0,7585
605,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430055141,https://github.com/NVIDIA/spark-rapids/issues/7585#issuecomment-1430055141,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7585,1430055141,IC_kwDOD7z77c5VPOjl,2023-02-14T16:45:04Z,2023-02-14T16:45:34Z,CONTRIBUTOR,"To add more context here, cuDF has the correct behavior when passed `TEST$`. The problem is that we are transcoding `TEST$` to `TEST(?:\r|\u0085|\u2028|\u2029|\r\n)?$` to work around differences between Java's regexp engine and cuDF's regexp engine related to line terminators, and this transcoding does not provide the desired result for the example given in this issue.

Possible solutions to resolve this issue:

1. Find a transcoding that works in all cases (I am not sure if this is possible).
2. Scan the input data and look for cases where we would produce incorrect results and fail the query.
3. Request new Java-compatibility features in cuDF's regexp engine so that we don't have to transcode in the first place.
4. Fork cuDF's regexp kernels and make them compatible with Java as part of `spark-rapids-jni`.

If we remove the transcoding, then the test fails on a different input. In this case, Java `$` ignores the final line terminator in the input, but cuDF does not.

```
javaPattern[0]=TEST$, cudfPattern=TEST$, input='aTEST\u0085', cpu=true, gpu=false
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430055141/reactions,0,0,0,0,0,0,0,0,0,7585
606,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430155613,https://github.com/NVIDIA/spark-rapids/issues/7585#issuecomment-1430155613,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7585,1430155613,IC_kwDOD7z77c5VPnFd,2023-02-14T18:00:07Z,2023-02-14T18:00:28Z,CONTRIBUTOR,"Here are some examples of mismatches when we remove the transcoding and use the same pattern `TEST$` on CPU and GPU.

| Input | CPU | GPU |
|-|-|-|
| `TEST\r` | true | false | 
| `TEST\r\n` | true | false |
| `TEST\u0085` | true | false |
| `TEST\u2028` | true | false |
| `TEST\u2029` | true | false |",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430155613/reactions,0,0,0,0,0,0,0,0,0,7585
607,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430321628,https://github.com/NVIDIA/spark-rapids/issues/7585#issuecomment-1430321628,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7585,1430321628,IC_kwDOD7z77c5VQPnc,2023-02-14T20:17:37Z,2023-02-14T20:17:37Z,NONE,"One option is to do a replace of these with a single `\n` on the input data before evaluating the regex.
This can be done with a single libcudf call to `cudf::strings::replace` like the following:
```
auto input   = cudf::test::strings_column_wrapper({""test\r\n"", ""test∞"", ""testφ"", ""test""});
auto targets = cudf::test::strings_column_wrapper({""\r\n"", ""∞"", ""φ""});
auto repls   = cudf::test::strings_column_wrapper({""\n"", ""\n"", ""\n""});

auto results = cudf::strings::replace(cudf::strings_column_view{input},
                                      cudf::strings_column_view(targets),
                                      cudf::strings_column_view(repls));
results => {""test\n"", ""test\n"", ""test\n"", ""test""}
```
And then apply `test$` pattern to that.
I believe the `\r\n` to `\n` replacement may already be occurring based on the comment here which was resolved.
https://github.com/rapidsai/cudf/issues/11979#issuecomment-1307025060",,davidwendt,45795991,MDQ6VXNlcjQ1Nzk1OTkx,https://avatars.githubusercontent.com/u/45795991?v=4,,https://api.github.com/users/davidwendt,https://github.com/davidwendt,https://api.github.com/users/davidwendt/followers,https://api.github.com/users/davidwendt/following{/other_user},https://api.github.com/users/davidwendt/gists{/gist_id},https://api.github.com/users/davidwendt/starred{/owner}{/repo},https://api.github.com/users/davidwendt/subscriptions,https://api.github.com/users/davidwendt/orgs,https://api.github.com/users/davidwendt/repos,https://api.github.com/users/davidwendt/events{/privacy},https://api.github.com/users/davidwendt/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430321628/reactions,0,0,0,0,0,0,0,0,0,7585
608,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411090895,https://github.com/NVIDIA/spark-rapids/issues/7587#issuecomment-1411090895,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7587,1411090895,IC_kwDOD7z77c5UG4nP,2023-01-31T21:26:22Z,2023-01-31T21:26:22Z,MEMBER,The core work to allow parallel testing with `--packages` was completed in #6590.  Therefore the only thing left to do for this issue is to star running tests in parallel that are using `--packages`.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411090895/reactions,0,0,0,0,0,0,0,0,0,7587
609,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1408810382,https://github.com/NVIDIA/spark-rapids/issues/7605#issuecomment-1408810382,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7605,1408810382,IC_kwDOD7z77c5T-L2O,2023-01-30T15:17:34Z,2023-01-30T15:17:34Z,MEMBER,Also relates to #6947 ,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1408810382/reactions,0,0,0,0,0,0,0,0,0,7605
610,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411095655,https://github.com/NVIDIA/spark-rapids/issues/7616#issuecomment-1411095655,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7616,1411095655,IC_kwDOD7z77c5UG5xn,2023-01-31T21:31:13Z,2023-01-31T21:31:13Z,COLLABORATOR,We should switch to the new JSON reader per issue #[7518](https://github.com/NVIDIA/spark-rapids/issues/7518),,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411095655/reactions,0,0,0,0,0,0,0,0,0,7616
611,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894714393,https://github.com/NVIDIA/spark-rapids/issues/7616#issuecomment-1894714393,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7616,1894714393,IC_kwDOD7z77c5w7wwZ,2024-01-17T00:06:33Z,2024-01-17T00:06:33Z,CONTRIBUTOR,"I just re-tested this, and it is still an issue even after switching to the new engine.

```
scala> spark.read.json(""no-body.json"").show
24/01/17 00:02:09 WARN GpuOverrides: 
!Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat

24/01/17 00:02:09 WARN GpuOverrides: 
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <FileSourceScanExec> will run on GPU

24/01/17 00:02:09 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)
java.lang.UnsupportedOperationException: empty.min
	at scala.collection.TraversableOnce.min(TraversableOnce.scala:227)
	at scala.collection.TraversableOnce.min$(TraversableOnce.scala:225)
	at org.apache.spark.sql.types.StructType.min(StructType.scala:102)
	at com.nvidia.spark.rapids.GpuTextBasedPartitionReader.readToTable(GpuTextBasedPartitionReader.scala:298)
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894714393/reactions,0,0,0,0,0,0,0,0,0,7616
612,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894735320,https://github.com/NVIDIA/spark-rapids/issues/7616#issuecomment-1894735320,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7616,1894735320,IC_kwDOD7z77c5w713Y,2024-01-17T00:29:15Z,2024-01-17T00:29:15Z,CONTRIBUTOR,"This only seems to be an issue for a JSON file that only contains empty entries.  If there is at least one non-empty row, then we match Spark.

```
$ cat with-body.json 
{}
{ ""a"": 4 }
```

```
scala> spark.read.json(""with-body.json"").show
24/01/17 00:26:26 WARN GpuOverrides: 
!Exec <FileSourceScanExec> cannot run on GPU because unsupported file format: org.apache.spark.sql.execution.datasources.text.TextFileFormat

24/01/17 00:26:26 WARN GpuOverrides: 
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> cast(a#22L as string) AS a#25 will run on GPU
      *Expression <Cast> cast(a#22L as string) will run on GPU
    !Exec <FileSourceScanExec> cannot run on GPU because JSON input and output has been disabled. To enable set spark.rapids.sql.format.json.enabled to true

+----+
|   a|
+----+
|null|
|   4|
+----+
```
",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894735320/reactions,0,0,0,0,0,0,0,0,0,7616
613,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995836099,https://github.com/NVIDIA/spark-rapids/issues/7616#issuecomment-1995836099,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7616,1995836099,IC_kwDOD7z77c529grD,2024-03-13T21:12:20Z,2024-03-13T21:12:20Z,COLLABORATOR,"@res-life are you still planning on working on this?

The failures are happening in two places. If you don't provide a schema, then schema discovery returns with an empty schema. CUDF does not like this so we try to make one up, and try to pull something out of the dataSchema, which is also empty and results in a crash.

If we do provide a schema, then we run into a null pointer exception when trying to read the data.

```
spark.read.schema(""a string"").json(""./no-body.json"").show
...
Caused by: java.lang.NullPointerException
  at ai.rapids.cudf.TableWithMeta.getColumnNames(TableWithMeta.java:132)
  at ai.rapids.cudf.Table.gatherJSONColumns(Table.java:1211)
  at ai.rapids.cudf.Table.readJSON(Table.java:1373)
  at org.apache.spark.sql.catalyst.json.rapids.JsonPartitionReader$.$anonfun$readToTable$2(GpuJsonScan.scala:325)
  at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
  at org.apache.spark.sql.catalyst.json.rapids.JsonPartitionReader$.$anonfun$readToTable$1(GpuJsonScan.scala:323)
  at com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:477)
  at com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:613)
  at com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:517)
```

We should not be trying to use the data schema if the read data schema is empty.  That might result in us reading in the wrong data if it actually succeeded, because the only time that readDataSchema is empty but data schema is not is if we have partition columns.

In the short term I think we just need to fall back to the CPU if the readDataSchema is empty, and we should concentrate on fixing the null pointer exception.

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995836099/reactions,0,0,0,0,0,0,0,0,0,7616
614,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998732252,https://github.com/NVIDIA/spark-rapids/issues/7616#issuecomment-1998732252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7616,1998732252,IC_kwDOD7z77c53Ijvc,2024-03-15T01:22:44Z,2024-03-15T01:23:04Z,COLLABORATOR,"> @res-life are you still planning on working on this?

No, I'm now focusing on `get-json-object` issues, maybe anyone else can take this.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998732252/reactions,0,0,0,0,0,0,0,0,0,7616
615,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1409962198,https://github.com/NVIDIA/spark-rapids/issues/7619#issuecomment-1409962198,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7619,1409962198,IC_kwDOD7z77c5UClDW,2023-01-31T08:35:32Z,2023-02-01T01:45:18Z,COLLABORATOR,~~https://github.com/NVIDIA/spark-rapids/issues/7624~~,,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1409962198/reactions,0,0,0,0,0,0,0,0,0,7619
616,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1410508120,https://github.com/NVIDIA/spark-rapids/issues/7619#issuecomment-1410508120,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7619,1410508120,IC_kwDOD7z77c5UEqVY,2023-01-31T14:54:12Z,2023-01-31T14:54:12Z,MEMBER,"@res-life not sure why you're pointing to that issue, it's not clear how it is related.  There are many examples of GPU implementations of `RunnableCommand` where we are passing tests without being flagged as not on the GPU despite executing and returning rows as it should.  See GpuWriteIntoDelta, GpuMergeIntoCommand, etc.  Also note that GpuExecutedCommandExec explicitly states it implements GpuExec yet does not want to be columnar since runnable commands return rows.  I think we need to remove GpuRunnableCommand and implement things in terms of RunnableCommand since we want to return rows, not columnar batches, for these operations per the discussion above.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1410508120/reactions,0,0,0,0,0,0,0,0,0,7619
617,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411317343,https://github.com/NVIDIA/spark-rapids/issues/7619#issuecomment-1411317343,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7619,1411317343,IC_kwDOD7z77c5UHv5f,2023-02-01T01:32:05Z,2023-02-01T01:47:09Z,COLLABORATOR,"Please ignore that I linked an unrelated issue.
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1411317343/reactions,0,0,0,0,0,0,0,0,0,7619
618,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1414505444,https://github.com/NVIDIA/spark-rapids/issues/7654#issuecomment-1414505444,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7654,1414505444,IC_kwDOD7z77c5UT6Pk,2023-02-02T23:23:51Z,2023-02-02T23:23:51Z,COLLABORATOR,@abellina ,,nvdbaranec,56695930,MDQ6VXNlcjU2Njk1OTMw,https://avatars.githubusercontent.com/u/56695930?v=4,,https://api.github.com/users/nvdbaranec,https://github.com/nvdbaranec,https://api.github.com/users/nvdbaranec/followers,https://api.github.com/users/nvdbaranec/following{/other_user},https://api.github.com/users/nvdbaranec/gists{/gist_id},https://api.github.com/users/nvdbaranec/starred{/owner}{/repo},https://api.github.com/users/nvdbaranec/subscriptions,https://api.github.com/users/nvdbaranec/orgs,https://api.github.com/users/nvdbaranec/repos,https://api.github.com/users/nvdbaranec/events{/privacy},https://api.github.com/users/nvdbaranec/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1414505444/reactions,0,0,0,0,0,0,0,0,0,7654
619,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421497645,https://github.com/NVIDIA/spark-rapids/issues/7654#issuecomment-1421497645,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7654,1421497645,IC_kwDOD7z77c5UulUt,2023-02-07T21:45:32Z,2023-02-07T21:45:32Z,COLLABORATOR,"Need to run integration tests single threaded with the java command line argument`-Dai.rapids.refcount.debug=true` and examine the logs.  Then need to grep the log for ""LEAK"" and debug.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421497645/reactions,0,0,0,0,0,0,0,0,0,7654
620,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1423678787,https://github.com/NVIDIA/spark-rapids/issues/7654#issuecomment-1423678787,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7654,1423678787,IC_kwDOD7z77c5U251D,2023-02-09T06:01:55Z,2023-02-09T06:01:55Z,COLLABORATOR,@nvdbaranec can you post the full command you ran for the integration tests? ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1423678787/reactions,0,0,0,0,0,0,0,0,0,7654
621,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1425996032,https://github.com/NVIDIA/spark-rapids/issues/7654#issuecomment-1425996032,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7654,1425996032,IC_kwDOD7z77c5U_vkA,2023-02-10T15:45:16Z,2023-02-10T15:45:16Z,COLLABORATOR,I just used the standard script. Inside `integration_tests`:  `./run_pyspark_from_build.sh`,,nvdbaranec,56695930,MDQ6VXNlcjU2Njk1OTMw,https://avatars.githubusercontent.com/u/56695930?v=4,,https://api.github.com/users/nvdbaranec,https://github.com/nvdbaranec,https://api.github.com/users/nvdbaranec/followers,https://api.github.com/users/nvdbaranec/following{/other_user},https://api.github.com/users/nvdbaranec/gists{/gist_id},https://api.github.com/users/nvdbaranec/starred{/owner}{/repo},https://api.github.com/users/nvdbaranec/subscriptions,https://api.github.com/users/nvdbaranec/orgs,https://api.github.com/users/nvdbaranec/repos,https://api.github.com/users/nvdbaranec/events{/privacy},https://api.github.com/users/nvdbaranec/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1425996032/reactions,0,0,0,0,0,0,0,0,0,7654
622,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421521924,https://github.com/NVIDIA/spark-rapids/issues/7661#issuecomment-1421521924,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661,1421521924,IC_kwDOD7z77c5UurQE,2023-02-07T22:04:41Z,2023-02-07T22:04:41Z,COLLABORATOR,Would need to add dictionary support.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421521924/reactions,0,0,0,0,0,0,0,0,0,7661
623,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1983705108,https://github.com/NVIDIA/spark-rapids/issues/7661#issuecomment-1983705108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661,1983705108,IC_kwDOD7z77c52PPAU,2024-03-07T15:05:48Z,2024-03-07T15:05:48Z,COLLABORATOR,"This could be a huge win for memory on cases like https://github.com/NVIDIA/spark-rapids/issues/10561 where we insert in a lot of null columns as place holders knowing that they will never be used, and likely replaced by more columns of only nulls.  Perhaps we can even ask CUDF for a scalar column as a further optimization.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1983705108/reactions,0,0,0,0,0,0,0,0,0,7661
624,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998512656,https://github.com/NVIDIA/spark-rapids/issues/7661#issuecomment-1998512656,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7661,1998512656,IC_kwDOD7z77c53HuIQ,2024-03-14T21:28:11Z,2024-03-14T21:28:11Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/15308 for this in CUDF. We will see what happens.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998512656/reactions,0,0,0,0,0,0,0,0,0,7661
625,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1416333944,https://github.com/NVIDIA/spark-rapids/issues/7662#issuecomment-1416333944,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7662,1416333944,IC_kwDOD7z77c5Ua4p4,2023-02-03T19:52:13Z,2023-02-03T19:52:13Z,COLLABORATOR,"I'm keeping this on low priority, since it affects only the tests.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1416333944/reactions,0,0,0,0,0,0,0,0,0,7662
626,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1489095210,https://github.com/NVIDIA/spark-rapids/issues/7669#issuecomment-1489095210,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7669,1489095210,IC_kwDOD7z77c5Ywcoq,2023-03-29T18:25:08Z,2023-03-29T18:25:08Z,COLLABORATOR,Spill callbacks were removed already. We just need to remove the spill priorities,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1489095210/reactions,0,0,0,0,0,0,0,0,0,7669
627,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1489109586,https://github.com/NVIDIA/spark-rapids/issues/7670#issuecomment-1489109586,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7670,1489109586,IC_kwDOD7z77c5YwgJS,2023-03-29T18:35:48Z,2023-03-29T18:35:48Z,COLLABORATOR,"Most of the metrics were done as a part of https://github.com/NVIDIA/spark-rapids/pull/7935 All that is left from this is ""unspill hit rate"" But this probably depends on us having unspill enabled by default.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1489109586/reactions,0,0,0,0,0,0,0,0,0,7670
628,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2051998667,https://github.com/NVIDIA/spark-rapids/issues/7670#issuecomment-2051998667,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7670,2051998667,IC_kwDOD7z77c56TwPL,2024-04-12T15:38:39Z,2024-04-12T15:38:39Z,COLLABORATOR,"I think we should consider not just ""unspill"" but also simply materializing a disk/host buffer as a +1. This would allow us to find operators that are stuck re-reading spilled buffers.. I can think of join doing this, not sure if others.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2051998667/reactions,0,0,0,0,0,0,0,0,0,7670
629,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421040551,https://github.com/NVIDIA/spark-rapids/issues/7689#issuecomment-1421040551,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7689,1421040551,IC_kwDOD7z77c5Us1un,2023-02-07T16:16:25Z,2023-02-07T16:16:25Z,COLLABORATOR,"the issue here is actually the node is hiding the contents.  It actually does two GpuScanParquet following by a Union but then it goes to row, deserializeToObject, mapPartitions, to WholeStagecodegen, then after that Scan node it goes to row to columnar and GpuFilter afterwards.  
I think the UI gets confused by the last thing in there being on CPU.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1421040551/reactions,0,0,0,0,0,0,0,0,0,7689
630,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550046216,https://github.com/NVIDIA/spark-rapids/issues/7698#issuecomment-1550046216,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7698,1550046216,IC_kwDOD7z77c5cY9QI,2023-05-16T17:01:35Z,2023-05-16T17:01:35Z,COLLABORATOR,"This is fixed by rapidsai/cudf#13335. 
@revans2  can you please confirm? ",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550046216/reactions,0,0,0,0,0,0,0,0,0,7698
631,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550078909,https://github.com/NVIDIA/spark-rapids/issues/7698#issuecomment-1550078909,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7698,1550078909,IC_kwDOD7z77c5cZFO9,2023-05-16T17:25:39Z,2023-05-16T17:25:39Z,COLLABORATOR,"@razajafri #7485 is fixed by https://github.com/rapidsai/cudf/pull/13335. 

It also helped to make the API safer and showed that there is a performance improvement over doing an if/else (20% to 40% better). So it makes me rethink this a bit, but I still don't like `bitwiseMergeAndSetValidity` as an API. 

It it is confusing to use, especially compared to something like if/else. So as an implementation I think  https://github.com/rapidsai/cudf/pull/13335 has addressed my concerns. And an API no it has not.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550078909/reactions,1,1,0,0,0,0,0,0,0,7698
632,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430403998,https://github.com/NVIDIA/spark-rapids/issues/7727#issuecomment-1430403998,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7727,1430403998,IC_kwDOD7z77c5VQjue,2023-02-14T21:30:01Z,2023-02-14T21:30:01Z,COLLABORATOR,Similar to https://github.com/NVIDIA/spark-rapids/issues/7501,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1430403998/reactions,0,0,0,0,0,0,0,0,0,7727
633,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487412016,https://github.com/NVIDIA/spark-rapids/issues/7727#issuecomment-1487412016,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7727,1487412016,IC_kwDOD7z77c5YqBsw,2023-03-28T18:27:43Z,2023-03-28T18:27:43Z,COLLABORATOR,"Tacking on https://github.com/apache/spark/pull/40137, because it's related. After `AliasAwareOutputExpression` is supported, it would be good to verify that partitioning/ordering clauses from non-existent attributes does not affect the output.

The (possibly contrived) test was added in this commit:
https://github.com/apache/spark/commit/149458c50d",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487412016/reactions,0,0,0,0,0,0,0,0,0,7727
634,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1426334389,https://github.com/NVIDIA/spark-rapids/issues/7733#issuecomment-1426334389,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7733,1426334389,IC_kwDOD7z77c5VBCK1,2023-02-10T20:55:42Z,2023-02-10T20:55:42Z,COLLABORATOR,"The way I was reproducing this on Apache doesn't work with Databricks, it seems to be checking different thus the test code doesn't work there.  ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1426334389/reactions,0,0,0,0,0,0,0,0,0,7733
635,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1428110552,https://github.com/NVIDIA/spark-rapids/issues/7744#issuecomment-1428110552,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7744,1428110552,IC_kwDOD7z77c5VHzzY,2023-02-13T15:12:37Z,2023-02-13T15:12:37Z,COLLABORATOR,I assume this is dup of https://github.com/NVIDIA/spark-rapids/issues/7741,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1428110552/reactions,0,0,0,0,0,0,0,0,0,7744
636,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1428127617,https://github.com/NVIDIA/spark-rapids/issues/7744#issuecomment-1428127617,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7744,1428127617,IC_kwDOD7z77c5VH3-B,2023-02-13T15:21:05Z,2023-02-13T15:21:05Z,COLLABORATOR,"Yeah, closed 7741 since I think this one had just a tad more info.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1428127617/reactions,0,0,0,0,0,0,0,0,0,7744
637,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487304706,https://github.com/NVIDIA/spark-rapids/issues/7765#issuecomment-1487304706,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7765,1487304706,IC_kwDOD7z77c5YpngC,2023-03-28T17:10:39Z,2023-03-28T17:10:39Z,NONE,"Hi, it's not a nccl issue as shared in https://github.com/dmlc/xgboost/issues/8746#issuecomment-1474435378 . ",,trivialfis,16746409,MDQ6VXNlcjE2NzQ2NDA5,https://avatars.githubusercontent.com/u/16746409?v=4,,https://api.github.com/users/trivialfis,https://github.com/trivialfis,https://api.github.com/users/trivialfis/followers,https://api.github.com/users/trivialfis/following{/other_user},https://api.github.com/users/trivialfis/gists{/gist_id},https://api.github.com/users/trivialfis/starred{/owner}{/repo},https://api.github.com/users/trivialfis/subscriptions,https://api.github.com/users/trivialfis/orgs,https://api.github.com/users/trivialfis/repos,https://api.github.com/users/trivialfis/events{/privacy},https://api.github.com/users/trivialfis/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487304706/reactions,0,0,0,0,0,0,0,0,0,7765
638,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1463986259,https://github.com/NVIDIA/spark-rapids/issues/7804#issuecomment-1463986259,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7804,1463986259,IC_kwDOD7z77c5XQqhT,2023-03-10T15:41:52Z,2023-03-10T15:41:52Z,COLLABORATOR,"Depends on:
 * https://github.com/rapidsai/cudf/pull/12879",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1463986259/reactions,0,0,0,0,0,0,0,0,0,7804
639,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1473873810,https://github.com/NVIDIA/spark-rapids/issues/7804#issuecomment-1473873810,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7804,1473873810,IC_kwDOD7z77c5X2YeS,2023-03-17T13:49:31Z,2023-03-17T13:49:31Z,MEMBER,"This is a bug in the logic in the plugin where we can produce an GPU-unsupported SortOrder.  Fixing that is what this issue tracks, and it is not dependent on a cudf change.  Allowing the GPU to support more SortOrder expressions is what the cudf issue is related to.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1473873810/reactions,0,0,0,0,0,0,0,0,0,7804
640,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1457172456,https://github.com/NVIDIA/spark-rapids/issues/7827#issuecomment-1457172456,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7827,1457172456,IC_kwDOD7z77c5W2q_o,2023-03-06T22:55:08Z,2023-03-06T22:55:08Z,COLLABORATOR,Related issue: https://github.com/NVIDIA/spark-rapids/issues/7850,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1457172456/reactions,0,0,0,0,0,0,0,0,0,7827
641,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1590005851,https://github.com/NVIDIA/spark-rapids/issues/7836#issuecomment-1590005851,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7836,1590005851,IC_kwDOD7z77c5exZBb,2023-06-13T20:48:35Z,2023-06-13T20:48:35Z,COLLABORATOR,"@tgravescs If we create a GPU version of spark.sql.files.maxPartitionBytes, i hope it can be dynamically adjusted to avoid OOM. 

The current challenge is different job may need a different value for this parameter, otherwise the table scan stage might OOM. 
If we can dynamically adjust the value for this GPU version spark.sql.files.maxPartitionBytes, do you think it doable?",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1590005851/reactions,0,0,0,0,0,0,0,0,0,7836
642,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1591270160,https://github.com/NVIDIA/spark-rapids/issues/7836#issuecomment-1591270160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7836,1591270160,IC_kwDOD7z77c5e2NsQ,2023-06-14T14:00:31Z,2023-06-14T14:00:31Z,COLLABORATOR,"@viadea what do you suggest as a way to know ahead of time if a task is going to OOM so we could adjust it dynamically?

We implemented the chunked reader for parquet and are in the middle of doing something similar for ORC to allow us to avoid OOM issues when reading highly compressed data where even a single row group/stripe might not fit in memory.  Beyond that the retry framework is there to try and allow us to detect problems as they happen and recover from them.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1591270160/reactions,0,0,0,0,0,0,0,0,0,7836
643,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675160225,https://github.com/NVIDIA/spark-rapids/issues/7836#issuecomment-1675160225,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7836,1675160225,IC_kwDOD7z77c5j2Oqh,2023-08-11T17:48:50Z,2023-08-11T17:48:50Z,COLLABORATOR,"That would work as long as it can help reduce the chances of OOM for table scan stage.
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675160225/reactions,0,0,0,0,0,0,0,0,0,7836
644,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1456324559,https://github.com/NVIDIA/spark-rapids/issues/7845#issuecomment-1456324559,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7845,1456324559,IC_kwDOD7z77c5Wzb_P,2023-03-06T15:15:51Z,2023-03-06T15:15:51Z,COLLABORATOR,"![Screenshot from 2023-03-06 09-15-27](https://user-images.githubusercontent.com/4563792/223151414-0b514afe-7702-4176-b78c-ed7e4423e40c.png)
",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1456324559/reactions,0,0,0,0,0,0,0,0,0,7845
645,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1457205027,https://github.com/NVIDIA/spark-rapids/issues/7851#issuecomment-1457205027,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7851,1457205027,IC_kwDOD7z77c5W2y8j,2023-03-06T23:25:09Z,2023-03-06T23:25:09Z,COLLABORATOR,cuDF issue: https://github.com/rapidsai/cudf/issues/12889,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1457205027/reactions,0,0,0,0,0,0,0,0,0,7851
646,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478583881,https://github.com/NVIDIA/spark-rapids/issues/7861#issuecomment-1478583881,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7861,1478583881,IC_kwDOD7z77c5YIWZJ,2023-03-21T21:09:56Z,2023-03-21T21:10:21Z,COLLABORATOR,"@mythrocks I removed the `Needs Triage` label.  If you've determined that this is an issue that needs to be fixed, then please re-add the `Needs Triage` label and then we can assess priority.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478583881/reactions,0,0,0,0,0,0,0,0,0,7861
647,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464326044,https://github.com/NVIDIA/spark-rapids/issues/7869#issuecomment-1464326044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7869,1464326044,IC_kwDOD7z77c5XR9ec,2023-03-10T19:43:45Z,2023-03-10T19:46:59Z,COLLABORATOR,"I think we can implement a specific exception class in the Java JNI layer. Then we have a dedicated API in JNI to check for column size. Something like:
```
static void throwIfSizeExceedCudfLimit(...)
  if (inputColumn.numRows() > long(INT_MAX)) {
    throw new ColumnSizeExceedCudfLimitException(...);
  }
}
```

The plugin just try+catch that specific exception for split+retry.

Probably that `throwIfSizeExceedCudfLimit` check is called in `ColumnVector` (`ColumnView`?) constructor so we always have new column being checked.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464326044/reactions,0,0,0,0,0,0,0,0,0,7869
648,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464374658,https://github.com/NVIDIA/spark-rapids/issues/7869#issuecomment-1464374658,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7869,1464374658,IC_kwDOD7z77c5XSJWC,2023-03-10T20:22:56Z,2023-03-10T20:22:56Z,COLLABORATOR,"The problem is not with the input, but the output of operations.  CUDF is the one that hopefully finds that the output is too large and throws an exception.

https://github.com/rapidsai/cudf/blob/2969b241c0654a11d1a61e29664bcaecd7bc4a15/cpp/include/cudf/strings/detail/strings_children.cuh#L82-L84

But that is not guaranteed in all cases because of overflow.  The ideal would be that when a string, or any other column type, would exceed the CUDF limits then there could be an exception class that is specific to this so we could know that it happened and then retry the operation with a smaller input to avoid the issue.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464374658/reactions,0,0,0,0,0,0,0,0,0,7869
649,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464475352,https://github.com/NVIDIA/spark-rapids/issues/7869#issuecomment-1464475352,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7869,1464475352,IC_kwDOD7z77c5XSh7Y,2023-03-10T21:09:32Z,2023-03-10T21:09:32Z,COLLABORATOR,"I see. Recently cudf introduced several new exception class (https://github.com/rapidsai/cudf/pull/12426) to throw in case of invalid input type. It should be reasonable to add a new exception type for our specific need.

I've filed a related issue: https://github.com/rapidsai/cudf/issues/12925.

",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1464475352/reactions,0,0,0,0,0,0,0,0,0,7869
650,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462232046,https://github.com/NVIDIA/spark-rapids/issues/7872#issuecomment-1462232046,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7872,1462232046,IC_kwDOD7z77c5XJ-Pu,2023-03-09T15:16:54Z,2023-03-09T15:16:54Z,COLLABORATOR,Looks like this one is already on by default: https://github.com/NVIDIA/spark-rapids/blob/branch-23.04/sql-plugin/src/main/scala/com/nvidia/spark/rapids/RapidsConf.scala#L730-L738,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462232046/reactions,0,0,0,0,0,0,0,0,0,7872
651,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1463744565,https://github.com/NVIDIA/spark-rapids/issues/7872#issuecomment-1463744565,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7872,1463744565,IC_kwDOD7z77c5XPvg1,2023-03-10T12:36:18Z,2023-03-10T12:36:18Z,COLLABORATOR,"The `spark.rapids.sql.hasExtendedYearValues=true` indicates your dataset has extended year values, which we do not support, and we will fall back to the CPU.  To run on the GPU this config needs to be set to false.  

We need to add support for handling extended years and remove the need for this configuration.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1463744565/reactions,0,0,0,0,0,0,0,0,0,7872
652,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1629918281,https://github.com/NVIDIA/spark-rapids/issues/7872#issuecomment-1629918281,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7872,1629918281,IC_kwDOD7z77c5hJpRJ,2023-07-11T00:31:20Z,2023-07-11T00:31:20Z,COLLABORATOR,"I have filed the following issues against [cudf](https://github.com/rapidsai/cudf/issues/13682) and [spark-rapids-jni](https://github.com/NVIDIA/spark-rapids-jni/issues/1258). Depending on where this fix will be, the other one should be closed out

",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1629918281/reactions,0,0,0,0,0,0,0,0,0,7872
653,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462402829,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1462402829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1462402829,IC_kwDOD7z77c5XKn8N,2023-03-09T16:49:52Z,2023-03-09T16:49:52Z,COLLABORATOR,"I have not seen a single place where rowBasedUDF is actually beneficial for performance. I would love to see some examples of where it actually helps.

If we do want to limit the impact of UDFs that cannot be on the GPU then we need to find ways to isolate from other GPU commands so we can release the semaphore while we are doing the CPU computation, and then grab it again afterwards. There are a few options on how we might be able to do that.  ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462402829/reactions,0,0,0,0,0,0,0,0,0,7873
654,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478600133,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1478600133,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1478600133,IC_kwDOD7z77c5YIaXF,2023-03-21T21:25:23Z,2023-03-21T21:25:23Z,COLLABORATOR,"Consider removing feature from the plugin given we haven't seen any use cases where there is a benefit.

@viadea: have you seen specific use cases where enabling this flag has been beneficial from a performance standpoint?",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478600133/reactions,0,0,0,0,0,0,0,0,0,7873
655,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675179097,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1675179097,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1675179097,IC_kwDOD7z77c5j2TRZ,2023-08-11T18:08:01Z,2023-08-11T18:08:01Z,COLLABORATOR,"@mattahrens  I believe it might help performance if it can avoid a huge CPU fallback.
But I do not have metrics number to prove that yet since we normally enable multiple parameters in one shot. So could not really tell if this specific change is making difference or not. 

Maybe next time we can try to test with only enabling this specific parameter to see if it can help perf or not. 
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675179097/reactions,0,0,0,0,0,0,0,0,0,7873
656,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675197275,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1675197275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1675197275,IC_kwDOD7z77c5j2Xtb,2023-08-11T18:25:24Z,2023-08-11T18:25:24Z,MEMBER,"As @revans2 mentioned, we've never seen a case where this helped.  We should not be asking users to turn this on unless it's isolated from other changes and we're seeing performance increases.  It should not be recommended generally at this point.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675197275/reactions,0,0,0,0,0,0,0,0,0,7873
657,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675201624,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1675201624,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1675201624,IC_kwDOD7z77c5j2YxY,2023-08-11T18:29:47Z,2023-08-11T18:29:47Z,COLLABORATOR,@nvliyuan @rwlee @SurajAralihalli  FYI here.,,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675201624/reactions,1,0,0,0,0,0,0,0,1,7873
658,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675202840,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1675202840,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1675202840,IC_kwDOD7z77c5j2ZEY,2023-08-11T18:30:58Z,2023-08-11T18:30:58Z,COLLABORATOR,I have plans to remove it because it causes a lot of problems with host memory management. I want to see a real world query where this actually helps. I have not seen a single one yet.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675202840/reactions,0,0,0,0,0,0,0,0,0,7873
659,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675372638,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1675372638,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1675372638,IC_kwDOD7z77c5j3Che,2023-08-11T20:31:48Z,2023-08-11T20:31:48Z,COLLABORATOR,"Okay so I did some tests and I gave rowBaseUDF the best chance possible. I created a very small UDF that just adds two longs together.  Then using NDS data for store sales I did the following querys.

```
spark.time(data.selectExpr(""SUM(ss_quantity + ss_store_sk) as something"").show())
spark.time(data.selectExpr(""SUM(my_add(ss_quantity, ss_store_sk)) as something"").show())
```

In the second query it is put in as a part of the hash aggregate so it prevents the aggregate from running on the GPU.

| op | + | my_add |
|-|-|-|
| CPU | about 2900 ms | about 3700 ms |
| GPU | 412 ms | about 5300 ms |
| GPU with rowBasedUDF | 412 ms | about 10300 ms (for concurrent of 4) about 7350 ms (for concurrent of 12) |

From this it is clear that it is not about preventing computation from falling back to the CPU. In fact there are cases where it makes it worse to have this enabled. 

It is all about preventing data movement. The one case that I can force this to be a benefit is when we have lots of columns that are not being touched bu just going along for the ride.

```
spark.time(data.selectExpr(""*"", ""my_add(ss_quantity, ss_store_sk) as tmp"").write.mode(""overwrite"").parquet(""/data/tmp/OUTPUT""))
spark.time(data.selectExpr(""*"", ""ss_quantity + ss_store_sk as tmp"").write.mode(""overwrite"").parquet(""/data/tmp/OUTPUT""))
```

| op | + | my_add |
|-|-|-|
| CPU | 323363 ms | 319553 ms |
| GPU |  about 28000 ms | about 86000 ms |
| GPU with rowBasedUDF | about 28000 ms | about 39000 ms (for concurrent of 4) |

I didn't do a concurrent of 12 here for the UDF because it stated to spill a lot.  But is this common enough that keep the config around? It is clear that we cannot turn it on all the time. There are just too many cases where it makes things worse.

I still want to rip out this code because it is going to make the host memory limits much harder to implement if we don't remove it.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675372638/reactions,0,0,0,0,0,0,0,0,0,7873
660,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849473975,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1849473975,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1849473975,IC_kwDOD7z77c5uPLu3,2023-12-11T07:35:10Z,2023-12-11T07:35:41Z,COLLABORATOR,Update a case in nvbug ***8068.,,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849473975/reactions,0,0,0,0,0,0,0,0,0,7873
661,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852825653,https://github.com/NVIDIA/spark-rapids/issues/7873#issuecomment-1852825653,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7873,1852825653,IC_kwDOD7z77c5ub-A1,2023-12-12T21:18:04Z,2023-12-12T21:18:04Z,COLLABORATOR,"Since @nvliyuan found one specific use case where rowBasedUDF might perform better, shall we avoid deleting this feature?
Instead, we keep this parameter so that advanced user can tune/test if needed.",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852825653/reactions,0,0,0,0,0,0,0,0,0,7873
662,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462326879,https://github.com/NVIDIA/spark-rapids/issues/7874#issuecomment-1462326879,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7874,1462326879,IC_kwDOD7z77c5XKVZf,2023-03-09T16:05:27Z,2023-03-09T16:05:27Z,COLLABORATOR,"We need to go through this on an operator by operator basis, and look at all of the types to see where there are gaps.

For example a udf that does a divide of integers could throw a divide by 0 exception, but the SQL divide by default does not.

We also have to see what happens if ANSI is enabled or not.  Can we force the divide to be an ANSI divide to get the behavior we want? There is a lot of testing and evaluation that we need to go through to understand what work needs to be done to really truly make this work.

What about Long vs long. In UDFs if the type is a long and a null shows up as input, then the UDF is not run and the output is null. But if the type is a Long, then the null is passed to the UDF and we need to be able to handle throwing exceptions for NPEs at the same time the java/scala code would.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1462326879/reactions,0,0,0,0,0,0,0,0,0,7874
663,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478609749,https://github.com/NVIDIA/spark-rapids/issues/7874#issuecomment-1478609749,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7874,1478609749,IC_kwDOD7z77c5YIctV,2023-03-21T21:31:03Z,2023-03-21T21:31:03Z,COLLABORATOR,"Functionality should be viewed as a prototype.  Additional validation would be needed before considering this by default.

@viadea: what are the use cases where you have seen that enabling this feature has had a major benefit?",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1478609749/reactions,0,0,0,0,0,0,0,0,0,7874
664,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675174004,https://github.com/NVIDIA/spark-rapids/issues/7874#issuecomment-1675174004,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7874,1675174004,IC_kwDOD7z77c5j2SB0,2023-08-11T18:02:51Z,2023-08-11T18:02:51Z,COLLABORATOR,"@mattahrens Regarding `spark.rapids.sql.udfCompiler.enabled`, I did not realize we might have different results for supported Scala UDFs -> Catalyst expression. 
sometimes we enable this parameter and hope it can help convert those simple enough Scala UDFs.
But if we have such potential result issue, I would suggest we firstly address those before enabling it by default.",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675174004/reactions,0,0,0,0,0,0,0,0,0,7874
665,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479714154,https://github.com/NVIDIA/spark-rapids/issues/7919#issuecomment-1479714154,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7919,1479714154,IC_kwDOD7z77c5YMqVq,2023-03-22T14:53:33Z,2023-03-22T14:53:33Z,COLLABORATOR,"@nvliyuan There are several things happening here and only one of them we should consider to be a bug.

First we are doing a write on the GPU for ORC when bloom filters are enabled. We do not support bloom filter writes on the GPU, so what we write out does not have any bloom filter data in it.  This could be a bug and we should at least fall back to the CPU until bloom filter writes can be supported by the GPU. I will file a separate issue for falling back to the CPU for bloom filter writes.

As for reading the data this works as we expect. We get the correct answer, but we have to scan more rows than we expect/want to scan. We should look into the bloom filter to understand if we are, or are not using it for filtering out stripes. I am not sure if that would be good enough or not.  It would require some more investigation.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479714154/reactions,0,0,0,0,0,0,0,0,0,7919
666,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479720516,https://github.com/NVIDIA/spark-rapids/issues/7919#issuecomment-1479720516,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7919,1479720516,IC_kwDOD7z77c5YMr5E,2023-03-22T14:57:34Z,2023-03-22T14:57:34Z,COLLABORATOR,"> We do not support bloom filter writes on the GPU, so what we write out does not have any bloom filter data in it.

sorry forget to mention in the issue that the normal dataset and bloom dataset are all generated by CPU.",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479720516/reactions,0,0,0,0,0,0,0,0,0,7919
667,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479730504,https://github.com/NVIDIA/spark-rapids/issues/7919#issuecomment-1479730504,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7919,1479730504,IC_kwDOD7z77c5YMuVI,2023-03-22T15:01:14Z,2023-03-22T15:01:14Z,COLLABORATOR,I filed https://github.com/NVIDIA/spark-rapids/issues/7921 for the bug where we need to fall back to the CPU.  As for bloom filter support on reads we need to do some investigation and see what it would take to support it at least for filtering out stripes.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1479730504/reactions,0,0,0,0,0,0,0,0,0,7919
668,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1608210575,https://github.com/NVIDIA/spark-rapids/issues/7919#issuecomment-1608210575,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7919,1608210575,IC_kwDOD7z77c5f21iP,2023-06-26T20:39:22Z,2023-06-26T20:39:22Z,MEMBER,"> We should look into the bloom filter to understand if we are, or are not using it for filtering out stripes.

I verified that the GPU read is leveraging bloom filters for eliminating stripes.  I tried to load data from an ORC file where the min/max statistics were unable to eliminate the stripe based on the ID being probed but the bloom filter was, and zero rows were emitted from the GPU OrcScan.  So the thing missing is filtering row groups within a stripe (i.e.: using the row index stats).

Filtering rowgroups within a stripe is possible, but it will be tricky given the existing APIs to cudf.  A row index entry has a triplet of info for compressed column streams: an offset the compression block containing the row group, the number of compression bytes to discard at the start of the block, and the number of RLE bytes to discard after that to get to the start of the rowgroup.  We'd either need to develop a new, lower-level API for feeding data to the libcudf ORC reader, allowing cudf to locate the data without actually needing the data to be there (so we could avoid the I/O), or we would need to cheaply fabricate stand-in data for the data that was skipped in the remote filesystem to keep the ORC file valid (probably necessitating re-encoding the ORC row index stream or potentially writing a lot of data on the CPU).",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1608210575/reactions,0,0,0,0,0,0,0,0,0,7919
669,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487555542,https://github.com/NVIDIA/spark-rapids/issues/7938#issuecomment-1487555542,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7938,1487555542,IC_kwDOD7z77c5YqkvW,2023-03-28T20:32:16Z,2023-03-28T20:32:16Z,COLLABORATOR,"Hi @nvliyuan perhaps we start with one rule at a time, and put up a PR and see if the changes look reasonable?  Would you be able to put up a PR for the suggested changes for rule MD032 and we can review?  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1487555542/reactions,0,0,0,0,0,0,0,0,0,7938
670,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507835638,https://github.com/NVIDIA/spark-rapids/issues/7938#issuecomment-1507835638,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7938,1507835638,IC_kwDOD7z77c5Z3772,2023-04-14T02:17:36Z,2023-04-14T02:17:36Z,COLLABORATOR,"> Hi @nvliyuan perhaps we start with one rule at a time, and put up a PR and see if the changes look reasonable? Would you be able to put up a PR for the suggested changes for rule MD032 and we can review?

sure, will do that in #7929 and change it to base v2306",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507835638/reactions,0,0,0,0,0,0,0,0,0,7938
671,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1490664701,https://github.com/NVIDIA/spark-rapids/issues/7981#issuecomment-1490664701,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7981,1490664701,IC_kwDOD7z77c5Y2bz9,2023-03-30T17:24:10Z,2023-03-30T17:24:10Z,COLLABORATOR,"why do we want this opt-in?  How do you know they don't need the private jar or won't need it in the future?  
What does including it make not possible or much harder as offset adding more complexity to the build and possibly introduce errors in building/testing? 

if it has side affects we want to know about it.  Yes it needs to be including in the build so might affect iteration a little bit but once its downloaded I can't believe that is very much time.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1490664701/reactions,0,0,0,0,0,0,0,0,0,7981
672,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1490772435,https://github.com/NVIDIA/spark-rapids/issues/7981#issuecomment-1490772435,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7981,1490772435,IC_kwDOD7z77c5Y22HT,2023-03-30T18:50:43Z,2023-03-30T20:39:10Z,COLLABORATOR,"> why do we want this opt-in? How do you know they don't need the private jar or won't need it in the future?

That was poor wording for the intent of this issue. I mean opt-out for when the user knows.

> What does including it make not possible or much harder as offset adding more complexity to the build and possibly introduce errors in building/testing?

I wish the user can skip this code altogether when the user knows it's not needed. I have no additional comments beyond making it more robust.  

> if it has side affects we want to know about it. 

This is covered by the default profiles where it's used. And again I just mis-wrote regarding opt-in vs opt-out

> Yes it needs to be including in the build so might affect iteration a little bit but once its downloaded I can't believe that is very much time.

Agree, although a secondary issue, but why deal with it when not absolutely necessary, and so easy to avoid.

",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1490772435/reactions,0,0,0,0,0,0,0,0,0,7981
673,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1494729607,https://github.com/NVIDIA/spark-rapids/issues/7994#issuecomment-1494729607,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7994,1494729607,IC_kwDOD7z77c5ZF8OH,2023-04-03T17:45:29Z,2023-04-03T17:45:29Z,COLLABORATOR,"The plugin has `spark.rapids.memory.gpu.oomDumpDir` for unrecoverable GPU OOM, which is equivalent to calling `jmap -dump` on the process. I am not sure if that's the crash you are interested in or if there are others?

We log warn all the threads active on the semaphore (the stack traces) when there is a GPU OOM as well. If it's GPU OOM we have some things in place but could probably do more if needed.

On JVM crashes (say when there is a bug in the native side of things and there is a segfault) we normally get an hs_err file with native stack trace and JVM stack trace. This file can be pretty useful to figure out what the executor was doing. There is also `XX:OnError` to call some command and I see some documentation of folks using it to drop into gdb for example, so perhaps that could be a trigger for more sophisticated actions.

It mostly depends on what the nature of the crash is IMHO.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1494729607/reactions,0,0,0,0,0,0,0,0,0,7994
674,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1494816981,https://github.com/NVIDIA/spark-rapids/issues/7994#issuecomment-1494816981,https://api.github.com/repos/NVIDIA/spark-rapids/issues/7994,1494816981,IC_kwDOD7z77c5ZGRjV,2023-04-03T18:55:49Z,2023-04-03T18:55:49Z,COLLABORATOR,"I think the goal with this is to make a big debug easy button for customers.  We have a lot of ways to dump debug information, but it is not controlled by a single config, and it is especially hard to do if we are in a distributed setup. Perhaps in some cases, like with YARN, we can just have some instructions on how to set specific things to make it simple to send us a crash report when we don't have direct access to a cluster.  But for other environments we might need to think more about how to make this super simple and as consistent as possible between different cluster types.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1494816981/reactions,1,1,0,0,0,0,0,0,0,7994
675,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1801228044,https://github.com/NVIDIA/spark-rapids/issues/8003#issuecomment-1801228044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8003,1801228044,IC_kwDOD7z77c5rXI8M,2023-11-08T07:22:27Z,2023-11-08T07:22:27Z,COLLABORATOR,Low priority,,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1801228044/reactions,0,0,0,0,0,0,0,0,0,8003
676,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1496636414,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1496636414,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1496636414,IC_kwDOD7z77c5ZNNv-,2023-04-04T21:37:39Z,2023-04-04T21:37:39Z,COLLABORATOR,"@mtsol 

When discussing this we were a little confused if this fails randomly like the GPU memory is near the limit on what it can support and some times it works, while other times it fails, or if this looks more like a memory leak where running in X times always works, but X+1 times crashes?

We are working on a way to mitigate situations like this #7778 The goal is to have this in the 23.06 release. If you want to try and test it sooner I can see if we can come up with a version you could try out.

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1496636414/reactions,1,1,0,0,0,0,0,0,0,8021
677,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1498510230,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1498510230,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1498510230,IC_kwDOD7z77c5ZUXOW,2023-04-06T05:25:58Z,2023-04-06T05:25:58Z,NONE,I would appreciate if you can provide me something to test it sooner.,,mtsol,20060589,MDQ6VXNlcjIwMDYwNTg5,https://avatars.githubusercontent.com/u/20060589?v=4,,https://api.github.com/users/mtsol,https://github.com/mtsol,https://api.github.com/users/mtsol/followers,https://api.github.com/users/mtsol/following{/other_user},https://api.github.com/users/mtsol/gists{/gist_id},https://api.github.com/users/mtsol/starred{/owner}{/repo},https://api.github.com/users/mtsol/subscriptions,https://api.github.com/users/mtsol/orgs,https://api.github.com/users/mtsol/repos,https://api.github.com/users/mtsol/events{/privacy},https://api.github.com/users/mtsol/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1498510230/reactions,0,0,0,0,0,0,0,0,0,8021
678,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1512825020,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1512825020,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1512825020,IC_kwDOD7z77c5aK-C8,2023-04-18T10:20:55Z,2023-04-18T10:22:01Z,NONE,"""if this fails randomly like the GPU memory is near the limit on what it can support and some times it works, while other times it fails, or if this looks more like a memory leak where running in X times always works, but X+1 times crashes?""

Ans: In my case, it crashes always X+1 time, like when I have 1.2 million rows in my dataset, everything works fine, but when I increase the data, it crashes with this error, it crashes on 1.5 million, 2 million rows as well, and any number of data between them. I cannot say if it is related to memory leak, but as I observed the error occurs when data increases a certain limit.

PS: I will appreciate if you can provide and jar prior to the release to test if that one works fine with our data.",,mtsol,20060589,MDQ6VXNlcjIwMDYwNTg5,https://avatars.githubusercontent.com/u/20060589?v=4,,https://api.github.com/users/mtsol,https://github.com/mtsol,https://api.github.com/users/mtsol/followers,https://api.github.com/users/mtsol/following{/other_user},https://api.github.com/users/mtsol/gists{/gist_id},https://api.github.com/users/mtsol/starred{/owner}{/repo},https://api.github.com/users/mtsol/subscriptions,https://api.github.com/users/mtsol/orgs,https://api.github.com/users/mtsol/repos,https://api.github.com/users/mtsol/events{/privacy},https://api.github.com/users/mtsol/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1512825020/reactions,0,0,0,0,0,0,0,0,0,8021
679,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1514520571,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1514520571,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1514520571,IC_kwDOD7z77c5aRb_7,2023-04-19T10:46:56Z,2023-04-19T10:46:56Z,NONE,"After debugging and analysis, I found in my code that this statement:

df = df.withColumn(self.output_col_name, concat_ws(col_sep, array(self.input_col_name_list)))

was causing the error on larger data on gpus. And I think there is some bug in the gpu optimization of concat function, which needs to be addressed.",,mtsol,20060589,MDQ6VXNlcjIwMDYwNTg5,https://avatars.githubusercontent.com/u/20060589?v=4,,https://api.github.com/users/mtsol,https://github.com/mtsol,https://api.github.com/users/mtsol/followers,https://api.github.com/users/mtsol/following{/other_user},https://api.github.com/users/mtsol/gists{/gist_id},https://api.github.com/users/mtsol/starred{/owner}{/repo},https://api.github.com/users/mtsol/subscriptions,https://api.github.com/users/mtsol/orgs,https://api.github.com/users/mtsol/repos,https://api.github.com/users/mtsol/events{/privacy},https://api.github.com/users/mtsol/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1514520571/reactions,0,0,0,0,0,0,0,0,0,8021
680,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1514815797,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1514815797,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1514815797,IC_kwDOD7z77c5aSkE1,2023-04-19T14:15:57Z,2023-04-19T14:15:57Z,COLLABORATOR,"@mstol thanks for the updated info. `concat_ws` can be a memory hog, especially if you are not also dropping the input columns after concatenating them together. We are aware that we have some problems with batch sizes when doing a ProjectExec that adds more rows. We have plans to work on this https://github.com/NVIDIA/spark-rapids/issues/7257 is the epic to work on it.

I am guessing that you just removed that line from your query, and because of that it dropped the total memory pressure at that point in time and for data being processed after it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1514815797/reactions,0,0,0,0,0,0,0,0,0,8021
681,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1516549902,https://github.com/NVIDIA/spark-rapids/issues/8021#issuecomment-1516549902,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8021,1516549902,IC_kwDOD7z77c5aZLcO,2023-04-20T15:38:18Z,2023-04-20T15:38:18Z,COLLABORATOR,"@mtsol I have a snapshot jar that you can try.

https://drive.google.com/file/d/15RyaI5OyeSJNEj5G-W4MnN8JeyQPq4ff/view?usp=sharing

Be aware that there are some known bugs with it.  Specifically https://github.com/NVIDIA/spark-rapids/issues/8147 which is caused by https://github.com/rapidsai/cudf/issues/13173 so it should go without saying, but don't use this in production, and avoid the substring command if you can.

If you want a better version I can upload another one once the issue is fixed.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1516549902/reactions,0,0,0,0,0,0,0,0,0,8021
682,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499204792,https://github.com/NVIDIA/spark-rapids/issues/8048#issuecomment-1499204792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8048,1499204792,IC_kwDOD7z77c5ZXAy4,2023-04-06T14:55:39Z,2023-04-06T14:55:39Z,COLLABORATOR,one of the biggest things for the delta_log json files is having a performant and accurate json parser.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499204792/reactions,0,0,0,0,0,0,0,0,0,8048
683,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499655620,https://github.com/NVIDIA/spark-rapids/issues/8048#issuecomment-1499655620,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8048,1499655620,IC_kwDOD7z77c5ZYu3E,2023-04-06T21:34:45Z,2023-04-06T21:34:45Z,MEMBER,"> one of the biggest things for the delta_log json files is having a performant and accurate json parser

Agreed.  Given the relatively small amount of data in each JSON file, we may need to look into coalescing files to get a bigger amount of data for the GPU to process per invocation.

We also probably need to cover the rest of the metadata query on the GPU.  Unless the JSON parser is _really_ fast compared to the CPU (unlikely unless there is a ton of JSON to process), there's immediately going to be a fallback to the CPU for the very next operation (e.g.: ObjectHashAggregate, MapPartitions, etc.) and that transition will likely kill the speed gains from the JSON parsing on the GPU.

There's also the possibility that the JSON parsing has been cached, and the metadata query will start with an ExstingRDD Scan, and that may be tricky to speed up given the initial CPU rows -> GPU columns conversion at the very start.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1499655620/reactions,0,0,0,0,0,0,0,0,0,8048
684,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538470394,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1538470394,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1538470394,IC_kwDOD7z77c5bszH6,2023-05-08T14:37:33Z,2023-05-08T14:37:33Z,COLLABORATOR,"Spilling to disk is done to keep the host store at a specific target size. So when the host store is full, this pauses host spills as well.

The `RapidsBufferCatalog` triggers the spill to disk here https://github.com/NVIDIA/spark-rapids/blob/branch-23.06/sql-plugin/src/main/scala/com/nvidia/spark/rapids/RapidsBufferCatalog.scala#L527, for the host store (the only store that has a maximum size as of today).

So I think instead of spilling (and after detecting that we are with `RmmRapidsRetryIterator`, so we need to set some sort of thread local state here), we would need to throw a special exception instead of spilling to disk, such that the JNI adapter could handle it and keep state for this thread that has ""wanted to spill to disk N times"". The `onAllocFailure` callback in `DeviceMemoryEventHandler` would need to get back some state from the state machine saying that it already paused this thread due to a prior disk spill attempt, and this time it means it so it should let it through.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538470394/reactions,0,0,0,0,0,0,0,0,0,8068
685,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538653093,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1538653093,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1538653093,IC_kwDOD7z77c5btful,2023-05-08T16:03:10Z,2023-05-08T16:03:10Z,COLLABORATOR,"Just a few notes about this. This is not supposed to produce production level code.  We have a lot of tech debt to pay off first in the spill framework before we get to putting this into production. I would love to have you do some experiments to see if this even makes since to go this route. If it does, then we can go back and figure out a long term plan to make it happen cleanly. When putting together changes for the experiments you don't have to worry about the UCX shuffle for the state machine, and you can hack up the spill code just to make something work so we can get an idea of what the performance might look like. You also don't have to worry about GPU Direct Storage for the spill code.

The current code will do the following to avoid an OOM

1. Spill to host memory
2. Spill to disk
3. pause the current thread
4. throw retry exception on lowest priority thread
5. throw split and retry exception

We want to change the order of these so that the priority is

1. Spill to host memory
2. pause the current thread
3. throw retry exception on lowest priority thread
4. spill to disk
5. throw split and retry exception

To do this I think the big change would have to be that the spill [callback](https://github.com/rapidsai/cudf/blob/0a5065fe03c40977016febb4b9f324f4902fa0dd/java/src/main/native/src/RmmJni.cpp#L163) code would have to move to the [state machine](https://github.com/NVIDIA/spark-rapids-jni/blob/branch-23.06/src/main/cpp/src/SparkResourceAdaptorJni.cpp) in Spark Rapids JNI. We would also have to provide an API to the callback that lets the spill code know if it is allowed to spill all the way to disk, or just spill to host memory. Then the state machine when an allocation fails it would first call the spill code saying spill only to host memory. If that succeeds and some memory is spilled/freed, then we retry the allocation, if nothing was freed then we got through the pause code like we do today. Exception if all of the threads are paused instead of throwing an `SplitAndRetryOOM` we try to spill again, this time we allow it to spill to disk. If that works, then we can go on (retry the allocation). If it does not work, then we throw the `SplitAndRetryOOM`.

For testing I really would like to see the performance of running a query that would normally spill to disk before and after the patch. I think we can do a lot of this with NDS queries and adjusting the maximum memory that the GPU has access to. The config `spark.rapids.memory.gpu.allocSize` is intended to only be used with testing, and it should give you a good approximation of running on a GPU with much lower memory than it actually has. The idea is that it is probably cheaper to pause a thread, and drop parallelism than it is to spill to disk.  But the way our operators work, and how they use memory does not make it super clear if that is true all the time. Especially with really fast disks. So because of that I would love to see the impact of testing this on an on prem cluster with fast disks, but I would also love to see it run on some other cloud where we know that the disk speeds are relatively slow (or at least are slow by default without adding NVMe store to them).",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1538653093/reactions,0,0,0,0,0,0,0,0,0,8068
686,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1606398999,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1606398999,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1606398999,IC_kwDOD7z77c5fv7QX,2023-06-26T01:27:58Z,2023-06-27T01:40:52Z,COLLABORATOR,"Figured out a way to do the POC things. Here are the two relevant changes in my personal repos.

https://github.com/firestarman/spark-rapids-jni/pull/1/files
https://github.com/firestarman/spark-rapids/pull/6/files

NOTE: We are NOT going to merge these and they are only used for POC.

Will run the perf tests next.

",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1606398999/reactions,1,0,0,0,1,0,0,0,0,8068
687,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612537817,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1612537817,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1612537817,IC_kwDOD7z77c5gHV_Z,2023-06-29T07:16:08Z,2023-07-03T06:05:03Z,COLLABORATOR,"So far I have tried a customer case where the spilling indeed happened on spark2a, but did not get any perf improvement. 
What's more, I got some OOM errors when running with the two PRs. Will investgate more.

And I ran the NDS on Dataproc (Spilling happened with setting `allocSize` to 8G on Tesla 4) for this and dit not get any perf improvement either.
 
[numbers](https://docs.google.com/spreadsheets/d/1-hmZTTslpWlEONSaKcjCXHqcTfGQRs_SJ0BiaRNEWpE/edit#gid=1228411614)",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612537817/reactions,0,0,0,0,0,0,0,0,0,8068
688,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612657472,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1612657472,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1612657472,IC_kwDOD7z77c5gHzNA,2023-06-29T08:53:38Z,2023-07-03T06:04:49Z,COLLABORATOR,"Pls correct me if wrong.
In theory, if the new pipeline wants to get better performance, we may need to have a good runtime data size that can be all held by the host memory store, which can avoid the disk writing during the spilling process to run faster. Otherwise, disk writing is still necessary in the new pipeline.

For most cases, seems it is impossible to have such good data size during runtime, then we may not get better performance.
",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612657472/reactions,0,0,0,0,0,0,0,0,0,8068
689,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1613178748,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1613178748,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1613178748,IC_kwDOD7z77c5gJyd8,2023-06-29T13:24:17Z,2023-06-29T13:24:17Z,COLLABORATOR,"@firestarman The experiment where there is lots of spilling is informative.  I was hoping that there would be a reduction in the total data spilled to disk, and hence a reduction in the runtime of the query.

I think we want to start out with some kind of a synthetic load.  We know that it should have no impact on performance if there is no spill at all, but I think we can come up with a few interesting situations to see what happens.

I would like to see times for running NDS (just because it is a good synthetic load).  Then we can start to adjust `spark.rapids.memory.gpu.allocSize` to artificially reduce the GPUs memory to induce a spill and also adjust `spark.rapids.memory.pinnedPool.size` and `spark.rapids.memory.host.spillStorageSize` to see what the impact would be. We should keep the concurrency at a set amount, ideally 4 so there are multiple threads to pause, but 2 works too.

I would love to see what the total `gpuSpillBlockTime`, `gpuSpillReadTime`, `gpuSemaphoreWait`, `gpuRetryBlockTime`, `gpuRetryCount`, and `gpuSplitAndRetryCount` in addition to the total run time, memory spill and disk spill for different settings. The first set of tests would be a baseline to measure the impact of spilling to memory on the runtime of the NDS. We start with setting where there is no spilling because everything fits in GPU memory.  Then we reduce the GPU memory and increase the host spill storage size at varying amounts keeping the total memory available for storage the same. The amount of memory for the GPU should not get below 4 to 6 GiB, just because I to be sure there is enough memory for a single task to execute on the GPU without any issues. We should try this with both your patch and the baseline jar. Because there would be no spill to disk there should be no retries happen, no OOM errors or anything like that. Just a clean curve that should ideally be the same for both jars.

Next we can take one of the situations where there is a decent amount of spilling to host memory, and start to reduce the amount of host memory available. My hope is that there is a rather wide range in which we can avoid spilling to disk by pausing some threads and in those cases the overall runtime will be faster. After that we can start to look at the data and decide on next steps.

It really comes down to a few different things. If pausing and retrying computation is more expensive than spilling to disk, then we have the order of operations correct. If spilling to disk is more expensive that pausing/retrying computation, then we should look at shifting the order of operations around. Or there is inherently something else that is happening that we are blocked on when running out of memory that we don't totally understand yet.  I hope that the metrics collected will help us understand which outcome it is.

Also these may be different for different environments.  a PCIe gen3 card in a setup with a really complicated PCIe switch structure (like many cloud service providers on a T4) and possibly slow disks will likely behave very differently compared to a V100 on PCIe gen4 with dedicated lanes directly to the CPU and very fast NVMe storage also directly connected to the CPU.  OR even a Grace Hopper setup where we have something close to a TiB/s of bandwidth between the CPU and GPU for spilling.

So we might want to explore a few different setups.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1613178748/reactions,1,0,0,0,0,0,0,0,1,8068
690,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724896384,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1724896384,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1724896384,IC_kwDOD7z77c5mz9SA,2023-09-19T06:23:39Z,2023-09-19T06:23:39Z,COLLABORATOR,"Hi @revans2, shall we move it to 23.12 ?
Seems there is a lot of testing work to do, so I probably can not make it for 23.08.
",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724896384/reactions,0,0,0,0,0,0,0,0,0,8068
691,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725903774,https://github.com/NVIDIA/spark-rapids/issues/8068#issuecomment-1725903774,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8068,1725903774,IC_kwDOD7z77c5m3zOe,2023-09-19T15:34:44Z,2023-09-19T15:34:44Z,COLLABORATOR,yes we can push this back. I think the work to add in retry support in more places is more important.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725903774/reactions,1,0,0,0,0,0,0,0,1,8068
692,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1506975003,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1506975003,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1506975003,IC_kwDOD7z77c5Z0p0b,2023-04-13T13:34:33Z,2023-04-13T13:34:33Z,COLLABORATOR,Looks like we have a lambda inside of GPuProjectExec that is pulling in the ProjectExec itself.  We likely have this problem in a lot of other places too.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1506975003/reactions,0,0,0,0,0,0,0,0,0,8095
693,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1506978176,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1506978176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1506978176,IC_kwDOD7z77c5Z0qmA,2023-04-13T13:36:44Z,2023-04-13T13:36:44Z,COLLABORATOR,"They have shown that `withResource` was the culprit in this particular case, since they removed `withResource` and the project exec didn't need serializing. But I agree that there may be other things we are doing causing serialization to trigger in other places. Kyuubi did fix a similar issue, but it is in an unreleased version (https://github.com/apache/kyuubi/issues/4617).",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1506978176/reactions,0,0,0,0,0,0,0,0,0,8095
694,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507175879,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1507175879,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1507175879,IC_kwDOD7z77c5Z1a3H,2023-04-13T15:30:21Z,2023-04-13T15:30:21Z,COLLABORATOR,"I can take this on to be done by next sprint. Virtually all files need to change, and it doesn't seem to be a trivial script that could handle it. So it is just going to take time.

If it is for 23.04 and is really important it would be great to know. @sameerz @GaryShen2008 ",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507175879/reactions,0,0,0,0,0,0,0,0,0,8095
695,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507713291,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1507713291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1507713291,IC_kwDOD7z77c5Z3eEL,2023-04-13T23:06:20Z,2023-04-14T01:10:32Z,COLLABORATOR,"This issue happened when using Kyuubi 1.7.0 with Ranger authorization.
Kyuubi has fixed the serializable issue in their latest code, but it's not released yet.
I hope to have a simple fixing in GpuProjectExec to make it unblocking our usage of Kyuubi 1.7.0 if possible. Otherwise we'll need to use their master-snapshot version to bypass this issue with our coming 23.04 release.
And I don't know when Apache Kyuubi will release the next version. I hope we don't need to depend on that.",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507713291/reactions,0,0,0,0,0,0,0,0,0,8095
696,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1508229844,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1508229844,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1508229844,IC_kwDOD7z77c5Z5cLU,2023-04-14T09:33:54Z,2023-04-14T09:33:54Z,COLLABORATOR,"Update one thing, I double tested the Kyuubi's master-snapshot image, the issue still occurred.
In that case, it seems a MUST fix in our plugin side.",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1508229844/reactions,0,0,0,0,0,0,0,0,0,8095
697,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513760443,https://github.com/NVIDIA/spark-rapids/issues/8095#issuecomment-1513760443,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8095,1513760443,IC_kwDOD7z77c5aOia7,2023-04-18T20:32:07Z,2023-04-18T20:32:07Z,COLLABORATOR,"@GaryShen2008 can you confirm that the merged PR resolves the issue?  If so, please close this issue.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513760443/reactions,0,0,0,0,0,0,0,0,0,8095
698,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507452629,https://github.com/NVIDIA/spark-rapids/issues/8099#issuecomment-1507452629,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8099,1507452629,IC_kwDOD7z77c5Z2ebV,2023-04-13T18:43:05Z,2023-04-13T18:43:05Z,COLLABORATOR,"I was able to build fine on my macbook pro, but perhaps its intel vs arm or python installation issue.  its unfortunately its an unknown error.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1507452629/reactions,0,0,0,0,0,0,0,0,0,8099
699,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513767091,https://github.com/NVIDIA/spark-rapids/issues/8113#issuecomment-1513767091,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8113,1513767091,IC_kwDOD7z77c5aOkCz,2023-04-18T20:38:53Z,2023-04-18T20:38:53Z,COLLABORATOR,Scope: validate what happens for this use case to ensure we fallback properly to CPU.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513767091/reactions,0,0,0,0,0,0,0,0,0,8113
700,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1706727863,https://github.com/NVIDIA/spark-rapids/issues/8113#issuecomment-1706727863,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8113,1706727863,IC_kwDOD7z77c5lupm3,2023-09-05T14:25:48Z,2023-09-05T14:25:48Z,COLLABORATOR,Note that there was a follow on patch around this https://github.com/apache/spark/commit/f0e18284d0. ,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1706727863/reactions,0,0,0,0,0,0,0,0,0,8113
701,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513769251,https://github.com/NVIDIA/spark-rapids/issues/8115#issuecomment-1513769251,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8115,1513769251,IC_kwDOD7z77c5aOkkj,2023-04-18T20:40:56Z,2023-04-18T20:40:56Z,COLLABORATOR,Scope: validate expressions are properly handled.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513769251/reactions,0,0,0,0,0,0,0,0,0,8115
702,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513770482,https://github.com/NVIDIA/spark-rapids/issues/8118#issuecomment-1513770482,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8118,1513770482,IC_kwDOD7z77c5aOk3y,2023-04-18T20:42:12Z,2023-04-18T20:42:12Z,COLLABORATOR,"Scope: validate behavior that Spark is inserting nulls ahead of our plugin.  If not, we may need code changes on our side.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513770482/reactions,0,0,0,0,0,0,0,0,0,8118
703,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513775161,https://github.com/NVIDIA/spark-rapids/issues/8124#issuecomment-1513775161,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8124,1513775161,IC_kwDOD7z77c5aOmA5,2023-04-18T20:46:57Z,2023-04-18T20:46:57Z,COLLABORATOR,Scope: have a manual build available for developers to use for release candidate branches of key Spark versions.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1513775161/reactions,0,0,0,0,0,0,0,0,0,8124
704,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1515631720,https://github.com/NVIDIA/spark-rapids/issues/8124#issuecomment-1515631720,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8124,1515631720,IC_kwDOD7z77c5aVrRo,2023-04-20T02:43:34Z,2023-04-20T02:49:41Z,COLLABORATOR,"Previously we tried temply target the dependency to new patch snapshot version but this could not be the same as RC build one.

> Scope: have a manual build available for developers to use for release candidate branches of key Spark versions.

Sounds good to me. 

One thing is that if we would like to have it for public CI usage, we will need to add an extra artifact ID to sonatype repo for differentiating from official snapshots, or we are fine to just deploy to internal artifactory for internal use only?",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1515631720/reactions,0,0,0,0,0,0,0,0,0,8124
705,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1532084815,https://github.com/NVIDIA/spark-rapids/issues/8200#issuecomment-1532084815,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8200,1532084815,IC_kwDOD7z77c5bUcJP,2023-05-02T20:12:14Z,2023-05-02T20:12:14Z,MEMBER,"> The GpuCoalesceBatches algo needs some performance upgrades, I can file something for that.. but presumably whoever looks at this may want to take a look at the performance while at it.

The algorithm update may be related to #3750.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1532084815/reactions,1,1,0,0,0,0,0,0,0,8200
706,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1529784912,https://github.com/NVIDIA/spark-rapids/issues/8205#issuecomment-1529784912,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8205,1529784912,IC_kwDOD7z77c5bLqpQ,2023-05-01T14:39:52Z,2023-05-01T14:39:52Z,COLLABORATOR,"Yes this is probably a corner case, but we use the ""null"" character to turn off comments in CUDF.  So it might be more relevant than we think. At least we should test it and document any differences.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1529784912/reactions,1,1,0,0,0,0,0,0,0,8205
707,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1530375237,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1530375237,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1530375237,IC_kwDOD7z77c5bN6xF,2023-05-01T21:56:29Z,2023-05-01T21:56:38Z,COLLABORATOR,Note the exception is related to unity catalog and the job that had this failure had that enabled.   ,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1530375237/reactions,0,0,0,0,0,0,0,0,0,8210
708,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1533534138,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1533534138,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1533534138,IC_kwDOD7z77c5bZ9-6,2023-05-03T18:47:41Z,2023-05-03T18:47:41Z,COLLABORATOR,Note the stack trace changes for the location it gets thrown when using the coalescing reader without parallel footer filtering.  So it seems it definitely just happens when we are doing the parsing in separate threads so likely something local or something we aren't passing on.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1533534138/reactions,0,0,0,0,0,0,0,0,0,8210
709,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1548566221,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1548566221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1548566221,IC_kwDOD7z77c5cTT7N,2023-05-15T20:58:25Z,2023-05-15T20:58:25Z,COLLABORATOR,the only way I've been able to reproduce at this point is to do a delta write to a one table and then a read from another one.  I have limited options as its customer env.  My guess might be that the write is setting up credentials scope to one thing and then when we go to do the read the scope is set wrong and we have to explicitly set it for the read to work.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1548566221/reactions,0,0,0,0,0,0,0,0,0,8210
710,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551485119,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1551485119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1551485119,IC_kwDOD7z77c5ceci_,2023-05-17T14:18:35Z,2023-05-17T14:18:35Z,COLLABORATOR,parquet was handled under PR 8296. Also filed https://github.com/NVIDIA/spark-rapids/issues/8242 to find a good way to integration test against Unity,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551485119/reactions,0,0,0,0,0,0,0,0,0,8210
711,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553620804,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1553620804,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1553620804,IC_kwDOD7z77c5cml9E,2023-05-18T20:43:34Z,2023-05-18T20:43:34Z,COLLABORATOR,Also need to make sure to test Unity with Alluxio,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553620804/reactions,0,0,0,0,0,0,0,0,0,8210
712,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675244983,https://github.com/NVIDIA/spark-rapids/issues/8210#issuecomment-1675244983,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8210,1675244983,IC_kwDOD7z77c5j2jW3,2023-08-11T19:09:34Z,2023-08-11T19:09:34Z,COLLABORATOR,I believe the thing left here was just to try to find a reproduce case to add specific test for this fix.,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1675244983/reactions,0,0,0,0,0,0,0,0,0,8210
713,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551905699,https://github.com/NVIDIA/spark-rapids/issues/8258#issuecomment-1551905699,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8258,1551905699,IC_kwDOD7z77c5cgDOj,2023-05-17T18:59:17Z,2023-10-31T03:26:02Z,COLLABORATOR,"Looking at Spark 3.3.0 I see the following Exec that we need to look through and deal with retry for. I will go through this list update it with either follow on issues to add full retry support, or an indication that it is done. 

   - [x] GpuTopN https://github.com/NVIDIA/spark-rapids/issues/8310 and https://github.com/NVIDIA/spark-rapids/issues/8352 
   - [x] GpuLocalLimitExec https://github.com/NVIDIA/spark-rapids/issues/8315
   - [x] GpuGlobalLimitExec https://github.com/NVIDIA/spark-rapids/issues/8315
   - [x] GpuRangeExec https://github.com/NVIDIA/spark-rapids/issues/8314
   - [x] GpuFilterExec https://github.com/NVIDIA/spark-rapids/issues/7865
   - [ ] GpuProjectExec https://github.com/NVIDIA/spark-rapids/issues/7865 and https://github.com/NVIDIA/spark-rapids/issues/7866
   - [x] GpuCachedDoublePassWindowExec https://github.com/NVIDIA/spark-rapids/issues/8217
   - [x] GpuColumnarToRowExec https://github.com/NVIDIA/spark-rapids/issues/8348
   - [x] GpuRowToColumnarExec https://github.com/NVIDIA/spark-rapids/issues/8349 and https://github.com/NVIDIA/spark-rapids/issues/8350
   - [x] GpuSortExec https://github.com/NVIDIA/spark-rapids/issues/8352 and https://github.com/NVIDIA/spark-rapids/issues/8351 (The second one is more important because the other one is configured off, except as a part of GpuTopN)
   - [x] GpuHashAggregateExec So many of them for performance and better algorithms, but the ones we really care about is https://github.com/NVIDIA/spark-rapids/issues/8382
   - [x] GpuExpandExec https://github.com/NVIDIA/spark-rapids/issues/8461
   - [x] GpuShuffleCoalesceExec https://github.com/NVIDIA/spark-rapids/issues/8501
   - [x] GpuShuffledHashJoinExec https://github.com/NVIDIA/spark-rapids/issues/8347
   - [x] GpuBroadcastHashJoinExec https://github.com/NVIDIA/spark-rapids/issues/8347
   - [x] GpuRunningWindowExec https://github.com/NVIDIA/spark-rapids/issues/8343 
   - [x] GpuBroadcastNestedLoopJoinExec https://github.com/NVIDIA/spark-rapids/issues/8344 and https://github.com/NVIDIA/spark-rapids/issues/8345
   - [x] GpuCartesianProductExec https://github.com/NVIDIA/spark-rapids/issues/8344 and https://github.com/NVIDIA/spark-rapids/issues/8345
   - [x] GpuProjectAstExec (This is really only used for testing, but...) https://github.com/NVIDIA/spark-rapids/issues/8311
   - [ ] GpuSampleExec https://github.com/NVIDIA/spark-rapids/issues/8312
   - [ ] GpuFastSampleExec https://github.com/NVIDIA/spark-rapids/issues/8313
   - [x] HostColumnarToGpu https://github.com/NVIDIA/spark-rapids/issues/8326
   - [ ] GpuHiveTableScanExec https://github.com/NVIDIA/spark-rapids/issues/8456
   - [ ] GpuInMemoryTableScanExec https://github.com/NVIDIA/spark-rapids/issues/8459
   - [ ] GpuGenerateExec https://github.com/NVIDIA/spark-rapids/issues/8506
   - [ ] GpuRapidsProcessDeltaMergeJoinExec https://github.com/NVIDIA/spark-rapids/issues/8505
   - [ ] GpuDeltaInvariantCheckerExec https://github.com/NVIDIA/spark-rapids/issues/8504
   - [x] GpuShuffleExchangeExec https://github.com/NVIDIA/spark-rapids/issues/8502
   - [ ] GpuDataWritingCommandExec
   - [ ] GpuPythonMapInArrowExec
   - [ ] GpuArrowEvalPythonExec
   - [ ] GpuAggregateInPandasExec
   - [ ] GpuFlatMapGroupsInPandasExec
   - [ ] GpuFlatMapCoGroupsInPandasExec
   - [ ] GpuMapInPandasExec
   - [ ] GpuWindowInPandasExec
   - [x] GpuRapidsDeltaWriteExec This exec is a node we had to add in order to get access to the child plan when AQE is enabled. IT doesn't do anything interesting, so we shouldn't need a retry here.
   - [x] GpuFileSourceScanExec
   - [x] GpuBatchScanExec
   - [x] GpuExecutedCommandExec
   - [x] GpuCustomShuffleReaderExec
   - [x] GpuRunnableCommandExec
   - [x] GpuBroadcastToRowExec Just copies data to the CPU.
   - [X] GpuWindowExec Instead of updating GpuWidnowExec to support SplitAndRetry generally I think it would be better for us to support more window optimizations so we don't need the entire window in a single batch and add retry/split and retry to them.
   - [X] GpuCoalesceBatches DONE by https://github.com/NVIDIA/spark-rapids/pull/7852
   - [x] GpuSubqueryBroadcastExec CPU only.
   - [x] GpuBroadcastExchangeExec does not allocate GPU memory. Only copies data to the CPU. When the data goes back to the GPU it is in a Broadcast and the exec reading that needs to handle that case.
   - [x] GpuBringBackToHost does not allocate GPU memory. Only copies data to the CPU.
   - [x] GpuCoalesceExec Coalesce is just an RDD level operation. It does not process any data.
   - [x] GpuUnionExec Union is just a logical operation. It only updated some metrics and does not actually process any data.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551905699/reactions,0,0,0,0,0,0,0,0,0,8258
714,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724903090,https://github.com/NVIDIA/spark-rapids/issues/8258#issuecomment-1724903090,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8258,1724903090,IC_kwDOD7z77c5mz-6y,2023-09-19T06:29:50Z,2023-09-19T06:29:50Z,COLLABORATOR,"Hi, I saw most of the follow on issues don't have the `roadmap` or `prioritiy` specified. 

I will work on these issues and it would be good to know their `roadmap` and `prioritiy`. Or just following the order of the list here is good enough ?",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724903090/reactions,0,0,0,0,0,0,0,0,0,8258
715,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725902445,https://github.com/NVIDIA/spark-rapids/issues/8258#issuecomment-1725902445,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8258,1725902445,IC_kwDOD7z77c5m3y5t,2023-09-19T15:33:54Z,2023-09-19T15:33:54Z,COLLABORATOR,@firestarman I think the order here is fine. Be aware that the non-determinisitc expressions for Project and Filter might be a little more difficult and so it is okay if you want to try and skip it for now.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1725902445/reactions,1,0,0,0,0,0,0,0,1,8258
716,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551950526,https://github.com/NVIDIA/spark-rapids/issues/8268#issuecomment-1551950526,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8268,1551950526,IC_kwDOD7z77c5cgOK-,2023-05-17T19:39:16Z,2023-05-17T19:39:16Z,COLLABORATOR,This error seems to be easier to reproduce when running the full benchmark,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1551950526/reactions,0,0,0,0,0,0,0,0,0,8268
717,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1552248461,https://github.com/NVIDIA/spark-rapids/issues/8268#issuecomment-1552248461,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8268,1552248461,IC_kwDOD7z77c5chW6N,2023-05-18T00:29:47Z,2023-05-18T00:29:47Z,COLLABORATOR,"This is a regression from the fix made for https://github.com/NVIDIA/spark-rapids/issues/6978, so something in EMR 6.10 might have been updated to perform things differently from EMR 6.8",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1552248461/reactions,0,0,0,0,0,0,0,0,0,8268
718,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1672832703,https://github.com/NVIDIA/spark-rapids/issues/8268#issuecomment-1672832703,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8268,1672832703,IC_kwDOD7z77c5jtWa_,2023-08-10T08:59:12Z,2023-08-10T08:59:12Z,NONE,"Updates from EMR Spark team:

EMR Spark team checked if there is any difference in SubqueryBroadcastExec and there is no change in emr-6.10 when compared to emr-6.8.
SubqueryBroadcastExec is expecting HashedRelation with child.executeBroadcast. (but got Lscala.runtime.Nothing$)

Looks like something similar to earlier issue fixed in RAPIDS - https://github.com/NVIDIA/spark-rapids/issues/6978
Like earlier can RAPIDS team get more details from Spark History server - Like Plan? did anything change in RAPIDS where child.executeBroadcast will give diff results other than HashedRelation?",,kevnzhao,120480682,U_kgDOBy5jqg,https://avatars.githubusercontent.com/u/120480682?v=4,,https://api.github.com/users/kevnzhao,https://github.com/kevnzhao,https://api.github.com/users/kevnzhao/followers,https://api.github.com/users/kevnzhao/following{/other_user},https://api.github.com/users/kevnzhao/gists{/gist_id},https://api.github.com/users/kevnzhao/starred{/owner}{/repo},https://api.github.com/users/kevnzhao/subscriptions,https://api.github.com/users/kevnzhao/orgs,https://api.github.com/users/kevnzhao/repos,https://api.github.com/users/kevnzhao/events{/privacy},https://api.github.com/users/kevnzhao/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1672832703/reactions,0,0,0,0,0,0,0,0,0,8268
719,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1686832073,https://github.com/NVIDIA/spark-rapids/issues/8268#issuecomment-1686832073,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8268,1686832073,IC_kwDOD7z77c5kiwPJ,2023-08-21T18:33:04Z,2023-08-21T18:33:04Z,MEMBER,This is also failing on emr-6.12.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1686832073/reactions,0,0,0,0,0,0,0,0,0,8268
720,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1546007740,https://github.com/NVIDIA/spark-rapids/issues/8284#issuecomment-1546007740,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8284,1546007740,IC_kwDOD7z77c5cJjS8,2023-05-12T16:36:11Z,2023-05-12T16:36:11Z,COLLABORATOR,"we already do this, we have integration tests that run on aws nightly and on azure on certain days.  ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1546007740/reactions,0,0,0,0,0,0,0,0,0,8284
721,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550351379,https://github.com/NVIDIA/spark-rapids/issues/8284#issuecomment-1550351379,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8284,1550351379,IC_kwDOD7z77c5caHwT,2023-05-16T20:56:48Z,2023-05-16T20:56:48Z,COLLABORATOR,"so looking some more, we do have a few notebooks that run real queries on a distributed cluster, although one of them only has 1 worker.  I thought we were running integration tests on the actual Databricks cluster but it looks like I was wrong. ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550351379/reactions,0,0,0,0,0,0,0,0,0,8284
722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550228448,https://github.com/NVIDIA/spark-rapids/issues/8289#issuecomment-1550228448,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8289,1550228448,IC_kwDOD7z77c5cZpvg,2023-05-16T19:20:08Z,2023-05-16T19:20:08Z,COLLABORATOR,https://github.com/gerashegalov/rapids-shell/blob/master/src/jupyter/RegexpRawStringsDemo.ipynb,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1550228448/reactions,0,0,0,0,0,0,0,0,0,8289
723,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1557361359,https://github.com/NVIDIA/spark-rapids/issues/8292#issuecomment-1557361359,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8292,1557361359,IC_kwDOD7z77c5c03LP,2023-05-22T14:51:33Z,2023-05-22T14:51:33Z,COLLABORATOR,"Actually I have some new information, which might help us a lot in all of the cases, but especially the case when we had to spill data to disk and there were too many partitions to keep the file handles open.

All of the compression codecs that Spark supports can be concatenated after the fact.

https://github.com/apache/spark/blob/46acedb3842484cb4eadb02f1c1e69e71c334748/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala#L62-L71

Also the default checksum used by Spark `adler32`  or `NONE` depending on the version of Spark we are on have a way to calculate the checksum in a distributed way. (The checksum is used as a part of the message handed back to the driver with the shuffle data is committed and written out to the metadata file as well)

For adler32 I need to do a bit more research but CERN did it so we should be able to do it too. http://cds.cern.ch/record/2673802/files/ATL-DAQ-PROC-2019-002.pdf

The only problem shows up if encryption is enabled or if the checksum is CRC32.  So we will need to support the algorithm I described originally for when encryption is enabled but we might be able to have a very fast common case for shuffle.  If creating a compression output stream is cheap enough we should be able to compress each batch as we see it come in, in parallel in a thread pool along with a checksum for the data. We can decide if that should stay in memory or go out to disk directly, but I think keeping it in memory might be the best. Then if we need to spill we can write it out to disk and keep a pointer to where it is along with the checksum.

If the checksum is CRC32 we can still do the compression in parallel, but we would have to calculate the checksum serially on the resulting data.

When we need to write out the final data we can move the compressed data to the proper location in the final file and start combining checksums together, if needed.

The details of the output stream used are.

```
SerializationStream( // Converts the objects into bytes
  CompressionOutputStream( // Optional compresses the data
    ErrorHandlingOutputStream(CryptoOutputStream( // Optional encrypts the data
      ManualCloseOutputStream( // Does a flush instead of a close and the close is separate
        BufferedOutputStream(
          ChecksumOutputStream( // Optional if empty none is set
            TimeTrackinOutputStream( // used for metrics
              FileOutputStream
            )
          )
        )
      )
    ))
  )
)
```

A few things to keep in mind.

First if `spark.file.transferTo` is enabled, and the there is no encryption and the compression codec can be concatenated, then spark will use NIO (file channels) to transfer the data to the final location.  Apparently there are some bugs in old versions of Linux that make this not work. I don't think we support any version of linux with this bug in it (2.6.32 according to the error message), so we might be able to always to the faster transfer.


Second `spark.shuffle.sync` is off by default, but if it is enabled then we need to make sure that the data hits the disk before we go on with shuffle (or we need to fall back to the old shuffle code)

The reason I am bringing this up is that in order to make this work we are likely going to have to copy over a lot more code from Spark at a lower level for shuffle.  We also are going to have to be much more careful about auditing any shuffle changes while we are at it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1557361359/reactions,0,0,0,0,0,0,0,0,0,8292
724,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553760496,https://github.com/NVIDIA/spark-rapids/issues/8324#issuecomment-1553760496,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8324,1553760496,IC_kwDOD7z77c5cnIDw,2023-05-18T23:00:59Z,2023-05-26T17:05:15Z,COLLABORATOR,"This is interesting. Here is the ""failing"" read plan:
```
GpuColumnarToRow false
+- GpuProject [foo#24, gpuconcat("", foo#24, "") AS concat("", foo, "")#21, length(foo#24) AS length(foo)#22]
   +- GpuRowToColumnar targetsize(1073741824)
      +- *(1) Project [staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, foo#20, 3, true, false, true) AS foo#24]
         +- GpuColumnarToRow false
            +- GpuFileGpuScan orc spark_catalog.default.foobar[foo#20] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[file:/tmp/foobar], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<foo:string>
```
The read isn't strictly failing on GPU. The GPU read presents right-trimmed strings (like in Spark 3.3). The CPU then adds the spaces back, to pad back to expected width, via `CharVarcharCodegenUtils.readSidePadding()`

The worst of this is that it falls off the GPU (`column->row->column`), and then does string padding on CPU.

We don't produce bad reads. But we could choose to go much faster, if we just presented what the CUDF reader reads. But then we would have to intercept code-gen. This might be a bit of work. ",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553760496/reactions,0,0,0,0,0,0,0,0,0,8324
725,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553761096,https://github.com/NVIDIA/spark-rapids/issues/8324#issuecomment-1553761096,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8324,1553761096,IC_kwDOD7z77c5cnINI,2023-05-18T23:01:54Z,2023-05-18T23:01:54Z,COLLABORATOR,"I've set this to low priority. There is no data corruption, bad read, etc.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1553761096/reactions,0,0,0,0,0,0,0,0,0,8324
726,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670446616,https://github.com/NVIDIA/spark-rapids/issues/8397#issuecomment-1670446616,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8397,1670446616,IC_kwDOD7z77c5jkP4Y,2023-08-08T23:31:49Z,2023-08-08T23:31:49Z,COLLABORATOR,@jlowe can we close this issue now that we have https://github.com/NVIDIA/spark-rapids/blob/618492d6773b36ce96f41361bda0bbd18d5cd0a6/integration_tests/src/main/python/cache_test.py#L357,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670446616/reactions,0,0,0,0,0,0,0,0,0,8397
727,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670488014,https://github.com/NVIDIA/spark-rapids/issues/8397#issuecomment-1670488014,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8397,1670488014,IC_kwDOD7z77c5jkZ_O,2023-08-09T00:36:24Z,2023-08-09T00:36:24Z,MEMBER,"Yes, assuming that is what reproduced the original divide by zero error.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670488014/reactions,0,0,0,0,0,0,0,0,0,8397
728,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1610263590,https://github.com/NVIDIA/spark-rapids/issues/8398#issuecomment-1610263590,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8398,1610263590,IC_kwDOD7z77c5f-qwm,2023-06-27T21:46:47Z,2023-06-27T21:46:47Z,COLLABORATOR,"#8618 covers a lot of this, but not everything. One key idea that we still need to look at is splitting the aggregation by column instead of row. If the input data size gets too large then right now we split it up into smaller batches, but it probably would be better to split it up by columns instead.  If that looks good, then we need to also need some guarantees from CUDF that the output would be ordered the same, and we might want a way to cache the offsets of there the key transitions happen so we don't have to redo that computation.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1610263590/reactions,0,0,0,0,0,0,0,0,0,8398
729,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564380305,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1564380305,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1564380305,IC_kwDOD7z77c5dPoyR,2023-05-26T13:16:02Z,2023-05-26T13:16:02Z,COLLABORATOR,This test was recently added here: https://github.com/NVIDIA/spark-rapids/pull/8371,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564380305/reactions,0,0,0,0,0,0,0,0,0,8403
730,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564429928,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1564429928,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1564429928,IC_kwDOD7z77c5dP05o,2023-05-26T13:53:30Z,2023-05-26T13:53:30Z,COLLABORATOR,This repros against a standalone cluster without the multi-threaded cluster.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564429928/reactions,0,0,0,0,0,0,0,0,0,8403
731,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564534488,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1564534488,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1564534488,IC_kwDOD7z77c5dQObY,2023-05-26T15:06:51Z,2023-05-26T15:06:51Z,COLLABORATOR,"Trying to come up with a smaller repro, but I ended up with this: https://github.com/NVIDIA/spark-rapids/issues/8411",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564534488/reactions,0,0,0,0,0,0,0,0,0,8403
732,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564567048,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1564567048,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1564567048,IC_kwDOD7z77c5dQWYI,2023-05-26T15:32:03Z,2023-05-26T15:32:03Z,COLLABORATOR,"Related (but not the whole) repro:

```
import org.apache.spark.sql.types._
val schema = StructType(Array(StructField(""k0"", LongType, false), StructField(""k1"", LongType, false), StructField(""k2"", LongType, false), StructField(""k3"", LongType, false), StructField(""v0"", LongType, true), StructField(""v1"", LongType, true), StructField(""v2"", LongType, true), StructField(""v3"", LongType, true)))
val d = spark.read.schema(schema).json(""file:///tmp/json_test_repro"")
d.show()
```

Where json_test_repro is a folder with a json file with contents:

```
{""v1"":5139590093827188858,""v2"":-1404180295742992878}
```

And partitioned directory structure, as such:

```
./k0=1
./k0=1/k1=1
./k0=1/k1=1/k2=2
./k0=1/k1=1/k2=2/k3=3
./k0=1/k1=1/k2=2/k3=3/part-00014-48dd252f-0dec-4969-ac53-79d4cb8a8331.c000.json
```

Note the schema passed to cuDF has columns v0, v3 that don't exist in the json itself. The issue with this we get back a table from cuDF with 2 columns, instead of 4 columns (after fixing https://github.com/NVIDIA/spark-rapids/issues/8411):

```
java.lang.ArrayIndexOutOfBoundsException: 2
        at ai.rapids.cudf.Table.getColumn(Table.java:124)
        at org.apache.spark.sql.catalyst.json.rapids.JsonPartitionReader.$anonfun$readToTable$4(GpuJsonScan.scala:326)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
        at org.apache.spark.sql.catalyst.json.rapids.JsonPartitionReader.$anonfun$readToTable$3(GpuJsonScan.scala:319)
        at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
```

Since the files can have any number of columns (if all rows are null, Spark can omit the column), then we need to make sure cuDF follows suit.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564567048/reactions,0,0,0,0,0,0,0,0,0,8403
733,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564581113,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1564581113,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1564581113,IC_kwDOD7z77c5dQZz5,2023-05-26T15:39:35Z,2023-05-26T15:39:35Z,COLLABORATOR,"We need to file a cuDF bug for this. 

My guess is that the integration test failure is an odd combination of contents of a source table. 

If the file in question had the following contents:

```
{""v0"":0, ""v1"":1, ""v2"":2, ""v3"":3}
{""v1"":5139590093827188858,""v2"":-1404180295742992878}
```

We get back 4 columns and all seems well:

```
+----+-------------------+--------------------+----+---+---+---+---+            
|  v0|                 v1|                  v2|  v3| k0| k1| k2| k3|
+----+-------------------+--------------------+----+---+---+---+---+
|   0|                  1|                   2|   3|  1|  1|  2|  3|
|null|5139590093827188858|-1404180295742992878|null|  1|  1|  2|  3|
+----+-------------------+--------------------+----+---+---+---+---+
```

The CPU consistently returns the above table with all columns and nulls where appropriate.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1564581113/reactions,0,0,0,0,0,0,0,0,0,8403
734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569010110,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1569010110,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1569010110,IC_kwDOD7z77c5dhTG-,2023-05-30T19:59:28Z,2023-05-30T19:59:28Z,COLLABORATOR,"Yes, this is a bug in our CUDF JNI layer.

https://github.com/rapidsai/cudf/blob/5e12c25ca40c41f21fc80d4cd36713c514fabaa4/java/src/main/native/src/TableJni.cpp#L1504-L1521

If we cannot find a column that matches the name, we return the original data from unchanged.  We really should be moving columns over to a new table and generating columns of nulls if we cannot find the column we want.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569010110/reactions,0,0,0,0,0,0,0,0,0,8403
735,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569037135,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1569037135,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1569037135,IC_kwDOD7z77c5dhZtP,2023-05-30T20:18:45Z,2023-05-30T20:18:45Z,COLLABORATOR,"From a performance/efficiency standpoint I think we want to modify where we reorder the columns.  I think we want to return a TableWithMeta, and let the java code handle the column names. This is so we can avoid copying the table multiple times and possibly making multiple copies of the same columns.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569037135/reactions,0,0,0,0,0,0,0,0,0,8403
736,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569089040,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1569089040,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1569089040,IC_kwDOD7z77c5dhmYQ,2023-05-30T20:55:04Z,2023-05-30T20:55:04Z,COLLABORATOR,"Crap. I don't know if we can fix it without pushing some of this back to the plugin or have CUDF actually return nulls for columns that were not in the file.

I'll file an issue and see what CUDF has to say about it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569089040/reactions,0,0,0,0,0,0,0,0,0,8403
737,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569148219,https://github.com/NVIDIA/spark-rapids/issues/8403#issuecomment-1569148219,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8403,1569148219,IC_kwDOD7z77c5dh007,2023-05-30T21:46:30Z,2023-05-30T21:46:30Z,COLLABORATOR,"I filed https://github.com/rapidsai/cudf/issues/13473 for the long term fix. In the short term I am going to do my best to make it work without it.  We will not be able to make the corner case of a file with no columns, only rows, work.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1569148219/reactions,0,0,0,0,0,0,0,0,0,8403
738,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1591757335,https://github.com/NVIDIA/spark-rapids/issues/8415#issuecomment-1591757335,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8415,1591757335,IC_kwDOD7z77c5e4EoX,2023-06-14T18:08:15Z,2023-06-14T20:47:16Z,CONTRIBUTOR,"Please ignore. This was unrelated to this issue and is now resolved.

~One challenge in implementing this is that we replace `JoinedRowProcessor` with a `RapidsProcessDeltaMergeJoin` logical operator that involves a non-deterministic expression (`monotonically_increasing_id`). Spark has an analyzer rule that only allows non-deterministic expressions in a hard-coded set of operators, resulting in the following error:~

```
23/06/14 16:55:53 ERROR GpuMergeIntoCommand: Fatal error in MERGE with materialized source in attempt 1.
org.apache.spark.sql.AnalysisException: nondeterministic expressions are only allowed in Project, Filter, Aggregate or Window, found:
a,b,c,_target_row_id_,_row_dropped_,_incr_row_count_,_change_type,(_source_row_present_ IS NULL),(_target_row_present_ IS NULL),true,a,b,c,_target_row_id_,true,(UDF() AND UDF()),CAST(NULL AS STRING),a,b,c,_target_row_id_,false,true,'delete',a,b,c,_target_row_id_,false,UDF(),CAST(NULL AS STRING),a,b,c,_target_row_id_,true,true,CAST(NULL AS STRING)
in operator RapidsProcessDeltaMergeJoin [a#304727, b#304728, c#304729, _target_row_id_#304730L, _row_dropped_#304731, _incr_row_count_#304732, _change_type#304733], isnull(_source_row_present_#304699), isnull(_target_row_present_#304702), [true], [[[a#304425, b#304426, c#304427, _target_row_id_#304707L, true, (UDF() AND UDF()), null], [a#304425, b#304426, c#304427, _target_row_id_#304707L, false, true, delete]]], [a#304425, b#304426, c#304427, _target_row_id_#304707L, false, UDF(), null], [a#304425, b#304426, c#304427, _target_row_id_#304707L, true, true, null].; line 1 pos 0;
```

Here is the Spark code from `CheckAnalysis`:

```scala
          case o if o.expressions.exists(!_.deterministic) &&
            !o.isInstanceOf[Project] && !o.isInstanceOf[Filter] &&
            !o.isInstanceOf[Aggregate] && !o.isInstanceOf[Window] &&
            // Lateral join is checked in checkSubqueryExpression.
            !o.isInstanceOf[LateralJoin] =>
            // The rule above is used to check Aggregate operator.
            o.failAnalysis(
              errorClass = ""_LEGACY_ERROR_TEMP_2439"",
              messageParameters = Map(
                ""sqlExprs"" -> o.expressions.map(_.sql).mkString("",""),
                ""operator"" -> operator.simpleString(SQLConf.get.maxToStringFields)))
```

For now, we avoid replacing the operator if `notMatchedBySourceClauses` is non-empty.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1591757335/reactions,0,0,0,0,0,0,0,0,0,8415
739,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1615103914,https://github.com/NVIDIA/spark-rapids/issues/8418#issuecomment-1615103914,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8418,1615103914,IC_kwDOD7z77c5gRIeq,2023-06-30T19:23:31Z,2023-06-30T21:16:17Z,COLLABORATOR,"I see it happening on my pre-merge for spark 3.4.0 but it's happening for ""compare CPU and GPU: replace digits"". 
",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1615103914/reactions,0,0,0,0,0,0,0,0,0,8418
740,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1645114774,https://github.com/NVIDIA/spark-rapids/issues/8418#issuecomment-1645114774,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8418,1645114774,IC_kwDOD7z77c5iDnWW,2023-07-21T07:29:57Z,2023-07-21T07:29:57Z,COLLABORATOR,"seems this appears more frequent than before
```
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] # A fatal error has been detected by the Java Runtime Environment:
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] #  SIGSEGV (0xb) at pc=0x00007fd3f782a606, pid=1093836, tid=0x00007fd402310700
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] # JRE version: OpenJDK Runtime Environment (8.0_362-b09) (build 1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09)
[2023-07-21T06:56:23.473Z] # Java VM: OpenJDK 64-Bit Server VM (25.362-b09 mixed mode linux-amd64 compressed oops)
[2023-07-21T06:56:23.473Z] # Problematic frame:
[2023-07-21T06:56:23.473Z] # J 70664 C2 ai.rapids.cudf.ColumnView.copyToHost()Lai/rapids/cudf/HostColumnVector; (1738 bytes) @ 0x00007fd3f782a606 [0x00007fd3f7829940+0xcc6]
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] # Core dump written. Default location: /home/jenkins/agent/workspace/jenkins-rapids_premerge-github-7579-ci-2/tests/core or core.1093836
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] # If you would like to submit a bug report, please visit:
[2023-07-21T06:56:23.473Z] #   http://bugreport.java.com/bugreport/crash.jsp
[2023-07-21T06:56:23.473Z] #
[2023-07-21T06:56:23.473Z] 
[2023-07-21T06:56:23.473Z] ---------------  T H R E A D  ---------------
[2023-07-21T06:56:23.473Z] 
[2023-07-21T06:56:23.473Z] Current thread (0x00007fd3fc015800):  JavaThread ""ScalaTest-main-running-RegularExpressionTranspilerSuite"" [_thread_in_Java, id=1093839, stack(0x00007fd401f11000,0x00007fd402311000)]
[2023-07-21T06:56:23.473Z] 
[2023-07-21T06:56:23.473Z] siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000018
[2023-07-21T06:56:23.473Z] 
[2023-07-21T06:56:23.473Z] Registers:
[2023-07-21T06:56:23.473Z] RAX=0x0000000000000000, RBX=0x000000000000006d, RCX=0x000000000000000b, RDX=0x000000077ed7d048
[2023-07-21T06:56:23.473Z] RSP=0x00007fd40230c700, RBP=0x00000006c0636c20, RSI=0x000000077ed7d048, RDI=0x00000000d80c6e5c
[2023-07-21T06:56:23.473Z] R8 =0x00000000d831cc59, R9 =0x00000000d831cc6d, R10=0x0000000000000000, R11=0x0000000000000000
[2023-07-21T06:56:23.473Z] R12=0x0000000000000000, R13=0x0000000000000001, R14=0x0000000000000008, R15=0x00007fd3fc015800
[2023-07-21T06:56:23.474Z] RIP=0x00007fd3f782a606, EFLAGS=0x0000000000010206, CSGSFS=0x002b000000000033, ERR=0x0000000000000004
[2023-07-21T06:56:23.474Z]   TRAPNO=0x000000000000000e
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] Top of Stack: (sp=0x00007fd40230c700)
[2023-07-21T06:56:23.474Z] 0x00007fd40230c700:   00000006d80c6d80 00000006c0636ac8
[2023-07-21T06:56:23.474Z] 0x00007fd40230c710:   00000006c18e7668 0000000000000000
[2023-07-21T06:56:23.474Z] 0x00007fd40230c720:   00000007c0011600 00000007c041d100
[2023-07-21T06:56:23.474Z] 0x00007fd40230c730:   00000006c1c15578 0000061600000616
[2023-07-21T06:56:23.474Z] 0x00007fd40230c740:   000000077ed7cfe8 000000077ed7cf30
[2023-07-21T06:56:23.474Z] 0x00007fd40230c750:   0000000000000000 d831cecdd8382aaf
[2023-07-21T06:56:23.474Z] 0x00007fd40230c760:   0000000000000000 000000077ed7bae0
[2023-07-21T06:56:23.474Z] 0x00007fd40230c770:   00000000000003e8 00000006c18e7668
[2023-07-21T06:56:23.474Z] 0x00007fd40230c780:   d83ad047d83e87e4 00000006c1f43f20
[2023-07-21T06:56:23.474Z] 0x00007fd40230c790:   00000017e8001000 00000003d83676a6
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7a0:   000000060000001e 000000077ed7bae0
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7b0:   00007fd40230c7f0 00007fd40230c7e0
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7c0:   00007fd2554d0938 00007fd25c5395d8
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7d0:   000000077ed7cf08 00007fd3f7829960
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7e0:   00000006c1c15578 0000000000000202
[2023-07-21T06:56:23.474Z] 0x00007fd40230c7f0:   000000077ed7bae0 00007fd3f781618c
[2023-07-21T06:56:23.474Z] 0x00007fd40230c800:   00007fd40230c870 0000000000000000
[2023-07-21T06:56:23.474Z] 0x00007fd40230c810:   00000000d83e87e4 00007fd3f25a0530
[2023-07-21T06:56:23.474Z] 0x00007fd40230c820:   000000077ed7bae0 000002ecd83e87e4
[2023-07-21T06:56:23.474Z] 0x00007fd40230c830:   00000047d81963c7 00000006c1f43f20
[2023-07-21T06:56:23.474Z] 0x00007fd40230c840:   00000006c0cb1e38 00000006c3fe52c8
[2023-07-21T06:56:23.474Z] 0x00007fd40230c850:   000000077ed7bb30 0000000000000000
[2023-07-21T06:56:23.474Z] 0x00007fd40230c860:   00000006c012cda8 00000000efdaf5c9
[2023-07-21T06:56:23.474Z] 0x00007fd40230c870:   000000077ed77178 00007fd3f7813ebc
[2023-07-21T06:56:23.474Z] 0x00007fd40230c880:   00007fd40230c8d0 00007fd3f07fd228
[2023-07-21T06:56:23.474Z] 0x00007fd40230c890:   000000077ed7ba90 eaad5e462c17a100
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8a0:   00007fd40230c8f0 00007fd265ed8878
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8b0:   00007fd3fc015800 00007fd40230fa90
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8c0:   000000077ed77178 00007fd3f77cf70c
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8d0:   00007fd3fc015800 0000000000000000
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8e0:   000000077ed77178 00007fd3f780142c
[2023-07-21T06:56:23.474Z] 0x00007fd40230c8f0:   000000077ed7bae0 000000077ed19c90 
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] Instructions: (pc=0x00007fd3f782a606)
[2023-07-21T06:56:23.474Z] 0x00007fd3f782a5e6:   25 05 00 00 45 33 d2 83 fb 42 0f 86 37 1b 00 00
[2023-07-21T06:56:23.474Z] 0x00007fd3f782a5f6:   4c 8b 54 24 50 4c 89 54 24 18 43 c6 44 c4 52 01
[2023-07-21T06:56:23.474Z] 0x00007fd3f782a606:   4d 8b 4a 18 41 ba 70 c8 09 f8 49 c1 e2 03 4c 89
[2023-07-21T06:56:23.474Z] 0x00007fd3f782a616:   94 24 80 00 00 00 49 ba b0 ae b9 c0 06 00 00 00 
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] Register to memory mapping:
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] RAX=0x0000000000000000 is an unknown value
[2023-07-21T06:56:23.474Z] RBX=0x000000000000006d is an unknown value
[2023-07-21T06:56:23.474Z] RCX=0x000000000000000b is an unknown value
[2023-07-21T06:56:23.474Z] RDX=0x000000077ed7d048 is an oop
[2023-07-21T06:56:23.474Z] org.mockito.internal.util.concurrent.WeakConcurrentMap$LatentKey 
[2023-07-21T06:56:23.474Z]  - klass: 'org/mockito/internal/util/concurrent/WeakConcurrentMap$LatentKey'
[2023-07-21T06:56:23.474Z] RSP=0x00007fd40230c700 is pointing into the stack for thread: 0x00007fd3fc015800
[2023-07-21T06:56:23.474Z] RBP=0x00000006c0636c20 is an oop
[2023-07-21T06:56:23.474Z] org.mockito.internal.creation.bytebuddy.MockMethodAdvice 
[2023-07-21T06:56:23.474Z]  - klass: 'org/mockito/internal/creation/bytebuddy/MockMethodAdvice'
[2023-07-21T06:56:23.474Z] RSI=0x000000077ed7d048 is an oop
[2023-07-21T06:56:23.474Z] org.mockito.internal.util.concurrent.WeakConcurrentMap$LatentKey 
[2023-07-21T06:56:23.474Z]  - klass: 'org/mockito/internal/util/concurrent/WeakConcurrentMap$LatentKey'
[2023-07-21T06:56:23.474Z] RDI=0x00000000d80c6e5c is an unknown value
[2023-07-21T06:56:23.474Z] R8 =0x00000000d831cc59 is an unknown value
[2023-07-21T06:56:23.474Z] R9 =0x00000000d831cc6d is an unknown value
[2023-07-21T06:56:23.474Z] R10=0x0000000000000000 is an unknown value
[2023-07-21T06:56:23.474Z] R11=0x0000000000000000 is an unknown value
[2023-07-21T06:56:23.474Z] R12=0x0000000000000000 is an unknown value
[2023-07-21T06:56:23.474Z] R13=0x0000000000000001 is an unknown value
[2023-07-21T06:56:23.474Z] R14=0x0000000000000008 is an unknown value
[2023-07-21T06:56:23.474Z] R15=0x00007fd3fc015800 is a thread
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] 
[2023-07-21T06:56:23.474Z] Stack: [0x00007fd401f11000,0x00007fd402311000],  sp=0x00007fd40230c700,  free space=4077k
[2023-07-21T06:56:23.474Z] Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
[2023-07-21T06:56:23.474Z] J 70664 C2 ai.rapids.cudf.ColumnView.copyToHost()Lai/rapids/cudf/HostColumnVector; (1738 bytes) @ 0x00007fd3f782a606 [0x00007fd3f7829940+0xcc6]
[2023-07-21T06:56:23.474Z] J 70656 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite$$Lambda$13416.apply(Ljava/lang/Object;)Ljava/lang/Object; (12 bytes) @ 0x00007fd3f781618c [0x00007fd3f7815ea0+0x2ec]
[2023-07-21T06:56:23.474Z] J 17927 C2 com.nvidia.spark.rapids.Arm$.withResource(Ljava/lang/AutoCloseable;Lscala/Function1;)Ljava/lang/Object; (78 bytes) @ 0x00007fd3edfb1a54 [0x00007fd3edfb1780+0x2d4]
[2023-07-21T06:56:23.474Z] J 70648 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.$anonfun$gpuContains$1(Ljava/lang/String;[ZLai/rapids/cudf/ColumnVector;)V (49 bytes) @ 0x00007fd3f781117c [0x00007fd3f7810820+0x95c]
[2023-07-21T06:56:23.474Z] J 70647 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite$$Lambda$13408.apply(Ljava/lang/Object;)Ljava/lang/Object; (16 bytes) @ 0x00007fd3f781210c [0x00007fd3f7811f20+0x1ec]
[2023-07-21T06:56:23.474Z] J 17927 C2 com.nvidia.spark.rapids.Arm$.withResource(Ljava/lang/AutoCloseable;Lscala/Function1;)Ljava/lang/Object; (78 bytes) @ 0x00007fd3edfb1a54 [0x00007fd3edfb1780+0x2d4]
[2023-07-21T06:56:23.474Z] J 70638 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.gpuContains(Ljava/lang/String;Lscala/collection/Seq;)[Z (62 bytes) @ 0x00007fd3f780981c [0x00007fd3f7809060+0x7bc]
[2023-07-21T06:56:23.474Z] J 70654 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.$anonfun$assertCpuGpuMatchesRegexpFind$2(Lcom/nvidia/spark/rapids/RegularExpressionTranspilerSuite;Lscala/collection/Seq;Lscala/Tuple2;)V (275 bytes) @ 0x00007fd3f781a8cc [0x00007fd3f781a240+0x68c]
[2023-07-21T06:56:23.474Z] J 70652 C1 com.nvidia.spark.rapids.RegularExpressionTranspilerSuite$$Lambda$13413.apply(Ljava/lang/Object;)Ljava/lang/Object; (16 bytes) @ 0x00007fd3f781594c [0x00007fd3f7815760+0x1ec]
[2023-07-21T06:56:23.474Z] J 12668 C2 scala.collection.TraversableLike$WithFilter$$Lambda$80.apply(Ljava/lang/Object;)Ljava/lang/Object; (13 bytes) @ 0x00007fd3ee3241ec [0x00007fd3ee323bc0+0x62c]
[2023-07-21T06:56:23.474Z] J 18321 C2 scala.collection.mutable.ArrayBuffer.foreach(Lscala/Function1;)V (6 bytes) @ 0x00007fd3ede9f428 [0x00007fd3ede9f3a0+0x88]
[2023-07-21T06:56:23.474Z] J 34962 C2 scala.collection.TraversableLike$WithFilter.foreach(Lscala/Function1;)V (17 bytes) @ 0x00007fd3f220992c [0x00007fd3f2209860+0xcc]
[2023-07-21T06:56:23.474Z] j  com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.assertCpuGpuMatchesRegexpFind(Lscala/collection/Seq;Lscala/collection/Seq;)V+36
[2023-07-21T06:56:23.475Z] j  com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.doFuzzTest(Lscala/Option;Lcom/nvidia/spark/rapids/RegexMode;)V+357
[2023-07-21T06:56:23.475Z] j  com.nvidia.spark.rapids.RegularExpressionTranspilerSuite.$anonfun$new$93(Lcom/nvidia/spark/rapids/RegularExpressionTranspilerSuite;)V+19
[2023-07-21T06:56:23.475Z] j  com.nvidia.spark.rapids.RegularExpressionTranspilerSuite$$Lambda$1815.apply$mcV$sp()V+4
[2023-07-21T06:56:23.475Z] J 24113 C2 scala.runtime.java8.JFunction0$mcV$sp.apply()Ljava/lang/Object; (10 bytes) @ 0x00007fd3f00eb19c [0x00007fd3f00eb160+0x3c]
[2023-07-21T06:56:23.475Z] J 61816 C1 org.scalatest.OutcomeOf.outcomeOf(Lscala/Function0;)Lorg/scalatest/Outcome; (138 bytes) @ 0x00007fd3f629250c [0x00007fd3f6292400+0x10c]
[2023-07-21T06:56:23.475Z] J 61812 C1 org.scalatest.Transformer.apply()Ljava/lang/Object; (5 bytes) @ 0x00007fd3f408285c [0x00007fd3f4082660+0x1fc]
[2023-07-21T06:56:23.475Z] J 62490 C1 org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply()Lorg/scalatest/Outcome; (19 bytes) @ 0x00007fd3f64abc84 [0x00007fd3f64aba60+0x224]
[2023-07-21T06:56:23.475Z] J 59622 C1 org.scalatest.funsuite.AnyFunSuite.withFixture(Lorg/scalatest/TestSuite$NoArgTest;)Lorg/scalatest/Outcome; (6 bytes) @ 0x00007fd3f140e96c [0x00007fd3f140e7e0+0x18c]
[2023-07-21T06:56:23.475Z] J 62485 C1 org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(Lorg/scalatest/SuperEngine$TestLeaf;Lorg/scalatest/Args;Ljava/lang/String;)Lorg/scalatest/Outcome; (35 bytes) @ 0x00007fd3f64a3ac4 [0x00007fd3f64a3860+0x264]
[2023-07-21T06:56:23.475Z] J 59611 C1 org.scalatest.funsuite.AnyFunSuiteLike$$Lambda$1987.apply(Ljava/lang/Object;)Ljava/lang/Object; (20 bytes) @ 0x00007fd3ef0227dc [0x00007fd3ef022620+0x1bc]
[2023-07-21T06:56:23.475Z] J 59591 C1 org.scalatest.SuperEngine.runTestImpl(Lorg/scalatest/Suite;Ljava/lang/String;Lorg/scalatest/Args;ZLscala/Function1;)Lorg/scalatest/Status; (1462 bytes) @ 0x00007fd3f1680144 [0x00007fd3f167b5c0+0x4b84]
[2023-07-21T06:56:23.475Z] J 58719 C1 org.scalatest.funsuite.AnyFunSuite.runTest(Ljava/lang/String;Lorg/scalatest/Args;)Lorg/scalatest/Status; (7 bytes) @ 0x00007fd3ef445e84 [0x00007fd3ef4459a0+0x4e4]
[2023-07-21T06:56:23.475Z] J 57904 C1 org.scalatest.funsuite.AnyFunSuiteLike$$Lambda$1981.apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (16 bytes) @ 0x00007fd3f100786c [0x00007fd3f1007560+0x30c]
[2023-07-21T06:56:23.475Z] J 57848 C1 org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Lorg/scalatest/SuperEngine;Lorg/scalatest/Args;Lorg/scalatest/Suite;Lorg/scalatest/SuperEngine$Branch;Lscala/collection/mutable/ListBuffer;Lscala/Function2;ZLorg/scalatest/SuperEngine$Node;)Ljava/lang/Object; (580 bytes) @ 0x00007fd3f100b95c [0x00007fd3f10080c0+0x389c]
[2023-07-21T06:56:23.475Z] J 57847 C1 org.scalatest.SuperEngine$$Lambda$1982.apply(Ljava/lang/Object;)Ljava/lang/Object; (36 bytes) @ 0x00007fd3ef1c062c [0x00007fd3ef1c0480+0x1ac]
[2023-07-21T06:56:23.475Z] J 17218 C2 scala.collection.immutable.List.foreach(Lscala/Function1;)V (32 bytes) @ 0x00007fd3eedbeb6c [0x00007fd3eedbeac0+0xac]
[2023-07-21T06:56:23.475Z] j  org.scalatest.SuperEngine.traverseSubNodes$1(Lorg/scalatest/SuperEngine$Branch;Lorg/scalatest/Args;Lorg/scalatest/Suite;Lscala/collection/mutable/ListBuffer;Lscala/Function2;Z)V+22
[2023-07-21T06:56:23.475Z] j  org.scalatest.SuperEngine.runTestsInBranch(Lorg/scalatest/Suite;Lorg/scalatest/SuperEngine$Branch;Lorg/scalatest/Args;ZLscala/Function2;)Lorg/scalatest/Status;+191
[2023-07-21T06:56:23.475Z] j  org.scalatest.SuperEngine.runTestsImpl(Lorg/scalatest/Suite;Lscala/Option;Lorg/scalatest/Args;Lorg/scalatest/Informer;ZLscala/Function2;)Lorg/scalatest/Status;+389
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike.runTests(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+22
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike.runTests$(Lorg/scalatest/funsuite/AnyFunSuiteLike;Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuite.runTests(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+214
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.run$(Lorg/scalatest/Suite;Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(Lorg/scalatest/funsuite/AnyFunSuiteLike;Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike$$Lambda$1975.apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+12
[2023-07-21T06:56:23.475Z] j  org.scalatest.SuperEngine.runImpl(Lorg/scalatest/Suite;Lscala/Option;Lorg/scalatest/Args;Lscala/Function2;)Lorg/scalatest/Status;+354
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike.run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+15
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuiteLike.run$(Lorg/scalatest/funsuite/AnyFunSuiteLike;Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.funsuite.AnyFunSuite.run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.callExecuteOnSuite$1(Lorg/scalatest/Suite;Lorg/scalatest/Args;Lorg/scalatest/Reporter;)Lorg/scalatest/Status;+185
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.$anonfun$runNestedSuites$1(Lorg/scalatest/Args;Lscala/collection/mutable/ListBuffer;Lorg/scalatest/Reporter;Lorg/scalatest/Suite;)Ljava/lang/Object;+16
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite$$Lambda$1973.apply(Ljava/lang/Object;)Ljava/lang/Object;+16
[2023-07-21T06:56:23.475Z] J 6908 C1 scala.collection.IndexedSeqOptimized.foreach(Lscala/Function1;)V (36 bytes) @ 0x00007fd3ed69a5a4 [0x00007fd3ed69a320+0x284]
[2023-07-21T06:56:23.475Z] J 7234 C1 scala.collection.mutable.ArrayOps$ofRef.foreach(Lscala/Function1;)V (6 bytes) @ 0x00007fd3ee127c3c [0x00007fd3ee127b80+0xbc]
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.runNestedSuites(Lorg/scalatest/Args;)Lorg/scalatest/Status;+157
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.runNestedSuites$(Lorg/scalatest/Suite;Lorg/scalatest/Args;)Lorg/scalatest/Status;+2
[2023-07-21T06:56:23.475Z] j  org.scalatest.tools.DiscoverySuite.runNestedSuites(Lorg/scalatest/Args;)Lorg/scalatest/Status;+2
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+170
[2023-07-21T06:56:23.475Z] j  org.scalatest.Suite.run$(Lorg/scalatest/Suite;Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.tools.DiscoverySuite.run(Lscala/Option;Lorg/scalatest/Args;)Lorg/scalatest/Status;+3
[2023-07-21T06:56:23.475Z] j  org.scalatest.tools.SuiteRunner.run()V+188
[2023-07-21T06:56:23.475Z] j  org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Lscala/collection/immutable/Set;Lscala/collection/immutable/Set;Lorg/scalatest/DispatchReporter;Lorg/scalatest/Stopper;Lorg/scalatest/ConfigMap;Lscala/runtime/ObjectRef;Lscala/collection/immutable/Set;Lorg/scalatest/tools/SuiteConfig;)V+162
[2023-07-21T06:56:23.475Z] j  org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Lscala/collection/immutable/Set;Lscala/collection/immutable/Set;Lorg/scalatest/DispatchReporter;Lorg/scalatest/Stopper;Lorg/scalatest/ConfigMap;Lscala/runtime/ObjectRef;Lscala/collection/immutable/Set;Lorg/scalatest/tools/SuiteConfig;)Ljava/lang/Object;+12
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$$$Lambda$1971.apply(Ljava/lang/Object;)Ljava/lang/Object;+32
[2023-07-21T06:56:23.476Z] J 7966 C1 scala.collection.immutable.List.foreach(Lscala/Function1;)V (32 bytes) @ 0x00007fd3ed773404 [0x00007fd3ed773240+0x1c4]
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Lorg/scalatest/DispatchReporter;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lorg/scalatest/Stopper;Lscala/collection/immutable/Set;Lscala/collection/immutable/Set;Lorg/scalatest/ConfigMap;ZLscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Ljava/lang/ClassLoader;Lorg/scalatest/tools/RunDoneListener;ILorg/scalatest/tools/ConcurrentConfig;Lscala/Option;Lscala/collection/immutable/Set;Lorg/scalatest/time/Span;)V+1456
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/Set;Lscala/collection/immutable/Set;Lorg/scalatest/ConfigMap;ZLscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lorg/scalatest/tools/ConcurrentConfig;Lscala/Option;Lscala/collection/immutable/Set;Lorg/scalatest/time/Span;Ljava/lang/ClassLoader;Lorg/scalatest/DispatchReporter;)V+49
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/Set;Lscala/collection/immutable/Set;Lorg/scalatest/ConfigMap;ZLscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lorg/scalatest/tools/ConcurrentConfig;Lscala/Option;Lscala/collection/immutable/Set;Lorg/scalatest/time/Span;Ljava/lang/ClassLoader;Lorg/scalatest/DispatchReporter;)Ljava/lang/Object;+32
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$$$Lambda$61.apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+72
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Lscala/collection/immutable/List;Lorg/scalatest/tools/ReporterConfigurations;Lscala/Option;Lscala/Option;ZJJLscala/Function2;)V+44
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter([Ljava/lang/String;Z)Z+2023
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner$.main([Ljava/lang/String;)V+154
[2023-07-21T06:56:23.476Z] j  org.scalatest.tools.Runner.main([Ljava/lang/String;)V+4
[2023-07-21T06:56:23.476Z] v  ~StubRoutines::call_stub
[2023-07-21T06:56:23.476Z] V  [libjvm.so+0x6a5da5]
[2023-07-21T06:56:23.476Z] V  [libjvm.so+0x72735d]
[2023-07-21T06:56:23.476Z] V  [libjvm.so+0x729f2e]
[2023-07-21T06:56:23.476Z] C  [libjli.so+0x4802]
[2023-07-21T06:56:23.476Z] C  [libjli.so+0x8dc1]
[2023-07-21T06:56:23.476Z] C  [libpthread.so.0+0x8609]  start_thread+0xd9
[2023-07-21T06:56:23.476Z] 
[2023-07-21T06:56:23.476Z] 
```",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1645114774/reactions,0,0,0,0,0,0,0,0,0,8418
741,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1737378479,https://github.com/NVIDIA/spark-rapids/issues/8418#issuecomment-1737378479,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8418,1737378479,IC_kwDOD7z77c5njkqv,2023-09-27T13:15:22Z,2023-09-27T13:15:22Z,COLLABORATOR,"saw this yesterday on our nightly tests:

```
[2023-09-26T12:44:09.931Z] ^[[32m- test simple OOM split and retry^[[0m
[2023-09-26T12:44:10.040Z] #
[2023-09-26T12:44:10.040Z] # A fatal error has been detected by the Java Runtime Environment:
[2023-09-26T12:44:10.040Z] #
[2023-09-26T12:44:10.040Z] #  SIGSEGV (0xb) at pc=0x00007fd3a7160369, pid=1996, tid=0x00007fd3b1190700
[2023-09-26T12:44:10.040Z] #
[2023-09-26T12:44:10.040Z] # JRE version: OpenJDK Runtime Environment (8.0_382-b05) (build 1.8.0_382-8u382-ga-1~20.04.1-b05)
[2023-09-26T12:44:10.040Z] # Java VM: OpenJDK 64-Bit Server VM (25.382-b05 mixed mode linux-amd64 compressed oops)
[2023-09-26T12:44:10.040Z] # Problematic frame:
[2023-09-26T12:44:10.040Z] # J 69312 C2 ai.rapids.cudf.ColumnView.copyToHost(Lai/rapids/cudf/HostMemoryAllocator;)Lai/rapids/cudf/HostColumnVector; (1913 bytes) @ 0x00007fd3a7160369 [0x00007fd3a715f6a0+0xcc9]
[2023-09-26T12:44:10.040Z] #
[2023-09-26T12:44:10.040Z] # Core dump written. Default location: /home/jenkins/agent/workspace/jenkins-rapids_nightly-dev-github-938-w2-938/tests/core or core.1996
[2023-09-26T12:44:10.040Z] #
```",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1737378479/reactions,0,0,0,0,0,0,0,0,0,8418
742,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1630106012,https://github.com/NVIDIA/spark-rapids/issues/8429#issuecomment-1630106012,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8429,1630106012,IC_kwDOD7z77c5hKXGc,2023-07-11T04:41:48Z,2023-07-11T04:41:48Z,COLLABORATOR,"Hi @advancedxy, apologies for the late reply.  In the near term we are not working on Iceberg.  If this is something important to your project, please let us know more or set up a meeting with us by contacting spark-rapids-support@nvidia.com.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1630106012/reactions,0,0,0,0,0,0,0,0,0,8429
743,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1585031504,https://github.com/NVIDIA/spark-rapids/issues/8447#issuecomment-1585031504,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8447,1585031504,IC_kwDOD7z77c5eealQ,2023-06-09T19:19:31Z,2023-06-09T19:19:31Z,COLLABORATOR,"Can we do file caching for test data? I.e., a generator will first try to load from a cached file. If the file doesn't exist, data will be generated and written down to file.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1585031504/reactions,0,0,0,0,0,0,0,0,0,8447
744,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1587380620,https://github.com/NVIDIA/spark-rapids/issues/8447#issuecomment-1587380620,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8447,1587380620,IC_kwDOD7z77c5enYGM,2023-06-12T13:48:33Z,2023-06-12T13:48:33Z,COLLABORATOR,"> Can we do file caching for test data? I.e., a generator will first try to load from a cached file. If the file doesn't exist, data will be generated and written down to file.

That would be another interesting option. I'm not sure how much it would help beyond the in memory caching for CI runs. It would help a lot for local runs where we can keep data cached for longer.  We would want to make sure that we add some kind of a ""version"" number to the generators so that if we change them, then we can invalidate the data saved to disk.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1587380620/reactions,0,0,0,0,0,0,0,0,0,8447
745,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1571506409,https://github.com/NVIDIA/spark-rapids/issues/8466#issuecomment-1571506409,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8466,1571506409,IC_kwDOD7z77c5dq0jp,2023-06-01T07:30:05Z,2023-06-01T07:30:05Z,COLLABORATOR,"Hi @abellina , any insight about this config?",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1571506409/reactions,0,0,0,0,0,0,0,0,0,8466
746,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572121025,https://github.com/NVIDIA/spark-rapids/issues/8466#issuecomment-1572121025,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8466,1572121025,IC_kwDOD7z77c5dtKnB,2023-06-01T14:04:37Z,2023-06-01T14:04:37Z,COLLABORATOR,"Agreed on the config complexity @nvliyuan. Please note that on Yarn and Databricks we only need to set `spark.shuffle.manager`, but ideally I'd like to set 0 configs and see if we can add smarts to the plugin (and Spark) to enable and disable acceleration when needed. We'll discuss how to prioritize this since we've discussed it for a while.

On the bug you found, I reproed. The writer is turned off, but not the reader. That is OK and safe because the MT reader is the regular reader with multiple threads, but definitely not intended. I'll file a separate bug for that and will push shortly.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572121025/reactions,0,0,0,0,0,0,0,0,0,8466
747,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579381964,https://github.com/NVIDIA/spark-rapids/issues/8466#issuecomment-1579381964,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8466,1579381964,IC_kwDOD7z77c5eI3TM,2023-06-06T20:12:22Z,2023-06-06T20:12:22Z,COLLABORATOR,Scope for this issue is to remove `spark.rapids.shuffle.enabled` setting from docs so it is internal-only.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579381964/reactions,0,0,0,0,0,0,0,0,0,8466
748,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572726710,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1572726710,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1572726710,IC_kwDOD7z77c5dvee2,2023-06-01T20:22:33Z,2023-06-01T20:22:33Z,COLLABORATOR,"I have not run this, but I am just guessing. The GPU does not support UserDefinedTypes right now, and VectorUDT is a user defined type, so we are going to fall back to the CPU to serialize and deserialize them.  This might not be ideal because the CPU is really bad at writing parquet in many cases compared to the GPU.  600x better (that is hard to believe so I need to do some testing).",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572726710/reactions,0,0,0,0,0,0,0,0,0,8474
749,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572844170,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1572844170,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1572844170,IC_kwDOD7z77c5dv7KK,2023-06-01T21:59:16Z,2023-06-01T21:59:16Z,COLLABORATOR,"@eordentlich do you have any instructions on how to get an environment setup to do this?  I tried to use conda to setup an environment following the instructions at https://xgboost.readthedocs.io/en/stable/tutorials/spark_estimator.html

```
conda create -y -n xgboost_env -c conda-forge conda-pack python=3.9
conda activate xgboost_env
# use conda when the supported version of xgboost (1.7) is released on conda-forge
pip install xgboost
conda install cudf pyarrow pandas -c rapids -c nvidia -c conda-forge
```

But it didn't work and I had to change the last command to

```
conda install cudf pyarrow pandas -c rapidsai -c nvidia -c conda-forge
```

Then when I tried to import xgboost in the notebook

```
from xgboost.spark import SparkXGBClassifier, SparkXGBClassifierModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
```

I got an error about no `sklearn` so I installed it

```
conda install scikit-learn
```

And now I am getting what appears to be CUDA mismatch of some kind.

```
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py"", line 830, in main
    process()
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py"", line 822, in process
    serializer.dump_stream(out_iter, outfile)
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 345, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 338, in init_stream_yield_batches
    for series in iterator:
  File ""spark_3.4.0/python/lib/pyspark.zip/pyspark/worker.py"", line 519, in func
    for result_batch, result_type in result_iter:
  File ""xgboost_env/lib/python3.9/site-packages/xgboost/spark/core.py"", line 795, in _train_booster
    use_qdm = use_hist and is_cudf_available()
  File ""xgboost_env/lib/python3.9/site-packages/xgboost/compat.py"", line 83, in is_cudf_available
    import cudf
  File ""xgboost_env/lib/python3.9/site-packages/cudf/__init__.py"", line 5, in <module>
    validate_setup()
  File ""xgboost_env/lib/python3.9/site-packages/cudf/utils/gpu_utils.py"", line 20, in validate_setup
    from rmm._cuda.gpu import (
  File ""xgboost_env/lib/python3.9/site-packages/rmm/__init__.py"", line 16, in <module>
    from rmm import mr
  File ""xgboost_env/lib/python3.9/site-packages/rmm/mr.py"", line 14, in <module>
    from rmm._lib.memory_resource import (
  File ""xgboost_env/lib/python3.9/site-packages/rmm/_lib/__init__.py"", line 15, in <module>
    from .device_buffer import DeviceBuffer
  File ""device_buffer.pyx"", line 1, in init rmm._lib.device_buffer
TypeError: C function cuda.ccudart.cudaStreamSynchronize has wrong signature (expected __pyx_t_4cuda_7ccudart_cudaError_t (__pyx_t_4cuda_7ccudart_cudaStream_t), got cudaError_t (cudaStream_t))
```

",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1572844170/reactions,0,0,0,0,0,0,0,0,0,8474
750,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1573254442,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1573254442,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1573254442,IC_kwDOD7z77c5dxfUq,2023-06-02T07:03:49Z,2023-06-02T07:03:49Z,CONTRIBUTOR,"Indeed, looks like those instructions can use some work.    I think a conda cudatoolkit package needs to be added to your conda environment create command: e.g cudatoolkit=11.5 with version ( >= 11.2, <= 11.8) that matches the one installed on your host.    If you are running on a single node, you can activate the conda environment and run in either local mode or standalone, with master and worker started in the environment.

That said, I think you can replicate the key issue via the following running in a pyspark shell started as
```
pyspark --master local[4] --conf spark.driver.memory=20g --jars rapids-4-spark_2.12-23.04.0.jar --conf spark.plugins=com.nvidia.spark.SQLPlugin --conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer
```
And then in the shell, paste
```
from pyspark.sql.functions import rand, element_at, sum
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.functions import vector_to_array
import timeit

df = spark.range(10000000)
df = df.select(rand().alias(""r0""),rand().alias(""r1""))

df_vec = VectorAssembler(inputCols=[""r0"",""r1""],outputCol=""vec"").transform(df).drop(""r0"",""r1"")
timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(""vec""), 1))).collect()), number=1)
df_vec.cache()
timeit.timeit(lambda: print(df_vec.select(sum(element_at(vector_to_array(""vec""), 1))).collect()), number=1)
```
The first `timeit` call finishes reasonably fast, while the second, after the `cache()`, takes ""forever"".    You can try dialing down the range size to compare running times.",,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1573254442/reactions,0,0,0,0,0,0,0,0,0,8474
751,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574164659,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1574164659,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1574164659,IC_kwDOD7z77c5d09iz,2023-06-02T18:44:55Z,2023-06-02T18:44:55Z,COLLABORATOR,"Thanks for the simplified setup. I was able to reproduce the caching issue. At least I was able to get the Spark to crash with a timeout when using the parquet cached batch serializer for your initial request (I made the data bigger because I was using more cores, but I guess I made it too big!!!).",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574164659/reactions,1,1,0,0,0,0,0,0,0,8474
752,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574224567,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1574224567,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1574224567,IC_kwDOD7z77c5d1MK3,2023-06-02T19:39:43Z,2023-06-02T19:39:43Z,COLLABORATOR,I found at least one really bad problem where we were doing code generation for each row in a specific code path.  I need to do some more profiling to see what else might be bad about it.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1574224567/reactions,0,0,0,0,0,0,0,0,0,8474
753,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577262107,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577262107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1577262107,IC_kwDOD7z77c5eAxwb,2023-06-05T18:21:11Z,2023-06-05T18:21:11Z,COLLABORATOR,@eordentlich do you have the ability to try out #8495?  It is not going to solve all of your problems but it would be good to know if it is good enough for now or if we have to start looking at some of the other optimizations too.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577262107/reactions,0,0,0,0,0,0,0,0,0,8474
754,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577359605,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577359605,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1577359605,IC_kwDOD7z77c5eBJj1,2023-06-05T19:30:36Z,2023-06-05T19:30:36Z,CONTRIBUTOR,Thanks.  I'll have to build the jar (unless it is already in cicd somewhere) and give it a try on that notebook.,,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577359605/reactions,0,0,0,0,0,0,0,0,0,8474
755,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577794331,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1577794331,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1577794331,IC_kwDOD7z77c5eCzsb,2023-06-06T02:20:54Z,2023-06-06T02:20:54Z,CONTRIBUTOR,"@revans2 I tested the PR and it is a huge improvement.   Still 4x slower than mapping vector to array type and back, with PCBS, and about 6x slower than regular non-PCBS caching for the notebook example eval stage.",,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1577794331/reactions,0,0,0,0,0,0,0,0,0,8474
756,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579004740,https://github.com/NVIDIA/spark-rapids/issues/8474#issuecomment-1579004740,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8474,1579004740,IC_kwDOD7z77c5eHbNE,2023-06-06T15:39:00Z,2023-06-06T15:39:00Z,COLLABORATOR,"> @revans2 I tested the PR and it is a huge improvement. Still 4x slower than mapping vector to array type and back, with PCBS, and about 6x slower than regular non-PCBS caching for the notebook example eval stage.


@eordentlich glad to hear that it is helping. I'll see if we can get some help in improving the performance even more.

@sameerz looks like we should spend some time on the other issues I filed especially https://github.com/NVIDIA/spark-rapids/issues/8496 I think we can make it work without too much difficulty.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579004740/reactions,0,0,0,0,0,0,0,0,0,8474
757,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579406206,https://github.com/NVIDIA/spark-rapids/issues/8511#issuecomment-1579406206,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8511,1579406206,IC_kwDOD7z77c5eI9N-,2023-06-06T20:31:50Z,2023-06-06T20:31:50Z,COLLABORATOR,Scope would include new JNI kernel,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1579406206/reactions,0,0,0,0,0,0,0,0,0,8511
758,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1608293690,https://github.com/NVIDIA/spark-rapids/issues/8511#issuecomment-1608293690,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8511,1608293690,IC_kwDOD7z77c5f3J06,2023-06-26T21:33:20Z,2023-06-27T17:04:26Z,COLLABORATOR,"This can be implemented using a generalized version of [`hex_to_integers`](https://docs.rapids.ai/api/libcudf/stable/group__strings__convert.html#ga31c8d3b529c9a7d00a1b00dd55e537c6) followed by [`integers_to_hex`](https://docs.rapids.ai/api/libcudf/stable/group__strings__convert.html#gacfaa40a6fd38d6d82ca3a15eaef58448) where the radices are not hard-coded to base=16

[Demo of current functionality](https://github.com/gerashegalov/rapids-shell/blob/master/src/jupyter/cudf-radix-conversion.ipynb) 
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1608293690/reactions,0,0,0,0,0,0,0,0,0,8511
759,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1583141380,https://github.com/NVIDIA/spark-rapids/issues/8532#issuecomment-1583141380,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8532,1583141380,IC_kwDOD7z77c5eXNIE,2023-06-08T18:31:14Z,2023-06-08T18:31:14Z,COLLABORATOR,"This is very difficult to do on the GPU generically. We have a few choices on how to implement this, and none of them are great.

We do not currently have AST support for array aggregations. I filed https://github.com/rapidsai/cudf/issues/8020 for that a long time ago, and even if we got it we would still be limited.  We could only do operations that could be effectively translated to AST. We don't support a lot of them right now. Second from a performance standpoint it would not be great for long arrays. This is because of how the agg model works. It is single threaded so you get an accumulator that is updated for each entry in the list/array. That is not a good access pattern for GPUs.

The other option is to try and generate PTX/C++ ourselves and use make_udf_aggregation, but that also would not be perfect because we cannot handle strings or nested types as output, and we have not done anything with JIT in a very long time.  It had a lot of issues. Also I don't know if it would work with a segmented reduction, and it also would likely have similar limitations that the AST implementation would.

I think the fastest way to get it done would be to try and do pattern matching to rewrite it into an existing segmented_reduction.  This would be very limited, even more so then the AST or PTX implementations.

The example above is doing a SUM followed by a multiplication of 10 at the end.  But that is not 100% correct, because if there is a null value in the array `+` is going to return a null, so would have to detect that and translate it into a make_sum_aggregation with a `null_policy.INCLUDE`, assuming that it works.

But if we got into something a little more difficult, like.

```(acc, x) -> if (x = ""FOO"", acc + 1, acc)```

we would have to do some very fancy pattern matching to understand what is happening there and translate it to something correctly.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1583141380/reactions,0,0,0,0,0,0,0,0,0,8532
760,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1654781170,https://github.com/NVIDIA/spark-rapids/issues/8534#issuecomment-1654781170,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8534,1654781170,IC_kwDOD7z77c5iofTy,2023-07-28T00:05:47Z,2023-07-28T00:05:47Z,COLLABORATOR,"I went through the Kubernetes documentation as well, and here are some additional thoughts:

- [ ] As a new user for Kubernetes, I wish there are more clear references to instructions on setting up a Kubernetes cluster. In the [Prerequisites](https://nvidia.github.io/spark-rapids/docs/get-started/getting-started-kubernetes.html#prerequisites) section, there is a link to ""how to install a Kubernetes cluster with NVIDIA GPU support"", but the link takes me to a ""NVIDIA Cloud Native Technologies"" overview page, and I could hardly find any resources for installing Kubernetes clusters there.",,cindyyuanjiang,47068112,MDQ6VXNlcjQ3MDY4MTEy,https://avatars.githubusercontent.com/u/47068112?v=4,,https://api.github.com/users/cindyyuanjiang,https://github.com/cindyyuanjiang,https://api.github.com/users/cindyyuanjiang/followers,https://api.github.com/users/cindyyuanjiang/following{/other_user},https://api.github.com/users/cindyyuanjiang/gists{/gist_id},https://api.github.com/users/cindyyuanjiang/starred{/owner}{/repo},https://api.github.com/users/cindyyuanjiang/subscriptions,https://api.github.com/users/cindyyuanjiang/orgs,https://api.github.com/users/cindyyuanjiang/repos,https://api.github.com/users/cindyyuanjiang/events{/privacy},https://api.github.com/users/cindyyuanjiang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1654781170/reactions,0,0,0,0,0,0,0,0,0,8534
761,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745199482,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1745199482,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1745199482,IC_kwDOD7z77c5oBaF6,2023-10-03T15:19:29Z,2023-10-03T15:19:29Z,CONTRIBUTOR,"I just picked this issue up and ran the test in the issue description. The behavior has changed since this issue was filed. The query falls back to CPU due to:

```
cannot run on GPU because expression JsonToStructs from_json(StructField(Zipcode,StringType,true), StructField(ZipCodeType,StringType,true), StructField(City,StringType,true), StructField(State,StringType,true), nest_jsonF#7, Some(UTC)) 
produces an unsupported type StructType(StructField(Zipcode,StringType,true),StructField(ZipCodeType,StringType,true),StructField(City,StringType,true),StructField(State,StringType,true)); 
from_json on GPU only supports MapType<StringType, StringType> input schema
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745199482/reactions,0,0,0,0,0,0,0,0,0,8558
762,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745217465,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1745217465,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1745217465,IC_kwDOD7z77c5oBee5,2023-10-03T15:29:17Z,2023-10-03T15:33:57Z,CONTRIBUTOR,"Changing the output schema to `Map<String,String>` fails.

```
val out_struct = df.withColumn(""output"", from_json(col(""nest_jsonF""), DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType)))

out_struct.select(""output"").show(false)
```

```
Caused by: ai.rapids.cudf.CudfException: CUDF failure at: /home/jenkins/agent/workspace/jenkins-spark-rapids-jni_nightly-pre_release-184-cuda11/src/main/cpp/src/map_utils.cu:94: Incorrect output size computation.
```

",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745217465/reactions,0,0,0,0,0,0,0,0,0,8558
763,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745341203,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1745341203,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1745341203,IC_kwDOD7z77c5oB8sT,2023-10-03T16:35:04Z,2023-10-03T16:35:04Z,CONTRIBUTOR,"The first issue here is that GpuOverrides checks are incorrect and disable the JSON to struct functionality. We should allow `StructType` here. This change was introduced in https://github.com/nvidia/spark-rapids/commit/2b2835ef36c0b6cf10363c7e730501f6f1213d7d

```
      (a, conf, p, r) => new UnaryExprMeta[JsonToStructs](a, conf, p, r) {
        override def tagExprForGpu(): Unit =
          a.schema match {
            case MapType(_: StringType, _: StringType, _) => ()
            case _ =>
              willNotWorkOnGpu(""from_json on GPU only supports MapType<StringType, StringType> "" +
                               ""input schema"")
          }
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745341203/reactions,0,0,0,0,0,0,0,0,0,8558
764,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745679119,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1745679119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1745679119,IC_kwDOD7z77c5oDPMP,2023-10-03T20:30:50Z,2023-10-03T20:30:50Z,CONTRIBUTOR,"Ok, I now realize I have gone full circle on this and now understand why the tests were xfailed and why the feature was disabled.

To support parsing JSON to struct and to support reading some parts of the JSON as string (per the example here), we will need something like https://github.com/rapidsai/cudf/issues/14239",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745679119/reactions,0,0,0,0,0,0,0,0,0,8558
765,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745770276,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1745770276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1745770276,IC_kwDOD7z77c5oDlck,2023-10-03T21:40:51Z,2023-10-03T21:40:51Z,CONTRIBUTOR,"we ask to read the `state` column as a string, but cuDF returns `List<Struct>` and we do not have access to the JSON key names, just the values.

```
COLUMN 3 - LIST
OFFSETS
0 NULL
1 [0 - 1)
2 NULL
COLUMN 3:DATA - STRUCT
COLUMN 3:DATA:CHILD_0 - INT64
0 706
COLUMN 3:DATA:CHILD_1 - STRING
0 ""SPECIAL"" 5350454349414c
COLUMN 3:DATA:CHILD_2 - STRING
0 ""San Jose"" 53616e204a6f7365
COLUMN 3:DATA:CHILD_3 - STRING
0 ""CA"" 4341
```

We either need cuDF to return this as unparsed string or we need to implement our own parsing, using cuDF for the JSON tokenizer.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745770276/reactions,0,0,0,0,0,0,0,0,0,8558
766,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1746876215,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1746876215,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1746876215,IC_kwDOD7z77c5oHzc3,2023-10-04T13:25:57Z,2023-10-04T13:25:57Z,COLLABORATOR,"The C++ for the JSON parser returns a table_with_metadata. https://github.com/rapidsai/cudf/blob/29556a2514f4d274164a27a80539410da7e132d6/cpp/include/cudf/io/types.hpp#L231

We strip off much of the metadata to try and make the API consistent with the other reader APIs that just return the data in the same layout as the schema we passed in. You could use that, but then what happens if you have mixed data types? Like if one of the rows it happens to be a string and the others are structs? I think the only long term solution is to have separate processing for JSOn after the tokenization similar to what we do for map.  The special map processing code already does this. But in speaking with some people in CUDF they are going to investigate if this is something that they want to support themselves or if we just need to write our own parser after the CUDF tokenization.  There are enough differences already that I am leaning towards our own custom parsing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1746876215/reactions,1,1,0,0,0,0,0,0,0,8558
767,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1936649645,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1936649645,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1936649645,IC_kwDOD7z77c5zbu2t,2024-02-09T21:48:49Z,2024-02-09T21:48:49Z,CONTRIBUTOR,I started on a prototype for this issue in https://github.com/NVIDIA/spark-rapids/pull/10326 and this needs updating now that https://github.com/rapidsai/cudf/pull/14954 has been merged,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1936649645/reactions,0,0,0,0,0,0,0,0,0,8558
768,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997450761,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1997450761,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1997450761,IC_kwDOD7z77c53Dq4J,2024-03-14T13:21:59Z,2024-03-14T13:21:59Z,COLLABORATOR,"With the most recent changes (including https://github.com/NVIDIA/spark-rapids/pull/10575) in we are now getting an exception instead of the wrong data.

With `spark.rapids.sql.json.read.mixedTypesAsString.enabled` set to true or false we get back 

```
Caused by: java.lang.IllegalStateException: Don't know how to transform ColumnVector{rows=1, type=LIST, nullCount=Optional.empty, offHeap=(ID: 98 7f7af001a3a0)} to StringType for JSON
  at org.apache.spark.sql.rapids.GpuJsonReadCommon$.throwMismatchException(GpuJsonReadCommon.scala:273)
  at org.apache.spark.sql.rapids.GpuJsonReadCommon$.nestedColumnViewMismatchTransform(GpuJsonReadCommon.scala:294)
  at org.apache.spark.sql.rapids.GpuJsonReadCommon$.$anonfun$convertToDesiredType$1(GpuJsonReadCommon.scala:317)
  at com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$9(ColumnCastUtil.scala:135)
  at scala.Option.map(Option.scala:230)
  at com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:134)
```

https://github.com/rapidsai/cudf/issues/15278 is the issue that was filed to fix it in CUDF.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997450761/reactions,0,0,0,0,0,0,0,0,0,8558
769,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997452408,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1997452408,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1997452408,IC_kwDOD7z77c53DrR4,2024-03-14T13:22:51Z,2024-03-14T13:22:51Z,COLLABORATOR,@andygrove do you still plan on trying to fix this?,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997452408/reactions,0,0,0,0,0,0,0,0,0,8558
770,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1999940168,https://github.com/NVIDIA/spark-rapids/issues/8558#issuecomment-1999940168,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8558,1999940168,IC_kwDOD7z77c53NKpI,2024-03-15T15:43:37Z,2024-03-15T15:43:37Z,CONTRIBUTOR,"> @andygrove do you still plan on trying to fix this?

I am not actively working on this, so have unassigned myself.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1999940168/reactions,0,0,0,0,0,0,0,0,0,8558
771,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1599432654,https://github.com/NVIDIA/spark-rapids/issues/8576#issuecomment-1599432654,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8576,1599432654,IC_kwDOD7z77c5fVWfO,2023-06-20T20:10:15Z,2023-06-20T20:10:15Z,COLLABORATOR,What is the expected data size for the strings in the example use case?,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1599432654/reactions,1,0,0,0,0,0,0,0,1,8576
772,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1614730118,https://github.com/NVIDIA/spark-rapids/issues/8576#issuecomment-1614730118,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8576,1614730118,IC_kwDOD7z77c5gPtOG,2023-06-30T14:23:58Z,2023-06-30T14:23:58Z,COLLABORATOR,"In some cases, the string may reach a length of 200.",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1614730118/reactions,0,0,0,0,0,0,0,0,0,8576
773,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881714947,https://github.com/NVIDIA/spark-rapids/issues/8587#issuecomment-1881714947,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8587,1881714947,IC_kwDOD7z77c5wKLED,2024-01-08T19:42:58Z,2024-01-08T19:42:58Z,COLLABORATOR,"We can also improve the reliability of the *released* spark-rapids jars using the nightly pipeline of the *pending* release. 

We know that semi-monthly/bi-weekly maintenance updates to Databricks Runtimes can break released spark-rapids plugin code. 
Usually it is more subtle than just breaking the API https://github.com/NVIDIA/spark-rapids/pull/10070#issuecomment-1862013962. 

Ideally we want to retest the jar version that is already used by customers upon every maintenance update. However, testing is time consuming. So we do not want to retest last `N`  releases nightly. Say we do it on a weekly schedule. And say due to an unfortunate sequencing the test runs just before the DBR update push, it may take another week for the next run to catch new issues.

However, we can utilize the fact that our pending release runs nightly tests on DBR to detect whether we need to kick off released  artifacts tests.

We can maintain a table mapping DB buildver to last tested build hashes

| DB buildver | DBR hashes tested | 
| - |  - |
| spark321db | 
| spark330db |
| spark332db | 
| spark341db |
 
Somewhere in the source code we will have a test or `./integration_tests/run_pyspark_from_build.sh` log the current values  `org.apache.spark.BuildInfo.gitHash`, `com.databricks.BuildInfo.gitHash`

then the  CI can compare it to the last known value for the DB shim based on the table and kick off a pipeline for released test jars automatically, then update the table. This should shorten the window of detection to a couple of a days. ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881714947/reactions,0,0,0,0,0,0,0,0,0,8587
774,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1611910309,https://github.com/NVIDIA/spark-rapids/issues/8626#issuecomment-1611910309,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8626,1611910309,IC_kwDOD7z77c5gE8yl,2023-06-28T18:46:36Z,2023-06-28T18:46:36Z,COLLABORATOR,"This is very similar to how we do percent_rank. But it is going to get to be a bit more complicated. It gets a little complicated because if the number of rows do not go evenly into the number of buckets, then there is extra and the extra is spread across the buckets, but it is not too complex and we can probably figure it out without too much difficulty. I don't think that we need anything from CUDF for this to work.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1611910309/reactions,0,0,0,0,0,0,0,0,0,8626
775,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1611877140,https://github.com/NVIDIA/spark-rapids/issues/8627#issuecomment-1611877140,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8627,1611877140,IC_kwDOD7z77c5gE0sU,2023-06-28T18:18:11Z,2023-06-28T18:18:11Z,COLLABORATOR,"Looks fairly simple to do. In the worst case we could split the data on commas and look for the value in the array. i.e. 

```
array_position(split(value, "",""), 'ab')
```

Although we don't currently support array_position, but we filed https://github.com/NVIDIA/spark-rapids/issues/5224 for it and it looks like that one would be simple to implement.


",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1611877140/reactions,0,0,0,0,0,0,0,0,0,8627
776,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612098245,https://github.com/NVIDIA/spark-rapids/issues/8627#issuecomment-1612098245,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8627,1612098245,IC_kwDOD7z77c5gFqrF,2023-06-28T21:00:11Z,2023-06-28T21:01:47Z,COLLABORATOR,"~~I believe that this relies on cudf API `lists::contains` rather than `lists::index_of` (`array_position`).~~

It seems that `find_in_set` returns index value rather than boolean so indeed we need `lists::index_of`.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1612098245/reactions,0,0,0,0,0,0,0,0,0,8627
777,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1678524426,https://github.com/NVIDIA/spark-rapids/issues/8627#issuecomment-1678524426,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8627,1678524426,IC_kwDOD7z77c5kDEAK,2023-08-15T07:23:54Z,2023-08-15T07:23:54Z,COLLABORATOR,This is not a very important feature request according to customers response.,,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1678524426/reactions,0,0,0,0,0,0,0,0,0,8627
778,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1622577997,https://github.com/NVIDIA/spark-rapids/issues/8629#issuecomment-1622577997,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8629,1622577997,IC_kwDOD7z77c5gtpNN,2023-07-05T21:52:52Z,2023-07-05T21:52:52Z,COLLABORATOR,We can document our plugin behavior in this use case as we differ from Spark CPU (we are behaving correctly).,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1622577997/reactions,0,0,0,0,0,0,0,0,0,8629
779,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1636181114,https://github.com/NVIDIA/spark-rapids/issues/8666#issuecomment-1636181114,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8666,1636181114,IC_kwDOD7z77c5hhiR6,2023-07-14T17:42:46Z,2023-07-14T17:43:12Z,COLLABORATOR,"Argh. Ignore the previous comment. The task descriptions are rendered unreadable if not logged in through Github Enterprise.

Sorry for the confusion. There isn't a problem here.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1636181114/reactions,0,0,0,0,0,0,0,0,0,8666
780,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1650526573,https://github.com/NVIDIA/spark-rapids/issues/8676#issuecomment-1650526573,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8676,1650526573,IC_kwDOD7z77c5iYQlt,2023-07-25T20:39:43Z,2023-07-25T20:39:43Z,COLLABORATOR,JNI issue for adding support for List[Structs]: https://github.com/NVIDIA/spark-rapids-jni/issues/1280,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1650526573/reactions,0,0,0,0,0,0,0,0,0,8676
781,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1782990713,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-1782990713,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,1782990713,IC_kwDOD7z77c5qRkd5,2023-10-27T14:14:10Z,2023-10-27T14:14:10Z,COLLABORATOR,Added https://github.com/apache/spark/commit/6468f96ea42 because they fixed a correctness bug in 3.5 when persist is used with StorageType.None.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1782990713/reactions,0,0,0,0,0,0,0,0,0,8678
782,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874497452,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-1874497452,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,1874497452,IC_kwDOD7z77c5vuo-s,2024-01-02T20:13:10Z,2024-01-02T20:13:10Z,COLLABORATOR,"> Added [apache/spark@6468f96ea42](https://github.com/apache/spark/commit/6468f96ea42) because they fixed a correctness bug in 3.5 when persist is used with StorageType.None.

Same for 4.0 https://github.com/apache/spark/commit/a0c9ab63f3b",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874497452/reactions,0,0,0,0,0,0,0,0,0,8678
783,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1986328044,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-1986328044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,1986328044,IC_kwDOD7z77c52ZPXs,2024-03-08T19:51:24Z,2024-03-11T20:55:10Z,COLLABORATOR,Related to https://github.com/NVIDIA/spark-rapids/issues/9876 and [#9250](https://github.com/NVIDIA/spark-rapids/issues/9250),,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1986328044/reactions,0,0,0,0,0,0,0,0,0,8678
784,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992394033,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-1992394033,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,1992394033,IC_kwDOD7z77c52wYUx,2024-03-12T19:33:48Z,2024-03-12T19:33:48Z,COLLABORATOR,It seems like this could cause problems in the plugin if not handled. Changing priority to P0. ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992394033/reactions,0,0,0,0,0,0,0,0,0,8678
785,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000024469,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-2000024469,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,2000024469,IC_kwDOD7z77c53NfOV,2024-03-15T16:29:55Z,2024-03-15T16:29:55Z,COLLABORATOR,"There is a pending PR in Spark that will make supporting this possible https://github.com/apache/spark/pull/45525
",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000024469/reactions,0,0,0,0,0,0,0,0,0,8678
786,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2045798062,https://github.com/NVIDIA/spark-rapids/issues/8678#issuecomment-2045798062,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8678,2045798062,IC_kwDOD7z77c558Gau,2024-04-09T18:01:09Z,2024-04-09T18:01:09Z,COLLABORATOR,The [PR](https://github.com/apache/spark/pull/45525) in Spark has been merged. We will have to make corresponding changes in our plugin having `GpuInMemoryTableScanExec` implement the new trait.,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2045798062/reactions,0,0,0,0,0,0,0,0,0,8678
787,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1643457748,https://github.com/NVIDIA/spark-rapids/issues/8717#issuecomment-1643457748,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8717,1643457748,IC_kwDOD7z77c5h9SzU,2023-07-20T07:58:36Z,2023-07-20T07:58:36Z,COLLABORATOR,"Hi, I would suggest prioritize  the `MapType` according to customer queries so far.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1643457748/reactions,0,0,0,0,0,0,0,0,0,8717
788,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1644223549,https://github.com/NVIDIA/spark-rapids/issues/8717#issuecomment-1644223549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8717,1644223549,IC_kwDOD7z77c5iANw9,2023-07-20T16:23:07Z,2023-07-20T16:23:07Z,COLLABORATOR,"> Hi, I would suggest prioritize the `MapType` according to customer queries so far.

It was simple enough that I decided to just put up a PR for it. https://github.com/NVIDIA/spark-rapids/pull/8765",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1644223549/reactions,1,0,0,0,0,0,0,1,0,8717
789,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1652319762,https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1652319762,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724,1652319762,IC_kwDOD7z77c5ifGYS,2023-07-26T18:46:39Z,2023-07-26T18:46:39Z,COLLABORATOR,"It should be noted that something similar to this already exists in the tools repo.

https://github.com/NVIDIA/spark-rapids-tools/tree/dev/data_validation

I'm not 100% sure if we want to have two tools or just one. But the code is not that complicated. The data_validation tool is not great. It requires a primary key, which should not be a requirement. It does not handle maps, nulls, duplicate rows, or floating point approximate matches correctly.

The above example can be updated to handle nulls by using the null equals operator '<=>' as the join condition for the anti-join columns.  Maps can be replaced with a `map_entries` that are array_sorted. And for now we are just going to punt on floats/doubles in both cases and not include them in our scale testing suite until this can support doing a fuzzy check for them.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1652319762/reactions,0,0,0,0,0,0,0,0,0,8724
790,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1713483581,https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1713483581,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724,1713483581,IC_kwDOD7z77c5mIa89,2023-09-11T09:10:17Z,2023-09-11T09:10:17Z,COLLABORATOR,"@revans2 I'm building pipelines to run preliminary scale test on our internal cluster and I got some numbers for this issue.

with `scale_factor=100, complexity=300`, I got the data at size
```
134.1 G  268.1 G  /data/scale-test/SCALE_100_300_parquet_1.0.0_41
```
GPU output size (0 means failed queries)
```
1.2 G     2.4 G    /data/scale-test/output/q10_1
46.2 G    92.4 G   /data/scale-test/output/q11_1
0         0        /data/scale-test/output/q12_1
46.2 G    92.4 G   /data/scale-test/output/q13_1
17.0 G    34.1 G   /data/scale-test/output/q14_1
12.9 G    25.7 G   /data/scale-test/output/q15_1
39.3 K    78.7 K   /data/scale-test/output/q16_1
9.8 G     19.7 G   /data/scale-test/output/q17_1
5.8 G     11.6 G   /data/scale-test/output/q18_1
38.5 K    77.1 K   /data/scale-test/output/q19_1
645.0 G   1.3 T    /data/scale-test/output/q1_1
38.5 K    77.1 K   /data/scale-test/output/q20_1
0         0        /data/scale-test/output/q21_1
11.1 G    22.2 G   /data/scale-test/output/q22_1
19.5 K    39.1 K   /data/scale-test/output/q23_1
16.5 M    33.1 M   /data/scale-test/output/q24_1
290.7 K   581.3 K  /data/scale-test/output/q25_1
775.5 M   1.5 G    /data/scale-test/output/q26_1
1.7 G     3.5 G    /data/scale-test/output/q27_1
1004.1 M  2.0 G    /data/scale-test/output/q28_1
1.2 M     2.4 M    /data/scale-test/output/q29_1
644.9 G   1.3 T    /data/scale-test/output/q2_1
1.4 G     2.9 G    /data/scale-test/output/q30_1
603.6 M   1.2 G    /data/scale-test/output/q31_1
841.9 K   1.6 M    /data/scale-test/output/q32_1
174.3 M   348.6 M  /data/scale-test/output/q33_1
191.7 M   383.3 M  /data/scale-test/output/q34_1
70.6 M    141.3 M  /data/scale-test/output/q35_1
26.8 M    53.5 M   /data/scale-test/output/q36_1
27.1 G    54.1 G   /data/scale-test/output/q37_1
26.5 G    53.1 G   /data/scale-test/output/q38_1
4.0 G     7.9 G    /data/scale-test/output/q39_1
644.9 G   1.3 T    /data/scale-test/output/q3_1
0         0        /data/scale-test/output/q40_1
15.7 M    31.4 M   /data/scale-test/output/q41_1
12.1 G    24.2 G   /data/scale-test/output/q4_1
12.1 G    24.2 G   /data/scale-test/output/q5_1
1.2 G     2.4 G    /data/scale-test/output/q6_1
1.2 G     2.4 G    /data/scale-test/output/q7_1
1.2 G     2.4 G    /data/scale-test/output/q8_1
1.2 G     2.4 G    /data/scale-test/output/q9_1
```

Lots of the output are in large size which I doubt the validation will take a very long time. Should we perform like what NDS queries do? they use `LIMIT` for all the queries that the output are quite small.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1713483581/reactions,0,0,0,0,0,0,0,0,0,8724
791,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716236248,https://github.com/NVIDIA/spark-rapids/issues/8724#issuecomment-1716236248,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8724,1716236248,IC_kwDOD7z77c5mS6_Y,2023-09-12T18:39:25Z,2023-09-12T18:39:25Z,COLLABORATOR,"Limit has problems, even if there is an order by before it there can be ambiguity in the order of the results.  I would much rather do something like the following.

```
import org.apache.spark.sql.DataFrame

def compare(left: DataFrame, right: DataFrame): DataFrame = {
  val leftCount = left.groupBy(left.columns.map(col(_)): _*).count
  val rightCount = right.groupBy(right.columns.map(col(_)): _*).count
  val joinOn = leftCount.columns.map(c => leftCount(c) <=> rightCount(c)).reduceLeft(_ and _)
  val onlyRight = rightCount.join(leftCount, joinOn, joinType=""left_anti"").withColumn(""_in_column"", lit(""right""))
  val onlyLeft = leftCount.join(rightCount, joinOn, joinType=""left_anti"").withColumn(""_in_column"", lit(""left""))
  onlyRight.union(onlyLeft)
}
```

This should run a query that compares the two dataframes. There is a lot we can do to improve it. It does not support Maps, and floating-point is not going to be compared very well, but that should be good enough for the current scale tests and should be a good starting point.

It would also be nice to explicitly verify that the schemas match before running the query and if we see a map to throw an exception, or if you see floating point to output a warning.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716236248/reactions,0,0,0,0,0,0,0,0,0,8724
792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1638967314,https://github.com/NVIDIA/spark-rapids/issues/8736#issuecomment-1638967314,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8736,1638967314,IC_kwDOD7z77c5hsKgS,2023-07-17T22:20:59Z,2023-07-17T22:21:43Z,COLLABORATOR,The refactor should be covered by tests and by validation: https://github.com/NVIDIA/spark-rapids/issues/8738,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1638967314/reactions,0,0,0,0,0,0,0,0,0,8736
793,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2132589677,https://github.com/NVIDIA/spark-rapids/issues/8750#issuecomment-2132589677,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8750,2132589677,IC_kwDOD7z77c5_HLxt,2024-05-27T03:48:02Z,2024-05-30T07:53:49Z,COLLABORATOR,"A Spark UT failed related to `substring_index`:
```
'Rapids - string substring_index function' offload to RAPIDS
- Rapids - string substring_index function *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 705.0 failed 1 times, most recent failure: Lost task 1.0 in stage 705.0 (TID 1411) (spark-haoyang executor driver): java.lang.AssertionError:  value at 0 is null
	at ai.rapids.cudf.HostColumnVectorCore.assertsForGet(HostColumnVectorCore.java:230)
	at ai.rapids.cudf.HostColumnVectorCore.getUTF8(HostColumnVectorCore.java:364)
	at com.nvidia.spark.rapids.RapidsHostColumnVectorCore.getUTF8String(RapidsHostColumnVectorCore.java:183)
	at org.apache.spark.sql.vectorized.ColumnarBatchRow.getUTF8String(ColumnarBatchRow.java:126)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:365)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
  at scala.Option.foreach(Option.scala:407)
  ...
  Cause: java.lang.AssertionError: value at 0 is null
  at ai.rapids.cudf.HostColumnVectorCore.assertsForGet(HostColumnVectorCore.java:230)
  at ai.rapids.cudf.HostColumnVectorCore.getUTF8(HostColumnVectorCore.java:364)
  at com.nvidia.spark.rapids.RapidsHostColumnVectorCore.getUTF8String(RapidsHostColumnVectorCore.java:183)
  at org.apache.spark.sql.vectorized.ColumnarBatchRow.getUTF8String(ColumnarBatchRow.java:126)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:365)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
```

It is from following case:
```
SubstringIndex(Literal(""""), Literal("".""), Literal(-2))
```

Spark-shell reproduce:
```
scala> Seq("""").toDF.write.mode(""overwrite"").parquet(""TEMP"")
24/05/30 07:47:38 WARN GpuOverrides:
*Exec <DataWritingCommandExec> will run on GPU
  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU
  ! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec
    @Expression <AttributeReference> value#1 could run on GPU


scala> val df = spark.read.parquet(""TEMP"")
df: org.apache.spark.sql.DataFrame = [value: string]

scala> df.selectExpr(""value"", ""substring_index(value, '.', -2)"").show()
24/05/30 07:48:21 WARN GpuOverrides:
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> substring_index(value#6, ., -2) AS substring_index(value, ., -2)#15 will run on GPU
      *Expression <SubstringIndex> substring_index(value#6, ., -2) will run on GPU
    *Exec <FileSourceScanExec> will run on GPU

+-----+-----------------------------+
|value|substring_index(value, ., -2)|
+-----+-----------------------------+
|     |                         null|
+-----+-----------------------------+


scala>

scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")

scala> df.selectExpr(""value"", ""substring_index(value, '.', -2)"").show()
+-----+-----------------------------+
|value|substring_index(value, ., -2)|
+-----+-----------------------------+
|     |                             |
+-----+-----------------------------+
```


Since `GpuSubstringIndex` needs reworking, I think we can fix this Spark UT by reworking instead of fixing on the original way.",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2132589677/reactions,0,0,0,0,0,0,0,0,0,8750
794,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1650477666,https://github.com/NVIDIA/spark-rapids/issues/8757#issuecomment-1650477666,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8757,1650477666,IC_kwDOD7z77c5iYEpi,2023-07-25T20:06:03Z,2023-07-25T20:06:03Z,COLLABORATOR,Relevant once we no longer support Spark 3.1.x.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1650477666/reactions,0,0,0,0,0,0,0,0,0,8757
795,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1644309313,https://github.com/NVIDIA/spark-rapids/issues/8759#issuecomment-1644309313,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8759,1644309313,IC_kwDOD7z77c5iAitB,2023-07-20T17:24:25Z,2023-07-20T17:24:25Z,COLLABORATOR,"This is not a requirement for us, but it would be nice to do something like this for all of our code. The idea is that using an ""Evaluator"" instead of a lambda means that you need to explicitly call out what goes into the closure for that operation. Which means we are much less likely to ship things we didn't expect (like the entire SQL plan).",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1644309313/reactions,0,0,0,0,0,0,0,0,0,8759
796,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703352718,https://github.com/NVIDIA/spark-rapids/issues/8759#issuecomment-1703352718,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8759,1703352718,IC_kwDOD7z77c5lhxmO,2023-09-01T21:49:51Z,2023-09-01T21:49:51Z,COLLABORATOR,"Here is another instance of this, where the API was used for C2R and R2C in apache spark.

https://github.com/apache/spark/commit/56b9f6cd46",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703352718/reactions,0,0,0,0,0,0,0,0,0,8759
797,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1659659247,https://github.com/NVIDIA/spark-rapids/issues/8826#issuecomment-1659659247,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8826,1659659247,IC_kwDOD7z77c5i7GPv,2023-08-01T06:36:48Z,2023-08-01T07:14:56Z,COLLABORATOR,"Created a cuDF issue, refer to https://github.com/rapidsai/cudf/issues/13793
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1659659247/reactions,0,0,0,0,0,0,0,0,0,8826
798,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1653931098,https://github.com/NVIDIA/spark-rapids/issues/8831#issuecomment-1653931098,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8831,1653931098,IC_kwDOD7z77c5ilPxa,2023-07-27T16:14:02Z,2023-07-27T16:14:02Z,COLLABORATOR,"I thought I filed something for this before, but I could not find it so I am doing it again.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1653931098/reactions,0,0,0,0,0,0,0,0,0,8831
799,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1655856411,https://github.com/NVIDIA/spark-rapids/issues/8846#issuecomment-1655856411,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8846,1655856411,IC_kwDOD7z77c5isl0b,2023-07-28T15:07:51Z,2023-07-28T15:07:51Z,COLLABORATOR,It looks like we have an output type from some operation that is saying the result cannot be null when it can be.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1655856411/reactions,0,0,0,0,0,0,0,0,0,8846
800,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998771501,https://github.com/NVIDIA/spark-rapids/issues/8849#issuecomment-1998771501,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8849,1998771501,IC_kwDOD7z77c53ItUt,2024-03-15T01:48:06Z,2024-03-15T01:48:06Z,COLLABORATOR,"Not planing work on this for release 24.04, unassign it from me.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998771501/reactions,0,0,0,0,0,0,0,0,0,8849
801,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670265202,https://github.com/NVIDIA/spark-rapids/issues/8865#issuecomment-1670265202,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8865,1670265202,IC_kwDOD7z77c5jjjly,2023-08-08T20:30:33Z,2023-08-08T20:30:33Z,COLLABORATOR,Need to verify that we fallback properly,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1670265202/reactions,0,0,0,0,0,0,0,0,0,8865
802,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2007687382,https://github.com/NVIDIA/spark-rapids/issues/8865#issuecomment-2007687382,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8865,2007687382,IC_kwDOD7z77c53quDW,2024-03-19T16:58:01Z,2024-03-19T16:58:35Z,COLLABORATOR,The two classes from this change that we rely on are FileFormat and FileSourceMetadataAttribute. We should get these changes for free IMO. @revans2 is my assessment correct? ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2007687382/reactions,0,0,0,0,0,0,0,0,0,8865
803,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836885041,https://github.com/NVIDIA/spark-rapids/issues/8887#issuecomment-1836885041,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8887,1836885041,IC_kwDOD7z77c5tfKQx,2023-12-01T22:54:07Z,2023-12-01T22:54:07Z,COLLABORATOR,I put up #9929 as a first step for GeneratedInternalRowToCudfRowIterator.,,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836885041/reactions,0,0,0,0,0,0,0,0,0,8887
804,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1956757748,https://github.com/NVIDIA/spark-rapids/issues/8887#issuecomment-1956757748,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8887,1956757748,IC_kwDOD7z77c50ocD0,2024-02-21T14:20:25Z,2024-02-21T14:20:25Z,COLLABORATOR,"I put up another pr to improve this:  #10450 
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1956757748/reactions,0,0,0,0,0,0,0,0,0,8887
805,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1971404732,https://github.com/NVIDIA/spark-rapids/issues/8887#issuecomment-1971404732,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8887,1971404732,IC_kwDOD7z77c51gT-8,2024-02-29T15:38:14Z,2024-02-29T15:38:14Z,COLLABORATOR,This should not have been closed.  Only the first part (`GeneratedInternalRowToCudfRowIterator`) is done.  I am still working on the second part (`RowToColumnarIterator`).,,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1971404732/reactions,0,0,0,0,0,0,0,0,0,8887
806,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2010764050,https://github.com/NVIDIA/spark-rapids/issues/8887#issuecomment-2010764050,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8887,2010764050,IC_kwDOD7z77c532dMS,2024-03-20T22:31:22Z,2024-03-20T22:31:22Z,COLLABORATOR,"I put up a draft PR https://github.com/NVIDIA/spark-rapids/pull/10617 that is a large part of the second part of this feature.
It allocates a single buffer up front and splices it up among the column builders.",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2010764050/reactions,0,0,0,0,0,0,0,0,0,8887
807,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2070232371,https://github.com/NVIDIA/spark-rapids/issues/8887#issuecomment-2070232371,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8887,2070232371,IC_kwDOD7z77c57ZT0z,2024-04-22T16:57:18Z,2024-04-22T16:57:18Z,COLLABORATOR,Assigning to @jlowe for triage after https://github.com/NVIDIA/spark-rapids/pull/10617 was reverted,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2070232371/reactions,0,0,0,0,0,0,0,0,0,8887
808,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1851215035,https://github.com/NVIDIA/spark-rapids/issues/8891#issuecomment-1851215035,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8891,1851215035,IC_kwDOD7z77c5uV0y7,2023-12-12T02:45:39Z,2023-12-12T02:45:39Z,COLLABORATOR,Just marked nvbug-***8068 here.,,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1851215035/reactions,0,0,0,0,0,0,0,0,0,8891
809,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1740966958,https://github.com/NVIDIA/spark-rapids/issues/8900#issuecomment-1740966958,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8900,1740966958,IC_kwDOD7z77c5nxQwu,2023-09-29T14:16:25Z,2023-09-29T14:16:25Z,COLLABORATOR,"This issue is a manifestation of not having good limits in the MT shuffle: https://github.com/NVIDIA/spark-rapids/issues/9153

As part of #8900, we should work out the pool of memory that we are allowing to be inflight. In terms of what I am finding in #9153, it seems 128MB is a better default per thread, so I'm going to PR that. That said, we need to work out if this is going to be a static amount dedicated for shuffle in #8900, or if we are going to allow the maxBytesInFlight to ""fluctuate"" as host memory usage is in demand.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1740966958/reactions,0,0,0,0,0,0,0,0,0,8900
810,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1664607202,https://github.com/NVIDIA/spark-rapids/issues/8910#issuecomment-1664607202,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8910,1664607202,IC_kwDOD7z77c5jN-Pi,2023-08-03T20:29:33Z,2023-08-03T21:00:21Z,CONTRIBUTOR,"Some more details on the failures:

## Column name resolution error with metadata fields

Affects `test_[csv|orc|parquet]_scan_with_hidden_metadata_fallback`

```
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_metadata`.`file_path` cannot be resolved. 

Did you mean one of the following? [`_c0`].; line 1 pos 0; 'Project [_c0#16965, '_metadata.file_path]
```

Possibly related to changes in Spark 3.5.0 in https://github.com/apache/spark/commit/3baf7f7b7106f3fd30257b793ff4908d0f1ec427

##  Fall back to BatchScanExec instead of FileSourceScanExec

Affects a number of fallback tests, such as `test_csv_datetime_parsing_fallback_cpu_fallback`

Test is expecting `FileSourceScanExec` but finds `v2.BatchScanExec`

## Test assertion failure in test_parquet_read_count

```
java.lang.AssertionError: assertion failed: Could not find GpuFileGpuScan parquet .* ReadSchema: struct<> in the Spark plan
 E                   GpuColumnarToRow false
 E                   +- GpuHashAggregate(keys=[], functions=[gpucount(1, false)], output=[count(1)#115634L])
 E                      +- GpuShuffleCoalesce 1073741824
 E                         +- GpuColumnarExchange gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=213897]
 E                            +- GpuHashAggregate(keys=[], functions=[partial_gpucount(1, false)], output=[count#115637L])
 E                               +- GpuBatchScan parquet hdfs://ip-172-31-0-176.us-west-2.compute.internal:8020/tmp/pyspark_tests/ip-172-31-8-237-main-840-848679351/PARQUET_DATA[] GpuParquetScan DataFilters: [], Format: gpuparquet, Location: InMemoryFileIndex(1 paths)[hdfs://ip-172-31-0-176.us-west-2.compute.internal:8020/tmp/pyspark_tes..., PartitionFilters: [], ReadSchema: struct<>, PushedFilters: [] RuntimeFilters: []
 E
```

## ArrowColumnarDataSourceV2 not found

Affects `test_read_*` tests

```
org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.nvidia.spark.rapids.tests.datasourcev2.parquet.ArrowColumnarDataSourceV2. Please find packages at `https://spark.apache.org/third-party-projects.html`.
```

## UDF loading issues

```
pyspark.errors.exceptions.captured.AnalysisException: [CANNOT_LOAD_FUNCTION_CLASS] Cannot load class com.nvidia.spark.rapids.tests.udf.hive.EmptyHiveSimpleUDF when registering the function `emptysimple`, please make sure it is on the classpath.
```


",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1664607202/reactions,0,0,0,0,0,0,0,0,0,8910
811,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1669768137,https://github.com/NVIDIA/spark-rapids/issues/8919#issuecomment-1669768137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8919,1669768137,IC_kwDOD7z77c5jhqPJ,2023-08-08T14:47:58Z,2023-08-08T14:47:58Z,COLLABORATOR,"The issue is all around state and where that state is stored, and if you want to generate the exact same data after modifying a table as you would before modifying it. I kind of took the route of wanting to not allow modifications to the tables because it is hard to produce the same data in different ways to get to the same result.  But we can adjust how all of this works to make it simpler.

The table id is a number in DBGen that just increments each time a table is added. It is used as part of the hash to generate the data in the table.  So if you do 
```
DBGen.addTable(""a"", ""c: int"", 100)
DBGen.addTable(""b"", ""c: int"", 100)
```

Tables a and b will have different values in them.  If you switched the order the tables were added to the database, then the data generated would also switch.

Each column has an ID associated with it too. That ID is generated by walking the table in a specific order. So with the current setup adding a column to the end of a table should be doable, because we would just need to get the ID of the table and the ID of the last column so we know where to start from.

Updating an existing column without replacing the entire table is harder because the column ID might change based off of the previous columns in the same table.

If you want an API to replace a table that is also not hard because we can use the table ID from the previous table.  But because you are replacing a table it would not preserve the any of the settings/modifications done to the columns in the table.

Perhaps the simplest thing to do is to change how we generate the table and column IDs. If we instead used the hash of the name of the table for the table ID, then the order of adding/changing tables would not matter.  If we also used a hash of a full column name, like for ""a map<string, list<struct<b int, c int>>>"" we would have `a`, `a.key`, `a.value`, `a.value.data`, `a.value.data.b`, and `a.value.data.c`. Then again we would not have to worry about the order of the columns in the table, but instead it would just be able the name of columns.  That would make it much simpler to be able to manipulate the generator after it is initially configured and get consistent results.

After that it is just a matter of plumbing and making sure that we can reference back to the original DBGen when needed.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1669768137/reactions,1,1,0,0,0,0,0,0,0,8919
812,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942570913,https://github.com/NVIDIA/spark-rapids/issues/8921#issuecomment-1942570913,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8921,1942570913,IC_kwDOD7z77c5zyUeh,2024-02-13T21:19:52Z,2024-03-14T20:46:47Z,COLLABORATOR,Initial scope is understanding impact of fallback with benchmarks on Databricks 11.3/12.2/13.3.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942570913/reactions,0,0,0,0,0,0,0,0,0,8921
813,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000372597,https://github.com/NVIDIA/spark-rapids/issues/8921#issuecomment-2000372597,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8921,2000372597,IC_kwDOD7z77c53O0N1,2024-03-15T20:06:28Z,2024-03-15T20:06:28Z,COLLABORATOR,"NDS SF3K benchmark experiment results with `spark.sql.optimizer.runtime.bloomFilter.enabled` set to true (test) and false (baseline):

```
--------------------------------------------------------------------
Name = query16
Means = 4938.2, 2899.4
Time diff = 2038.7999999999997
Speedup = 1.703179968269297
T-Test (test statistic, p value, df) = 30.182426342750624, 1.5758462055072497e-09, 8.0
T-Test Confidence Interval = 1883.0311702473427, 2194.5688297526567
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
--------------------------------------------------------------------
Name = query64
Means = 18665.8, 12114.6
Time diff = 6551.199999999999
Speedup = 1.5407689894837633
T-Test (test statistic, p value, df) = 48.586627207936644, 3.562824507885676e-11, 8.0
T-Test Confidence Interval = 6240.268882580042, 6862.131117419955
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
--------------------------------------------------------------------
Name = query93
Means = 12480.4, 11776.6
Time diff = 703.7999999999993
Speedup = 1.059762580031588
T-Test (test statistic, p value, df) = 2.7663393196104678, 0.024434393189381686, 8.0
T-Test Confidence Interval = 117.11647251971294, 1290.4835274802856
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
--------------------------------------------------------------------
Name = query94
Means = 5247.2, 3190.4
Time diff = 2056.7999999999997
Speedup = 1.6446840521564694
T-Test (test statistic, p value, df) = 36.89898739574463, 3.1911147346807857e-10, 8.0
T-Test Confidence Interval = 1928.2601771027562, 2185.3398228972433
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
--------------------------------------------------------------------
Name = query95
Means = 8282.2, 6988.6
Time diff = 1293.6000000000004
Speedup = 1.185101450934379
T-Test (test statistic, p value, df) = 5.730056177701596, 0.0004389295644865668, 8.0
T-Test Confidence Interval = 773.0035422047629, 1814.1964577952378
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
--------------------------------------------------------------------
Name = benchmark
Means = 441800.0, 429000.0
Time diff = 12800.0
Speedup = 1.02983682983683
T-Test (test statistic, p value, df) = 11.494739329713592, 2.973528840991338e-06, 8.0
T-Test Confidence Interval = 10232.142471284506, 15367.857528715494
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
```

Overall 3% improvement with the setting fully working and significant difference in queries 16, 64, 94, 95.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000372597/reactions,0,0,0,0,0,0,0,0,0,8921
814,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000389291,https://github.com/NVIDIA/spark-rapids/issues/8921#issuecomment-2000389291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8921,2000389291,IC_kwDOD7z77c53O4Sr,2024-03-15T20:21:30Z,2024-03-15T20:21:30Z,MEMBER,"@mattahrens significant speedup is expected with just that setting, since that's comparing non-Bloom filter joins vs. Bloom filter joins, with no GPU fallbacks in either case.

For the purposes of estimating the fallback cost of not implementing the new Bloom filter type, we should compare
```
spark.sql.optimizer.runtime.bloomFilter.enabled=true
```
as a baseline vs.
```
spark.sql.optimizer.runtime.bloomFilter.enabled=true
spark.rapids.sql.expression.BloomFilterMightContain=false
spark.rapids.sql.expression.BloomFilterAggregate=false
```

This will compare Bloom filter joins with GPU acceleration vs. Bloom filter joins where the build and probe of the Bloom filter falls back to the CPU.  ",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000389291/reactions,1,1,0,0,0,0,0,0,0,8921
815,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000620212,https://github.com/NVIDIA/spark-rapids/issues/8921#issuecomment-2000620212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8921,2000620212,IC_kwDOD7z77c53Pwq0,2024-03-15T23:22:59Z,2024-03-15T23:22:59Z,COLLABORATOR,"Ran that benchmark and it had a bigger impact (~6%): 
```
Name = benchmark
Means = 454600.0, 427800.0
Time diff = 26800.0
Speedup = 1.0626460963066853
T-Test (test statistic, p value, df) = 23.50515491742838, 1.1413900109260772e-08, 8.0
T-Test Confidence Interval = 24170.750755057958, 29429.249244942042
ALERT: significant change has been detected (p-value < 0.05)
ALERT: improvement in performance has been observed
```

Regressions noted in queries 13, 16, 64, 80, 94, 95.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000620212/reactions,0,0,0,0,0,0,0,0,0,8921
816,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1668616315,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1668616315,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1668616315,IC_kwDOD7z77c5jdRB7,2023-08-07T21:44:56Z,2023-08-07T21:44:56Z,COLLABORATOR,"cudf sees the 2 x double quotes as a way to escape a single `""`.  Spark sees 2 x double as an empty field.  

Possibly related issue https://github.com/rapidsai/cudf/issues/12145",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1668616315/reactions,0,0,0,0,0,0,0,0,0,8926
817,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1678574704,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1678574704,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1678574704,IC_kwDOD7z77c5kDQRw,2023-08-15T08:16:48Z,2023-08-15T08:16:48Z,NONE,"@tgravescs can you please share the steps to repro the issue? I'm only seeing nulls returned for the fields containing only `""""`.",,vuule,16005690,MDQ6VXNlcjE2MDA1Njkw,https://avatars.githubusercontent.com/u/16005690?v=4,,https://api.github.com/users/vuule,https://github.com/vuule,https://api.github.com/users/vuule/followers,https://api.github.com/users/vuule/following{/other_user},https://api.github.com/users/vuule/gists{/gist_id},https://api.github.com/users/vuule/starred{/owner}{/repo},https://api.github.com/users/vuule/subscriptions,https://api.github.com/users/vuule/orgs,https://api.github.com/users/vuule/repos,https://api.github.com/users/vuule/events{/privacy},https://api.github.com/users/vuule/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1678574704/reactions,0,0,0,0,0,0,0,0,0,8926
818,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679423979,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1679423979,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1679423979,IC_kwDOD7z77c5kGfnr,2023-08-15T18:45:46Z,2023-08-15T18:45:46Z,COLLABORATOR,"for me with Spark I just read a file with the contents in the description and it comes back wrong.  I did not create a CUDF reproduce case.  

I just tested this again and there might be multiple things going on with one of them me messing up.

The original user query looked something like:
`val df = spark.read.format(""csv"").option(""delimiter"", ""\t"").load(""./csvfileDir/"")`

When I was testing to get the small repro case I left off the delimiter specification.

CPU:
```
scala> val df = spark.read.csv(""./foo.csv"")

scala> spark.sql(""set spark.rapids.sql.enabled=false"")
res3: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> df.count()
res4: Long = 2

scala> df.show()
+-------------+
|          _c0|
+-------------+
| row1\t10\t""""|
|rows2\t11\t""""|
+-------------+

```

GPU:
```
scala> spark.sql(""set spark.rapids.sql.enabled=true"")
res6: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> val df = spark.read.csv(""./foo.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string]

scala> df.count()
res7: Long = 1

scala> df.show()
+-----------+
|        _c0|
+-----------+
|row1\t10\t""|
+-----------+

```

Going back to specifying delimiter actually doesn't show the same problem with my reproduce case.
GPU:
```
scala> val df = spark.read.format(""csv"").option(""delimiter"", ""\t"").load(""./foo.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]

scala> df.count()
res9: Long = 2

scala> df.show()
+-----+---+----+
|  _c0|_c1| _c2|
+-----+---+----+
| row1| 10|null|
|rows2| 11|null|
+-----+---+----+

```
CPU:

```
scala> spark.sql(""set spark.rapids.sql.enabled=false"")
res15: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> val df = spark.read.format(""csv"").option(""delimiter"", ""\t"").load(""./foo.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]

scala> df.count()
res16: Long = 2

scala> df.show()
+-----+---+----+
|  _c0|_c1| _c2|
+-----+---+----+
| row1| 10|null|
|rows2| 11|null|
+-----+---+----+

```

Now if I go back to the actual customer data though using the delimiter option then it still does report the wrong number of rows. 

```
scala> spark.sql(""set spark.rapids.sql.enabled=true"")
scala> val df = spark.read.format(""csv"").option(""delimiter"", ""\t"").load(""./customerdata/"")

scala> df.count()
res12: Long = 7600375     

scala> spark.sql(""set spark.rapids.sql.enabled=false"")

scala> df.count()
res14: Long = 7916199       
```  

So to summarize:

1) There is some issue when delimiter is not specified. Not sure if its plugin or cudf
2) I need to go back to customer data to find repro case with delimiter ",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679423979/reactions,0,0,0,0,0,0,0,0,0,8926
819,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679482502,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1679482502,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1679482502,IC_kwDOD7z77c5kGt6G,2023-08-15T19:29:24Z,2023-08-15T19:29:24Z,COLLABORATOR,"Ok so I was able to narrow down the customer data to the following to reproduce it:

Note there are tabs between 27 and ""foo\"""" and 2 and ""bar""
```
27      ""foo\""""
2       ""bar""
```


```
// CPU
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")
scala> val df = spark.read.format(""csv"").option(""delimiter"", ""\t"").load(""./testRepro.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]

scala> df.show(false)
+---+----+
|_c0|_c1 |
+---+----+
|27 |foo""|
|2  |bar |
+---+----+

// GPU
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""true"")

scala> df.show(false)


+---+------------------+
|_c0|_c1               |
+---+------------------+
|27 |""foo\""\n2\t""bar""\n|
+---+------------------+
```

Bobby pointed out this looks like https://github.com/NVIDIA/spark-rapids/issues/6435 as well.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679482502/reactions,0,0,0,0,0,0,0,0,0,8926
820,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679484687,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1679484687,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1679484687,IC_kwDOD7z77c5kGucP,2023-08-15T19:30:58Z,2023-08-15T19:33:43Z,COLLABORATOR,"Note the second issue with customer data and where delimiter is specified as tab  with the reproduce data containing `""foo\""""` seems this is actually the same as https://github.com/NVIDIA/spark-rapids/issues/129",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1679484687/reactions,0,0,0,0,0,0,0,0,0,8926
821,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681112544,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1681112544,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1681112544,IC_kwDOD7z77c5kM73g,2023-08-16T18:49:09Z,2023-08-16T18:58:52Z,COLLABORATOR,This is a dupe of #6435 and the CUDF issue https://github.com/rapidsai/cudf/issues/11948 is to fix it.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681112544/reactions,0,0,0,0,0,0,0,0,0,8926
822,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681331217,https://github.com/NVIDIA/spark-rapids/issues/8926#issuecomment-1681331217,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8926,1681331217,IC_kwDOD7z77c5kNxQR,2023-08-16T22:07:12Z,2023-08-16T22:07:12Z,NONE,"I suspect that I had trouble reproing this because the issue only reproes when a field has other characters except `""""`. If that's the case, this is a duplicate of #11948.
I looked into that issue earlier and found that the state machine is insufficient to deal with this case, but I can't find any comments about my findings ;(",,vuule,16005690,MDQ6VXNlcjE2MDA1Njkw,https://avatars.githubusercontent.com/u/16005690?v=4,,https://api.github.com/users/vuule,https://github.com/vuule,https://api.github.com/users/vuule/followers,https://api.github.com/users/vuule/following{/other_user},https://api.github.com/users/vuule/gists{/gist_id},https://api.github.com/users/vuule/starred{/owner}{/repo},https://api.github.com/users/vuule/subscriptions,https://api.github.com/users/vuule/orgs,https://api.github.com/users/vuule/repos,https://api.github.com/users/vuule/events{/privacy},https://api.github.com/users/vuule/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1681331217/reactions,0,0,0,0,0,0,0,0,0,8926
823,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1673735187,https://github.com/NVIDIA/spark-rapids/issues/8954#issuecomment-1673735187,https://api.github.com/repos/NVIDIA/spark-rapids/issues/8954,1673735187,IC_kwDOD7z77c5jwywT,2023-08-10T18:48:09Z,2023-08-10T18:48:09Z,COLLABORATOR,We can start by experimenting with AST project config turned on.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1673735187/reactions,0,0,0,0,0,0,0,0,0,8954
824,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1674840477,https://github.com/NVIDIA/spark-rapids/issues/9004#issuecomment-1674840477,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9004,1674840477,IC_kwDOD7z77c5j1Amd,2023-08-11T13:58:22Z,2023-08-11T13:58:22Z,COLLABORATOR,"This can happen today also. 

```
my_udf(a, c + another_udf(d))
```

because there is a `+` in between the two UDFs they end up being run as separate processes.  The CPU memory overhead has similar limits.  This is one of the ugly problems with pyspark, besides the fact that the UDFs are all dog slow.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1674840477/reactions,0,0,0,0,0,0,0,0,0,9004
825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000151184,https://github.com/NVIDIA/spark-rapids/issues/9058#issuecomment-2000151184,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9058,2000151184,IC_kwDOD7z77c53N-KQ,2024-03-15T17:46:11Z,2024-03-15T17:46:11Z,COLLABORATOR,"I added needs triage back on this because it looks like a lot of work has gone into CUDF for writing parquet V2 data, which can make the files a lot smaller compared to V1. We should take another look at implementing this and see if it could be a good solution for some of our customers.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000151184/reactions,0,0,0,0,0,0,0,0,0,9058
826,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688862885,https://github.com/NVIDIA/spark-rapids/issues/9063#issuecomment-1688862885,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9063,1688862885,IC_kwDOD7z77c5kqgCl,2023-08-22T20:15:55Z,2023-08-22T20:15:55Z,COLLABORATOR,This will accelerate places where we cannot get pinned memory.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688862885/reactions,0,0,0,0,0,0,0,0,0,9063
827,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688905389,https://github.com/NVIDIA/spark-rapids/issues/9077#issuecomment-1688905389,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9077,1688905389,IC_kwDOD7z77c5kqqat,2023-08-22T20:42:38Z,2023-08-22T20:42:38Z,COLLABORATOR,Spark also supports `bit_and` and `bit_xor` as reductions.  When we implement `bit_or` we should discuss whether to implement the other two as well.   ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688905389/reactions,0,0,0,0,0,0,0,0,0,9077
828,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688914782,https://github.com/NVIDIA/spark-rapids/issues/9080#issuecomment-1688914782,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9080,1688914782,IC_kwDOD7z77c5kqste,2023-08-22T20:50:55Z,2023-08-22T20:50:55Z,COLLABORATOR,"We could leverage https://github.com/rapidsai/cudf/pull/9215 , but could also implement the sha-2 algorithm in spark-rapids-jni.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1688914782/reactions,0,0,0,0,0,0,0,0,0,9080
829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1696383743,https://github.com/NVIDIA/spark-rapids/issues/9121#issuecomment-1696383743,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9121,1696383743,IC_kwDOD7z77c5lHML_,2023-08-28T20:42:32Z,2023-08-28T20:42:32Z,COLLABORATOR,"With this, we should also make clear the deduping that is being done in order to attach event handlers to unique columns (https://github.com/NVIDIA/spark-rapids/pull/9098#discussion_r1306097097)",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1696383743/reactions,0,0,0,0,0,0,0,0,0,9121
830,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699726388,https://github.com/NVIDIA/spark-rapids/issues/9126#issuecomment-1699726388,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9126,1699726388,IC_kwDOD7z77c5lT8Q0,2023-08-30T19:34:04Z,2023-08-30T19:34:04Z,COLLABORATOR,"`set_row_group_size_bytes` does not map cleanly to `parquet.block.size`.  `set_row_group_size_bytes` sets the size of the rowgroup before any compression happens, and even then it is an approximation based off of the average row size for the input table.

https://github.com/rapidsai/cudf/blob/f999e1c5ed183253585606fdfc7552a224aee2d7/cpp/src/io/parquet/writer_impl.cu#L1522-L1528

`parquet.block.size` actually stops building a row group with it adds a row that would, after compression, go over that size.  They are not the same thing.  Because of how CUDF works I don't think we can ever truly support this the same way. I would not mind exposing cudf specific configs that a user could modify, but they would not map 1 to 1 with the actual parquet configs.

Instead we need to at least warn users if they set this to a non-default value.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699726388/reactions,0,0,0,0,0,0,0,0,0,9126
831,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699895663,https://github.com/NVIDIA/spark-rapids/issues/9126#issuecomment-1699895663,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9126,1699895663,IC_kwDOD7z77c5lUllv,2023-08-30T21:49:52Z,2023-08-30T21:49:52Z,COLLABORATOR,"As @revans2 noted, we want to warn users if they set this that RAPIDS Spark will behave differently from Spark, and let them know the configs they can set to fall back Parquet writing to the CPU if they really want the behavior.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699895663/reactions,0,0,0,0,0,0,0,0,0,9126
832,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699896698,https://github.com/NVIDIA/spark-rapids/issues/9126#issuecomment-1699896698,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9126,1699896698,IC_kwDOD7z77c5lUl16,2023-08-30T21:50:57Z,2023-08-30T21:50:57Z,COLLABORATOR,It may be worthwhile to go through all the Parquet and ORC configs at the same time and provide the same warning.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699896698/reactions,0,0,0,0,0,0,0,0,0,9126
833,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697573233,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697573233,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697573233,IC_kwDOD7z77c5lLulx,2023-08-29T14:36:35Z,2023-08-29T14:36:35Z,COLLABORATOR,"I got an nsys trace of it happening, and it looks like a join gather can do a lot of work without having the GPU semaphore. I saw all 12 threads doing things, many of them in join gathers without the semaphore held.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697573233/reactions,1,0,0,0,1,0,0,0,0,9130
834,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697576750,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697576750,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697576750,IC_kwDOD7z77c5lLvcu,2023-08-29T14:38:28Z,2023-08-29T14:38:28Z,COLLABORATOR,"I should correct this a bit. It was not all 12 threads. Some of the threads were ""unscheduled"" but I did see some running without the GPU semaphore.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697576750/reactions,0,0,0,0,0,0,0,0,0,9130
835,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697695334,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697695334,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697695334,IC_kwDOD7z77c5lMMZm,2023-08-29T15:38:36Z,2023-08-29T15:38:36Z,COLLABORATOR,"A little more information. The join itself was not the problem, although it showed up in the metrics as a problem because of the semaphore not being grabbed. Most of the time the join didn't even start to do anything. The problem appears to be the parquet writes. When 4 of them would go at once, we have enough memory to do the write, but only if we keep switching it between different threads. But the device synchronize is a very expensive way of doing this. I'm not 100% sure we can fix the problem without some help from the team doing the ASYNC allocator.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697695334/reactions,0,0,0,0,0,0,0,0,0,9130
836,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697722228,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697722228,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697722228,IC_kwDOD7z77c5lMS90,2023-08-29T15:52:34Z,2023-08-29T15:52:34Z,COLLABORATOR,"> But the device synchronize is a very expensive way of doing this. I'm not 100% sure we can fix the problem without some help from the team doing the ASYNC allocator.

Is the time to synchronize not accounted for in the `spillTime` metric? Or do you see that being high as well.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697722228/reactions,0,0,0,0,0,0,0,0,0,9130
837,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697732489,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697732489,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697732489,IC_kwDOD7z77c5lMVeJ,2023-08-29T15:58:12Z,2023-08-29T15:58:12Z,COLLABORATOR,"If I switch the target batch size from 2g to 1g it fixes the problem because it drops the memory pressure (took 100.279 seconds instead of 5+ mins)
If I switch the parallelism to 3 instead of 4 it also mostly fixes the problem (took 131.437 seconds)

Both together is even faster still 96.760 seconds.

Oddly if I drop the total memory allowed to 16 GiB instead of the full 48 available it is still faster to spill to host memory in this case than to continually do the device synchronize (247.655 seconds).
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697732489/reactions,0,0,0,0,0,0,0,0,0,9130
838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697742087,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697742087,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697742087,IC_kwDOD7z77c5lMX0H,2023-08-29T16:03:57Z,2023-08-29T16:03:57Z,COLLABORATOR,"> Is the time to synchronize not accounted for in the `spillTime` metric? Or do you see that being high as well.

Why would it be a part of spillTime?  We don't grab the semaphore except when we are reading back in spilled data. I ""fixed"" the semaphore problem by just grabbing it in hasNext for the join iterator.

```
diff --git a/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala b/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala
index 76f6c1ed9..92003ac46 100644
--- a/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala
+++ b/sql-plugin/src/main/scala/com/nvidia/spark/rapids/AbstractGpuJoinIterator.scala
@@ -85,6 +85,7 @@ abstract class AbstractGpuJoinIterator(
     if (closed) {
       return false
     }
+    GpuSemaphore.acquireIfNecessary(TaskContext.get())
     var mayContinue = true
     while (nextCb.isEmpty && mayContinue) {
       if (gathererStore.exists(!_.isDone)) {
```

This is arguably not the right place to do this because we might grab it in some situations where we don't need to, but it is better than not grabbing it at the right time. I think there are likely a number of other places where we need to grab it too. I'll try to come up with a patch for it generally.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697742087/reactions,0,0,0,0,0,0,0,0,0,9130
839,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697746619,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697746619,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697746619,IC_kwDOD7z77c5lMY67,2023-08-29T16:06:33Z,2023-08-29T16:06:33Z,COLLABORATOR,"I was thinking about the synchronize within this `spillTime` counter:

https://github.com/NVIDIA/spark-rapids/blob/branch-23.10/sql-plugin/src/main/scala/com/nvidia/spark/rapids/DeviceMemoryEventHandler.scala#L120

We start the `spillTime` on line 120, and the synchronize happens on line 138, and we stop the timer on line 170. You were wondering why this time wasn't in the Spark UI metrics, and so I am curious if this is the same synchronize or something different.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697746619/reactions,0,0,0,0,0,0,0,0,0,9130
840,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697998078,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1697998078,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1697998078,IC_kwDOD7z77c5lNWT-,2023-08-29T19:17:20Z,2023-08-29T19:17:20Z,COLLABORATOR,"I don't think that it is the deviceSynchronize that is causing problems. I think that this is the perfect storm in terms of memory allocation. If I disable the device synchronize the performance is much worse. If I try to use arena the performance is much worse. If I use no pooling the performance is not great, but at least it is consistent.

I got an nsys trace of a bad run and a good run (where I tuned the parallelism and batch size).  The CUDA API summary report is quite interesting.

On the bad run `cudaDeviceSynchronize` took 5.3 seconds, and the good run never called it.  But 5 seconds does not explain much about the time taken. I am going to ignore `cudaStramSynchronize_ptsz`, and instead look at the next highest time for the slow run`cudaMallocFromPoolAsync_ptsz_v11020`. On the slow run this took 702.952 seconds vs 28.798 seconds for the good run. What is more the good run actually called it more often, because the batch size is smaller. That is enough to even offset the number or retried calls. So the average time per call changed from 0.0057 ms to 0.1541 ms. That is 27x slower. The other memory related APIs are also similarly slower. I think that as the GPU fills up the allocator has to do more work to find space for an allocation, which slows the allocations down by a lot.

But I am not 100% sure on that. Because it is not just the cuda*Aync memory APIs that are slower. cudaMallocHost is 10x slower. cudaFreeHost is 3x slower.  The kernels are actually a little faster, but not massively so. I think the speedup is because less of them are running, fewer batches. 

It is almost as if interacting with the page table on the GPU from the CPU is the bottleneck. Like we are locking it when we try to do an allocation, and for large GPUs it takes forever to find a free page, and in turn slows all of the other allocations down.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1697998078/reactions,1,1,0,0,0,0,0,0,0,9130
841,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175203,https://github.com/NVIDIA/spark-rapids/issues/9130#issuecomment-1699175203,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9130,1699175203,IC_kwDOD7z77c5lR1sj,2023-08-30T13:26:37Z,2023-08-30T13:26:37Z,COLLABORATOR,"I filed https://github.com/NVIDIA/spark-rapids/issues/9139 to look at and fix the semaphore problem properly.  I honestly don't know how to fix the performance problems here without a major change to how we deal with low memory setups. I don't know if the is something the cuda team can do, or if we need a way to detect that we are in a situation like this and try to throttle down the parallelism like we do with split and retry.

It is also looks like retry might be too aggressive with trying to have threads start to run again, because if I disable the device synchronize and just rely on retry to throttle the threads it is not doing it very well. The performance is worse than the device synchronize, and far worse than running with 3 in parallel to begin with.

Perhaps also there are metrics we could produce to detect this type of situation so the auto-tuner could also help avoid this type of situation. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175203/reactions,0,0,0,0,0,0,0,0,0,9130
842,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698196089,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698196089,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1698196089,IC_kwDOD7z77c5lOGp5,2023-08-29T21:54:03Z,2023-08-29T21:54:03Z,COLLABORATOR,I should point out that this is a pre-existing test (to stress-test coalesced reads in Parquet). Literally 3 years ago: 7ac919b4eea9a40c39bbf583a10bd8f6b19648bb.,,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698196089/reactions,0,0,0,0,0,0,0,0,0,9135
843,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698203774,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698203774,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1698203774,IC_kwDOD7z77c5lOIh-,2023-08-29T22:00:59Z,2023-08-29T22:00:59Z,COLLABORATOR,"I'm wondering if this message has anything to do with the problem:
```
23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 2.0 in stage 5.0 (TID 2015)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20
23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 3.0 in stage 5.0 (TID 2016)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20
23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 0.0 in stage 5.0 (TID 2013)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20
23/08/29 22:00:16 WARN  rapids.MultiFileReaderThreadPool: [Executor task launch worker for task 1.0 in stage 5.0 (TID 2014)]: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20
```",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698203774/reactions,0,0,0,0,0,0,0,0,0,9135
844,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698253520,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698253520,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1698253520,IC_kwDOD7z77c5lOUrQ,2023-08-29T23:02:19Z,2023-08-29T23:13:27Z,COLLABORATOR,"I've done some more digging. I think this might have to do with resetting the thread pool size or something.

Here is an easy repro:
```
spark-shell --jars /home/cloudera/mithunr/spark-rapids/dist/target/rapids-4-spark_2.12-23.10.0-SNAPSHOT-cuda11.jar --conf sparns=com.nvidia.spark.SQLPlugin --conf spark.rapids.sql.explain=ALL --conf spark.kryo.registrator=com.nvidia.spark.rapids.GpuKryoRegistrator --conf fs.defaultFS=""file:///"" --conf spark.rapids.sql.format.parquet.reader.type=COALESCING --conf spark.sql.sources.useV1SourceList="""" --conf spark.sql.files.maxPartitionBytes=1g --master local[1]
```
```scala
// Write corpus.
spark.conf.set(""spark.rapids.sql.enabled"", false)
(0 to 2048).toDF.repartition(2000).write.mode(""overwrite"").parquet(""file:///tmp/myth_parq_ints"")

// Read for boom.
spark.conf.set(""spark.rapids.sql.enabled"", true)
spark.read.parquet(""file:///tmp/myth_parq_ints"").show
```
1. It does not matter if the data resides on HDFS or local disk.
2. `PERFILE` and `MULTITHREADED` readers are fine.  Only `COALESCING` reader is conking.

I'm not yet convinced that this behaviour is specific to CDH.  I wonder if this could be an artifact of having a large number of threads, maybe?
```
$ lscpu | fgrep CPU\(s\) | head -1
CPU(s):                128
```",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698253520/reactions,0,0,0,0,0,0,0,0,0,9135
845,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698575865,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1698575865,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1698575865,IC_kwDOD7z77c5lPjX5,2023-08-30T06:32:12Z,2023-08-30T06:35:44Z,COLLABORATOR,"More experiments:  
1. Reproduced same crash with Apache Spark 3.3.0, on the same node.  Looks to be independent of CDH. 
2. The read succeeds if Spark is started up with `PERFILE` reader, and then changed to `COALESCING`.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1698575865/reactions,0,0,0,0,0,0,0,0,0,9135
846,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699450468,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699450468,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699450468,IC_kwDOD7z77c5lS45k,2023-08-30T16:01:57Z,2023-08-30T16:01:57Z,MEMBER,What is the executor core count set to on the cluster?  Is the test running in local mode or in cluster mode?,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699450468/reactions,0,0,0,0,0,0,0,0,0,9135
847,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699463279,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699463279,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699463279,IC_kwDOD7z77c5lS8Bv,2023-08-30T16:09:47Z,2023-08-30T16:09:47Z,MEMBER,"I think this is related to #9051.  I suspect we were using the incorrect driver core count instead of the (I suspect larger) executor core count when setting up the multithreaded reader pool.  Easiest fix is to give the executors more heap space on the cloudera cluster setup if that's possible.  Long-term fix is budgeting host memory usage.  This initiative is underway for off-heap memory, but this is a heap OOM.  Would be interesting to get a heap dump on OOM to see what's taking up all the heap space.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699463279/reactions,0,0,0,0,0,0,0,0,0,9135
848,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699866015,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699866015,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699866015,IC_kwDOD7z77c5lUeWf,2023-08-30T21:27:16Z,2023-08-30T21:27:16Z,COLLABORATOR,"> What is the executor core count set to on the cluster? Is the test running in local mode or in cluster mode?

I've run it in local mode, for both CDH and Apache Spark. I have set `--master local[1]`.  That should imply a single core, no? 

I've captured a heap-dump on OOM. Analyzing it now.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699866015/reactions,0,0,0,0,0,0,0,0,0,9135
849,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699921748,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699921748,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699921748,IC_kwDOD7z77c5lUr9U,2023-08-30T22:13:18Z,2023-08-30T22:15:28Z,COLLABORATOR,"Heap dump indicates that there are 128 `java.lang.Thread`s, each holding about 8MB Java local `byte[]`.  That exhausts the default 1GB heap in my runs.  IMO this implies that the large core count might be part of the problem.

I see that the `COALESCING` multi-threaded Parquet reader decides to use `128` threads in its memory pool, thereby exhausting the heap.
```
23/08/30 22:09:33 WARN MultiFileReaderThreadPool: Configuring the file reader thread pool with a max of 128 threads instead of spark.rapids.sql.multiThreadedRead.numThreads = 20
```
This is with `spark.executor.cores` not set to anything, probably causing the pool to consume all available CPUs.

There might be a couple of ways around the OOM:
1. As @jlowe suggested already, we could bump the heap for the test.  (In my run, `--driver-memory 8g` worked, even with 128 threads.)
2. We could set `spark.executor.cores` to a specific, smaller value.

I'm inclined to try the latter.  I'll update here with results.

Edit: There might be a longer term solution here, to size the `GpuMultiFileReader`'s thread-pool as a function of both number of cores *and* available memory.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699921748/reactions,0,0,0,0,0,0,0,0,0,9135
850,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699931149,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699931149,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699931149,IC_kwDOD7z77c5lUuQN,2023-08-30T22:21:46Z,2023-08-30T22:21:46Z,MEMBER,"We may want to revisit the 8MB heap buffer that is being used.  This was copied from some existing Parquet reading code and could make sense to produce large, chunky reads, especially for cloud applications, but asking for 8MB when there are hundreds of threads is...problematic.  We may want to automatically scale that buffer size based on the number of threads or simply use a smaller value for the temporary buffer.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699931149/reactions,1,1,0,0,0,0,0,0,0,9135
851,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699954181,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1699954181,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1699954181,IC_kwDOD7z77c5lUz4F,2023-08-30T22:48:56Z,2023-08-30T22:49:26Z,COLLABORATOR,"Ah, it turns out that one can't simply set `spark.executor.cores` for a specific test (`test_small_file_memory`). The Spark application is already up by that point.

One option is to set `spark.executor.cores` in `run_pyspark_from_build.sh`. I don't know if that would be acceptable. I did verify that that allowed this test to run.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699954181/reactions,0,0,0,0,0,0,0,0,0,9135
852,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703309347,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1703309347,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1703309347,IC_kwDOD7z77c5lhnAj,2023-09-01T20:54:11Z,2023-09-01T20:54:11Z,COLLABORATOR,"There is a tentative workaround to get this test to run on the large CDH nodes here:
https://github.com/NVIDIA/spark-rapids/pull/9177.

The more robust fix (to size the Parquet reader's buffers ""appropriately"") may be tackled later on.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703309347/reactions,0,0,0,0,0,0,0,0,0,9135
853,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726637088,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1726637088,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1726637088,IC_kwDOD7z77c5m6mQg,2023-09-19T22:44:37Z,2023-09-19T22:44:37Z,COLLABORATOR,I have raised #9269 and #9271 to mitigate this problem.,,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726637088/reactions,0,0,0,0,0,0,0,0,0,9135
854,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736849150,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1736849150,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1736849150,IC_kwDOD7z77c5nhjb-,2023-09-27T07:25:58Z,2023-09-27T07:48:24Z,COLLABORATOR,"We also saw this case failed OOM constantly while testing in arm environment 
(CI HW specs: GPU: A30, CPU 128 cores which is similar to CDH node)

```
00:28:11      def test_small_file_memory(spark_tmp_path, v1_enabled_list):
00:28:11          # stress the memory usage by creating a lot of small files.
00:28:11          # The more files we combine the more the offsets will be different which will cause
00:28:11          # footer size to change.
00:28:11          # Without the addition of extraMemory in GpuParquetScan this would cause reallocations
00:28:11          # of the host memory buffers.
00:28:11          cols = [string_gen] * 4
00:28:11          gen_list = [('_c' + str(i), gen ) for i, gen in enumerate(cols)]
00:28:11          first_data_path = spark_tmp_path + '/PARQUET_DATA'
00:28:11          with_cpu_session(
00:28:11                  lambda spark : gen_df(spark, gen_list).repartition(2000).write.parquet(first_data_path),
00:28:11                  conf=rebase_write_corrected_conf)
00:28:11          data_path = spark_tmp_path + '/PARQUET_DATA'
00:28:11  >       assert_gpu_and_cpu_are_equal_collect(
00:28:11                  lambda spark : spark.read.parquet(data_path),
00:28:11                  conf={'spark.rapids.sql.format.parquet.reader.type': 'COALESCING',
00:28:11                        'spark.sql.sources.useV1SourceList': v1_enabled_list,
00:28:11                        'spark.sql.files.maxPartitionBytes': ""1g""})
```

```
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 19511.0 failed 1 times, most recent failure: Lost task 3.0 in stage 19511.0 (TID 77495) (verify-pxli-mvn-verify-github-jtzwk-6nhg1 executor driver): java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space

00:28:11  E                   Driver stacktrace:
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
00:28:11  E                   	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
00:28:11  E                   	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
00:28:11  E                   	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
00:28:11  E                   	at scala.Option.foreach(Option.scala:407)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
00:28:11  E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
00:28:11  E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
00:28:11  E                   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
00:28:11  E                   	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
00:28:11  E                   	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
00:28:11  E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
00:28:11  E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)
00:28:11  E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)
00:28:11  E                   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)
00:28:11  E                   	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
00:28:11  E                   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
00:28:11  E                   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
00:28:11  E                   	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
00:28:11  E                   	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
00:28:11  E                   	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
00:28:11  E                   	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
00:28:11  E                   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
00:28:11  E                   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
00:28:11  E                   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
00:28:11  E                   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
00:28:11  E                   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
00:28:11  E                   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
00:28:11  E                   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
00:28:11  E                   	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
00:28:11  E                   	at sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)
00:28:11  E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
00:28:11  E                   	at java.lang.reflect.Method.invoke(Method.java:498)
00:28:11  E                   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
00:28:11  E                   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
00:28:11  E                   	at py4j.Gateway.invoke(Gateway.java:282)
00:28:11  E                   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
00:28:11  E                   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
00:28:11  E                   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
00:28:11  E                   	at java.lang.Thread.run(Thread.java:750)
00:28:11  E                   Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
00:28:11  E                   	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
00:28:11  E                   	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7(GpuMultiFileReader.scala:1216)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$7$adapted(GpuMultiFileReader.scala:1215)
00:28:11  E                   	at scala.collection.Iterator.foreach(Iterator.scala:941)
00:28:11  E                   	at scala.collection.Iterator.foreach$(Iterator.scala:941)
00:28:11  E                   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
00:28:11  E                   	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
00:28:11  E                   	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
00:28:11  E                   	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$4(GpuMultiFileReader.scala:1215)
00:28:11  E                   	at com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:88)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readPartFiles$1(GpuMultiFileReader.scala:1198)
00:28:11  E                   	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.readPartFiles(GpuMultiFileReader.scala:1185)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.$anonfun$readBatch$1(GpuMultiFileReader.scala:1146)
00:28:11  E                   	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.readBatch(GpuMultiFileReader.scala:1125)
00:28:11  E                   	at com.nvidia.spark.rapids.MultiFileCoalescingPartitionReaderBase.next(GpuMultiFileReader.scala:1098)
00:28:11  E                   	at com.nvidia.spark.rapids.PartitionIterator.hasNext(dataSourceUtil.scala:29)
00:28:11  E                   	at com.nvidia.spark.rapids.MetricsBatchIterator.hasNext(dataSourceUtil.scala:46)
00:28:11  E                   	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
00:28:11  E                   	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
00:28:11  E                   	at com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$3(GpuColumnarToRowExec.scala:285)
00:28:11  E                   	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
00:28:11  E                   	at com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:284)
00:28:11  E                   	at com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:257)
00:28:11  E                   	at com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:301)
00:28:11  E                   	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
00:28:11  E                   	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
00:28:11  E                   	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
00:28:11  E                   	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
00:28:11  E                   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
00:28:11  E                   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
00:28:11  E                   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
00:28:11  E                   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
00:28:11  E                   	at org.apache.spark.scheduler.Task.run(Task.scala:131)
00:28:11  E                   	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
00:28:11  E                   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
00:28:11  E                   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
00:28:11  E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
00:28:11  E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
00:28:11  E                   	... 1 more
00:28:11  E                   Caused by: java.lang.OutOfMemoryError: Java heap space
```

I will re-verify after #9269 #9271 are resolved",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736849150/reactions,1,1,0,0,0,0,0,0,0,9135
855,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1738399296,https://github.com/NVIDIA/spark-rapids/issues/9135#issuecomment-1738399296,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9135,1738399296,IC_kwDOD7z77c5nnd5A,2023-09-28T04:00:55Z,2023-09-28T04:00:55Z,COLLABORATOR,"It stands to reason that it fails.  :/ High core count, with low heap allocation. ",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1738399296/reactions,0,0,0,0,0,0,0,0,0,9135
856,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175388,https://github.com/NVIDIA/spark-rapids/issues/9139#issuecomment-1699175388,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9139,1699175388,IC_kwDOD7z77c5lR1vc,2023-08-30T13:26:44Z,2023-08-30T13:26:44Z,COLLABORATOR,"> We have code in SpillableColumnarBatch to grab the semaphore before reading the data back to the GPU. But that is not enough. It does not happen when we acquire a batch that did not spill.

So we have code that doesn't call `getColumnarBatch`? Because we make sure we get the semaphore even if the `RapidsBuffer` didn't spill.
https://github.com/NVIDIA/spark-rapids/blob/branch-23.10/sql-plugin/src/main/scala/com/nvidia/spark/rapids/SpillableColumnarBatch.scala#L111

Now `LazySpillableColumnarBatch` that one will definitely not get the semaphore, unless you made sure to call `allowSpilling`.

That said, it would be fairly easy to add a call to grab the semaphore during the retry, it seems like a good place to handle it for all callers.
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699175388/reactions,0,0,0,0,0,0,0,0,0,9139
857,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699178177,https://github.com/NVIDIA/spark-rapids/issues/9139#issuecomment-1699178177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9139,1699178177,IC_kwDOD7z77c5lR2bB,2023-08-30T13:28:22Z,2023-08-30T13:28:22Z,COLLABORATOR,Then perhaps it is just join that is not grabbing the semaphore.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1699178177/reactions,0,0,0,0,0,0,0,0,0,9139
858,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1709045846,https://github.com/NVIDIA/spark-rapids/issues/9156#issuecomment-1709045846,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9156,1709045846,IC_kwDOD7z77c5l3fhW,2023-09-06T20:19:00Z,2023-09-06T20:19:00Z,COLLABORATOR,"Need to document limitation for lz4raw compression.  File issues for downstream teams for support.

Ref: https://issues.apache.org/jira/browse/PARQUET-2032.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1709045846/reactions,0,0,0,0,0,0,0,0,0,9156
859,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2029906521,https://github.com/NVIDIA/spark-rapids/issues/9156#issuecomment-2029906521,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9156,2029906521,IC_kwDOD7z77c54_epZ,2024-04-01T15:04:40Z,2024-04-01T15:04:40Z,COLLABORATOR,Added an issue in cuda-hpc-libraries project in gitlab-master ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2029906521/reactions,0,0,0,0,0,0,0,0,0,9156
860,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1701619543,https://github.com/NVIDIA/spark-rapids/issues/9160#issuecomment-1701619543,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9160,1701619543,IC_kwDOD7z77c5lbKdX,2023-08-31T19:13:31Z,2023-08-31T19:13:31Z,COLLABORATOR,We need to make sure that the docs are clear about the default and they if they set the target batch size they should also set the parallelism because it can be problematic.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1701619543/reactions,0,0,0,0,0,0,0,0,0,9160
861,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1702798722,https://github.com/NVIDIA/spark-rapids/issues/9161#issuecomment-1702798722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9161,1702798722,IC_kwDOD7z77c5lfqWC,2023-09-01T14:02:16Z,2023-09-01T14:02:16Z,COLLABORATOR,"I'm not sure how critical this is. We will fall back to the CPU if we see this happen, and ShuffledHashJoins are off by default in Spark, so it is more likely to be a sort merge join that we see, and because we don't have any info about the side of the join, we just pick a build side that works for us.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1702798722/reactions,0,0,0,0,0,0,0,0,0,9161
862,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1702831275,https://github.com/NVIDIA/spark-rapids/issues/9162#issuecomment-1702831275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9162,1702831275,IC_kwDOD7z77c5lfySr,2023-09-01T14:23:37Z,2023-09-01T14:23:37Z,NONE,I can fix this bug. Can you assign this to me?,,Sweetdevil144,117591942,U_kgDOBwJPhg,https://avatars.githubusercontent.com/u/117591942?v=4,,https://api.github.com/users/Sweetdevil144,https://github.com/Sweetdevil144,https://api.github.com/users/Sweetdevil144/followers,https://api.github.com/users/Sweetdevil144/following{/other_user},https://api.github.com/users/Sweetdevil144/gists{/gist_id},https://api.github.com/users/Sweetdevil144/starred{/owner}{/repo},https://api.github.com/users/Sweetdevil144/subscriptions,https://api.github.com/users/Sweetdevil144/orgs,https://api.github.com/users/Sweetdevil144/repos,https://api.github.com/users/Sweetdevil144/events{/privacy},https://api.github.com/users/Sweetdevil144/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1702831275/reactions,0,0,0,0,0,0,0,0,0,9162
863,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703223603,https://github.com/NVIDIA/spark-rapids/issues/9162#issuecomment-1703223603,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9162,1703223603,IC_kwDOD7z77c5lhSEz,2023-09-01T19:20:42Z,2023-09-01T19:20:42Z,NONE,"some test are failing can you tell the reason
",,shivamxsurya,47709579,MDQ6VXNlcjQ3NzA5NTc5,https://avatars.githubusercontent.com/u/47709579?v=4,,https://api.github.com/users/shivamxsurya,https://github.com/shivamxsurya,https://api.github.com/users/shivamxsurya/followers,https://api.github.com/users/shivamxsurya/following{/other_user},https://api.github.com/users/shivamxsurya/gists{/gist_id},https://api.github.com/users/shivamxsurya/starred{/owner}{/repo},https://api.github.com/users/shivamxsurya/subscriptions,https://api.github.com/users/shivamxsurya/orgs,https://api.github.com/users/shivamxsurya/repos,https://api.github.com/users/shivamxsurya/events{/privacy},https://api.github.com/users/shivamxsurya/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703223603/reactions,0,0,0,0,0,0,0,0,0,9162
864,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703241886,https://github.com/NVIDIA/spark-rapids/issues/9162#issuecomment-1703241886,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9162,1703241886,IC_kwDOD7z77c5lhWie,2023-09-01T19:39:58Z,2023-09-01T19:39:58Z,NONE,"https://github.com/NVIDIA/spark-rapids/pull/9174     review on this
",,shivamxsurya,47709579,MDQ6VXNlcjQ3NzA5NTc5,https://avatars.githubusercontent.com/u/47709579?v=4,,https://api.github.com/users/shivamxsurya,https://github.com/shivamxsurya,https://api.github.com/users/shivamxsurya/followers,https://api.github.com/users/shivamxsurya/following{/other_user},https://api.github.com/users/shivamxsurya/gists{/gist_id},https://api.github.com/users/shivamxsurya/starred{/owner}{/repo},https://api.github.com/users/shivamxsurya/subscriptions,https://api.github.com/users/shivamxsurya/orgs,https://api.github.com/users/shivamxsurya/repos,https://api.github.com/users/shivamxsurya/events{/privacy},https://api.github.com/users/shivamxsurya/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1703241886/reactions,0,0,0,0,0,0,0,0,0,9162
865,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1706948274,https://github.com/NVIDIA/spark-rapids/issues/9178#issuecomment-1706948274,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9178,1706948274,IC_kwDOD7z77c5lvfay,2023-09-05T16:23:31Z,2023-09-05T16:23:31Z,COLLABORATOR,unfortunately most of these are written as a replacement that calls a static java method instead of actually making a new expression with codegen/etc.  This makes it especially difficult to replace even if we wanted to.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1706948274/reactions,0,0,0,0,0,0,0,0,0,9178
866,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1707205947,https://github.com/NVIDIA/spark-rapids/issues/9179#issuecomment-1707205947,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9179,1707205947,IC_kwDOD7z77c5lweU7,2023-09-05T19:36:50Z,2023-09-05T19:36:50Z,COLLABORATOR,I tried to reproduce it today and could not.  Will keep trying periodically.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1707205947/reactions,0,0,0,0,0,0,0,0,0,9179
867,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1793277595,https://github.com/NVIDIA/spark-rapids/issues/9201#issuecomment-1793277595,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9201,1793277595,IC_kwDOD7z77c5q4z6b,2023-11-04T00:56:08Z,2023-11-04T00:57:06Z,COLLABORATOR,"After looking further into this, I don't think we should make this change in the plugin side.

The ""SQL func"" column in file `supportedExpr.csv` is generated based on a map called [expressions](https://github.com/apache/spark/blob/28961a6ce001e0c25c780a39a726fdd825542cee/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L357) in `FunctionRegistry.scala` in the Apache Spark Github repo, which is a mapping from expressions to SQL functions. Since we rely on this map as the ground truth for the ""SQL func"" column, we should not modify the meaning of this map by adding more entries into it.

For instance, `promote_precision` is not a SQL function and therefore it should not have a ""SQL func"" column by definition. In order for the tools repo to recognize supported expressions correctly, one approach would be to add another file/column to store the expressions and their corresponding names (that show up in event logs), instead of changing the ""SQL func"" column.

cc: @mattahrens ",,cindyyuanjiang,47068112,MDQ6VXNlcjQ3MDY4MTEy,https://avatars.githubusercontent.com/u/47068112?v=4,,https://api.github.com/users/cindyyuanjiang,https://github.com/cindyyuanjiang,https://api.github.com/users/cindyyuanjiang/followers,https://api.github.com/users/cindyyuanjiang/following{/other_user},https://api.github.com/users/cindyyuanjiang/gists{/gist_id},https://api.github.com/users/cindyyuanjiang/starred{/owner}{/repo},https://api.github.com/users/cindyyuanjiang/subscriptions,https://api.github.com/users/cindyyuanjiang/orgs,https://api.github.com/users/cindyyuanjiang/repos,https://api.github.com/users/cindyyuanjiang/events{/privacy},https://api.github.com/users/cindyyuanjiang/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1793277595/reactions,1,1,0,0,0,0,0,0,0,9201
868,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716399850,https://github.com/NVIDIA/spark-rapids/issues/9205#issuecomment-1716399850,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9205,1716399850,IC_kwDOD7z77c5mTi7q,2023-09-12T20:40:51Z,2023-09-12T20:40:51Z,COLLABORATOR,Proposed scope: LocalTableScanExec should be ignored for displaying what cannot run on GPU.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1716399850/reactions,0,0,0,0,0,0,0,0,0,9205
869,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1813090148,https://github.com/NVIDIA/spark-rapids/issues/9205#issuecomment-1813090148,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9205,1813090148,IC_kwDOD7z77c5sEY9k,2023-11-15T18:56:54Z,2023-11-15T18:56:54Z,COLLABORATOR,"We could add all metastore operations here as well.
```sql
CREATE TABLE foobar AS SELECT ...
```
Even if everything in the `SELECT` runs on the GPU, the actual table creation will still be flagged as `NOT_ON_GPU`.
This holds for table/partition addition/modification/deletion.
This might also hold for certain parts of delta-table operations.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1813090148/reactions,0,0,0,0,0,0,0,0,0,9205
870,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1813334484,https://github.com/NVIDIA/spark-rapids/issues/9205#issuecomment-1813334484,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9205,1813334484,IC_kwDOD7z77c5sFUnU,2023-11-15T22:04:14Z,2023-11-15T22:04:14Z,MEMBER,"> We could add all metastore operations here as well.

Note that we already explicitly do not complain about a number of ""auxiliary"" SQL operations.  See the use of GpuOverrides.neverReplaceExec, e.g.: https://github.com/NVIDIA/spark-rapids/blob/branch-23.12/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuOverrides.scala#L4277",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1813334484/reactions,1,1,0,0,0,0,0,0,0,9205
871,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1743324110,https://github.com/NVIDIA/spark-rapids/issues/9210#issuecomment-1743324110,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9210,1743324110,IC_kwDOD7z77c5n6QPO,2023-10-02T16:20:15Z,2023-10-02T16:20:15Z,COLLABORATOR,Fix targeted for tuning guide on docs.nvidia.com: https://docs.nvidia.com/spark-rapids/user-guide/latest/tuning-guide.html,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1743324110/reactions,0,0,0,0,0,0,0,0,0,9210
872,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1749700194,https://github.com/NVIDIA/spark-rapids/issues/9210#issuecomment-1749700194,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9210,1749700194,IC_kwDOD7z77c5oSk5i,2023-10-05T21:48:47Z,2023-10-05T21:48:47Z,COLLABORATOR,"Do you know if the example where `readFsTime` was larger than `buffer time` and `scan total time` was with ORC data?  It looks like our logic for the `readFsTime` and `buffer time` metric is different in our ORC scan versus Parquet and Avro.  In Parquet and Avro, it is clear that the `buffer time` metric is wrapped around a function where `readFsTime` and `write buffer time` are calculated.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1749700194/reactions,0,0,0,0,0,0,0,0,0,9210
873,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1750706697,https://github.com/NVIDIA/spark-rapids/issues/9210#issuecomment-1750706697,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9210,1750706697,IC_kwDOD7z77c5oWaoJ,2023-10-06T13:44:03Z,2023-10-06T13:44:03Z,COLLABORATOR,"I don't recall which format this was unfortunately. Sorry. But what you found is ringing bells.

There's a lot of variation on how the metrics are used, especially with the multi-threaded readers. I think we want to reset a bit and make sure each metric is doing what we expect for all formats.
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1750706697/reactions,0,0,0,0,0,0,0,0,0,9210
874,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1715906279,https://github.com/NVIDIA/spark-rapids/issues/9219#issuecomment-1715906279,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9219,1715906279,IC_kwDOD7z77c5mRqbn,2023-09-12T15:06:03Z,2023-09-12T15:06:03Z,COLLABORATOR,"This could end up being kind of difficult because `M` in java changes based on the locale that is set. But looking at the code here it looks like that locale might be hardcoded to `Locale.US`, so we might be okay here. But we need to dig in deeply to all of the versions that we support to be sure that is correct. We also need to add in tests that set the JVM to a different local so we can be sure that we cover this and can detect if something goes wrong.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1715906279/reactions,0,0,0,0,0,0,0,0,0,9219
875,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783224564,https://github.com/NVIDIA/spark-rapids/issues/9219#issuecomment-1783224564,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9219,1783224564,IC_kwDOD7z77c5qSdj0,2023-10-27T16:53:49Z,2023-10-27T16:53:49Z,COLLABORATOR,"Also this needs cudf work, as the current cudf's timestamp parser only processes numeric based timestamp.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783224564/reactions,0,0,0,0,0,0,0,0,0,9219
876,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724771107,https://github.com/NVIDIA/spark-rapids/issues/9226#issuecomment-1724771107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9226,1724771107,IC_kwDOD7z77c5mzesj,2023-09-19T03:15:43Z,2023-09-19T03:15:43Z,COLLABORATOR,"Discussed a little about this issue with Peixin.
The original #9213 issue was caused because the default jar of the DB's aggregate jar was changed to empty content, but the deploy script was to deploy the default jar. 
I think we don't want to do all the same things in nightly pipeline to the premerge pipeline.
The premerge pipeline is required to be fast enough to run. So currently we run the parrallel builds on DB and the other shims. If we change as nightly pipeline, it'll require some sequential builds, which spend much time.
So, the problem seems the premerge doesn't verify what deploy.sh does and what the last packaging in dist needs.
I wonder if we can create a validation script to check the aggregate jar(which is used to deploy in deploy.sh) about what the minimum requirement in the dist packaging step, for example, the properties file should exist. And integrate this script into the profile for each aggregate jar or added into the premerge build script. So that we don't do the real deployment, but still check the requirement from deployment point of view.",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1724771107/reactions,0,0,0,0,0,0,0,0,0,9226
877,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736089171,https://github.com/NVIDIA/spark-rapids/issues/9226#issuecomment-1736089171,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9226,1736089171,IC_kwDOD7z77c5nep5T,2023-09-26T18:43:12Z,2023-09-26T18:43:12Z,COLLABORATOR,"> The original #9213 issue was caused because the default jar of the DB's aggregate jar was changed to empty content, but the deploy script was to deploy the default jar.

This already points at the deficiency that there is no single source of truth for what modules require deployment.

> I think we don't want to do all the same things in nightly pipeline to the premerge pipeline.
> The premerge pipeline is required to be fast enough to run. So currently we run the parrallel builds on DB and the other shims. If we change as nightly pipeline, it'll require some sequential builds, which spend much time.

Think of this as Spark or `buildall`. We should have a single way of coding the build flow logic. The speed should be determined  just by a scalability tunables. This includes the list of shims required for the given pipeline instance, and the degree of parallelism available for that pipeline instance.   

> So, the problem seems the premerge doesn't verify what deploy.sh does and what the last packaging in dist needs.
> I wonder if we can create a validation script to check the aggregate jar(which is used to deploy in deploy.sh) about what the minimum requirement in the dist packaging step, for example, the properties file should exist. And integrate this script into the profile for each aggregate jar or added into the premerge build script. So that we don't do the real deployment, but still check the requirement from deployment point of view.

Addressing specific regressions does not take us to a significantly higher level of confidence offered by the request in this issue if the build logic in the nightly and pre-merge continue to diverge.

Once the pre-merge passed we should only worry about test failures from the set of tests ""nightly minus pre-merge"", but not about whether the nightly jar build is working at all. 
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736089171/reactions,0,0,0,0,0,0,0,0,0,9226
878,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1730017525,https://github.com/NVIDIA/spark-rapids/issues/9241#issuecomment-1730017525,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9241,1730017525,IC_kwDOD7z77c5nHfj1,2023-09-21T17:34:07Z,2023-09-21T17:34:07Z,COLLABORATOR,The init_cap issue is something that CUDF says that they will fix.  All of the others have had fixes merged in.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1730017525/reactions,0,0,0,0,0,0,0,0,0,9241
879,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726389629,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1726389629,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1726389629,IC_kwDOD7z77c5m5p19,2023-09-19T19:59:37Z,2023-09-19T19:59:37Z,COLLABORATOR,This looks like we found a character that we do not translate to upper case properly. I'll try to do some more digging here.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726389629/reactions,0,0,0,0,0,0,0,0,0,9247
880,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728263523,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1728263523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1728263523,IC_kwDOD7z77c5nAzVj,2023-09-20T18:49:54Z,2023-09-20T18:49:54Z,COLLABORATOR,So the characters in the output do not match the characters listed in the regexp. I don't think sre_yield is doing what we want/expect here and going off of bytes instead of characters. Need to dig in a little bit more to understand if this is expected or not.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728263523/reactions,0,0,0,0,0,0,0,0,0,9247
881,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728291144,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1728291144,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1728291144,IC_kwDOD7z77c5nA6FI,2023-09-20T19:12:23Z,2023-09-20T19:12:23Z,COLLABORATOR,"Okay, now I am even more confused. I added in upper and lower in addition to initcap just to see what would happen, and the CPU is doing something rather odd.

```
-Row(a='ß«¾îí\x87\x04Av~', IC='ß«¾îí\x87\x04av~', U='SS«¾ÎÍ\x87\x04AV~', L='ß«¾îí\x87\x04av~')
+Row(a='ß«¾îí\x87\x04Av~', IC='SS«¾îí\x87\x04av~', U='SS«¾ÎÍ\x87\x04AV~', L='ß«¾îí\x87\x04av~')
```

We are consistent and make ß upper case as the first character, but the CPU keeps it lower case and I really want to understand why...",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728291144/reactions,0,0,0,0,0,0,0,0,0,9247
882,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728308405,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1728308405,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1728308405,IC_kwDOD7z77c5nA-S1,2023-09-20T19:27:03Z,2023-09-20T19:27:03Z,COLLABORATOR,"OK on further digging this is a real bug in our code. title case and upper case are not the same thing. We are converting characters to upper case and lower case using the cudf strings::capitalize function.  But it converts the values to upper case, which at least in the case of ß is not the same as title case. We are likely going to need some help from cudf to make this work properly.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728308405/reactions,0,0,0,0,0,0,0,0,0,9247
883,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728309818,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1728309818,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1728309818,IC_kwDOD7z77c5nA-o6,2023-09-20T19:28:17Z,2023-09-20T19:28:17Z,COLLABORATOR,https://unicode.org/faq/casemap_charprop.html#4,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728309818/reactions,0,0,0,0,0,0,0,0,0,9247
884,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728313342,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1728313342,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1728313342,IC_kwDOD7z77c5nA_f-,2023-09-20T19:31:25Z,2023-09-20T19:31:25Z,COLLABORATOR,https://github.com/rapidsai/cudf/issues/14144,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728313342/reactions,0,0,0,0,0,0,0,0,0,9247
885,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1730018205,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1730018205,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1730018205,IC_kwDOD7z77c5nHfud,2023-09-21T17:34:40Z,2023-09-21T17:34:40Z,COLLABORATOR,CUDF agreed to fix the issue so I will keep this assigned to me to verify the fix.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1730018205/reactions,0,0,0,0,0,0,0,0,0,9247
886,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834244172,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1834244172,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1834244172,IC_kwDOD7z77c5tVFhM,2023-11-30T17:33:19Z,2023-11-30T17:33:19Z,MEMBER,"Saw another instance of this with a different seed:
```
[2023-11-29T23:15:09.589Z] FAILED ../../../../integration_tests/src/main/python/string_test.py::test_initcap[DATAGEN_SEED=1701294231, INCOMPAT] - AssertionError: GPU and CPU string values are different at [1854, 'initcap(a)']
```",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834244172/reactions,0,0,0,0,0,0,0,0,0,9247
887,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849261085,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1849261085,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1849261085,IC_kwDOD7z77c5uOXwd,2023-12-11T03:18:45Z,2023-12-11T03:18:45Z,COLLABORATOR,moved to 24.02,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849261085/reactions,0,0,0,0,0,0,0,0,0,9247
888,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904834104,https://github.com/NVIDIA/spark-rapids/issues/9247#issuecomment-1904834104,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9247,1904834104,IC_kwDOD7z77c5xiXY4,2024-01-22T21:17:45Z,2024-01-22T21:17:45Z,COLLABORATOR,"Just for reference there are a number of characters that do not match between the CPU and the GPU. It is not too hard to do an exhaustive check in scala.

```
(0 to 65536).map(_.toChar.toString).toDF(""a"").repartition(1).selectExpr(""a"", ""initCap(a) as b"")
```

This showed 265 characters where we got the wrong answer. The code point values for these are.

```
(223, 304, 329, 452, 454, 455, 457, 458, 460, 496, 497, 499, 604, 609, 618, 620, 642, 647, 669, 670, 912, 944, 1011, 1012, 1321, 1323, 1325, 1327, 1415, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4349, 4350, 4351, 5112, 5113, 5114, 5115, 5116, 5117, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7566, 7830, 7831, 7832, 7833, 7834, 7838, 8016, 8018, 8020, 8022, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8114, 8115, 8116, 8118, 8119, 8130, 8131, 8132, 8134, 8135, 8146, 8147, 8150, 8151, 8162, 8163, 8164, 8166, 8167, 8178, 8179, 8180, 8182, 8183, 8486, 8490, 8491, 42649, 42651, 42900, 42903, 42905, 42907, 42909, 42911, 42933, 42935, 42937, 42939, 42941, 42943, 42947, 43859, 43888, 43889, 43890, 43891, 43892, 43893, 43894, 43895, 43896, 43897, 43898, 43899, 43900, 43901, 43902, 43903, 43904, 43905, 43906, 43907, 43908, 43909, 43910, 43911, 43912, 43913, 43914, 43915, 43916, 43917, 43918, 43919, 43920, 43921, 43922, 43923, 43924, 43925, 43926, 43927, 43928, 43929, 43930, 43931, 43932, 43933, 43934, 43935, 43936, 43937, 43938, 43939, 43940, 43941, 43942, 43943, 43944, 43945, 43946, 43947, 43948, 43949, 43950, 43951, 43952, 43953, 43954, 43955, 43956, 43957, 43958, 43959, 43960, 43961, 43962, 43963, 43964, 43965, 43966, 43967, 64256, 64257, 64258, 64259, 64260, 64261, 64262, 64265, 64266, 64267, 64268, 64269, 64275, 64276, 64277, 64278, 64279)
```",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904834104/reactions,0,0,0,0,0,0,0,0,0,9247
889,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726360985,https://github.com/NVIDIA/spark-rapids/issues/9265#issuecomment-1726360985,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9265,1726360985,IC_kwDOD7z77c5m5i2Z,2023-09-19T19:36:23Z,2023-09-19T19:36:23Z,COLLABORATOR,Consider having these metrics behind a debug flag.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1726360985/reactions,0,0,0,0,0,0,0,0,0,9265
890,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728014963,https://github.com/NVIDIA/spark-rapids/issues/9268#issuecomment-1728014963,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9268,1728014963,IC_kwDOD7z77c5m_2pz,2023-09-20T16:00:04Z,2023-09-20T16:00:04Z,COLLABORATOR,Note that tasks already have this exposed as `BYTES_READ`. In the event log we should find: `totalBytesRead` as the per task amount. @mattahrens this is not in the SQL tab but it is in the task metrics (stage page). ,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1728014963/reactions,0,0,0,0,0,0,0,0,0,9268
891,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1749088212,https://github.com/NVIDIA/spark-rapids/issues/9268#issuecomment-1749088212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9268,1749088212,IC_kwDOD7z77c5oQPfU,2023-10-05T15:02:21Z,2023-10-05T15:45:22Z,COLLABORATOR,"I see this in an event log at the task-level: 
```
""Input Metrics"":{""Bytes Read"":236628968,""Records Read"":10000}
```",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1749088212/reactions,0,0,0,0,0,0,0,0,0,9268
892,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736246554,https://github.com/NVIDIA/spark-rapids/issues/9275#issuecomment-1736246554,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9275,1736246554,IC_kwDOD7z77c5nfQUa,2023-09-26T20:28:47Z,2023-09-26T20:28:47Z,COLLABORATOR,"We may not consider this a high priority since Spark does not have a stable sort in all cases.  For example, https://issues.apache.org/jira/browse/SPARK-45243 . ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736246554/reactions,0,0,0,0,0,0,0,0,0,9275
893,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735657282,https://github.com/NVIDIA/spark-rapids/issues/9300#issuecomment-1735657282,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9300,1735657282,IC_kwDOD7z77c5ndAdC,2023-09-26T14:26:38Z,2023-09-26T14:26:38Z,COLLABORATOR,This is very similar to https://github.com/NVIDIA/spark-rapids/issues/9301,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735657282/reactions,0,0,0,0,0,0,0,0,0,9300
894,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735659939,https://github.com/NVIDIA/spark-rapids/issues/9301#issuecomment-1735659939,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9301,1735659939,IC_kwDOD7z77c5ndBGj,2023-09-26T14:27:58Z,2023-09-26T14:27:58Z,COLLABORATOR,Note that we need to also have/do a reduction if there are no partition by columns to make this work properly.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735659939/reactions,0,0,0,0,0,0,0,0,0,9301
895,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735682177,https://github.com/NVIDIA/spark-rapids/issues/9301#issuecomment-1735682177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9301,1735682177,IC_kwDOD7z77c5ndGiB,2023-09-26T14:39:13Z,2023-09-26T14:39:13Z,COLLABORATOR,"Also we should implement at least one operation to make sure that it is working. COUNT, MIN, or MAX would probably be the simplest from #9303, but FIRST or LAST from #9302 would also work.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1735682177/reactions,0,0,0,0,0,0,0,0,0,9301
896,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736225055,https://github.com/NVIDIA/spark-rapids/issues/9302#issuecomment-1736225055,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9302,1736225055,IC_kwDOD7z77c5nfLEf,2023-09-26T20:14:10Z,2023-09-26T20:14:10Z,COLLABORATOR,"Actually the regular window operation appears to be fairly good already, so this may not be needed.

```
spark.time(spark.range(1, 20000000L, 1, 1).selectExpr(""id"", ""(id * 0) + 1 as p"", ""id DIV 100 as v"").selectExpr(""*"", ""FIRST(v) over (partition by p order by id RANGE between unbounded preceding and current row) as m"").orderBy(desc(""m""), desc(""id"")).show())
```

took 289 ms

```
spark.time(spark.range(1, 20000000L, 1, 1).selectExpr(""id"", ""(id * 0) + 1 as p"", ""id DIV 100 as v"").groupBy(""p"").agg(first(col(""v"")).alias(""m"")).show())
```

took 123 ms but it also didn't have to do the sorting/top n operations that the window one did, and that appeared to account for about 100 ms of the 289 ms, so it is likely to be very very close to what we have toady.
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1736225055/reactions,0,0,0,0,0,0,0,0,0,9302
897,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1776794080,https://github.com/NVIDIA/spark-rapids/issues/9332#issuecomment-1776794080,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9332,1776794080,IC_kwDOD7z77c5p57ng,2023-10-24T08:53:03Z,2023-10-24T08:53:03Z,COLLABORATOR,Do we also need to remove Alluxio related code?,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1776794080/reactions,0,0,0,0,0,0,0,0,0,9332
898,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777952590,https://github.com/NVIDIA/spark-rapids/issues/9332#issuecomment-1777952590,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9332,1777952590,IC_kwDOD7z77c5p-WdO,2023-10-24T20:03:34Z,2023-10-24T20:03:34Z,COLLABORATOR,"Yes, let's also remove Alluxio code.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777952590/reactions,0,0,0,0,0,0,0,0,0,9332
899,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1740980442,https://github.com/NVIDIA/spark-rapids/issues/9337#issuecomment-1740980442,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9337,1740980442,IC_kwDOD7z77c5nxUDa,2023-09-29T14:25:37Z,2023-09-29T14:25:37Z,COLLABORATOR,"This is very similar to https://github.com/NVIDIA/spark-rapids/issues/8831 we might also want to look into window dropping columns. I know that it can happen, especially if the column is just used to create the window and does not need to continue afterwards. I don't think window would be a performance improvement. We are not going to gather the data for window. But it would be a potential memory reduction by dropping the column slightly earlier. But maybe not that big of a win.

The only other operator I could think of that might drop a column after it is used and we could avoid a gather is hash aggregate, but that would require changes to CUDF, and I don't think it is that common of an operation.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1740980442/reactions,0,0,0,0,0,0,0,0,0,9337
900,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745738619,https://github.com/NVIDIA/spark-rapids/issues/9340#issuecomment-1745738619,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9340,1745738619,IC_kwDOD7z77c5oDdt7,2023-10-03T21:16:14Z,2023-10-03T21:16:14Z,COLLABORATOR,Explanation should explicitly say: `cannot run on GPU because <user-value> is not supported; only literal 10 or 16...`,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745738619/reactions,0,0,0,0,0,0,0,0,0,9340
901,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1741030273,https://github.com/NVIDIA/spark-rapids/issues/9343#issuecomment-1741030273,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9343,1741030273,IC_kwDOD7z77c5nxgOB,2023-09-29T14:59:28Z,2023-09-29T14:59:28Z,COLLABORATOR,Note that this is probably not a high severity bug because double/float format_number is off by default.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1741030273/reactions,0,0,0,0,0,0,0,0,0,9343
902,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832751679,https://github.com/NVIDIA/spark-rapids/issues/9350#issuecomment-1832751679,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9350,1832751679,IC_kwDOD7z77c5tPZI_,2023-11-29T21:47:12Z,2023-11-29T21:47:12Z,MEMBER,"For the single precision floating point case, https://github.com/rapidsai/cudf/issues/14528 is related.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832751679/reactions,0,0,0,0,0,0,0,0,0,9350
903,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1760971171,https://github.com/NVIDIA/spark-rapids/issues/9356#issuecomment-1760971171,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9356,1760971171,IC_kwDOD7z77c5o9kmj,2023-10-13T06:25:32Z,2023-10-13T06:25:32Z,COLLABORATOR,potential dupe of #8759 ,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1760971171/reactions,0,0,0,0,0,0,0,0,0,9356
904,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745788228,https://github.com/NVIDIA/spark-rapids/issues/9370#issuecomment-1745788228,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9370,1745788228,IC_kwDOD7z77c5oDp1E,2023-10-03T21:55:48Z,2023-10-03T21:55:48Z,COLLABORATOR,"As part of this, we can start compiling on CUDA 12.2 (R535).",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1745788228/reactions,0,0,0,0,0,0,0,0,0,9370
905,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1754284219,https://github.com/NVIDIA/spark-rapids/issues/9370#issuecomment-1754284219,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9370,1754284219,IC_kwDOD7z77c5okEC7,2023-10-10T03:21:57Z,2023-10-10T03:34:32Z,COLLABORATOR,"Hi @jlowe, just to double confirm, even after we build JNI on cuda 12.2, for this core dump feature, if it's only supported from Drivers that are CUDA 12.1+, your code will automatically detect the driver version like 12.0.x then disable the feature. Make sure no failure on the old Driver with CUDA 12.0.x. Am I right?",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1754284219/reactions,0,0,0,0,0,0,0,0,0,9370
906,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1755455587,https://github.com/NVIDIA/spark-rapids/issues/9370#issuecomment-1755455587,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9370,1755455587,IC_kwDOD7z77c5ooiBj,2023-10-10T13:42:08Z,2023-10-10T13:42:08Z,MEMBER,"> Make sure no failure on the old Driver with CUDA 12.0.x. Am I right?

Yes.  We will need a test pipeline against a CUDA 12.0 driver to help verify there are no regressions there.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1755455587/reactions,0,0,0,0,0,0,0,0,0,9370
907,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756554571,https://github.com/NVIDIA/spark-rapids/issues/9370#issuecomment-1756554571,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9370,1756554571,IC_kwDOD7z77c5osuVL,2023-10-11T01:00:33Z,2023-10-11T01:03:13Z,COLLABORATOR,"> > Make sure no failure on the old Driver with CUDA 12.0.x. Am I right?
> 
> Yes. We will need a test pipeline against a CUDA 12.0 driver to help verify there are no regressions there.

Thanks for the clarification! 
Can we also add some flags for CI to mark those cases that should be verified in older drivers? This would help save much resources and time by enabling tests only w/ specific labels and no need to run others",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756554571/reactions,0,0,0,0,0,0,0,0,0,9370
908,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989431777,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-1989431777,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,1989431777,IC_kwDOD7z77c52lFHh,2024-03-11T20:57:48Z,2024-03-11T20:57:48Z,COLLABORATOR,"@mattahrens @sameerz 
How important do you guys think this feature is for 24.04? ",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989431777/reactions,0,0,0,0,0,0,0,0,0,9375
909,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992162779,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-1992162779,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,1992162779,IC_kwDOD7z77c52vf3b,2024-03-12T17:14:05Z,2024-03-12T17:14:05Z,COLLABORATOR,"I presume the main driver for connect will be support for an LTS of Databricks 14 https://docs.databricks.com/en/release-notes/runtime/14.0.html. 14.3 LTS was released on Feb 1 https://docs.databricks.com/en/release-notes/runtime/index.html
",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992162779/reactions,0,0,0,0,0,0,0,0,0,9375
910,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992166335,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-1992166335,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,1992166335,IC_kwDOD7z77c52vgu_,2024-03-12T17:16:05Z,2024-03-12T17:16:05Z,COLLABORATOR,What about Apache Spark 3.5.1? Since 14.3 is branching off of it.,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992166335/reactions,0,0,0,0,0,0,0,0,0,9375
911,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992177357,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-1992177357,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,1992177357,IC_kwDOD7z77c52vjbN,2024-03-12T17:21:55Z,2024-03-12T17:21:55Z,COLLABORATOR,"> What about Apache Spark 3.5.1? Since 14.3 is branching off of it.

Upstream users have  a choice, whereas it does not look like Shared Cluster users can opt out https://docs.databricks.com/en/release-notes/runtime/14.0.html#introducing-spark-connect-in-shared-cluster-architecture",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992177357/reactions,0,0,0,0,0,0,0,0,0,9375
912,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998721008,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-1998721008,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,1998721008,IC_kwDOD7z77c53Ig_w,2024-03-15T01:06:35Z,2024-03-15T01:06:35Z,COLLABORATOR,We can revisit when we work on 4.0 or Databricks 14.x shims.  For 3.5.1 it is not critical.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998721008/reactions,0,0,0,0,0,0,0,0,0,9375
913,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2010350301,https://github.com/NVIDIA/spark-rapids/issues/9375#issuecomment-2010350301,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9375,2010350301,IC_kwDOD7z77c5304Ld,2024-03-20T18:46:33Z,2024-03-20T18:46:33Z,COLLABORATOR,Related user question #10611,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2010350301/reactions,0,0,0,0,0,0,0,0,0,9375
914,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756145310,https://github.com/NVIDIA/spark-rapids/issues/9377#issuecomment-1756145310,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9377,1756145310,IC_kwDOD7z77c5orKae,2023-10-10T20:06:17Z,2023-10-10T20:06:17Z,COLLABORATOR,@gerashegalov can you add the Spark commit/issue that this is related to?  That will help us assess priority and scope.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756145310/reactions,1,1,0,0,0,0,0,0,0,9377
915,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756318096,https://github.com/NVIDIA/spark-rapids/issues/9377#issuecomment-1756318096,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9377,1756318096,IC_kwDOD7z77c5or0mQ,2023-10-10T21:50:34Z,2023-10-10T21:50:34Z,COLLABORATOR,@mattahrens done!,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756318096/reactions,1,1,0,0,0,0,0,0,0,9377
916,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1751089734,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1751089734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1751089734,IC_kwDOD7z77c5oX4JG,2023-10-06T16:41:31Z,2023-10-06T16:41:31Z,COLLABORATOR,"Hello @Nikhilpa1!

It seems that the TCP ports in the range logged are failing to be bound by our socket.

You can set the starting port we will attempt using `spark.rapids.shuffle.ucx.listenerStartPort`. By default we will try 16 times (`spark.port.maxRetries`) by starting at a port and going up by one in case a retry is needed. 

If you don't set `listenerStartPort` we start trying at a random port above 1024.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1751089734/reactions,0,0,0,0,0,0,0,0,0,9386
917,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1752273950,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1752273950,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1752273950,IC_kwDOD7z77c5ocZQe,2023-10-09T02:42:38Z,2023-10-09T02:42:38Z,NONE,"I tried mentioning the listener start port(through `spark.rapids.shuffle.ucx.listenerStartPort`) that I used when running the perf_test, and even then, I am encountering the same error. Do you have any suggestions?",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1752273950/reactions,0,0,0,0,0,0,0,0,0,9386
918,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756177059,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1756177059,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1756177059,IC_kwDOD7z77c5orSKj,2023-10-10T20:29:19Z,2023-10-10T20:29:19Z,COLLABORATOR,"@Nikhilpa1 for some reason the executors are not able to bind to the tcp ports and I am not sure why. Is this running under a container by any chance?

One thing we can try is looking at the UCX logs (`spark.executorEnv.UCX_LOG_LEVEL=info`, then `spark.executorEnv.UCX_LOG_FILE=/tmp/ucx_%h_%p.log`) should give us some UCX-level logs.

Also, I'd try without:
```
""--conf"" ""spark.executorEnv.UCX_TLS=tcp,cuda_copy,cuda_ipc,rc""
""--conf"" ""spark.executorEnv.UCX_NET_DEVICES=en0,mlx5_0:1""
```
to see if UCX will pick the right interfaces to use.

",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1756177059/reactions,0,0,0,0,0,0,0,0,0,9386
919,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764921159,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1764921159,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1764921159,IC_kwDOD7z77c5pMo9H,2023-10-16T17:13:16Z,2023-10-16T17:13:16Z,NONE,"No, it is not running on a container. 
Trying without 
`""--conf"" ""spark.executorEnv.UCX_TLS=tcp,cuda_copy,cuda_ipc,rc""
""--conf"" ""spark.executorEnv.UCX_NET_DEVICES=en0,mlx5_0:1""`
didn't help. Please let me know if you need more info about the system or on the run.

Additionally, Can I get the details of the configuration specifics that were used to compare ucx on vs off performance.",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764921159/reactions,0,0,0,0,0,0,0,0,0,9386
920,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764951597,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1764951597,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1764951597,IC_kwDOD7z77c5pMwYt,2023-10-16T17:33:15Z,2023-10-16T17:33:15Z,COLLABORATOR,"> One thing we can try is looking at the UCX logs (spark.executorEnv.UCX_LOG_LEVEL=info, then spark.executorEnv.UCX_LOG_FILE=/tmp/ucx_%h_%p.log) should give us some UCX-level logs.

@Nikhilpa1 were you able to generate a log file with the above? This would be helpful.

> Additionally, Can I get the details of the configuration specifics that were used to compare ucx on vs off performance.

Please take a look a this GTC presentation for results we have presented in the past: https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31822/. 
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1764951597/reactions,0,0,0,0,0,0,0,0,0,9386
921,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766553492,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766553492,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766553492,IC_kwDOD7z77c5pS3eU,2023-10-17T14:37:12Z,2023-10-17T14:37:20Z,NONE,"> Were you able to generate a log file with the above? This would be helpful.

Yes.
[1697552387.833492] [gpu11:19890:0]          parser.c:2000 UCX  INFO  UCX_* env variables: UCX_MEMTYPE_CACHE=n UCX_MAX_RNDV_RAILS=1 UCX_LOG_FILE=/home/kanaka.3/tmp/ucx_with_only_net.log UCX_RNDV_SCHEME=put_zcopy UCX_NET_DEVICES=en0,mlx5_0:1 UCX_LOG_LEVEL=info UCX_ERROR_SIGNALS= UCX_IB_RX_QUEUE_LEN=1024
[1697552387.850624] [gpu11:19890:0] rdmacm_listener.c:83   UCX  DIAG  rdma_bind_addr(addr=10.1.1.11:50956) failed: No such device
[1697552387.850642] [gpu11:19890:0]    ucp_listener.c:245  UCX  DIAG  failed to create UCT listener on CM 0x2ac175ab25b0 (component rdmacm) with address 10.1.1.11:50956 status No such device
[1697552387.871581] [gpu11:19890:a]       rdmacm_cm.c:712  UCX  DIAG  [cep 0x2ac1bc029990 <invalid>->10.1.1.10:8368 client Success] got error event RDMA_CM_EVENT_ADDR_ERROR, event status No such device (-19)
[1697552387.871606] [gpu11:19890:a]          uct_cm.c:100  UCX  DIAG  resolve callback failed with error: Destination is unreachable
[1697552387.871939] [gpu11:19890:1]       wireup_cm.c:91   UCX  DIAG  ep 0x2abe4ffa20b0: client switching from rdmacm to tcp in attempt to connect to the server
[1697552387.872051] [gpu11:19890:a]       rdmacm_cm.c:712  UCX  DIAG  [cep 0x2ac1bc02a040 <invalid>->10.1.1.12:23238 client Success] got error event RDMA_CM_EVENT_ADDR_ERROR, event status No such device (-19)
[1697552387.872061] [gpu11:19890:a]          uct_cm.c:100  UCX  DIAG  resolve callback failed with error: Destination is unreachable
[1697552387.872078] [gpu11:19890:1]       wireup_cm.c:91   UCX  DIAG  ep 0x2abe4ffa2108: client switching from rdmacm to tcp in attempt to connect to the server
[1697552387.872843] [gpu11:19890:1]      ucp_worker.c:1783 UCX  INFO  ep_cfg[2]: tag(tcp/en0) am(tcp/en0)
[1697552387.876788] [gpu11:19890:1]      ucp_worker.c:1783 UCX  INFO      ep_cfg[3]: tag(tcp/en0) am(tcp/en0)
[1697552387.877323] [gpu11:19890:1]      ucp_worker.c:1783 UCX  INFO    ep_cfg[4]: tag(rc_mlx5/mlx5_0:1) am(rc_mlx5/mlx5_0:1) ka(ud_mlx5/mlx5_0:1)
",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766553492/reactions,0,0,0,0,0,0,0,0,0,9386
922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766657433,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766657433,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766657433,IC_kwDOD7z77c5pTQ2Z,2023-10-17T15:28:24Z,2023-10-17T15:28:24Z,COLLABORATOR,"@yosefe @petro-rudenko any ideas on what is happening with this?
```
[1697552387.850624] [gpu11:19890:0] rdmacm_listener.c:83 UCX DIAG rdma_bind_addr(addr=10.1.1.11:50956) failed: No such device
[1697552387.850642] [gpu11:19890:0] ucp_listener.c:245 UCX DIAG failed to create UCT listener on CM 0x2ac175ab25b0 (component rdmacm) with address 10.1.1.11:50956 status No such device
```

It looks like the NIC with IP address 10.1.1.11 doesn't support RDMACM but we are attempting it? 

@Nikhilpa1 one attempt you can make is disabling RDMACM, as a test:

Please set `spark.executorEnv.UCX_SOCKADDR_TLS_PRIORITY=tcp,sockcm`. Also please share logs when you run with ths environment variable. And thanks!!!",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766657433/reactions,0,0,0,0,0,0,0,0,0,9386
923,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766702899,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766702899,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766702899,IC_kwDOD7z77c5pTb8z,2023-10-17T15:52:11Z,2023-10-17T15:52:11Z,NONE,"Thanks for the response.
Disabling rdma with `spark.executorEnv.UCX_SOCKADDR_TLS_PRIORITY=tcp,sockcm` worked fine..
[1697557791.108393] [gpu12:8008 :0]          parser.c:2000 UCX  INFO  UCX_* env variables: UCX_MEMTYPE_CACHE=n UCX_MAX_RNDV_RAILS=1 UCX_RNDV_SCHEME=put_zcopy UCX_SOCKADDR_TLS_PRIORITY=tcp,sockcm UCX_NET_DEVICES=en0,mlx5_0:1 UCX_LOG_LEVEL=info UCX_ERROR_SIGNALS= UCX_IB_RX_QUEUE_LEN=1024
[1697557791.174517] [gpu12:8008 :1]      ucp_worker.c:1783 UCX  INFO  ep_cfg[2]: tag(tcp/en0) am(tcp/en0)
[1697557791.178327] [gpu12:8008 :1]      ucp_worker.c:1783 UCX  INFO      ep_cfg[3]: tag(tcp/en0) am(tcp/en0)
[1697557791.178942] [gpu12:8008 :1]      ucp_worker.c:1783 UCX  INFO    ep_cfg[4]: tag(rc_mlx5/mlx5_0:1) am(rc_mlx5/mlx5_0:1) ka(ud_mlx5/mlx5_0:1)
But, I am still facing
23/10/17 11:48:02 ERROR TaskSchedulerImpl: Lost executor 11 on 10.1.1.12: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766702899/reactions,0,0,0,0,0,0,0,0,0,9386
924,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766724605,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766724605,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766724605,IC_kwDOD7z77c5pThP9,2023-10-17T16:04:18Z,2023-10-17T16:04:18Z,COLLABORATOR,"Thanks @Nikhilpa1. Progress! 

Do you know if there are other executor logs within executor 11 that would tell us what happened? The error here could mean too many things to pin it down without looking at the executor log (stderr AND stdout).",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766724605/reactions,0,0,0,0,0,0,0,0,0,9386
925,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766734307,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766734307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766734307,IC_kwDOD7z77c5pTjnj,2023-10-17T16:09:58Z,2023-10-17T16:09:58Z,NONE,"In executor logs,

ERROR BlockManagerStorageEndpoint: Error in removing shuffle 109
java.lang.IllegalStateException: An executor with RapidsShuffleManager is trying to use a ShuffleBufferCatalog that isn't initialized.
        at org.apache.spark.sql.rapids.RapidsShuffleInternalManagerBase.resolver$lzycompute(RapidsShuffleInternalManagerBase.scala:1152)
        at org.apache.spark.sql.rapids.RapidsShuffleInternalManagerBase.resolver(RapidsShuffleInternalManagerBase.scala:1136)
        at org.apache.spark.sql.rapids.RapidsShuffleInternalManagerBase.shuffleBlockResolver(RapidsShuffleInternalManagerBase.scala:1409)
        at org.apache.spark.sql.rapids.RapidsShuffleInternalManagerBase.unregisterShuffle(RapidsShuffleInternalManagerBase.scala:1393)
        at org.apache.spark.sql.rapids.ProxyRapidsShuffleInternalManagerBase.unregisterShuffle(RapidsShuffleInternalManagerBase.scala:1486)
        at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:61)
        at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
        at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
        at scala.util.Success.$anonfun$map$1(Try.scala:255)
        at scala.util.Success.map(Try.scala:213)
        at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
        at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
        at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766734307/reactions,0,0,0,0,0,0,0,0,0,9386
926,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766747813,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766747813,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766747813,IC_kwDOD7z77c5pTm6l,2023-10-17T16:17:58Z,2023-10-17T16:17:58Z,COLLABORATOR,"Ok, mind providing the beginning of the executor log? (Up until when you start getting assigned tasks). 

We should see us start to get memory for the GPU and what seems to be happening is that we are not fully initializing. This could happen if the version of Spark running in the executors is somehow different than the one running on the driver as we are looking to match that the classname provided under `spark.shuffle.manager` is the one that would match our shim (for Spark 3.3.0 in this case).",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766747813/reactions,0,0,0,0,0,0,0,0,0,9386
927,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766766488,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766766488,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766766488,IC_kwDOD7z77c5pTreY,2023-10-17T16:28:02Z,2023-10-17T16:28:02Z,NONE,"I've double-checked the version of Spark with the shim, and it is 3.3.0.
I have the executor logs here. Please use it for your reference.
[stdout.zip](https://github.com/NVIDIA/spark-rapids/files/12936525/stdout.zip)",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766766488/reactions,1,1,0,0,0,0,0,0,0,9386
928,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766801718,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766801718,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766801718,IC_kwDOD7z77c5pT0E2,2023-10-17T16:48:01Z,2023-10-17T16:48:01Z,COLLABORATOR,"The log here shows that executorId 2 died had an issue, so we stopped the connection. I don't see the exception you are seeing above.

I think we should try `UCX_TLS=tcp,cuda_copy,cuda_ipc` to remove RDMA alltogether, and I'll try to repro this on my side.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766801718/reactions,0,0,0,0,0,0,0,0,0,9386
929,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766876125,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766876125,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766876125,IC_kwDOD7z77c5pUGPd,2023-10-17T17:39:27Z,2023-10-17T17:39:27Z,NONE,"Using `UCX_TLS=tcp,cuda_copy,cuda_ipc` fixed the issue of lost executors, but the performance improvement with UCX is minimal.",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766876125/reactions,0,0,0,0,0,0,0,0,0,9386
930,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766924342,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1766924342,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1766924342,IC_kwDOD7z77c5pUSA2,2023-10-17T18:12:59Z,2023-10-17T18:12:59Z,COLLABORATOR,"Ok, that's good and bad :) 

At least we know there's some sort of issue with the RDMA part and that it is inside of UCX. Can you remove the UCX_TLS setting and share the UCX logs for all executors? That will help triage what is happening there.

In terms of the improvement: how much are you shuffling? Do you spend a lot of time blocked in shuffle before?

One of the big benefits we get with UCX is in the writes: because we are caching all the GPU blocks instead of writing them to files. That said, if the size of shuffle is rather large and causes us to spill, we will copy those raw bytes to disk on the spill (it doesn't go through the regular Spark compression/encryption (if enabled) pipe).

That said, if you are spending enough time writing shuffle before and all the bytes were to fit in GPU memory, I'd expect some performance improvement here because we are not writing to disk.

On the read, there seems to be an issue in your setup where RDMA isn't 100% there. Could you share the output of `ucx_info -d`? That would help the UCX team tell us what is missing. ",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1766924342/reactions,0,0,0,0,0,0,0,0,0,9386
931,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767002610,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767002610,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767002610,IC_kwDOD7z77c5pUlHy,2023-10-17T19:08:01Z,2023-10-17T19:08:12Z,NONE,">Can you remove the UCX_TLS setting and share the UCX logs for all executors?

When run without UCX_TLS setting, facing lost executor,
[Archive.zip](https://github.com/NVIDIA/spark-rapids/files/12940505/Archive.zip)

>Could you share the output of ucx_info -d?

Please find it here: [ucx_info_d.txt](https://github.com/NVIDIA/spark-rapids/files/12941147/ucx_info_d.txt)
",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767002610/reactions,0,0,0,0,0,0,0,0,0,9386
932,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767047032,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767047032,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767047032,IC_kwDOD7z77c5pUv94,2023-10-17T19:35:38Z,2023-10-17T19:35:38Z,COLLABORATOR,"It seems you have 3 machines: 10.1.1.10, 10.1.1.11, 10.1.1.12. I see 1 executor in .10, 3 executors in .11, and 3 executors in .12. 

Could you please run ucx_info -d in each machine?

Could you also, please capture the UCX log for each machine? 

For the` ucx_info -d` you provided, it seems to be OK. I see both en0 and mlx5_0:1. @yosefe @petro-rudenko fyi.


",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767047032/reactions,0,0,0,0,0,0,0,0,0,9386
933,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767062838,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767062838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767062838,IC_kwDOD7z77c5pUz02,2023-10-17T19:48:28Z,2023-10-17T19:48:37Z,NONE,"> Could you please run ucx_info -d in each machine?

[ucx_info.zip](https://github.com/NVIDIA/spark-rapids/files/12946785/ucx_info.zip)

> Could you also, please capture the UCX log for each machine?

[ucx_logs_all_executors.log](https://github.com/NVIDIA/spark-rapids/files/12947513/ucx_logs_all_executors.log)
",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767062838/reactions,0,0,0,0,0,0,0,0,0,9386
934,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767078803,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767078803,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767078803,IC_kwDOD7z77c5pU3uT,2023-10-17T19:59:38Z,2023-10-17T19:59:38Z,COLLABORATOR,"Thanks @Nikhilpa1 I meant without UCX_TLS or UCX_SOCKADDR_TLS_PRIORITY changes, so it would fail. I just want to see what each executor is seeing... thank you!",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767078803/reactions,0,0,0,0,0,0,0,0,0,9386
935,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767123129,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767123129,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767123129,IC_kwDOD7z77c5pVCi5,2023-10-17T20:30:33Z,2023-10-17T20:30:33Z,NONE,"Thanks, Without UCX_TLS and UCX_SOCKADDR_TLS_PRIORITY,
[1697574255.756477] [gpu10:12245:a]       wireup_cm.c:147  UCX  DIAG  client ep 0x2acf1ffbf0b0 failed to connect to 10.1.1.11:26905 using rdmacm,tcp cms
[1697574269.299272] [gpu10:12245:a]       rdmacm_cm.c:712  UCX  DIAG  [cep 0x2acfa0140bf0 <invalid>->10.1.1.11:30943 client Success] got error event RDMA_CM_EVENT_ADDR_ERROR, event status No such device (-19)
[1697574269.299303] [gpu10:12245:a]          uct_cm.c:100  UCX  DIAG  resolve callback failed with error: Destination is unreachable
[1697574269.299329] [gpu10:12245:1]       wireup_cm.c:91   UCX  DIAG  ep 0x2acf1ffbf0b0: client switching from rdmacm to tcp in attempt to connect to the server
[1697574269.299766] [gpu10:12245:a]       wireup_cm.c:147  UCX  DIAG  client ep 0x2acf1ffbf0b0 failed to connect to 10.1.1.11:30943 using rdmacm,tcp cms
[1697574269.299775] [gpu10:12245:a]          uct_cm.c:100  UCX  DIAG  resolve callback failed with error: Endpoint is not connected

Application Logs:
[app-20231017162403-0006.zip](https://github.com/NVIDIA/spark-rapids/files/12953420/app-20231017162403-0006.zip",,Nikhilpa1,36602072,MDQ6VXNlcjM2NjAyMDcy,https://avatars.githubusercontent.com/u/36602072?v=4,,https://api.github.com/users/Nikhilpa1,https://github.com/Nikhilpa1,https://api.github.com/users/Nikhilpa1/followers,https://api.github.com/users/Nikhilpa1/following{/other_user},https://api.github.com/users/Nikhilpa1/gists{/gist_id},https://api.github.com/users/Nikhilpa1/starred{/owner}{/repo},https://api.github.com/users/Nikhilpa1/subscriptions,https://api.github.com/users/Nikhilpa1/orgs,https://api.github.com/users/Nikhilpa1/repos,https://api.github.com/users/Nikhilpa1/events{/privacy},https://api.github.com/users/Nikhilpa1/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767123129/reactions,0,0,0,0,0,0,0,0,0,9386
936,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767127654,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1767127654,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1767127654,IC_kwDOD7z77c5pVDpm,2023-10-17T20:33:32Z,2023-10-17T20:33:32Z,COLLABORATOR,"This is great @Nikhilpa1 !

```
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[gpu12:2007 :1:2106]     ib_mlx5.c:1114 Assertion `(num_bb > 0) && (num_bb <= 4)' failed: num_bb=6

/home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/mlx5/ib_mlx5.c: [ uct_ib_mlx5_txwq_validate_always() ]
      ...
     1110     uint16_t hw_ci;
     1111 
     1112     /* num_bb must be non-zero and not larger than MAX_BB */
==>  1113     ucs_assertv((num_bb > 0) && (num_bb <= UCT_IB_MLX5_MAX_BB), ""num_bb=%u"",
     1114                 num_bb);
     1115 
     1116     /* bb_max must be smaller than the full QP length */

==== backtrace (tid:   2106) ====
 0 0x000000000002af1a uct_ib_mlx5_txwq_validate_always()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/mlx5/ib_mlx5.c:1113
 1 0x000000000003f024 uct_ib_mlx5_txwq_validate()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/mlx5/ib_mlx5.inl:193
 2 0x000000000003f024 uct_rc_mlx5_common_post_send()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/rc/accel/rc_mlx5.inl:462
 3 0x000000000003f024 uct_rc_mlx5_txqp_inline_iov_post()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/rc/accel/rc_mlx5.inl:499
 4 0x000000000003f024 uct_rc_mlx5_ep_am_short_iov_inline()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/rc/accel/rc_mlx5_ep.c:111
 5 0x000000000003f024 uct_rc_mlx5_ep_am_short_iov()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/ib/rc/accel/rc_mlx5_ep.c:309
 6 0x000000000002cd78 uct_ep_am_short_iov()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/api/uct.h:3008
 7 0x000000000002cd78 ucp_am_contig_short()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/core/ucp_am.c:582
 8 0x00000000000ec2b5 ucp_request_try_send()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/core/ucp_request.inl:334
 9 0x00000000000ec2b5 ucp_request_send()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/core/ucp_request.inl:357
10 0x00000000000ec2b5 ucp_wireup_replay_pending_request()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/wireup/wireup.c:984
11 0x00000000000ec2b5 ucp_wireup_replay_pending_requests()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/wireup/wireup.c:996
12 0x00000000000ec78d ucp_wireup_eps_progress()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/wireup/wireup.c:503
13 0x0000000000055593 ucs_callbackq_slow_proxy()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucs/datastruct/callbackq.c:404
14 0x000000000005ba5a ucs_callbackq_dispatch()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucs/datastruct/callbackq.h:211
15 0x000000000005ba5a uct_worker_progress()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/uct/api/uct.h:2768
16 0x000000000005ba5a ucp_worker_progress()  /home/kanaka.3/projects/sources/ucx-1.14.0/src/ucp/core/ucp_worker.c:2807
=================================
```
^^^^ @petro-rudenko @yosefe @sameerz  FYI. I need some help from the UCX team to figure out what is wrong in this scenario, but @Nikhilpa1's environment is triggering asserts and we are seeing this stack trace in several executors. Please see logs provided.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767127654/reactions,0,0,0,0,0,0,0,0,0,9386
937,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1772999137,https://github.com/NVIDIA/spark-rapids/issues/9386#issuecomment-1772999137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9386,1772999137,IC_kwDOD7z77c5prdHh,2023-10-20T15:58:12Z,2023-10-20T15:58:12Z,COLLABORATOR,cc: @gleon99 who also might be able to help look at the logs.  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1772999137/reactions,0,0,0,0,0,0,0,0,0,9386
938,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1748957914,https://github.com/NVIDIA/spark-rapids/issues/9387#issuecomment-1748957914,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9387,1748957914,IC_kwDOD7z77c5oPvra,2023-10-05T13:58:17Z,2023-10-05T13:58:17Z,COLLABORATOR,"Ya I tested this out myself and it looks like it has to be some kind of a bug in CUDF. CUDF thinks the length of the last string is 14, but spark sees it as 6. Perhaps there is some odd padding that is happening. I am going to try and dump the raw data that we got from CUDF.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1748957914/reactions,0,0,0,0,0,0,0,0,0,9387
939,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1748987906,https://github.com/NVIDIA/spark-rapids/issues/9387#issuecomment-1748987906,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9387,1748987906,IC_kwDOD7z77c5oP3AC,2023-10-05T14:13:01Z,2023-10-05T14:13:01Z,COLLABORATOR,"So it looks like there are nulls at the end of the last string.

```
GPU COLUMN LENGTH - NC: 0 DATA: DeviceMemoryBufferView{address=0x30a003400, length=20, id=-1} VAL: DeviceMemoryBufferView{address=0x30a001e00, length=64, id=-1}
COLUMN LENGTH - STRING
0 ""all"" 616c6c
1 ""the"" 746865
2 ""leaves"" 6c65617665730000000000000000
```

The parquet-mr cli tool appears to dump the data correctly like Spark does.

This even shows up when we try to read str as a binary column instead of a string column. The next step should be to see if we can get a repro case in pure cudf C++ and if so then we need to file an issue with CUDF for this.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1748987906/reactions,1,1,0,0,0,0,0,0,0,9387
940,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1750990323,https://github.com/NVIDIA/spark-rapids/issues/9390#issuecomment-1750990323,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390,1750990323,IC_kwDOD7z77c5oXf3z,2023-10-06T16:00:49Z,2023-10-06T16:00:49Z,COLLABORATOR,"Just FYI this is a different issue in 3.3.0. Spark will gladly parse the incorrect timestamp in both CORRECTED and LEGACY modes. It will parse it slightly differently in each mode, but it does parse it.  We just return null in those cases. Not 100% sure what is going on here. I tried to track down the spark issue that changed this, but I have not been able to do that yet.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1750990323/reactions,0,0,0,0,0,0,0,0,0,9390
941,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783502809,https://github.com/NVIDIA/spark-rapids/issues/9390#issuecomment-1783502809,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9390,1783502809,IC_kwDOD7z77c5qThfZ,2023-10-27T20:55:45Z,2023-10-27T20:55:45Z,COLLABORATOR,Note that there is another bug fix having to do with the type inference when a partition has 1 row vs 2+: https://github.com/apache/spark/commit/7e3ddc1e582. I think we need to add a test that covers that as well.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1783502809/reactions,0,0,0,0,0,0,0,0,0,9390
942,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2013576107,https://github.com/NVIDIA/spark-rapids/issues/9429#issuecomment-2013576107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9429,2013576107,IC_kwDOD7z77c54BLur,2024-03-21T19:59:21Z,2024-03-21T19:59:21Z,COLLABORATOR,The Spark change only affects. 4.0+,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2013576107/reactions,0,0,0,0,0,0,0,0,0,9429
943,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767099117,https://github.com/NVIDIA/spark-rapids/issues/9435#issuecomment-1767099117,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9435,1767099117,IC_kwDOD7z77c5pU8rt,2023-10-17T20:13:53Z,2023-10-17T20:13:53Z,COLLABORATOR,@jlowe to confirm that no action is needed for the plugin,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767099117/reactions,0,0,0,0,0,0,0,0,0,9435
944,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1771283070,https://github.com/NVIDIA/spark-rapids/issues/9435#issuecomment-1771283070,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9435,1771283070,IC_kwDOD7z77c5pk6J-,2023-10-19T15:58:24Z,2023-10-19T15:58:24Z,MEMBER,"Code very similar to the code that was updated in Apache Spark exists in GpuRowBasedScalaUDF.  That code will need to be updated, or this could be resolved by removing the row-based UDF functionality as has been proposed by @revans2.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1771283070/reactions,0,0,0,0,0,0,0,0,0,9435
945,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767108143,https://github.com/NVIDIA/spark-rapids/issues/9436#issuecomment-1767108143,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9436,1767108143,IC_kwDOD7z77c5pU-4v,2023-10-17T20:19:29Z,2023-10-17T20:19:29Z,COLLABORATOR,Need to manually test or add a unit test for canceling a GPU broadcast exchange.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767108143/reactions,0,0,0,0,0,0,0,0,0,9436
946,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767118020,https://github.com/NVIDIA/spark-rapids/issues/9448#issuecomment-1767118020,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9448,1767118020,IC_kwDOD7z77c5pVBTE,2023-10-17T20:26:46Z,2023-10-17T20:26:46Z,COLLABORATOR,Related issue for this metric generated by the plugin: https://github.com/NVIDIA/spark-rapids/issues/6745,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767118020/reactions,0,0,0,0,0,0,0,0,0,9448
947,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767118930,https://github.com/NVIDIA/spark-rapids/issues/9448#issuecomment-1767118930,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9448,1767118930,IC_kwDOD7z77c5pVBhS,2023-10-17T20:27:28Z,2023-10-17T20:27:28Z,COLLABORATOR,What is the use case for monitoring GPU memory?  The preference is to have GPU memory metrics available in the plugin for profiler recommendations.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767118930/reactions,0,0,0,0,0,0,0,0,0,9448
948,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767822675,https://github.com/NVIDIA/spark-rapids/issues/9448#issuecomment-1767822675,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9448,1767822675,IC_kwDOD7z77c5pXtVT,2023-10-18T07:20:35Z,2023-10-18T07:20:35Z,COLLABORATOR,"It looks like https://github.com/NVIDIA/spark-rapids/issues/6745 can satisfy our requirement. We are now trying to profile all queries in the Scale Test. Our use case is just to know the peak GPU memory when running a query. Thanks! 
@winningsix according to the issue mentioned by Matt, nsys now supports the track for GPU memory, all we need to do is to launch nsys to profile the spark application, and use nsys to open the output qdrep file, we should be able to see the nice metrics.",,wjxiz1992,20476954,MDQ6VXNlcjIwNDc2OTU0,https://avatars.githubusercontent.com/u/20476954?v=4,,https://api.github.com/users/wjxiz1992,https://github.com/wjxiz1992,https://api.github.com/users/wjxiz1992/followers,https://api.github.com/users/wjxiz1992/following{/other_user},https://api.github.com/users/wjxiz1992/gists{/gist_id},https://api.github.com/users/wjxiz1992/starred{/owner}{/repo},https://api.github.com/users/wjxiz1992/subscriptions,https://api.github.com/users/wjxiz1992/orgs,https://api.github.com/users/wjxiz1992/repos,https://api.github.com/users/wjxiz1992/events{/privacy},https://api.github.com/users/wjxiz1992/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767822675/reactions,0,0,0,0,0,0,0,0,0,9448
949,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767178114,https://github.com/NVIDIA/spark-rapids/issues/9451#issuecomment-1767178114,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9451,1767178114,IC_kwDOD7z77c5pVP-C,2023-10-17T21:03:37Z,2023-10-17T21:03:37Z,COLLABORATOR,"Note, we should drop testing cloudera 3.2.1 from integration tests and we should just remove that shim (which I will file a separate issues for).",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767178114/reactions,0,0,0,0,0,0,0,0,0,9451
950,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767427086,https://github.com/NVIDIA/spark-rapids/issues/9451#issuecomment-1767427086,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9451,1767427086,IC_kwDOD7z77c5pWMwO,2023-10-18T00:58:29Z,2023-10-18T12:59:20Z,COLLABORATOR,"Previously we test Cloudera shims against dedicated clusters (raplab), 
I am unfamiliar with that part, how should we simply switch CDH version on existing clusters or request a new cluster? cloudera2 IT pipeline (321cdh) has not been enabled for 7 months (after the migration at Feb.) @GaryShen2008 @sameerz 

@tgravescs is it possible for us to test against CDH shim locally? like inside a single container, so we could run it in our jenkins cluster for testing purposes which would not require extra resources (yarn cluster) and maintenance overhead, thanks

",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1767427086/reactions,0,0,0,0,0,0,0,0,0,9451
951,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1768921408,https://github.com/NVIDIA/spark-rapids/issues/9451#issuecomment-1768921408,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9451,1768921408,IC_kwDOD7z77c5pb5lA,2023-10-18T16:32:12Z,2023-10-18T16:32:12Z,COLLABORATOR,"The idea behind running on the Cloudera cluster itself was were are running the test in both YARN environment and using their specific jars.  If you want to skip the YARN environment then we would have to figure out how to just use their jars in a local cluster or something.  That is a call for @sameerz .  We have a cluster sitting there idle, so why not use it, or we should repurpose that cluster.",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1768921408/reactions,0,0,0,0,0,0,0,0,0,9451
952,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777957059,https://github.com/NVIDIA/spark-rapids/issues/9462#issuecomment-1777957059,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9462,1777957059,IC_kwDOD7z77c5p-XjD,2023-10-24T20:06:54Z,2023-10-24T20:06:54Z,COLLABORATOR,"@pxLi we will start working on this soon.  As part of this, CI/CD jobs for Cloudera 3.2.1 will need to be removed.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777957059/reactions,1,1,0,0,0,0,0,0,0,9462
953,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1773040872,https://github.com/NVIDIA/spark-rapids/issues/9494#issuecomment-1773040872,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9494,1773040872,IC_kwDOD7z77c5prnTo,2023-10-20T16:27:31Z,2023-10-20T16:27:31Z,COLLABORATOR,"I did some quick tests, and the plugin is complaining that 

```
 cannot run on GPU because mixing CPU and GPU aggregations is not supported
```

for the first SortAggregateExec, but all of the children show that they are on the GPU. I'll try to dig a little deeper.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1773040872/reactions,0,0,0,0,0,0,0,0,0,9494
954,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1773050409,https://github.com/NVIDIA/spark-rapids/issues/9494#issuecomment-1773050409,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9494,1773050409,IC_kwDOD7z77c5prpop,2023-10-20T16:34:13Z,2023-10-20T16:34:13Z,COLLABORATOR,"Okay I traced it down. We put in some code for databricks to prevent partial replacement in some cases and it is hitting here

https://github.com/NVIDIA/spark-rapids/issues/4963

is the issue to fix this. So we can decide if we are hitting this in error or not.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1773050409/reactions,0,0,0,0,0,0,0,0,0,9494
955,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778002922,https://github.com/NVIDIA/spark-rapids/issues/9495#issuecomment-1778002922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9495,1778002922,IC_kwDOD7z77c5p-ivq,2023-10-24T20:40:37Z,2023-10-24T20:40:37Z,COLLABORATOR,Scope should be to remove `improvedTimeOps` setting completely.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778002922/reactions,0,0,0,0,0,0,0,0,0,9495
956,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1775516010,https://github.com/NVIDIA/spark-rapids/issues/9517#issuecomment-1775516010,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9517,1775516010,IC_kwDOD7z77c5p1Dlq,2023-10-23T15:57:27Z,2023-10-23T15:57:27Z,COLLABORATOR,The default is to have this be false. Is there a customer asking for this? If not then this should not be a blocker for enabling the feature by default.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1775516010/reactions,0,0,0,0,0,0,0,0,0,9517
957,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1775621839,https://github.com/NVIDIA/spark-rapids/issues/9517#issuecomment-1775621839,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9517,1775621839,IC_kwDOD7z77c5p1dbP,2023-10-23T16:57:21Z,2023-10-23T16:57:21Z,CONTRIBUTOR,It seems unlikely that any customers would need this. I am just filing for completeness.,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1775621839/reactions,0,0,0,0,0,0,0,0,0,9517
958,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777908107,https://github.com/NVIDIA/spark-rapids/issues/9526#issuecomment-1777908107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9526,1777908107,IC_kwDOD7z77c5p-LmL,2023-10-24T19:38:00Z,2023-10-24T19:38:00Z,CONTRIBUTOR,I'll take a look and see if we can report a bug against Arrow for this,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1777908107/reactions,0,0,0,0,0,0,0,0,0,9526
959,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778030630,https://github.com/NVIDIA/spark-rapids/issues/9530#issuecomment-1778030630,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9530,1778030630,IC_kwDOD7z77c5p-pgm,2023-10-24T21:00:56Z,2023-10-24T21:00:56Z,COLLABORATOR,Make sure it works with IDEs.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1778030630/reactions,0,0,0,0,0,0,0,0,0,9530
960,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1779448307,https://github.com/NVIDIA/spark-rapids/issues/9535#issuecomment-1779448307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9535,1779448307,IC_kwDOD7z77c5qEDnz,2023-10-25T14:47:20Z,2023-10-25T14:47:20Z,COLLABORATOR,"This is a known issue.

https://github.com/NVIDIA/spark-rapids/issues/1860

The problem is that right now when we compute window operations, by default, we do it one partition at a time. This is the same that Spark does on the CPU.  We have put in a few special optimizations for running window (ROWS between unbounded preceding and current row) and for unbounded preceding to unbounded following.  But in the more general case of N preceding and M following we don't have a memory optimization yet.  The plan would be to take an input batch and process it. Then we would have to strip off M rows from the output batch because they were not complete. We would also have to save N rows from the input batch because they would be needed to fully compute the rows that we stripped off from the output. The hard part with this is really just making sure that we have covered all of the odd corner cases. Like what happens if preceding or following are negative?

It is not that terribly difficult of a problem to solve. Just need to sit down and do it.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1779448307/reactions,0,0,0,0,0,0,0,0,0,9535
961,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787972521,https://github.com/NVIDIA/spark-rapids/issues/9560#issuecomment-1787972521,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9560,1787972521,IC_kwDOD7z77c5qkkup,2023-10-31T20:18:46Z,2023-10-31T20:18:46Z,COLLABORATOR,Initial scope will be to document the issue and then later on can fix the bug.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787972521/reactions,0,0,0,0,0,0,0,0,0,9560
962,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2052067419,https://github.com/NVIDIA/spark-rapids/issues/9565#issuecomment-2052067419,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9565,2052067419,IC_kwDOD7z77c56UBBb,2024-04-12T16:17:09Z,2024-04-12T16:17:09Z,COLLABORATOR,"Follow up for this work: Additional check was added to handle NPE.


Spark Commit: https://github.com/apache/spark/commit/aa566ff3b9f",,parthosa,13639815,MDQ6VXNlcjEzNjM5ODE1,https://avatars.githubusercontent.com/u/13639815?v=4,,https://api.github.com/users/parthosa,https://github.com/parthosa,https://api.github.com/users/parthosa/followers,https://api.github.com/users/parthosa/following{/other_user},https://api.github.com/users/parthosa/gists{/gist_id},https://api.github.com/users/parthosa/starred{/owner}{/repo},https://api.github.com/users/parthosa/subscriptions,https://api.github.com/users/parthosa/orgs,https://api.github.com/users/parthosa/repos,https://api.github.com/users/parthosa/events{/privacy},https://api.github.com/users/parthosa/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2052067419/reactions,0,0,0,0,0,0,0,0,0,9565
963,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1846005359,https://github.com/NVIDIA/spark-rapids/issues/9567#issuecomment-1846005359,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9567,1846005359,IC_kwDOD7z77c5uB85v,2023-12-07T19:51:29Z,2023-12-07T19:51:29Z,COLLABORATOR,"We don't support TimestampNTZType on the GPU. 
```
!Exec <FileSourceScanExec> cannot run on GPU because unsupported data types ArrayType(TimestampNTZType,true) [_2] in read for Parquet; unsupported data types in output: ArrayType(TimestampNTZType,true) [_2#23]
```",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1846005359/reactions,0,0,0,0,0,0,0,0,0,9567
964,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1804799564,https://github.com/NVIDIA/spark-rapids/issues/9580#issuecomment-1804799564,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9580,1804799564,IC_kwDOD7z77c5rkw5M,2023-11-09T22:44:48Z,2023-11-09T22:45:04Z,COLLABORATOR,FYI: Currently we keep track of remaining nested type FEAs in this issue: https://github.com/NVIDIA/spark-rapids/issues/8550,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1804799564/reactions,0,0,0,0,0,0,0,0,0,9580
965,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1854352807,https://github.com/NVIDIA/spark-rapids/issues/9580#issuecomment-1854352807,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9580,1854352807,IC_kwDOD7z77c5uhy2n,2023-12-13T16:58:13Z,2023-12-13T16:58:13Z,COLLABORATOR,"Thanks @revans2 for opening this issue. Do you think it would be appropriate to close https://github.com/rapidsai/cudf/issues/11844, or are you expecting to find some significant gaps?",,GregoryKimball,12725111,MDQ6VXNlcjEyNzI1MTEx,https://avatars.githubusercontent.com/u/12725111?v=4,,https://api.github.com/users/GregoryKimball,https://github.com/GregoryKimball,https://api.github.com/users/GregoryKimball/followers,https://api.github.com/users/GregoryKimball/following{/other_user},https://api.github.com/users/GregoryKimball/gists{/gist_id},https://api.github.com/users/GregoryKimball/starred{/owner}{/repo},https://api.github.com/users/GregoryKimball/subscriptions,https://api.github.com/users/GregoryKimball/orgs,https://api.github.com/users/GregoryKimball/repos,https://api.github.com/users/GregoryKimball/events{/privacy},https://api.github.com/users/GregoryKimball/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1854352807/reactions,0,0,0,0,0,0,0,0,0,9580
966,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1854371526,https://github.com/NVIDIA/spark-rapids/issues/9580#issuecomment-1854371526,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9580,1854371526,IC_kwDOD7z77c5uh3bG,2023-12-13T17:04:11Z,2023-12-13T17:04:11Z,COLLABORATOR,@GregoryKimball I am fine if you want to close https://github.com/rapidsai/cudf/issues/11844. We have not even started on this as far as I can tell. SO who knows when it will actually be done.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1854371526/reactions,0,0,0,0,0,0,0,0,0,9580
967,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1786343243,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1786343243,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1786343243,IC_kwDOD7z77c5qeW9L,2023-10-31T02:35:02Z,2023-10-31T02:35:02Z,COLLABORATOR,should be fixed directly in the #9576 ,,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1786343243/reactions,0,0,0,0,0,0,0,0,0,9582
968,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787213413,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1787213413,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1787213413,IC_kwDOD7z77c5qhrZl,2023-10-31T13:24:32Z,2023-10-31T13:24:32Z,COLLABORATOR,"@pxLi how does #9576, which is a build issue that in practice should not cause any runtime problems, just build problems is the cause of a core dump from sort? I just want to be sure that this was not closed on accident because this is a very serious and scary issue.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787213413/reactions,0,0,0,0,0,0,0,0,0,9582
969,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787247914,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1787247914,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1787247914,IC_kwDOD7z77c5qhz0q,2023-10-31T13:43:31Z,2023-10-31T13:43:31Z,COLLABORATOR,"From the stack trace it looks like the test failed, and then the bad access happens as we try to close the catalog.

https://github.com/NVIDIA/spark-rapids/blob/d1573060301b70d187147697e3202fc26c8a8097/tests/src/test/scala/com/nvidia/spark/rapids/RmmSparkRetrySuiteBase.scala#L58

The test failed because of 

https://github.com/NVIDIA/spark-rapids/blob/d1573060301b70d187147697e3202fc26c8a8097/sql-plugin/src/main/scala/com/nvidia/spark/rapids/SortUtils.scala#L248

Which is something that we should look into, but I am much more concerned about the segfault that happened.  The secondary code appears to be a SEGV_ACCERR, which means we tried to access something that we don't have permission to access. My guess is that it is a use after free, but I am not 100% sure. It could also be that RMM had shut down already before we tried to clean up the memory.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1787247914/reactions,0,0,0,0,0,0,0,0,0,9582
970,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788023880,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1788023880,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1788023880,IC_kwDOD7z77c5qkxRI,2023-10-31T20:56:30Z,2023-10-31T20:56:46Z,COLLABORATOR,"https://github.com/NVIDIA/spark-rapids/commit/9f12c272259e2c3c355e95ece980535f020bd93a is the patch that fixed the failure to happen, so I am hopeful that I can understand where the segfault came from. One of the scary things is that this got past our assertions and still got the segfault.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788023880/reactions,0,0,0,0,0,0,0,0,0,9582
971,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788279587,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1788279587,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1788279587,IC_kwDOD7z77c5qlvsj,2023-11-01T01:56:37Z,2023-11-01T01:56:37Z,COLLABORATOR,"> @pxLi how does #9576, which is a build issue that in practice should not cause any runtime problems, just build problems is the cause of a core dump from sort? I just want to be sure that this was not closed on accident because this is a very serious and scary issue.

I was told the issue was caused by the change, cc @NVnavkumar could share more",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788279587/reactions,0,0,0,0,0,0,0,0,0,9582
972,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788428391,https://github.com/NVIDIA/spark-rapids/issues/9582#issuecomment-1788428391,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9582,1788428391,IC_kwDOD7z77c5qmUBn,2023-11-01T05:23:01Z,2023-11-01T05:24:12Z,COLLABORATOR,"We only see this core dump in https://github.com/NVIDIA/spark-rapids/pull/9576 before the patch https://github.com/NVIDIA/spark-rapids/commit/9f12c272259e2c3c355e95ece980535f020bd93a and 100% repro in CI. (did not see any other place to report the same core dump)

so previous commits in the change might actually help trigger the segfault",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1788428391/reactions,0,0,0,0,0,0,0,0,0,9582
973,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791566426,https://github.com/NVIDIA/spark-rapids/issues/9592#issuecomment-1791566426,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9592,1791566426,IC_kwDOD7z77c5qySJa,2023-11-02T21:31:56Z,2023-11-02T21:31:56Z,CONTRIBUTOR,"I ran a test in spark-rapids-jni:

```java
ColumnVector input = ColumnVector.fromStrings(""{}"", ""BAD"", ""{\""A\"": 100}"");
ColumnVector outputMap = MapUtils.extractRawMapFromJsonString(input);
TableDebug.get().debug(""outputMap"", outputMap);
```

This shows the following output:

```
GPU COLUMN outputMap - NC: 0 DATA: null VAL: null
GPU COLUMN outputMap:DATA - NC: 0 DATA: null VAL: null
GPU COLUMN outputMap:DATA:CHILD_0 - NC: 0 DATA: DeviceMemoryBufferView{address=0x7f48a3c01200, length=1, id=-1} VAL: null
GPU COLUMN outputMap:DATA:CHILD_1 - NC: 0 DATA: DeviceMemoryBufferView{address=0x7f48a3c01a00, length=3, id=-1} VAL: null
COLUMN outputMap - LIST
OFFSETS
0 [0 - 0)
1 [0 - 0)
2 [0 - 1)
COLUMN outputMap:DATA - STRUCT
COLUMN outputMap:DATA:CHILD_0 - STRING
0 ""A"" 41
COLUMN outputMap:DATA:CHILD_1 - STRING
0 ""100"" 313030
```

There is no differentiation between the source strings `{}` and `BAD`. None of the values are null.

@ttnghia Would it be possible to detect the invalid item and set the corresponding entry to null?
",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791566426/reactions,0,0,0,0,0,0,0,0,0,9592
974,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791580478,https://github.com/NVIDIA/spark-rapids/issues/9592#issuecomment-1791580478,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9592,1791580478,IC_kwDOD7z77c5qyVk-,2023-11-02T21:44:03Z,2023-11-02T21:44:03Z,COLLABORATOR,"Yes that is possible but it seems not trivial at this point because the underlying JNI code doesn't work based on lines JSON (it concatenates all the input rows into one giant JSON string).

We need to rework it to have this fixed.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791580478/reactions,1,1,0,0,0,0,0,0,0,9592
975,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791394760,https://github.com/NVIDIA/spark-rapids/issues/9596#issuecomment-1791394760,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9596,1791394760,IC_kwDOD7z77c5qxoPI,2023-11-02T19:09:30Z,2023-11-02T19:09:30Z,COLLABORATOR,This work can help inform this autotuner enhancement: https://github.com/NVIDIA/spark-rapids-tools/issues/644.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791394760/reactions,0,0,0,0,0,0,0,0,0,9596
976,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1800142106,https://github.com/NVIDIA/spark-rapids/issues/9602#issuecomment-1800142106,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9602,1800142106,IC_kwDOD7z77c5rS_0a,2023-11-07T21:15:02Z,2023-11-07T21:15:02Z,COLLABORATOR,Should this be added to the JSON epic?  https://github.com/NVIDIA/spark-rapids/issues/9458,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1800142106/reactions,0,0,0,0,0,0,0,0,0,9602
977,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1800330199,https://github.com/NVIDIA/spark-rapids/issues/9602#issuecomment-1800330199,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9602,1800330199,IC_kwDOD7z77c5rTtvX,2023-11-07T22:54:58Z,2023-11-07T22:54:58Z,CONTRIBUTOR,"> Should this be added to the JSON epic? #9458

I went ahead and added it. This epic was initially supposed to be focused on the highest-priority items based on customer requirements, and I am not aware that this issue qualifies for that, but it seems like a good place to track all these related tasks.

",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1800330199/reactions,0,0,0,0,0,0,0,0,0,9602
978,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815387969,https://github.com/NVIDIA/spark-rapids/issues/9614#issuecomment-1815387969,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9614,1815387969,IC_kwDOD7z77c5sNJ9B,2023-11-16T22:05:00Z,2023-11-16T22:05:00Z,COLLABORATOR,"Is the idea to have a tool (entirely separate from the `spark-plugin`) that can be run to check the GPU (memory, etc.) for failures?",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815387969/reactions,0,0,0,0,0,0,0,0,0,9614
979,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852833152,https://github.com/NVIDIA/spark-rapids/issues/9614#issuecomment-1852833152,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9614,1852833152,IC_kwDOD7z77c5ub_2A,2023-12-12T21:24:14Z,2023-12-12T21:24:14Z,COLLABORATOR,"@mythrocks My intention for this issue/feature request is to print whatever useful debug information when cudaErrorIllegalAddress is happening. I knew ECC errors are not the hard evidence for a bad GPU but at least it can provide some insights.

I do not want to have a separate tool to diagnostic this because the cluster may not be active after the job completed/failed. Imagine if this is a one time Dataproc job or a Databricks job cluster. 
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852833152/reactions,0,0,0,0,0,0,0,0,0,9614
980,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852867141,https://github.com/NVIDIA/spark-rapids/issues/9614#issuecomment-1852867141,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9614,1852867141,IC_kwDOD7z77c5ucIJF,2023-12-12T21:52:19Z,2023-12-12T21:52:19Z,COLLABORATOR,How does this differ from #5028?   We should already be printing `nvidia-smi` output when executors fail on a fatal exception.,,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1852867141/reactions,0,0,0,0,0,0,0,0,0,9614
981,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791452285,https://github.com/NVIDIA/spark-rapids/issues/9616#issuecomment-1791452285,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9616,1791452285,IC_kwDOD7z77c5qx2R9,2023-11-02T19:53:13Z,2023-11-02T19:53:13Z,MEMBER,"IMO this is no different fundamentally than a CPU segfault, and Spark's not killing the entire job when that occurs.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791452285/reactions,0,0,0,0,0,0,0,0,0,9616
982,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791751922,https://github.com/NVIDIA/spark-rapids/issues/9619#issuecomment-1791751922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9619,1791751922,IC_kwDOD7z77c5qy_by,2023-11-03T00:48:21Z,2023-11-03T00:48:21Z,COLLABORATOR,"hi @sameerz @GaryShen2008 any thoughts about this one?

I think existing scala2.12 + jdk11&17 would be enough, do we need to cover all combinations as limited resources? seems we are trying to cover work for JVM team too",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791751922/reactions,0,0,0,0,0,0,0,0,0,9619
983,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791752242,https://github.com/NVIDIA/spark-rapids/issues/9619#issuecomment-1791752242,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9619,1791752242,IC_kwDOD7z77c5qy_gy,2023-11-03T00:48:50Z,2023-11-03T00:50:20Z,COLLABORATOR,We can try start with adding some extra github actions. Lets mark this one lower priority for now cc @YanxuanLiu to help later,,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791752242/reactions,0,0,0,0,0,0,0,0,0,9619
984,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791756920,https://github.com/NVIDIA/spark-rapids/issues/9619#issuecomment-1791756920,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9619,1791756920,IC_kwDOD7z77c5qzAp4,2023-11-03T00:53:10Z,2023-11-03T00:53:10Z,COLLABORATOR,"> We can try start with adding some extra github actions

This will help already. The bug was not caught exactly for the reason that we don't have any check making sure Scala2.13 at least builds with JDK11+",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1791756920/reactions,0,0,0,0,0,0,0,0,0,9619
985,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998770351,https://github.com/NVIDIA/spark-rapids/issues/9627#issuecomment-1998770351,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9627,1998770351,IC_kwDOD7z77c53ItCv,2024-03-15T01:47:41Z,2024-03-15T01:47:41Z,COLLABORATOR,"Not planing work on this for release 24.04, unassign it from me.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998770351/reactions,0,0,0,0,0,0,0,0,0,9627
986,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1794945226,https://github.com/NVIDIA/spark-rapids/issues/9633#issuecomment-1794945226,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9633,1794945226,IC_kwDOD7z77c5q_LDK,2023-11-06T14:24:07Z,2023-11-06T14:24:07Z,COLLABORATOR,Just for reference we might need to translate some of these to things java understands.  https://code2care.org/pages/java-timezone-list-utc-gmt-offset/,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1794945226/reactions,0,0,0,0,0,0,0,0,0,9633
987,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1846767208,https://github.com/NVIDIA/spark-rapids/issues/9633#issuecomment-1846767208,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9633,1846767208,IC_kwDOD7z77c5uE25o,2023-12-08T08:34:42Z,2023-12-08T08:35:17Z,COLLABORATOR,"Firstly, I ran the UT TimeZoneSuite with AllTimezones (but not AllYears). It took about 30min to finish all zones successfully.

Then, I tried to pick up some ""weird zones"" according to some simple rules:
1. Zones contain the most transitions
```scala
ZoneId.getAvailableZoneIds().asScala.map { id => (ZoneId.of(id).normalized().getRules().getTransitions().asScala.size, id) }.toList.sortBy(-_._1).slice(0, 10).foreach(println)
```
```
(307,Asia/Hebron)
(305,Asia/Gaza)
(196,Africa/Casablanca)
```
2.  Zones contain the intra-hour transitions ( Ex: Transition[Overlap at 1921-02-13T00:00-05:30 to -06:00])
```scala
ZoneId.getAvailableZoneIds().asScala.map { id => (ZoneId.of(id).normalized().getRules().getTransitions().asScala.filter(e => e.getDuration().getSeconds() % 3600 > 0).size, id) }.toList.sortBy(-_._1).slice(0,10).foreach(println)
```
```
(45,America/Belize)
(26,Australia/Lord_Howe)
(26,Australia/LHI)
```
3.  Zones contain the intra-minute transitions ( Ex: Transition[Gap at 1890-01-01T00:00-04:43:40 to -04:42:45])
```scala
ZoneId.getAvailableZoneIds().asScala.map { id => (ZoneId.of(id).normalized().getRules().getTransitions().asScala.filter(e => e.getDuration().getSeconds() % 60 > 0).size, id) }.toList.sortBy(-_._1).slice(0,10).foreach(println)
```
```
(4,America/Punta_Arenas)
(3,America/Santiago)
(3,Chile/Continental)
```

4. According to https://www.oracle.com/java/technologies/tzdata-versions.html,  we choose `Morocco` because it frequently changes its transition rule.


Maybe we can pick up several of them into our pre-merge and nightly tests?  Such as https://github.com/NVIDIA/spark-rapids/pull/9999
",,sperlingxx,6276118,MDQ6VXNlcjYyNzYxMTg=,https://avatars.githubusercontent.com/u/6276118?v=4,,https://api.github.com/users/sperlingxx,https://github.com/sperlingxx,https://api.github.com/users/sperlingxx/followers,https://api.github.com/users/sperlingxx/following{/other_user},https://api.github.com/users/sperlingxx/gists{/gist_id},https://api.github.com/users/sperlingxx/starred{/owner}{/repo},https://api.github.com/users/sperlingxx/subscriptions,https://api.github.com/users/sperlingxx/orgs,https://api.github.com/users/sperlingxx/repos,https://api.github.com/users/sperlingxx/events{/privacy},https://api.github.com/users/sperlingxx/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1846767208/reactions,0,0,0,0,0,0,0,0,0,9633
988,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1809368573,https://github.com/NVIDIA/spark-rapids/issues/9661#issuecomment-1809368573,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9661,1809368573,IC_kwDOD7z77c5r2MX9,2023-11-14T00:52:12Z,2023-11-14T00:52:12Z,COLLABORATOR,"Regards to ""Support conditional equi-joins with non-AST rewrite"" above, it seems not applicable to have non-ast-split there. It seems we should close it.

Given this ```left.join(broadcast(right), f.round(left.a).cast('integer') == f.round(f.log(right.r_a).cast('integer')), join_type)``` as an example, join condition will be split into left and right key expressions in GpuBroadcastHashJoinExec.scala
https://github.com/NVIDIA/spark-rapids/blob/792b4c14f27b73cf86012d3220b61b2c6ca23f0f/sql-plugin/src/main/spark340/scala/org/apache/spark/sql/rapids/execution/GpuBroadcastHashJoinExec.scala#L65-L66

And it will be evaluated in separated project nodes for build and stream sides accordingly.

Build side:
https://github.com/NVIDIA/spark-rapids/blob/792b4c14f27b73cf86012d3220b61b2c6ca23f0f/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/execution/GpuHashJoin.scala#L324-L327

Stream side:
https://github.com/NVIDIA/spark-rapids/blob/792b4c14f27b73cf86012d3220b61b2c6ca23f0f/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/execution/GpuHashJoin.scala#L394-L398

The remain case I came up is that a join condition involves columns across both sides. In that case, it's also not a split-able case.

Explained Plan
```
GpuBroadcastHashJoin [gpuround(a#22, 0, IntegerType)], [gpuround(cast(LOG(cast(r_a#30 as double)) as int), 0, IntegerType)], Cross, GpuBuildRight
:- GpuRowToColumnar targetsize(104857600)
:  +- *(1) Scan ExistingRDD[a#22,b#23]
+- GpuBroadcastExchange HashedRelationBroadcastMode(List(cast(round(cast(ln(cast(input[0, int, true] as double)) as int), 0) as bigint)),false), [plan_id=107]
   +- GpuProject [a#26 AS r_a#30, b#27 AS r_b#33]
      +- GpuRowToColumnar targetsize(104857600)
         +- *(2) Scan ExistingRDD[a#26,b#27]
```

@revans2 @jlowe any thoughts here?",,winningsix,2278268,MDQ6VXNlcjIyNzgyNjg=,https://avatars.githubusercontent.com/u/2278268?v=4,,https://api.github.com/users/winningsix,https://github.com/winningsix,https://api.github.com/users/winningsix/followers,https://api.github.com/users/winningsix/following{/other_user},https://api.github.com/users/winningsix/gists{/gist_id},https://api.github.com/users/winningsix/starred{/owner}{/repo},https://api.github.com/users/winningsix/subscriptions,https://api.github.com/users/winningsix/orgs,https://api.github.com/users/winningsix/repos,https://api.github.com/users/winningsix/events{/privacy},https://api.github.com/users/winningsix/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1809368573/reactions,0,0,0,0,0,0,0,0,0,9661
989,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1802826661,https://github.com/NVIDIA/spark-rapids/issues/9664#issuecomment-1802826661,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9664,1802826661,IC_kwDOD7z77c5rdPOl,2023-11-08T22:52:22Z,2023-11-08T22:52:22Z,CONTRIBUTOR,This behavior may only apply to certain Spark versions. I am investigating.,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1802826661/reactions,0,0,0,0,0,0,0,0,0,9664
990,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1804278037,https://github.com/NVIDIA/spark-rapids/issues/9664#issuecomment-1804278037,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9664,1804278037,IC_kwDOD7z77c5rixkV,2023-11-09T17:42:09Z,2023-11-09T17:42:09Z,CONTRIBUTOR,"As of Spark 3.4, five-digit numbers result in an error such as `ValueError: year 99346 is out of range`",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1804278037/reactions,0,0,0,0,0,0,0,0,0,9664
991,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1812489985,https://github.com/NVIDIA/spark-rapids/issues/9671#issuecomment-1812489985,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9671,1812489985,IC_kwDOD7z77c5sCGcB,2023-11-15T12:56:23Z,2023-11-15T12:56:23Z,COLLABORATOR,"I also met this today, it's a random problem.
We may need to increase the timeout.
From Liangcai:
> Is this related to the compression/decompression for disk spilling which is added recently ?",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1812489985/reactions,0,0,0,0,0,0,0,0,0,9671
992,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836767469,https://github.com/NVIDIA/spark-rapids/issues/9671#issuecomment-1836767469,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9671,1836767469,IC_kwDOD7z77c5tetjt,2023-12-01T20:58:19Z,2023-12-01T20:58:19Z,MEMBER,"A similar test recently failed in a nightly test, not sure if it's the same root cause.  Happy to file a separate issue if deemed likely separate:
```
[2023-12-01T18:19:36.815Z] - simple mixed blocking alloc *** FAILED ***
[2023-12-01T18:19:36.815Z]   java.util.concurrent.TimeoutException:
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.HostAllocSuite$TaskThread$TaskThreadTrackingOp.get(HostAllocSuite.scala:111)
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.HostAllocSuite$AllocOnAnotherThread.waitForAlloc(HostAllocSuite.scala:220)
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$34(HostAllocSuite.scala:530)
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$34$adapted(HostAllocSuite.scala:524)
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
[2023-12-01T18:19:36.815Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$33(HostAllocSuite.scala:524)
[2023-12-01T18:19:36.816Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$33$adapted(HostAllocSuite.scala:520)
[2023-12-01T18:19:36.816Z]   at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
[2023-12-01T18:19:36.816Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$32(HostAllocSuite.scala:520)
[2023-12-01T18:19:36.816Z]   at com.nvidia.spark.rapids.HostAllocSuite.$anonfun$new$32$adapted(HostAllocSuite.scala:516)
[2023-12-01T18:19:36.816Z]   ...
```",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836767469/reactions,0,0,0,0,0,0,0,0,0,9671
993,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823569187,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1823569187,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1823569187,IC_kwDOD7z77c5ssXUj,2023-11-22T22:08:46Z,2023-11-27T23:13:06Z,COLLABORATOR,"Attached [herewith](https://github.com/NVIDIA/spark-rapids/files/13444584/decimals_avg.parquet.zip) is a zipped Parquet file with 102 rows in a single `Decimal(8,3)` column.

Taking the window functions out of the equation, one sees that running `AVG()` produces slightly different results on Apache Spark and the plugin:
```scala
// On Spark.
scala> spark.read.parquet(""/tmp/decimals_avg.parquet"").select( expr(""avg(c)"") ).show
+------------+
|      avg(c)|
+------------+
|3527.6195313|
+------------+

// On the plugin:
+------------+
|      avg(c)|
+------------+
|3527.6195312|
+------------+
```
The behaviour seems to be consistent on Spark ~3.4.2~ 3.4.1 and Spark 3.2.3. The plugin result is off by `0.000001`.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823569187/reactions,0,0,0,0,0,0,0,0,0,9682
994,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828781810,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1828781810,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1828781810,IC_kwDOD7z77c5tAP7y,2023-11-27T23:12:26Z,2023-11-27T23:12:26Z,COLLABORATOR,"I have filed https://github.com/rapidsai/cudf/issues/14507 to track the CUDF side of this.

I was able to repro this on CUDF by writing the input as `DECIMAL(12,7)` to Parquet, and then running the `MEAN` aggregation on it.  The bug I filed has the repro details.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828781810/reactions,0,0,0,0,0,0,0,0,0,9682
995,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828821221,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1828821221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1828821221,IC_kwDOD7z77c5tAZjl,2023-11-27T23:50:00Z,2023-11-27T23:50:00Z,COLLABORATOR,"A couple of other findings. I tried querying `SUM`, `COUNT`, `AVG`, etc. as follows:
```sql
select sum(c),  count(c), sum(c)/count(c), avg(c), cast(avg(c) as DECIMAL(12,8)) , cast(sum(c)/count(c) as decimal(12,7)) from foobar 
```
On CPU, those results tally up:
```
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+
|sum(c)    |count(c)|(sum(c) / count(c))         |avg(c)      |CAST(avg(c) AS DECIMAL(12,8))|CAST((sum(c) / count(c)) AS DECIMAL(12,7))|
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+
|338651.475|96      |3527.61953125000000000000000|3527.6195313|3527.61953130                |3527.6195313                              |
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+

```

Here's what one finds on GPU:
```
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+
|sum(c)    |count(c)|(sum(c) / count(c))         |avg(c)      |CAST(avg(c) AS DECIMAL(12,8))|CAST((sum(c) / count(c)) AS DECIMAL(12,7))|
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+
|338651.475|96      |3527.61953125000000000000000|3527.6195312|3527.61953120                |3527.6195313                              |
+----------+--------+----------------------------+------------+-----------------------------+------------------------------------------+
```",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1828821221/reactions,0,0,0,0,0,0,0,0,0,9682
996,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833138166,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1833138166,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1833138166,IC_kwDOD7z77c5tQ3f2,2023-11-30T05:36:34Z,2023-11-30T05:37:11Z,COLLABORATOR,"There were some red herrings in investigating this bug.

First off, I have closed the CUDF bug (rapidsai/cudf#14507) I raised for this.  CUDF is not at fault; it consistently truncates additional decimal digits.

It looked like this might have to do with `GpuDecimalAverage` and `GpuDecimalDivide`, but those operators are not involved. Consider this query:
```sql
SELECT avg(c) FROM foobar; -- c is a DECIMAL(8,3).
```
The execution plan indicates `GpuDecimalAverage` isn't invoked at all. The operation is done completely in integer/double, and the result is cast to `DECIMAL(12,7)`:
```
== Optimized Logical Plan ==Aggregate [cast((avg(UnscaledValue(c#9)) / 1000.0) as decimal(12,7)) AS avg(c)#888]
+- Relation [c#9] parquet

== Physical Plan ==
GpuColumnarToRow false
+- GpuHashAggregate(keys=[], functions=[avg(UnscaledValue(c#9), DoubleType)], output=[avg(c)#888])
   +- GpuShuffleCoalesce 1073741824
      +- GpuColumnarExchange gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=1819]
         +- GpuHashAggregate(keys=[], functions=[partial_avg(UnscaledValue(c#9), DoubleType)], output=[sum#892, count#893L])
            +- GpuFileGpuScan parquet [c#9] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/tmp/decimals_avg.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c:decimal(8,3)>
```
The `avg()` is computed on the unscaled decimal reps, and the average is divided by `1000.0`.  The result is cast to `DECIMAL(12,7)`.

The `avg()` and the divide produce the same results on CPU and GPU.  So this amounts to a casting problem.

Here's the simplest repro for the problem:
```scala
Seq(3527.61953125).toDF(""d"").repartition(1).selectExpr(""d"", ""CAST(d AS DECIMAL(12,7))"").show
```
On CPU:
```sql
+-------------+---------------+
|            d|as_decimal_12_7|
+-------------+---------------+
|3527.61953125|   3527.6195313|
+-------------+---------------+
```
On GPU:
```sql
+-----------+---------------+
|          d|as_decimal_12_7|
+-----------+---------------+
|3527.619531|   3527.6195312|
+-----------+---------------+
```
(Ignore how `d` is printed as `3527.619531` on GPU.  I think that's just a string/display formatting issue. The correct/complete value is written if serialized to file.)

All mention of window functions, aggregation, `GpuDecimalAverage` and everything else is a distraction.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833138166/reactions,0,0,0,0,0,0,0,0,0,9682
997,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833903840,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1833903840,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1833903840,IC_kwDOD7z77c5tTybg,2023-11-30T14:39:24Z,2023-11-30T14:39:24Z,COLLABORATOR,"This is a performance optimization in Spark that is only supposed to happen when the value would not be impacted by potential floating point issues.

https://github.com/apache/spark/blob/9bb358b51e30b5041c0cd20e27cf995aca5ed4c7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L2110-L2142

So if the precision is less than 15. 15 requires 50 bits to store it and a double has 52 bits in the significant section so the result should produce the correct answer without any possibility of errors.

https://en.wikipedia.org/wiki/Double-precision_floating-point_format

So if we are getting the wrong answer back, then the problem is some where in the computation that the average was replaced with.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833903840/reactions,0,0,0,0,0,0,0,0,0,9682
998,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833933174,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1833933174,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1833933174,IC_kwDOD7z77c5tT5l2,2023-11-30T14:51:01Z,2023-11-30T14:51:01Z,COLLABORATOR,"I am remembering more now.  Converting a double to a decimal value has problems because they do it by going from a double to a string to a decimal. This is inherent in how scala does it in their BigDecimal class, and it is even a bit of magic with an implicit method that just makes it happen behind the scenes. But going from a Double to a String we cannot match what java does. It is not standard which is why we have `spark.rapids.sql.castStringToFloat.enabled`, which is off by default. Now I really want to understand what would happen if this optimization is disabled and what the result really would be. Is Spark right and we are wrong, or is Spark wrong and we are right? ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833933174/reactions,0,0,0,0,0,0,0,0,0,9682
999,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834050352,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1834050352,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1834050352,IC_kwDOD7z77c5tUWMw,2023-11-30T15:55:59Z,2023-11-30T15:55:59Z,COLLABORATOR,"So java does odd things when interpreting floating point values compared to the rest of the world. They try to fix the problem that some decimal values cannot be represented as floating point values.

https://docs.oracle.com/javase/8/docs/api/java/lang/Double.html#toString-double-

https://docs.oracle.com/javase/8/docs/api/java/lang/Double.html#valueOf-java.lang.String-

They are self consistent, but it is not standard.  The number we are trying to convert is one of them that cannot be accurately represented as a double.

https://binaryconvert.com/result_double.html?decimal=051053050055046054049057053051049050053

So technically the Spark performance optimization is wrong in the general case. But I think how Java/Scala convert double to Strings and in turn decimal values ""fixes"" it.

So there are two options that we have to fix the problem ourselves.  We either undo the optimization and just to the average on Decimal values, or we find a way to replicate what Java is doing.  None of these are simple.

In the case of a Window operation it is not that hard to undo the optimization because it is self contained in a single exec.  We can do the pattern matching see the UnscaledValue(e) being manipulated. But for a hash aggregation or a reduction it gets to be much harder.  Especially if the optimization later went through other transformations related to distinct/etc it could get to be really hard to detect and undo this. We might be able to just find the final Divide by a constant followed by a cast to a Decimal and try to rewrite that part. Just because we get the rest of it right. That might be the simplest way to make this work.

Matching java code is really difficult because it is GPL Licensed so we cannot copy or even read it and try to apply it.

I think if we can try and detect the case of `cast(Long/Double Scalar that is 10 ^ x as Decimal(SOMETHING, X + 4)` and replace it with `CAST(LONG as Decimal(19,2))/DecimalScalar(10^X) and then have the divide output the desired precision and scale like we do for other decimal average conversions, then we are good.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834050352/reactions,1,1,0,0,0,0,0,0,0,9682
1000,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834055305,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1834055305,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1834055305,IC_kwDOD7z77c5tUXaJ,2023-11-30T15:58:43Z,2023-11-30T15:58:43Z,COLLABORATOR,But before we do any of that I want to be sure that we know what the original input long was before the divide happened and what the double was that we are dividing? I am assuming that it was `(352761953125 / 10 ^ 8)`,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834055305/reactions,1,1,0,0,0,0,0,0,0,9682
1001,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834182804,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1834182804,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1834182804,IC_kwDOD7z77c5tU2iU,2023-11-30T17:07:42Z,2023-11-30T17:07:42Z,COLLABORATOR,"> know what the original input long was before the divide happened and what the double was that we are dividing? I am assuming that it was `(352761953125/ 10 ^ 8)`

Not exactly. The result of the average (of the unscaled decimals) was `3527619.53125L`. That was then divided by `1000.0L` to rescale it. ",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834182804/reactions,0,0,0,0,0,0,0,0,0,9682
1002,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834650958,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1834650958,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1834650958,IC_kwDOD7z77c5tWo1O,2023-11-30T22:19:07Z,2023-11-30T22:21:24Z,COLLABORATOR,"I can confirm here that [`GpuCast::castFloatsToDecimal()`](https://github.com/NVIDIA/spark-rapids/blob/fcad2279c31aa1d53e0c132cf90432169e212446/sql-plugin/src/main/scala/com/nvidia/spark/rapids/GpuCast.scala#L1620) seems to be the one producing the differing output:
```scala
    // Approach to minimize difference between CPUCast and GPUCast:
    // step 1. cast input to FLOAT64 (if necessary)
    // step 2. cast FLOAT64 to container DECIMAL (who keeps one more digit for rounding)
    // step 3. perform HALF_UP rounding on container DECIMAL
    val checkedInput = withResource(input.castTo(DType.FLOAT64)) { double =>
      val roundedDouble = double.round(dt.scale, cudf.RoundMode.HALF_UP)
      withResource(roundedDouble) { rounded =>
      // ...
      }
    }
```
The second step (i.e. after ensuring the input is `FLOAT64`) is to `round(RoundMode.HALF_UP)`. This causes the following change in the input row:
```scala
3527.61953125 -> 3527.6195312
```
The (final) CPU output for this row is `3527.6195313`.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1834650958/reactions,0,0,0,0,0,0,0,0,0,9682
1003,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1957736523,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-1957736523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,1957736523,IC_kwDOD7z77c50sLBL,2024-02-21T19:17:11Z,2024-02-21T19:17:11Z,COLLABORATOR,I've relinquished ownership on this bug. I'm not actively working on this one.,,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1957736523/reactions,0,0,0,0,0,0,0,0,0,9682
1004,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2126616418,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-2126616418,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,2126616418,IC_kwDOD7z77c5-wZdi,2024-05-23T09:11:38Z,2024-05-23T09:11:38Z,COLLABORATOR,"> I am remembering more now. Converting a double to a decimal value has problems because they do it by going from a double to a string to a decimal. This is inherent in how scala does it in their BigDecimal class, and it is even a bit of magic with an implicit method that just makes it happen behind the scenes. But going from a Double to a String we cannot match what java does. It is not standard which is why we have `spark.rapids.sql.castStringToFloat.enabled`, which is off by default. Now I really want to understand what would happen if this optimization is disabled and what the result really would be. Is Spark right and we are wrong, or is Spark wrong and we are right?

Since we have an _almost match_ float to string kernel in [jni](https://github.com/NVIDIA/spark-rapids-jni/blob/branch-24.06/src/main/cpp/src/cast_float_to_string.cu), does that means we can also _almost match_ float to decimal easily by follow Spark's _float => string => decimal_ way?
",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2126616418/reactions,0,0,0,0,0,0,0,0,0,9682
1005,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127064316,https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-2127064316,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9682,2127064316,IC_kwDOD7z77c5-yGz8,2024-05-23T13:07:13Z,2024-05-23T13:07:13Z,COLLABORATOR,"@thirtiseven yes that is one possibility, but again it is almost match. That is up to management about how close is good enough.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127064316/reactions,0,0,0,0,0,0,0,0,0,9682
1006,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1811369694,https://github.com/NVIDIA/spark-rapids/issues/9705#issuecomment-1811369694,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9705,1811369694,IC_kwDOD7z77c5r907e,2023-11-14T21:42:23Z,2023-11-14T21:42:23Z,COLLABORATOR,Scope should be to fully support StringGen for all characters.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1811369694/reactions,0,0,0,0,0,0,0,0,0,9705
1007,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1811370436,https://github.com/NVIDIA/spark-rapids/issues/9705#issuecomment-1811370436,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9705,1811370436,IC_kwDOD7z77c5r91HE,2023-11-14T21:42:59Z,2023-11-14T21:42:59Z,COLLABORATOR,Added as low priority to this epic: https://github.com/NVIDIA/spark-rapids/issues/9.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1811370436/reactions,0,0,0,0,0,0,0,0,0,9705
1008,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819505329,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1819505329,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1819505329,IC_kwDOD7z77c5sc3Kx,2023-11-20T17:26:02Z,2023-11-20T17:26:02Z,CONTRIBUTOR,"I modified the test to show the input that causes this error:

```
-Row(a='oNÍ[\x87\x01áe>\x85', regexp_replace(a, .*$, PROD, 1)='PRODPROD\x85PROD')
+Row(a='oNÍ[\x87\x01áe>\x85', regexp_replace(a, .*$, PROD, 1)='PROD\x85PROD')
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819505329/reactions,0,0,0,0,0,0,0,0,0,9731
1009,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819519369,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1819519369,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1819519369,IC_kwDOD7z77c5sc6mJ,2023-11-20T17:35:28Z,2023-11-20T17:52:59Z,CONTRIBUTOR,"Simpler repro:

```python
    .with_special_case('a\x85')
```

```
-Row(a='a\x85', regexp_replace(a, .*$, PROD, 1)='PRODPROD\x85PROD')
+Row(a='a\x85', regexp_replace(a, .*$, PROD, 1)='PROD\x85PROD')
```

Note that `a\x84` works fine, so this appears to be specific to certain characters.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819519369/reactions,0,0,0,0,0,0,0,0,0,9731
1010,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863591532,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1863591532,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1863591532,IC_kwDOD7z77c5vFCZs,2023-12-19T23:02:45Z,2023-12-19T23:02:45Z,CONTRIBUTOR,Adding the triage label back now that there is a summary of the issue. ,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863591532/reactions,0,0,0,0,0,0,0,0,0,9731
1011,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869776982,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1869776982,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1869776982,IC_kwDOD7z77c5vcohW,2023-12-26T21:18:42Z,2023-12-26T21:18:42Z,COLLABORATOR,"Need to investigate why we got \x85 as an input string, since it is not a valid UTF-8 string.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869776982/reactions,0,0,0,0,0,0,0,0,0,9731
1012,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869800880,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1869800880,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1869800880,IC_kwDOD7z77c5vcuWw,2023-12-26T22:25:39Z,2023-12-26T22:25:39Z,COLLABORATOR,"It looks like this might be related to how Spark/Python interprets the string 'a\x85'. 

```
import pyspark.sql.types

df = spark.createDataFrame(SparkContext.getOrCreate().parallelize([(""a\x85"")]), pyspark.sql.types.StringType())
spark.conf.set(""spark.rapids.sql.enabled"", False)
df.selectExpr('CAST(value as BINARY)').show()
+----------+
|     value|
+----------+
|[61 C2 85]|
+----------+

```

[C2 85] is the UTF-8 encoded character that is the same as ""\u0085"", which is Next Line - NEL. The result, when it is pulled back into Spark converts the `[C2 85]` back to latin1 as an ""\x85"" in python.

So it looks like a kind of odd situation where `.*$` is behaving differently for `NEL` compared to `\n`.

```
import pyspark.sql.types
df = spark.createDataFrame(SparkContext.getOrCreate().parallelize([(""a\x85""), (""a\n"")]), pyspark.sql.types.StringType())
spark.conf.set(""spark.rapids.sql.enabled"", True)
df.selectExpr(""CAST(regexp_replace(value, '.*$', 'P', 1) AS BINARY) as p"").show(truncate=False)
+-------------+
|p            |
+-------------+
|[50 C2 85 50]|
|[50 50 0A 50]|
+-------------+

spark.conf.set(""spark.rapids.sql.enabled"", False)
df.selectExpr(""CAST(regexp_replace(value, '.*$', 'P', 1) AS BINARY) as p"").show(truncate=False)
+----------------+
|p               |
+----------------+
|[50 50 C2 85 50]|
|[50 50 0A 50]   |
+----------------+
```",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869800880/reactions,0,0,0,0,0,0,0,0,0,9731
1013,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1883963913,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1883963913,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1883963913,IC_kwDOD7z77c5wSwIJ,2024-01-09T23:38:02Z,2024-01-09T23:38:02Z,COLLABORATOR,"Note that `\u0085` is recognized as a valid line terminator: https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html

```
A line terminator is a one- or two-character sequence that marks the end of a line of the input character sequence. The following are recognized as line terminators:

A newline (line feed) character ('\n'),
A carriage-return character followed immediately by a newline character (""\r\n""),
A standalone carriage-return character ('\r'),
A next-line character ('\u0085'),
A line-separator character ('\u2028'), or
A paragraph-separator character ('\u2029).
```

So the replace in this case is not actually working properly on the GPU. 

Also, Python 3 strings are by default UTF-8, so `a\x85` and `a\u0085` are equivalent as strings, Python just displays as `'a\x85'`, so it's not latin1.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1883963913/reactions,0,0,0,0,0,0,0,0,0,9731
1014,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905058986,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1905058986,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1905058986,IC_kwDOD7z77c5xjOSq,2024-01-23T00:10:43Z,2024-01-23T00:10:43Z,CONTRIBUTOR,"Note that we transpile the pattern `.*$` to `[^\n\r\u0085\u2028\u2029]*(?:\r|\u0085|\u2028|\u2029|\r\n)?$` before passing to cuDF, so maybe we need to update this to handle `NEL`. I am investigating.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905058986/reactions,0,0,0,0,0,0,0,0,0,9731
1015,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905103059,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1905103059,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1905103059,IC_kwDOD7z77c5xjZDT,2024-01-23T00:57:57Z,2024-01-23T00:57:57Z,CONTRIBUTOR,"cuDF repro:

```java
@Test
void testStringReplaceEdgeCase() {
    TableDebug debug = TableDebug.builder().build();
    RegexProgram target = new RegexProgram(
            ""[^\n\r\u0085\uc285\u2028\u2029]*(?:\r|\u0085|\uc285|\u2028|\u2029|\r\n)?$"");
    try (ColumnVector input = ColumnVector.fromStrings(""a\n"", ""a\u0085"");
         ColumnVector expected = ColumnVector.fromStrings(""PRODPROD\nPROD"", ""PRODPROD\u0085PROD"");
         Scalar replace = Scalar.fromString(""PROD"");
         ColumnVector output = input.replaceRegex(target, replace)) {
        debug.debug(""input"", input);
        debug.debug(""output"", output);
        assertColumnsAreEqual(expected, output);
    }
}
```

Output:

```
GPU COLUMN input - NC: 0 DATA: DeviceMemoryBufferView{address=0x7f7e86000000, length=5, id=-1} VAL: null
COLUMN input - STRING
0 ""a
"" 610a
1 ""a<85>"" 61c285
GPU COLUMN output - NC: 0 DATA: DeviceMemoryBufferView{address=0x7f7e86000a00, length=21, id=-1} VAL: null
COLUMN output - STRING
0 ""PRODPROD
PROD"" 50524f4450524f440a50524f44
1 ""PRODPROD"" 50524f4450524f44
HOST PADDING SIZE: 32
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905103059/reactions,0,0,0,0,0,0,0,0,0,9731
1016,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905106209,https://github.com/NVIDIA/spark-rapids/issues/9731#issuecomment-1905106209,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9731,1905106209,IC_kwDOD7z77c5xjZ0h,2024-01-23T01:01:55Z,2024-01-23T01:01:55Z,CONTRIBUTOR,"@NVnavkumar based on the test I posted here, I am not sure if this is really a bug in cuDF or not. What do you think?",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1905106209/reactions,0,0,0,0,0,0,0,0,0,9731
1017,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1814816858,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1814816858,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1814816858,IC_kwDOD7z77c5sK-ha,2023-11-16T16:37:45Z,2023-11-16T16:37:45Z,COLLABORATOR,"Actually I reproduced this and it looks like it is not a small diff. It looks like Spark overflowed and produced an inf, when we produced a very large value instead.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1814816858/reactions,0,0,0,0,0,0,0,0,0,9744
1018,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1814820523,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1814820523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1814820523,IC_kwDOD7z77c5sK_ar,2023-11-16T16:39:57Z,2023-11-16T16:39:57Z,COLLABORATOR,"```
-Row(a=-1.7976931348623157e+308, b=3.049517727610612e+300, HYPOT(a, b)=inf)
+Row(a=-1.7976931348623157e+308, b=3.049517727610612e+300, HYPOT(a, b)=1.7976931348623157e+308)
```",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1814820523/reactions,0,0,0,0,0,0,0,0,0,9744
1019,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836954055,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1836954055,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1836954055,IC_kwDOD7z77c5tfbHH,2023-12-02T00:24:38Z,2023-12-02T00:25:17Z,COLLABORATOR,"I've checked out the `hypot` code from openjdk (https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/lang/FdLibm.java#L1968) and realized that our implementation (`GpuHypot`) is completely different so it is almost impossible to guarantee a match, unless we reimplement our code. We can only try to make our output close to it as much as we can. I'll continue to study this to see what can we do for it.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836954055/reactions,0,0,0,0,0,0,0,0,0,9744
1020,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1838964051,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1838964051,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1838964051,IC_kwDOD7z77c5tnF1T,2023-12-04T16:07:01Z,2023-12-04T16:07:01Z,COLLABORATOR,@ttnghia be very careful here. That code is GPL licensed and we cannot copy it in any way.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1838964051/reactions,0,0,0,0,0,0,0,0,0,9744
1021,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1839035462,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1839035462,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1839035462,IC_kwDOD7z77c5tnXRG,2023-12-04T16:40:26Z,2023-12-04T16:40:26Z,COLLABORATOR,"Yes, and that is also one of the biggest problems now.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1839035462/reactions,0,0,0,0,0,0,0,0,0,9744
1022,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849310703,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1849310703,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1849310703,IC_kwDOD7z77c5uOj3v,2023-12-11T04:33:46Z,2023-12-11T04:33:46Z,COLLABORATOR,moved to 24.02,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1849310703/reactions,0,0,0,0,0,0,0,0,0,9744
1023,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1907106199,https://github.com/NVIDIA/spark-rapids/issues/9744#issuecomment-1907106199,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9744,1907106199,IC_kwDOD7z77c5xrCGX,2024-01-23T23:50:37Z,2024-01-23T23:50:37Z,COLLABORATOR,"Removing the target release, as we do not have a near term fix, nor do we know of anyone using this feature. ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1907106199/reactions,0,0,0,0,0,0,0,0,0,9744
1024,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1919785565,https://github.com/NVIDIA/spark-rapids/issues/9753#issuecomment-1919785565,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9753,1919785565,IC_kwDOD7z77c5ybZpd,2024-01-31T19:27:52Z,2024-01-31T19:27:52Z,COLLABORATOR,"I tested this against Spark 3.5.0 and was unable to repro the bug on the GPU. The bug is reproducible on the CPU when I did the `:paste` 

@NVnavkumar also tested this but saw an interesting thing where the bug wasn't reproducing even on the CPU when he did a `:load` but was able to see the bug when using `:paste`

Based on @tgravescs 's suggestion we can retarget this for 24.04",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1919785565/reactions,0,0,0,0,0,0,0,0,0,9753
1025,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970044992,https://github.com/NVIDIA/spark-rapids/issues/9753#issuecomment-1970044992,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9753,1970044992,IC_kwDOD7z77c51bIBA,2024-02-28T22:44:08Z,2024-02-28T23:16:05Z,CONTRIBUTOR,"I think that we do need to update our version of the shuffle reader to match Spark. I updated locally to match 3.5.0 (before the fix) to try and reproduce the issue on the GPU but found that we fall back to CPU with:

```
 ! <TableCacheQueryStageExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.adaptive.TableCacheQueryStageExec
```

I will create a PR to update the shuffle reader to match ~Spark 3.5.1~  all the Spark versions, which will involve some shim work.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970044992/reactions,0,0,0,0,0,0,0,0,0,9753
1026,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1816675351,https://github.com/NVIDIA/spark-rapids/issues/9759#issuecomment-1816675351,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9759,1816675351,IC_kwDOD7z77c5sSEQX,2023-11-17T15:54:00Z,2023-11-17T15:54:00Z,COLLABORATOR,Why just broadcast? Why not make it generic for all of the joins that can use AST?,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1816675351/reactions,0,0,0,0,0,0,0,0,0,9759
1027,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815550430,https://github.com/NVIDIA/spark-rapids/issues/9763#issuecomment-1815550430,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9763,1815550430,IC_kwDOD7z77c5sNxne,2023-11-17T00:36:52Z,2023-11-17T00:36:52Z,COLLABORATOR,"For example, if we have two tests using `IntegerGen()` then they can share exactly the same input.
Special generators such as `IntegerGen().with_special_case()` need to be considered as different from the pure `IntegerGen()` through internal object hashing.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815550430/reactions,0,0,0,0,0,0,0,0,0,9763
1028,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815574435,https://github.com/NVIDIA/spark-rapids/issues/9763#issuecomment-1815574435,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9763,1815574435,IC_kwDOD7z77c5sN3ej,2023-11-17T01:06:10Z,2023-11-17T01:06:10Z,COLLABORATOR,"Currently we already have cached dataframes:
https://github.com/NVIDIA/spark-rapids/blob/30c3df35ab7e87f71ecd84789529d25d9a289848/integration_tests/src/main/python/data_gen.py#L758-L763

And have a `_cache_repr` in dataGens for hashing, is that the same thing as this issue?",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815574435/reactions,0,0,0,0,0,0,0,0,0,9763
1029,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815738829,https://github.com/NVIDIA/spark-rapids/issues/9763#issuecomment-1815738829,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9763,1815738829,IC_kwDOD7z77c5sOfnN,2023-11-17T04:40:52Z,2023-11-17T04:40:52Z,COLLABORATOR,"Oh wow, I didn't know that. But we still have all tests running very slow. However, such decoration is only caching python lists of values, not Spark dataframes. Is it possible to cache Spark dataframes instead? ",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815738829/reactions,0,0,0,0,0,0,0,0,0,9763
1030,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815746201,https://github.com/NVIDIA/spark-rapids/issues/9763#issuecomment-1815746201,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9763,1815746201,IC_kwDOD7z77c5sOhaZ,2023-11-17T04:52:54Z,2023-11-17T04:59:03Z,COLLABORATOR,"> However, such decoration is only caching python lists of values, not Spark dataframes. Is it possible to cache Spark dataframes instead?

Yes, it is possible, we are not doing this now because the runtime config in spark session may affect the result dataframes. It will make the IT run slightly faster under my tests. It is tracked by https://github.com/NVIDIA/spark-rapids/issues/8524

The current performance bottleneck of IT seems to be https://github.com/NVIDIA/spark-rapids/issues/8447",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1815746201/reactions,0,0,0,0,0,0,0,0,0,9763
1031,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819444006,https://github.com/NVIDIA/spark-rapids/issues/9767#issuecomment-1819444006,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9767,1819444006,IC_kwDOD7z77c5scoMm,2023-11-20T16:49:16Z,2023-11-20T16:49:16Z,MEMBER,Duplicated by #9778 which has some details on why this fails.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819444006/reactions,0,0,0,0,0,0,0,0,0,9767
1032,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819459981,https://github.com/NVIDIA/spark-rapids/issues/9767#issuecomment-1819459981,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9767,1819459981,IC_kwDOD7z77c5scsGN,2023-11-20T16:58:43Z,2023-11-20T16:58:43Z,COLLABORATOR,"Thank you, @jlowe. Looks like another xfail case. There are similar xfails in the tests already, pertaining to dataframe conversions through Pandas. ",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819459981/reactions,0,0,0,0,0,0,0,0,0,9767
1033,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821762716,https://github.com/NVIDIA/spark-rapids/issues/9767#issuecomment-1821762716,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9767,1821762716,IC_kwDOD7z77c5sleSc,2023-11-21T21:57:11Z,2023-11-21T21:57:11Z,COLLABORATOR,"The xfail is being handled in #9677.  From Issue #9778 , Jason pointed out: 
> On Databricks 13.3, nulls in the Pandas DataFrame (represented as NaNs) are being honored as nulls in the resulting Spark DataFrame when converting a Pandas DataFrame to a Spark DataFrame. Pandas thinks there are nulls in the data, and those nulls are propagating to the Spark DataFrame.
>
> fastparquet loads the NaNs properly, but then when converting the data to pandas, pandas thinks the NaN values are null. This, in turn, causes spark.createDataFrame to produce corresponding nulls. When comparing this to the GPU direct load of the data that contains NaNs (not nulls), the test fails. The problem is not in the way the GPU loads the data, it's the way the NaNs get converted into nulls due to sending the data through pandas before converting to a Spark DataFrame.

Basically he is asking to fix the test case so NaNs do not become nulls when passing through Pandas.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821762716/reactions,0,0,0,0,0,0,0,0,0,9767
1034,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821819727,https://github.com/NVIDIA/spark-rapids/issues/9767#issuecomment-1821819727,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9767,1821819727,IC_kwDOD7z77c5slsNP,2023-11-21T22:47:07Z,2023-11-21T22:47:07Z,COLLABORATOR,"> The xfail is being handled in #9677.

Do we mean #9776?  Then, no, it's not. #9776 is a different `xfail` case, for timestamps.  This current issue was for floating-point failures.

> so NaNs do not become nulls when passing through Pandas...

I cannot do that in the short term.  The shortest path to constructing a Spark dataframe from what's read by `fastparquet` is to route it through Pandas.  And Pandas does not distinguish between NaN and null values.

This will go into the already long list of `fastparquet`/Pandas/Spark incompatible xfail conditions.  We can revisit routing around Pandas when the higher priority tasks are sorted.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821819727/reactions,0,0,0,0,0,0,0,0,0,9767
1035,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821830098,https://github.com/NVIDIA/spark-rapids/issues/9767#issuecomment-1821830098,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9767,1821830098,IC_kwDOD7z77c5sluvS,2023-11-21T22:58:14Z,2023-11-21T22:58:14Z,COLLABORATOR,"> Do we mean #9776? Then, no, it's not. #9776 is a different xfail case, for timestamps. This current issue was for floating-point failures.

Ah, I just spoke with @jlowe, and looked more closely at #9677.  I understand now: @jlowe has that `xfailed` out already.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821830098/reactions,0,0,0,0,0,0,0,0,0,9767
1036,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997836469,https://github.com/NVIDIA/spark-rapids/issues/9774#issuecomment-1997836469,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9774,1997836469,IC_kwDOD7z77c53FJC1,2024-03-14T16:20:03Z,2024-03-14T16:20:03Z,COLLABORATOR,"The implementation of JsonToStructs and ScanJson have been combined, so now we need to validate this for both of them.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997836469/reactions,0,0,0,0,0,0,0,0,0,9774
1037,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904970455,https://github.com/NVIDIA/spark-rapids/issues/9775#issuecomment-1904970455,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9775,1904970455,IC_kwDOD7z77c5xi4rX,2024-01-22T22:52:43Z,2024-01-22T22:52:43Z,CONTRIBUTOR,"Note that we currently fall back to CPU in GpuJsonSchema if the schema contains nested types, so adding recursive checks is not necessary until we support reading nested types in https://github.com/NVIDIA/spark-rapids/issues/10241",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904970455/reactions,0,0,0,0,0,0,0,0,0,9775
1038,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819833537,https://github.com/NVIDIA/spark-rapids/issues/9787#issuecomment-1819833537,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9787,1819833537,IC_kwDOD7z77c5seHTB,2023-11-20T21:30:55Z,2023-11-20T21:30:55Z,COLLABORATOR,cc @pxLi ,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1819833537/reactions,0,0,0,0,0,0,0,0,0,9787
1039,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1820072594,https://github.com/NVIDIA/spark-rapids/issues/9787#issuecomment-1820072594,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9787,1820072594,IC_kwDOD7z77c5sfBqS,2023-11-21T01:33:56Z,2023-11-21T01:33:56Z,COLLABORATOR,"> cc @pxLi

Looks promising! Please let us know when the tests are ready to go (good to have some easy-to-poke flag to change the forkCount in script), we can help with the resource planning and the CI-related updates later, thanks!",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1820072594/reactions,0,0,0,0,0,0,0,0,0,9787
1040,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821539874,https://github.com/NVIDIA/spark-rapids/issues/9787#issuecomment-1821539874,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9787,1821539874,IC_kwDOD7z77c5skn4i,2023-11-21T19:18:45Z,2023-11-21T19:18:45Z,COLLABORATOR,I was hoping to get your help in taking it over the finish line.,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821539874/reactions,0,0,0,0,0,0,0,0,0,9787
1041,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821943238,https://github.com/NVIDIA/spark-rapids/issues/9787#issuecomment-1821943238,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9787,1821943238,IC_kwDOD7z77c5smKXG,2023-11-22T01:23:06Z,2023-11-22T01:23:06Z,COLLABORATOR,"> I was hoping to get your help in taking it over the finish line.

Currently we may not have time for this one, 
cc @GaryShen2008 to help with the potential UT speed-up approach thanks",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821943238/reactions,0,0,0,0,0,0,0,0,0,9787
1042,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1907263798,https://github.com/NVIDIA/spark-rapids/issues/9787#issuecomment-1907263798,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9787,1907263798,IC_kwDOD7z77c5xrok2,2024-01-24T02:52:11Z,2024-01-24T02:52:11Z,COLLABORATOR,Postpone to 24.04,,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1907263798/reactions,0,0,0,0,0,0,0,0,0,9787
1043,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1891388770,https://github.com/NVIDIA/spark-rapids/issues/9792#issuecomment-1891388770,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9792,1891388770,IC_kwDOD7z77c5wvE1i,2024-01-15T06:34:47Z,2024-01-15T06:34:47Z,COLLABORATOR,"Can we change this to release 24.04.
For US costomers, they are using DST (Day Light Saving) time zone, we can not support DST time zone in realse 24.02.
For China customers, they do not use Databricks.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1891388770/reactions,0,0,0,0,0,0,0,0,0,9792
1044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998769544,https://github.com/NVIDIA/spark-rapids/issues/9792#issuecomment-1998769544,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9792,1998769544,IC_kwDOD7z77c53Is2I,2024-03-15T01:47:23Z,2024-03-15T01:47:23Z,COLLABORATOR,"Not planing work on this for release 24.04, unassign it from me.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998769544/reactions,0,0,0,0,0,0,0,0,0,9792
1045,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821016297,https://github.com/NVIDIA/spark-rapids/issues/9807#issuecomment-1821016297,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9807,1821016297,IC_kwDOD7z77c5sioDp,2023-11-21T14:20:46Z,2023-11-21T14:20:46Z,COLLABORATOR,I had this fail outside of cdh.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821016297/reactions,0,0,0,0,0,0,0,0,0,9807
1046,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821799311,https://github.com/NVIDIA/spark-rapids/issues/9807#issuecomment-1821799311,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9807,1821799311,IC_kwDOD7z77c5slnOP,2023-11-21T22:26:33Z,2023-11-21T22:26:33Z,COLLABORATOR,Is this a case where the timezone is not set in the CDH test environment?  ,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821799311/reactions,0,0,0,0,0,0,0,0,0,9807
1047,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823143769,https://github.com/NVIDIA/spark-rapids/issues/9807#issuecomment-1823143769,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9807,1823143769,IC_kwDOD7z77c5sqvdZ,2023-11-22T16:55:06Z,2023-11-22T16:55:06Z,MEMBER,"The problem relates to an arguably invalid date being generated during the test.  During the failed run, the date `1582-10-13` is generated.  According to the Gregorian calendar, that date doesn't exist, because it falls in the 10 lost days during the transition from Julian to Gregorian. 1582-10-04 is followed by 1582-10-15.  When the CPU writes this to an ORC file, it marshals the date to 10-15 on both a CPU and GPU read:
```
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")

scala> Seq(""1582-10-13"").toDF(""s"").selectExpr(""cast(s as date) as d"").repartition(1).write.orc(""/tmp/orccpu"")

scala> spark.read.orc(""/tmp/orccpu"").show()
+----------+
|         d|
+----------+
|1582-10-15|
+----------+

scala> spark.conf.set(""spark.rapids.sql.enabled"", ""true"")

scala> spark.read.orc(""/tmp/orccpu"").show()
23/11/22 16:28:01 WARN GpuOverrides: 
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU

+----------+
|         d|
+----------+
|1582-10-15|
+----------+
```

However when the GPU writes the date, it encodes it in such a way that the CPU and GPU readers differ on the value:
```
scala> spark.conf.set(""spark.rapids.sql.enabled"", ""true"")

scala> Seq(""1582-10-13"").toDF(""s"").selectExpr(""cast(s as date) as d"").repartition(1).write.orc(""/tmp/orcgpu"")
23/11/22 16:26:39 WARN GpuOverrides: 
!Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient
  @Partitioning <SinglePartition$> could run on GPU
  ! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec
    @Expression <AttributeReference> d#27 could run on GPU

scala> spark.read.orc(""/tmp/orcgpu"").show()
23/11/22 16:28:08 WARN GpuOverrides: 
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU

+----------+
|         d|
+----------+
|1582-10-13|
+----------+

scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")

scala> spark.read.orc(""/tmp/orcgpu"").show()
+----------+
|         d|
+----------+
|1582-10-03|
+----------+
```

I tried pandas and the ORC Java tools against the CPU and GPU generated ORC files.  All readers agree the CPU-written file has the date `1582-10-15`.  With the GPU-written file, Pandas and the libcudf reader see the date as `1582-10-13` whereas Spark CPU and the ORC Java tools read the date as `1582-10-03`.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823143769/reactions,0,0,0,0,0,0,0,0,0,9807
1048,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823149200,https://github.com/NVIDIA/spark-rapids/issues/9807#issuecomment-1823149200,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9807,1823149200,IC_kwDOD7z77c5sqwyQ,2023-11-22T16:58:45Z,2023-11-22T16:58:45Z,MEMBER,Relates to #131 ,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1823149200/reactions,0,0,0,0,0,0,0,0,0,9807
1049,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1843779292,https://github.com/NVIDIA/spark-rapids/issues/9807#issuecomment-1843779292,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9807,1843779292,IC_kwDOD7z77c5t5dbc,2023-12-06T22:18:42Z,2023-12-06T22:18:42Z,MEMBER,Updated the compatibility docs to describe the incompatibilities with the lost days associated with the switch to the Gregorian calendar.  Putting this back into the backlog since fixing is deemed low priority.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1843779292/reactions,0,0,0,0,0,0,0,0,0,9807
1050,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821639570,https://github.com/NVIDIA/spark-rapids/issues/9822#issuecomment-1821639570,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9822,1821639570,IC_kwDOD7z77c5slAOS,2023-11-21T20:36:25Z,2023-11-21T20:36:25Z,COLLABORATOR,"I think this case might even change run to run. Our aggregations do not guarantee an order that the sum will happen. And floating point is not truly commutative. My guess is that the GPU got unlucky in this case and it overflowed, where the CPU which did the SUM in a strict order, didn't have the same problem.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1821639570/reactions,0,0,0,0,0,0,0,0,0,9822
1051,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875889215,https://github.com/NVIDIA/spark-rapids/issues/9822#issuecomment-1875889215,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9822,1875889215,IC_kwDOD7z77c5vz8w_,2024-01-03T19:53:24Z,2024-01-03T19:53:24Z,COLLABORATOR,"See comments in https://github.com/NVIDIA/spark-rapids/issues/10026, which I believe is another instance of this issue.",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875889215/reactions,0,0,0,0,0,0,0,0,0,9822
1052,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1878843177,https://github.com/NVIDIA/spark-rapids/issues/9822#issuecomment-1878843177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9822,1878843177,IC_kwDOD7z77c5v_N8p,2024-01-05T15:22:54Z,2024-01-05T15:22:54Z,COLLABORATOR,"Here is a smaller version of the test case from #10026 that still shows a difference:
```
+-------------+
|b            |
+-------------+
|8.2052361E17 |
|-9.615448E24 |
|3.4028235E38 |
|-8.9130315E23|
|-3.4028235E38|
|4.86208753E14|
|2.825404E25  |
|9.7615254E-4 |
+-------------+

SUM CPU: 1.7713718887409737e+25
SUM GPU: 1.7718319043726909e+25
APPROX EQ: False

SORTED SUM CPU: 1.7718319043726909e+25
SORTED SUM GPU: 1.7718319043726909e+25
APPROX EQ: True

REVERSE SORTED SUM CPU: 1.7718319043726909e+25
REVERSE SORTED SUM GPU: 1.7718319043726909e+25
APPROX EQ: True
```
Note that in this case I got the same result on GPU every time.  It's the CPU that gave a different result in the unsorted case.
Once again, if you add an accumulation column, GPU matches the CPU for the unsorted case, even though I get a different value for GPU when I do a SUM.
```
+----------------------+----------------------+----------------------+
|b                     |cpu_accum             |gpu_accum             |
+----------------------+----------------------+----------------------+
|8.2052360892841984E17 |8.2052360892841984E17 |8.2052360892841984E17 |
|-9.615447782308683E24 |-9.615446961785074E24 |-9.615446961785074E24 |
|3.4028234663852886E38 |3.4028234663851923E38 |3.4028234663851923E38 |
|-8.913031508548466E23 |3.4028234663851832E38 |3.4028234663851832E38 |
|-3.4028234663852886E38|-1.0540321989765048E25|-1.0540321989765048E25|
|4.862087528448E14     |-1.0540321989278838E25|-1.0540321989278838E25|
|2.8254040876688576E25 |1.7713718887409737E25 |1.7713718887409737E25 |
|9.761525434441864E-4  |1.7713718887409737E25 |1.7713718887409737E25 |
+----------------------+----------------------+----------------------+
```
It appears that for the SUM, the order of operations in the CPU matches the input data order, but for GPU, it does not, so we sometimes get different results.
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1878843177/reactions,0,0,0,0,0,0,0,0,0,9822
1053,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2029835974,https://github.com/NVIDIA/spark-rapids/issues/9822#issuecomment-2029835974,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9822,2029835974,IC_kwDOD7z77c54_NbG,2024-04-01T14:21:39Z,2024-04-01T14:21:39Z,COLLABORATOR,"We might want to split out the float and double datagens for this test into a separate test, so that all of the deterministic ones are in one test, and the ones affected by order are in a separate test.

One observation I had when I was looking into this for floats/doubles is that order really matters when the magnitudes of the values are very different.   Adding very small numbers to very large numbers can effectively ignore the small numbers, and the order can have an impact if enough small numbers add together to impact the final result.  One way to make the test more resilient to this would be to ensure that all of the random numbers are within a smaller range.  Maybe select an initial random starting value and then select a more restricted range around that initial value.",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2029835974/reactions,0,0,0,0,0,0,0,0,0,9822
1054,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832768235,https://github.com/NVIDIA/spark-rapids/issues/9847#issuecomment-1832768235,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9847,1832768235,IC_kwDOD7z77c5tPdLr,2023-11-29T21:59:44Z,2023-11-29T21:59:44Z,MEMBER,"The -0.0 vs 0.0 is not an actual mismatch in the test (it's using approx float comparison already), that's an artifact of the difftool used to produce the output when the test fails which is just looking at raw text and no longer doing the floating point approx comparison logic.

The real issue is more along the lines of #9350.  In this specific case with DATAGEN_SEED=3, the problem is only with single-precision floating point value 6.121944898040965e-05f which gets rounded incorrectly up instead of down.  I tracked this down to a problem in libcudf where the calculations for single-precision floating point rounding are forced to use single-precision floating point throughout which introduces enough error to cause the issue.  Filed https://github.com/rapidsai/cudf/issues/14528",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832768235/reactions,0,0,0,0,0,0,0,0,0,9847
1055,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832791921,https://github.com/NVIDIA/spark-rapids/issues/9847#issuecomment-1832791921,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9847,1832791921,IC_kwDOD7z77c5tPi9x,2023-11-29T22:19:52Z,2023-11-29T22:19:52Z,COLLABORATOR,"@jlowe do you think that this might happen with double values too for rounding to larger numbers of digits?  Right now we go up to 10, which exposed a problem for float. But should we try larger values for double?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832791921/reactions,0,0,0,0,0,0,0,0,0,9847
1056,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832800276,https://github.com/NVIDIA/spark-rapids/issues/9847#issuecomment-1832800276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9847,1832800276,IC_kwDOD7z77c5tPlAU,2023-11-29T22:27:40Z,2023-11-29T22:27:40Z,MEMBER,"> do you think that this might happen with double values too for rounding to larger numbers of digits?

Yes, I'm sure that will happen at some point for specific values and rounding places, even for doubles.  We'll won't be able to match Spark's behavior here if we stick with floating-point types, since Spark goes to BigDecimal and thus will have a lot more precision to wield.  Fixing the cudf issue I filed for floats may give us enough additional precision to handle most (all?) of the FLOAT32 cases, but it won't do anything to improve the known limitations when rounding with doubles to high numbers of decimal places.  We may want to do some experiments to know how many decimal places we can reliably round to and consider fallback to the CPU when we see rounding to a decimal place we cannot reliably match.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1832800276/reactions,0,0,0,0,0,0,0,0,0,9847
1057,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833970323,https://github.com/NVIDIA/spark-rapids/issues/9847#issuecomment-1833970323,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9847,1833970323,IC_kwDOD7z77c5tUCqT,2023-11-30T15:11:38Z,2023-11-30T15:11:38Z,NONE,"> We may want to do some experiments to know how many decimal places we can reliably round to and consider fallback to the CPU when we see rounding to a decimal place we cannot reliably match.

I think for float32 trying to round anything more than 7 decimal places (inclusive) might result in some ""strange"" results (even if you use double precision).

Consider a float32 value and its two closest neighbours:

```
x.1234567890 // decimal digits
1.00009977817535400391
1.00009989738464355469 // <-- we're trying to round this one
1.00010001659393310547
```

Rounding to six digits produces `1.00010001658383310547`, rounding to seven digits reproduces the input (you might want to see 1.0000999 as the ""correctly rounded in infinite precision"" value). But none of the nearby floats are a good representation of the value rounded to 7 digits. So you'd need to produce a double result to represent the rounding ""correctly"" (whatever that means).",,wence-,1126981,MDQ6VXNlcjExMjY5ODE=,https://avatars.githubusercontent.com/u/1126981?v=4,,https://api.github.com/users/wence-,https://github.com/wence-,https://api.github.com/users/wence-/followers,https://api.github.com/users/wence-/following{/other_user},https://api.github.com/users/wence-/gists{/gist_id},https://api.github.com/users/wence-/starred{/owner}{/repo},https://api.github.com/users/wence-/subscriptions,https://api.github.com/users/wence-/orgs,https://api.github.com/users/wence-/repos,https://api.github.com/users/wence-/events{/privacy},https://api.github.com/users/wence-/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1833970323/reactions,0,0,0,0,0,0,0,0,0,9847
1058,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1830810070,https://github.com/NVIDIA/spark-rapids/issues/9876#issuecomment-1830810070,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9876,1830810070,IC_kwDOD7z77c5tH_HW,2023-11-28T21:51:42Z,2023-11-28T21:51:42Z,COLLABORATOR,@razajafri: can you check if our integration tests for cache are passing for Spark 3.5 as they should be hitting this same issue?,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1830810070/reactions,0,0,0,0,0,0,0,0,0,9876
1059,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1910746997,https://github.com/NVIDIA/spark-rapids/issues/9876#issuecomment-1910746997,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9876,1910746997,IC_kwDOD7z77c5x4691,2024-01-25T18:18:42Z,2024-01-25T18:18:42Z,COLLABORATOR,"I ran the tests but forgot to post here. All of our current cache tests are passing. There is something else in the plan that's causing the TableCacheQueryStageExec to kick in. Will post my findings once I have investigated this more
",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1910746997/reactions,0,0,0,0,0,0,0,0,0,9876
1060,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989037157,https://github.com/NVIDIA/spark-rapids/issues/9876#issuecomment-1989037157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9876,1989037157,IC_kwDOD7z77c52jkxl,2024-03-11T17:33:41Z,2024-03-11T17:33:41Z,COLLABORATOR,According to @revans2 this is not a trivial task because internally Spark is looking for InMemoryTableScanExec. ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989037157/reactions,0,0,0,0,0,0,0,0,0,9876
1061,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1841630387,https://github.com/NVIDIA/spark-rapids/issues/9878#issuecomment-1841630387,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9878,1841630387,IC_kwDOD7z77c5txQyz,2023-12-05T21:21:17Z,2023-12-05T21:21:17Z,COLLABORATOR,We can test when we're doing Spark 4 integration.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1841630387/reactions,0,0,0,0,0,0,0,0,0,9878
1062,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1830889811,https://github.com/NVIDIA/spark-rapids/issues/9880#issuecomment-1830889811,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9880,1830889811,IC_kwDOD7z77c5tISlT,2023-11-28T22:57:03Z,2023-11-28T22:57:03Z,MEMBER,See https://github.com/apache/spark/commit/81639090622 for changes that were needed to the CPU BroadcastHashJoinExec that are probably relevant to the changes likely needed for the GPU version.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1830889811/reactions,0,0,0,0,0,0,0,0,0,9880
1063,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989442585,https://github.com/NVIDIA/spark-rapids/issues/9880#issuecomment-1989442585,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9880,1989442585,IC_kwDOD7z77c52lHwZ,2024-03-11T21:04:49Z,2024-03-11T21:04:49Z,COLLABORATOR,related to #9753 ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989442585/reactions,0,0,0,0,0,0,0,0,0,9880
1064,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2005182760,https://github.com/NVIDIA/spark-rapids/issues/9880#issuecomment-2005182760,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9880,2005182760,IC_kwDOD7z77c53hKko,2024-03-18T22:45:56Z,2024-03-18T22:45:56Z,CONTRIBUTOR,"> See [apache/spark@81639090622](https://github.com/apache/spark/commit/81639090622) for changes that were needed to the CPU BroadcastHashJoinExec that are probably relevant to the changes likely needed for the GPU version.

This commit updated the `outputPartitioning` logic in `BroadcastHashJoinExec`, but we do not currently implement this method in the GPU equivalent class. I filed https://github.com/NVIDIA/spark-rapids/issues/10609 to discuss whether we should also implement this, but as @revans2 pointed out, AQE will call the CPU version of this method anyway during replanning and before we replace these operators, so maybe we don't really need them.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2005182760/reactions,0,0,0,0,0,0,0,0,0,9880
1065,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1841641474,https://github.com/NVIDIA/spark-rapids/issues/9907#issuecomment-1841641474,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9907,1841641474,IC_kwDOD7z77c5txTgC,2023-12-05T21:30:07Z,2023-12-05T21:30:07Z,COLLABORATOR,Verify that fallback happens appropriately.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1841641474/reactions,0,0,0,0,0,0,0,0,0,9907
1066,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836237362,https://github.com/NVIDIA/spark-rapids/issues/9924#issuecomment-1836237362,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9924,1836237362,IC_kwDOD7z77c5tcsIy,2023-12-01T14:47:00Z,2023-12-01T14:47:00Z,COLLABORATOR,I think you filed this already https://github.com/NVIDIA/spark-rapids/issues/9671,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836237362/reactions,0,0,0,0,0,0,0,0,0,9924
1067,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836268758,https://github.com/NVIDIA/spark-rapids/issues/9924#issuecomment-1836268758,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9924,1836268758,IC_kwDOD7z77c5tczzW,2023-12-01T15:02:30Z,2023-12-01T15:02:30Z,MEMBER,"It's a different test failing, and after asking @revans2 before filing he was not convinced they were the same root cause.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1836268758/reactions,0,0,0,0,0,0,0,0,0,9924
1068,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1839720040,https://github.com/NVIDIA/spark-rapids/issues/9952#issuecomment-1839720040,https://api.github.com/repos/NVIDIA/spark-rapids/issues/9952,1839720040,IC_kwDOD7z77c5tp-Zo,2023-12-04T23:31:15Z,2023-12-04T23:31:15Z,COLLABORATOR,"Note that we don't necessarily want to replace all instances. If immutable semantics are used throughout the usage, than it is better to stick with the immutable sequence as that will be more thread-safe and could potentially be lighter on memory usage since it's immutable.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1839720040/reactions,0,0,0,0,0,0,0,0,0,9952
1069,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1864989799,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1864989799,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1864989799,IC_kwDOD7z77c5vKXxn,2023-12-20T19:01:34Z,2023-12-20T20:18:30Z,CONTRIBUTOR,"This is one expensive workaround that we have, that we could remove with additional work in cuDF:

```scala
// if the last entry in a column is incomplete or invalid, then cuDF
// will drop the row rather than replace with null if there is no newline, so we
// add a newline here to prevent that
val joined = withResource(cleaned.joinStrings(lineSep, emptyRow)) { joined =>
  withResource(ColumnVector.fromStrings(""\n"")) { newline =>
    ColumnVector.stringConcatenate(Array[ColumnView](joined, newline))
  }
}
```

EDIT: It is just the `stringConcatenate` that we could potentially remove. We still have to call joinStrings, which is expensive, unless we can have cuDF parse a column of JSON entries rather than provide a ""file"" in one row.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1864989799/reactions,0,0,0,0,0,0,0,0,0,10001
1070,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874532811,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1874532811,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1874532811,IC_kwDOD7z77c5vuxnL,2024-01-02T20:49:57Z,2024-01-02T20:49:57Z,CONTRIBUTOR,"I added some debug logging to show the size of the inputs being passed to `readJSON` in my perf test and see two tasks both trying to allocate ~500 MB and running into OOM.

```
Table.readJSON start=0, length=528729598                           (0 + 8) / 14]
Table.readJSON start=0, length=528884953
24/01/02 20:43:02 WARN DeviceMemoryEventHandler: [RETRY 1] Retrying allocation of 2115539824 after a synchronize. Total RMM allocated is 6502158080 bytes.
24/01/02 20:43:02 WARN DeviceMemoryEventHandler: [RETRY 2] Retrying allocation of 2115539824 after a synchronize. Total RMM allocated is 6500297984 bytes.
24/01/02 20:43:02 WARN DeviceMemoryEventHandler: Device store exhausted, unable to allocate 2115539824 bytes. Total RMM allocated is 6500297984 bytes.
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874532811/reactions,0,0,0,0,0,0,0,0,0,10001
1071,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876006221,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1876006221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1876006221,IC_kwDOD7z77c5v0ZVN,2024-01-03T21:39:18Z,2024-01-03T21:39:18Z,CONTRIBUTOR,"The earlier OOM was happening when running on a workstation with an RTX 3080 which only has 10GB RAM so I am not convinced that this is really an issue. I did not run into any OOM/retry when using a workstation with a RTX Quadro 6000. 

The GPU version of `from_json` performed slightly better than running on CPU in this environment.

GPU: 176s
CPU: 213s

Here is the script that I use for testing.

```
## to_json

import org.apache.spark.sql.SaveMode

val t1 = spark.read.parquet(""/home/andygrove/web_sales.parquet"")
val df = t1.select(to_json(struct(t1.columns.map(col): _*)).alias(""my_json""))

spark.conf.set(""spark.rapids.sql.expression.StructsToJson"", true)
spark.time(df.write.mode(SaveMode.Overwrite).parquet(""temp.parquet""))

## from_json

import org.apache.spark.sql.SaveMode
val t1 = spark.read.parquet(""/home/andygrove/web_sales.parquet"")
val t2 = spark.read.parquet(""temp.parquet"")
val df = t2.select(from_json(col(""my_json""), t1.schema))

spark.conf.set(""spark.rapids.sql.expression.JsonToStructs"", true)
spark.time(df.collect())
```

",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876006221/reactions,0,0,0,0,0,0,0,0,0,10001
1072,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876008033,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1876008033,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1876008033,IC_kwDOD7z77c5v0Zxh,2024-01-03T21:41:14Z,2024-01-03T21:41:14Z,CONTRIBUTOR,"@revans2 I could use a sanity check on my conclusions here before closing this issue. Also, let me know if there are other benchmarks that you would like to see.",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876008033/reactions,0,0,0,0,0,0,0,0,0,10001
1073,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876083320,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1876083320,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1876083320,IC_kwDOD7z77c5v0sJ4,2024-01-03T22:58:49Z,2024-01-03T22:58:49Z,COLLABORATOR,"I think we still have problems, but the underlying problem is being masked by bugs in the retry framework. I tried to run on a 48 GiB GPU with concurrent set to 1. It failed if maxPartitionBytes was set to 256 MiB, but worked if it was set to 128 MiB. The amount of memory used by the 128 MiB use case was very high, but enough that it would risk using up all of the memory on the GPU. Instead I think we are hitting the limit of what a string can hold in CUDF. This gets treated like a split and retry oom exception, but with retry framework eats the original exception so we cannot see what really caused the problem.

I suspect that for your 10 GiB GPU that you really did run out of memory and it was mostly due to fragmentation that it could not finish. I don't think an RTX 3080 support the ARENA allocator. But I could be wrong.

Either way I think we would need to solve this in a generic way with project, and have it support splitting the input, eventually.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876083320/reactions,0,0,0,0,0,0,0,0,0,10001
1074,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1885022299,https://github.com/NVIDIA/spark-rapids/issues/10001#issuecomment-1885022299,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10001,1885022299,IC_kwDOD7z77c5wWyhb,2024-01-10T15:05:33Z,2024-01-10T15:05:33Z,COLLABORATOR,To be clear here the ultimate right fix here is to implement https://github.com/NVIDIA/spark-rapids/issues/7866,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1885022299/reactions,0,0,0,0,0,0,0,0,0,10001
1075,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1856602790,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1856602790,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1856602790,IC_kwDOD7z77c5uqYKm,2023-12-14T21:12:25Z,2023-12-14T21:12:25Z,COLLABORATOR,"Another maybe related issue is even for a `ReusedExchange`, it sometimes has a different id and different set of accumulators, causing `SparkPlanGraph` to treat it as different from the original `Exchange`, which causes a discrepancy with the corresponding cpu plan.

Here is an example from query 56. The subquery contains the original `GpuBroadcastExchange`, while the `ReusedExchange` should be reusing it, but it seems to creating a new `GpuBroadcastExchange`.

[q56_subquery.json](https://github.com/NVIDIA/spark-rapids/files/13678372/q56_subquery.json)
[q56_reused_exchange.json](https://github.com/NVIDIA/spark-rapids/files/13678373/q56_reused_exchange.json)
",,rongou,497101,MDQ6VXNlcjQ5NzEwMQ==,https://avatars.githubusercontent.com/u/497101?v=4,,https://api.github.com/users/rongou,https://github.com/rongou,https://api.github.com/users/rongou/followers,https://api.github.com/users/rongou/following{/other_user},https://api.github.com/users/rongou/gists{/gist_id},https://api.github.com/users/rongou/starred{/owner}{/repo},https://api.github.com/users/rongou/subscriptions,https://api.github.com/users/rongou/orgs,https://api.github.com/users/rongou/repos,https://api.github.com/users/rongou/events{/privacy},https://api.github.com/users/rongou/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1856602790/reactions,0,0,0,0,0,0,0,0,0,10005
1076,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1860729613,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1860729613,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1860729613,IC_kwDOD7z77c5u6HsN,2023-12-18T14:59:27Z,2023-12-18T14:59:27Z,COLLABORATOR,This bug may be fixed by https://github.com/firestarman/spark-rapids/pull/16/files.,,winningsix,2278268,MDQ6VXNlcjIyNzgyNjg=,https://avatars.githubusercontent.com/u/2278268?v=4,,https://api.github.com/users/winningsix,https://github.com/winningsix,https://api.github.com/users/winningsix/followers,https://api.github.com/users/winningsix/following{/other_user},https://api.github.com/users/winningsix/gists{/gist_id},https://api.github.com/users/winningsix/starred{/owner}{/repo},https://api.github.com/users/winningsix/subscriptions,https://api.github.com/users/winningsix/orgs,https://api.github.com/users/winningsix/repos,https://api.github.com/users/winningsix/events{/privacy},https://api.github.com/users/winningsix/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1860729613/reactions,0,0,0,0,0,0,0,0,0,10005
1077,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865195226,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1865195226,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1865195226,IC_kwDOD7z77c5vLJ7a,2023-12-20T21:56:18Z,2023-12-20T21:56:18Z,COLLABORATOR,"@winningsix I am confused about the PR from @firestarman. Was it created to address this issue (#10004)? It was PRed and merged to @firestarman's private repo, is it going to be PRed against our regular repo??",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865195226/reactions,0,0,0,0,0,0,0,0,0,10005
1078,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1868913036,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1868913036,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1868913036,IC_kwDOD7z77c5vZVmM,2023-12-25T10:23:46Z,2023-12-25T10:24:43Z,COLLABORATOR,"That PR tried to address a customer issue on reused exchanges, and we wanted to verify it quickly so merged to my own repo.
But it does not work according to customer's feedback. I do not reproduce this missing ReusedExchange issue locally by running query 56, so i do not know whether it is a fix.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1868913036/reactions,1,1,0,0,0,0,0,0,0,10005
1079,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1873963841,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1873963841,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1873963841,IC_kwDOD7z77c5vsmtB,2024-01-02T12:25:05Z,2024-01-02T12:37:13Z,COLLABORATOR,"This may be related to https://github.com/NVIDIA/spark-rapids/issues/10136, but i am not 100% sure.

@rongou You can do a quick verfication by setting `spark.rapids.sql.fileScanPrunePartition.enabled` to `false`. Since I can not repro this locally.
If no repro, then https://github.com/NVIDIA/spark-rapids/issues/10136 should be the root cause.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1873963841/reactions,0,0,0,0,0,0,0,0,0,10005
1080,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1910920012,https://github.com/NVIDIA/spark-rapids/issues/10005#issuecomment-1910920012,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10005,1910920012,IC_kwDOD7z77c5x5lNM,2024-01-25T20:14:41Z,2024-01-25T20:14:41Z,COLLABORATOR,@rongou do you still see the issue after #10136 was merged as @firestarman suggests?,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1910920012/reactions,0,0,0,0,0,0,0,0,0,10005
1081,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869292525,https://github.com/NVIDIA/spark-rapids/issues/10032#issuecomment-1869292525,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10032,1869292525,IC_kwDOD7z77c5vayPt,2023-12-26T06:23:08Z,2023-12-26T06:23:39Z,COLLABORATOR,"One related issue: https://github.com/NVIDIA/spark-rapids/issues/10083

Java API gets:
`+10000-01-01`
And cuDF gets:
`0000-01-01`
for date: `10000-01-01` when the format is `yyyy-MM-dd`",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869292525/reactions,0,0,0,0,0,0,0,0,0,10032
1082,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881716947,https://github.com/NVIDIA/spark-rapids/issues/10032#issuecomment-1881716947,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10032,1881716947,IC_kwDOD7z77c5wKLjT,2024-01-08T19:44:37Z,2024-01-08T20:09:43Z,CONTRIBUTOR,"Some notes on parsing dates from JSON, based on https://github.com/NVIDIA/spark-rapids/pull/9975

Depending on the Spark version, there can be different code paths depending on whether a dateFormat is specified or not. Some of the differences that we need to be able to handle are:

- We sometimes need to support single-digit months and days, and sometimes we require two digits
- We sometimes need to trim all leading and trailing whitespace, sometimes we only trim specific whitespace chars, sometimes we don't trim at all
- Sometimes we perform a cast instead of a parse, and in that case we support special values ""epoch"", ""now"", ""today"", ""yesterday"", and ""tomorrow"" (definitely an edge case because it doesn't much sense to store relative terms like this in a json file)",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881716947/reactions,1,1,0,0,0,0,0,0,0,10032
1083,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1897892696,https://github.com/NVIDIA/spark-rapids/issues/10032#issuecomment-1897892696,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10032,1897892696,IC_kwDOD7z77c5xH4tY,2024-01-18T06:42:11Z,2024-01-18T06:42:11Z,COLLABORATOR,"> Sometimes we perform a cast instead of a parse, and in that case we support special values ""epoch"", ""now"", ""today"", ""yesterday"", and ""tomorrow"" (definitely an edge case because it doesn't much sense to store relative terms like this in a json file)

When cast string to timestamp, only Spark31x supports special values you mentioned, Spark 320 and 320+ do not support special values.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1897892696/reactions,0,0,0,0,0,0,0,0,0,10032
1084,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1901093076,https://github.com/NVIDIA/spark-rapids/issues/10032#issuecomment-1901093076,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10032,1901093076,IC_kwDOD7z77c5xUGDU,2024-01-19T20:47:33Z,2024-01-19T20:47:33Z,COLLABORATOR,"> only Spark31x supports special values you mentioned

Are we just not going to support the special values in Spark 3.1 and document it? or are we going to do special post processing to fix them up?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1901093076/reactions,0,0,0,0,0,0,0,0,0,10032
1085,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1909854070,https://github.com/NVIDIA/spark-rapids/issues/10032#issuecomment-1909854070,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10032,1909854070,IC_kwDOD7z77c5x1g92,2024-01-25T10:28:56Z,2024-01-25T10:28:56Z,COLLABORATOR,"> Are we just not going to support the special values in Spark 3.1 and document it? or are we going to do special post processing to fix them up?

I suggest do special post processing:
For `now` and `epoch` they are not time zone awared.
For `today/tomorrow/yesterday` they are time zone awared. Generate them in Java in the default time zone, then replace the mached string to the values.
 ",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1909854070/reactions,0,0,0,0,0,0,0,0,0,10032
1086,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869776813,https://github.com/NVIDIA/spark-rapids/issues/10040#issuecomment-1869776813,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10040,1869776813,IC_kwDOD7z77c5vcoet,2023-12-26T21:18:15Z,2023-12-26T21:18:15Z,COLLABORATOR,"Another aspect to consider: When you pass a Python `datetime` object with timezone information, then it will be converted to UTC before sending it Spark. This produces an `date value out of range` Python exception.  

However, this also means that the effective range for testing dates for timestamps which have positive offset from UTC is actually restricted. You can only start `datetime` values at `0001-01-01 00:00:00.000000` UTC time, so a time that is `0001-01-01 00:00:00.000000` in a local timezone ahead of UTC cannot actually be sent back to Spark from Python because it will be converted to UTC before sending to Spark (which will be a Year 0 timestamp in UTC) that is out of range.  That value is still valid though for ANSI purposes (`0001-01-01 00:00:00.000000` to `9999-12-31 23:59:59.999999` is the valid range) and Spark is okay with those values.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869776813/reactions,0,0,0,0,0,0,0,0,0,10040
1087,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110955385,https://github.com/NVIDIA/spark-rapids/issues/10040#issuecomment-2110955385,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10040,2110955385,IC_kwDOD7z77c590p95,2024-05-14T19:06:09Z,2024-05-14T19:06:09Z,COLLABORATOR,I think the best course of action here is to write tests in Scala to handle multiple non-UTC timezones and dates that are in the invalid (non-positive and >9999 years) instead of trying to do this in Python because it's difficult to change how Python handles things in the PythonRunner on the executor.,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110955385/reactions,0,0,0,0,0,0,0,0,0,10040
1088,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863515964,https://github.com/NVIDIA/spark-rapids/issues/10045#issuecomment-1863515964,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10045,1863515964,IC_kwDOD7z77c5vEv88,2023-12-19T21:45:21Z,2023-12-19T21:45:21Z,COLLABORATOR,"Is the issue that the current output should not be recommending `spark.rapids.sql.format.hive.text.write.enabled to true` because it is already true?  Instead, you want it to recommend this: `spark.sql.hive.convertMetastoreParquet=true`.  Just want to confirm that is sufficient for the desired output.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863515964/reactions,0,0,0,0,0,0,0,0,0,10045
1089,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870567998,https://github.com/NVIDIA/spark-rapids/issues/10045#issuecomment-1870567998,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10045,1870567998,IC_kwDOD7z77c5vfpo-,2023-12-27T19:22:35Z,2023-12-27T19:22:35Z,COLLABORATOR,"@mattahrens Currently Spark RAPIDS can only support Hive table write when spark.sql.hive.convertMetastoreParquet=true. This is default true as well.
So if a Spark user disabled spark.sql.hive.convertMetastoreParquet and then above original driver log message will show up.
To the user, they did not know what parameter to turn on to avoid the CPU fallback. 

That is why I suggest we should mention spark.sql.hive.convertMetastoreParquet=true in the driver log to make sure user enables this parameter to avoid CPU fallback.



",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870567998/reactions,0,0,0,0,0,0,0,0,0,10045
1090,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870664862,https://github.com/NVIDIA/spark-rapids/issues/10045#issuecomment-1870664862,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10045,1870664862,IC_kwDOD7z77c5vgBSe,2023-12-27T22:26:12Z,2023-12-27T22:26:12Z,COLLABORATOR,"There are multiple different config settings that go into this on the Spark side.

`spark.sql.hive.convertMetastoreParquet` and `spark.sql.hive.convertInsertingPartitionedTable` are a few of them. Spark can even throw an exception telling the user to set `spark.sql.hive.convertMetastoreParquet` to false as a work around to potential errors in how Spark tries to determine the write schema. I don't think we want to tell the user to turn any of these configs on, if someone decided that they should be off.

In addition to that we would need to replicate the logic in https://github.com/apache/spark/blob/5430c700ba64b07cf0c32b906a3328df8a7bef71/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala#L164-L168 to be able to tell the user what is the correct config to turn on. It might be good to just explain that we cannot support this at this time and leave it at that.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1870664862/reactions,0,0,0,0,0,0,0,0,0,10045
1091,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876201419,https://github.com/NVIDIA/spark-rapids/issues/10045#issuecomment-1876201419,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10045,1876201419,IC_kwDOD7z77c5v1I_L,2024-01-04T01:50:16Z,2024-01-04T01:50:16Z,COLLABORATOR,"I would suggest make this warning message more straightforward. 
Currently it will mention:
1. unsupported output-format found: Some(org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat)
2. writing Hive delimited text tables has been disabled, to enable this, set spark.rapids.sql.format.hive.text.write.enabled to true;
3. unsupported serde found: Some(org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe), only org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe is currently supported

It might be confusing to customer on what parameter needs to be turned on. Such as user might blindly enable `spark.rapids.sql.format.hive.text.write.enabled=true` but actually the parameter is unrelated.

I think based on my experience, there are only below possibilities:
a. It is a Hive parquet table but user disabled `spark.sql.hive.convertMetastoreParquet ` or some other Spark parameter as @revans2  mentioned;
b. It is a Hive parquet table but user customized Spark so that it somehow could not be translated into a Spark native parque write.
c. It is a Hive Text table. (Which means setting `spark.rapids.sql.format.hive.text.write.enabled=true` is the right solution)

Not sure if our plugin can firstly detect the Hive formats(based on serde) and then show the message accordingly based on a, b, c possibilities? (I know #b is hard, but at least distinguish a and c is doable? )
",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876201419/reactions,0,0,0,0,0,0,0,0,0,10045
1092,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874070961,https://github.com/NVIDIA/spark-rapids/issues/10051#issuecomment-1874070961,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10051,1874070961,IC_kwDOD7z77c5vtA2x,2024-01-02T14:08:15Z,2024-01-02T14:08:15Z,COLLABORATOR,"@gerashegalov  I'm adding dependency check for the pre-merge and nightly build jobs, according to our discuss method

```
1, Create a fresh local dir such as /tmp/local-release-repo
2, mvn deploy:deploy-file -Durl=file:/tmp/local-release-repo  xxx \
3, Create another fresh local directory for /tmp/test-get-dest
4, Create a fresh empty dir /tmp/m2-cache every iteration such that there are no cached side-effects from one 
 depenendcy:get to another
5, execute depenency:get xxx
    -Dmaven.repo.local=/tmp/m2-cache
```

The dependency check still failed on `jdk-profiles` on current branch-24.02, are you going to fix it first?",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874070961/reactions,0,0,0,0,0,0,0,0,0,10051
1093,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876342144,https://github.com/NVIDIA/spark-rapids/issues/10051#issuecomment-1876342144,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10051,1876342144,IC_kwDOD7z77c5v1rWA,2024-01-04T05:28:47Z,2024-01-04T05:28:47Z,COLLABORATOR,"Hi @NvTimLiu

> The dependency check still failed on jdk-profiles on current branch-24.02, are you going to fix it first?

The test should mimic the release deploy logic irrespective whether this issue (eliminating parent and intermediate poms) is fixed.

If the test is failing it means it still does not reflect that currently deploy-file for jdk-profiles is required.

Ideally the test should share the same script  
https://github.com/NVIDIA/spark-rapids/blob/ed1fa9f0c92f8c7c1a4cbdedf2b4d61db6d1beca/jenkins/deploy.sh#L97-L107

just with different parameters. It would be helpful to create a file with the list of the artifacts required for the release. Then we could use it as the input in the release/test script. 

Once this issue is fixed we will drop the jdk-profiles module from this list, but the test remains valid.


  ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1876342144/reactions,0,0,0,0,0,0,0,0,0,10051
1094,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1877121816,https://github.com/NVIDIA/spark-rapids/issues/10051#issuecomment-1877121816,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10051,1877121816,IC_kwDOD7z77c5v4psY,2024-01-04T13:47:22Z,2024-01-04T13:52:53Z,COLLABORATOR,"> Hi @NvTimLiu
> 
> > The dependency check still failed on jdk-profiles on current branch-24.02, are you going to fix it first?
> 
> The test should mimic the release deploy logic irrespective whether this issue (eliminating parent and intermediate poms) is fixed.
> 
> If the test is failing it means it still does not reflect that currently deploy-file for jdk-profiles is required.
> 
> Ideally the test should share the same script
> 
> https://github.com/NVIDIA/spark-rapids/blob/ed1fa9f0c92f8c7c1a4cbdedf2b4d61db6d1beca/jenkins/deploy.sh#L97-L107
> 
> just with different parameters. It would be helpful to create a file with the list of the artifacts required for the release. Then we could use it as the input in the release/test script.
> 
> Once this issue is fixed we will drop the jdk-profiles module from this list, but the test remains valid.


 @gerashegalov Thanks for the explanation.

I agree we can reuse the  deploy script `[spark-rapids/jenkins/deploy.sh](https://github.com/NVIDIA/spark-rapids/blob/ed1fa9f0c92f8c7c1a4cbdedf2b4d61db6d1beca/jenkins/deploy.sh#L97-L107)` to do dependency check for the to-be-released artifacts.

A file list of artifacts needs extra script for deploy.sh to work with, e.g.
file list: ` pom.xml, dist/pom.xml, rapids.jar, rapids-cuda11.jar`,  then `deploy.sh` should parse from the `file list` the parent pom, dist pom, classifier jar, main jar, etc.

Over all, we need to refactor  deploy.sh `[spark-rapids/jenkins/deploy.sh](https://github.com/NVIDIA/spark-rapids/blob/ed1fa9f0c92f8c7c1a4cbdedf2b4d61db6d1beca/jenkins/deploy.sh#L97-L107)` to work with `list file`.

Let me check if we should break this issue into diff tasks.",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1877121816/reactions,1,1,0,0,0,0,0,0,0,10051
1095,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879449470,https://github.com/NVIDIA/spark-rapids/issues/10051#issuecomment-1879449470,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10051,1879449470,IC_kwDOD7z77c5wBh9-,2024-01-06T00:17:33Z,2024-01-08T20:07:27Z,COLLABORATOR,"@NvTimLiu I'll file some other issue for discussing a reproducible deploy test. To reiterate, we will need this test regardless of what our poms look like. 

This issue #10051 is about a potential simplification of published poms ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879449470/reactions,1,1,0,0,0,0,0,0,0,10051
1096,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863517766,https://github.com/NVIDIA/spark-rapids/issues/10058#issuecomment-1863517766,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10058,1863517766,IC_kwDOD7z77c5vEwZG,2023-12-19T21:47:08Z,2023-12-19T21:47:08Z,COLLABORATOR,Scope would be to validate that fallback occurs as expected.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1863517766/reactions,0,0,0,0,0,0,0,0,0,10058
1097,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871748626,https://github.com/NVIDIA/spark-rapids/issues/10062#issuecomment-1871748626,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10062,1871748626,IC_kwDOD7z77c5vkJ4S,2023-12-29T05:54:57Z,2023-12-29T05:54:57Z,COLLABORATOR,Also fails with `DATAGEN_SEED=1703803344` as CI just showed here https://github.com/NVIDIA/spark-rapids/pull/10109. I am adding a temporary seed=0 override.,,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871748626/reactions,1,1,0,0,0,0,0,0,0,10062
1098,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1918665956,https://github.com/NVIDIA/spark-rapids/issues/10062#issuecomment-1918665956,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10062,1918665956,IC_kwDOD7z77c5yXITk,2024-01-31T08:59:47Z,2024-01-31T09:13:09Z,COLLABORATOR,"This issue is quite weird to me, here is what I know about it now:

**repro**
Many thing in this test is irrelevant to repro this failure, like the type of agg functions, configs, incompat and approximate_float marks, here is a minimal repro:
```python
@ignore_order(local=True)
def test_hash_multiple_grpby_pivot():
    assert_gpu_and_cpu_are_equal_collect(
        lambda spark: gen_df(spark, [('a', LongGen()), ('b', IntegerGen()), ('c', LongGen())], length=100)
            .groupby('a','b')
            .pivot('b')
            .agg(f.count('c'), f.max('c')))
```
And many other things seem to be required to reproduce this failure, like the length of the data (around 100), the number of group by keys (2) , and the number of agg functions (2) .

I tried to find a repro with shorter/simpler data to find a pattern in the data that could lead to this failure, but I failed to find one.

**failure**

The test failed because the second group by key ""b"" is wrong in the result, most of the values are 0 in the gpu result, but they are not in the cpu result.

```
Row(a=-9223372036854775808, b=0, null_count(c)=None, null_max(c)=None, -2147483648_count(c)=None, ...
Row(a=-9223372036854775808, b=0, null_count(c)=None, null_max(c)=None, -2147483648_count(c)=None, ...
Row(a=-9223372036854775808, b=167810688, null_count(c)=None, null_max(c)=None, -2147483648_count(c)=None, ...
Row(a=-9018433402683060375, b=0, null_count(c)=None, null_max(c)=None, -2147483648_count(c)=None, ...
......
```

**different results and plans for different test methods**

An interesting thing is that if you run the data with `collect` or `take` method, the result is wrong, but if you run it with `show` method, the result is correct.

The plan for them are also slightly different, the main difference is:

take(1000):
```
......
24/01/31 11:47:50 WARN GpuOverrides:
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> __pivot_count(c) AS `count(c)`#209[0] AS null_count(c)#402L will run on GPU
      *Expression <GetArrayItem> __pivot_count(c) AS `count(c)`#209[0] will run on GPU
    *Expression <Alias> __pivot_max(c) AS `max(c)`#401[0] AS null_max(c)#403L will run on GPU
      *Expression <GetArrayItem> __pivot_max(c) AS `max(c)`#401[0] will run on GPU
    *Expression <Alias> __pivot_count(c) AS `count(c)`#209[1] AS -2147483648_count(c)#404L will run on GPU
      *Expression <GetArrayItem> __pivot_count(c) AS `count(c)`#209[1] will run on GPU
......
```

show:
```
......
24/01/30 18:25:40 WARN GpuOverrides:
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> cast(a#0L as string) AS a#1356 will run on GPU
      *Expression <Cast> cast(a#0L as string) will run on GPU
    *Expression <Alias> cast(b#1 as string) AS b#1357 will run on GPU
      *Expression <Cast> cast(b#1 as string) will run on GPU
    *Expression <Alias> cast(__pivot_count(c) AS `count(c)`#209[0] as string) AS null_count(c)#1358 will run on GPU
      *Expression <Cast> cast(__pivot_count(c) AS `count(c)`#209[0] as string) will run on GPU
        *Expression <GetArrayItem> __pivot_count(c) AS `count(c)`#209[0] will run on GPU
    *Expression <Alias> cast(__pivot_max(c) AS `max(c)`#401[0] as string) AS null_max(c)#1359 will run on GPU
      *Expression <Cast> cast(__pivot_max(c) AS `max(c)`#401[0] as string) will run on GPU
        *Expression <GetArrayItem> __pivot_max(c) AS `max(c)`#401[0] will run on GPU
......
```

There is a `Cast` node in the plan for the `show` method, but not for the `take` method, I'm not sure if this is the reason for the different results.

Also, the results for `take` are not always the same every time. It will produce different and wrong results with same data.

Here is the script I used to run the test:

```python
from pyspark.sql.types import *

import pyspark.sql.functions as f

data = [(-4845943781170555252, -1, 845188426061929355), (7350351203911181009, -1834450793, -8329883561935981210), (-6474373677523860195, -199278150, 9152903973058550378), (-3932771648075861385, -1678969406, 598714061780169842), (None, 801881452, -4466412165968923070), (-3652577983080905586, 1540102964, 47268640572211013), (4545717671238472990, 1947938927, None), (-8298661160006568201, None, 6854480935057369726), (-7616214595191846290, -1949955096, 0), (-7289328883642000118, 272299328, 2884476978744622358), (None, 1160260740, -6052237597774996739), (-9223372036854775808, -493879135, 6203771033123822312), (-6614098684129949450, -584802307, -3071555407446211853), (-9003145117425481384, -757788614, 7929562070840347149), (-9223372036854775808, -228071647, 2906400750731819474), (5732395791703362947, -2147483648, -3970425384979120781), (-1564274735105258826, 996967482, -1484437957559815242), (5526867606258508741, -1575882867, 6239854566349019265), (-9223372036854775808, -334761671, -4112306351109120101), (4244671825864794863, 981450318, 7465382293241571108), (-448440415840524957, 394207378, None), (-4073146087272737548, 208913219, 2562680179324889398), (-7321272388955223843, 732061080, -5794879109021556367), (860502963007932720, 1402155938, -3954084267791354691), (-4821486473017204446, 1315353396, 1347206884473686984), (3016210088861611029, -1456892421, 3583186219866217910), (-8846374268221322244, 0, -6858830960164213997), (-518664528608335071, -1268577466, 5720376452508829361), (2586987693939428128, -1968521231, 4186863789914269115), (4315088642471474100, -318667079, None), (-5113086506103862658, 1192991751, 3739428740990641749), (-2120010059318291246, -140200524, -3868910921495005039), (6836723189324294794, -1441161803, 9145430657210176353), (-3404440819830080875, 311543893, -2121016320289730914), (-8267335710692130228, 101892561, -7989940178650011946), (-154826107042624785, -1805786100, 167944445750808798), (4310854391155890700, 1025117499, 8102289978239659013), (3691614150816130938, -1222608032, 0), (-4898102975906729231, 1206436070, -1156663457190611290), (5780839774576522093, -938054806, 215654183084134840), (-3990677385327099340, -60005380, 2446406909182780340), (3558684623224462755, 1100654780, -3561235334131684560), (None, 598840759, 6880829832614805175), (2587651536519945797, -848441474, -8944550910905869417), (None, -1867029195, 7292092528479130246), (6483492345442125574, 1423197063, -7562444454753075438), (-309680615801512125, 635403706, None), (-2660137861258255564, -1580709119, -1317365564804702773), (-4805714711362703732, 1977765468, 5355716940028229540), (806134238988331049, -21753199, -5982077157730834146), (-2320179344785250342, 915815532, -924102870416173362), (None, -1638025670, None), (-5335012076263087262, 998390663, 7614229944302801432), (-8003442832786541311, 895630555, 1), (8962746789636867990, None, 7838149056358927259), (-3474654754894809539, None, 3134278270126018378), (-8855593812501704849, 2054085988, 2855154093732021075), (-7091849916600089089, None, -6417530363343090288), (-222013359233028811, -1006587236, 5138405147626324023), (6147863345995281135, -1343496055, -5605747624660684039), (2412508725397414376, 1750760724, 0), (6746130822817653164, 1119642812, 4321955607259744754), (-4522555907557220644, 747294963, 1902336294480137280), (-7124479271795897431, 618657031, -6384732039486542304), (-3016538980443125232, -971305592, 5961924950591751584), (36773415627206426, 1566835821, 3212457428261779419), (-3760560561437195364, -380125779, 4269719979332759241), (-4248829775503459426, 1070853851, 3760713768439915058), (-5923932160264549958, 1594768906, -393161711644537657), (1157815137441274869, 2084691734, -7858213111740624985), (-2199684093812713901, 11523829, -1594686429977057152), (0, -814837712, 4596534695065757359), (-3849151964918279414, -68879409, 5986326621238269570), (-7398018555027505285, -1631793792, -3473982566533875127), (4932966234951301523, 736936302, -4363781202623852301), (-3580853073542804157, -883765153, 8365114787068696630), (-50917646897929622, 1826610949, 8079244073851440946), (-3677281010338202811, 1360663342, 1990897617129644958), (5462740808011170473, 361204030, None), (None, -158654865, 8715253034384927926), (-4823423393556183648, 1863243438, -4120580680305907585), (-2094967163472758289, 1746245479, 4379449978340264045), (-7317633614294497645, 1126381026, -4684053213402982061), (1038911458266308727, -731208496, -1357558607203886826), (6238027844923655492, None, -1365950448597480252), (-463854078867593653, -1770484043, -6170126774260820756), (7022405428698357996, 1047662067, -6459373991120808345), (3224891226649524714, 85681293, 1), (1497870743195662602, -225772735, 6706375643237237900), (-2270318763267742179, -1246742999, -7480138770689422386), (-1, -1221487735, 7193799890519656756), (9033902131006707423, 1313705388, None), (9074230953825881152, -1378244309, -3631319698309137830), (-1227617950894814242, 88538877, 8888801943813690819), (-4943246025205215864, -820790410, 6939343468978651959), (4655205048401744152, -1202186285, 8659688436647848697), (-9018433402683060375, 865863905, 57130699251065081), (6984748109983071784, 0, 5106579162908166122), (None, 857006179, -783620458450576957), (-3380402600221637564, -1832167886, -4323246307621457475)]

schema = StructType([StructField('a', LongType(), True), StructField('b', IntegerType(), True), StructField('c', LongType(), True)])

df = spark.createDataFrame(data, schema)

# collect or take, wrong and unstable results

collect = df.groupby('a','b').pivot('b').agg(f.count('c'), f.max('c')).take(1000)

for row in collect:
    print(row)

# show, correct results

df.groupby('a','b').pivot('b').agg(f.count('c'), f.max('c')).show(1000, False)
```",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1918665956/reactions,0,0,0,0,0,0,0,0,0,10062
1099,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970726606,https://github.com/NVIDIA/spark-rapids/issues/10062#issuecomment-1970726606,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10062,1970726606,IC_kwDOD7z77c51dubO,2024-02-29T09:20:17Z,2024-02-29T09:22:08Z,COLLABORATOR,"Seems not to be a bug in `PivotFirst` or `AggregateExpression`, the test still fails when them are disabled.

Now I think it is an issue from `GetArrayItem` because when I disabled it by setting `spark.rapids.sql.expression.GetArrayItem` to false, the test passed.",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970726606/reactions,0,0,0,0,0,0,0,0,0,10062
1100,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865180567,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1865180567,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1865180567,IC_kwDOD7z77c5vLGWX,2023-12-20T21:40:42Z,2023-12-20T21:40:42Z,COLLABORATOR,"Confirmed reproduce via:

```
TZ=Iran ./run_pyspark_from_build.sh -k 'test_date_format_for_time or test_date_format_maybe_incompat'
```

Seed value doesn't seem to matter",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865180567/reactions,1,0,0,0,1,0,0,0,0,10083
1101,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869015813,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869015813,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869015813,IC_kwDOD7z77c5vZusF,2023-12-25T14:52:53Z,2023-12-25T14:53:39Z,COLLABORATOR,"It seems like an overflow for the values. The year values should be up to `9999`.
```
11:47:30  -Row(date_format(a, yyyy-MM-dd HH:mm:ss.SSSSSS)='+10000-01-01 03:29:59.999999')
11:47:30  +Row(date_format(a, yyyy-MM-dd HH:mm:ss.SSSSSS)='0000-01-01 03:29:59.999999')
```",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869015813/reactions,1,1,0,0,0,0,0,0,0,10083
1102,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869221726,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869221726,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869221726,IC_kwDOD7z77c5vag9e,2023-12-26T03:15:59Z,2023-12-26T03:31:09Z,COLLABORATOR,"cuDF can not handle years that are bigger than 9999.

```
  test(""test year 9999"") {
    println(""my debug: begin: "")
    val maxSecond = Instant.parse(""9999-12-31T23:59:59z"").getEpochSecond
    // plus 8 hours
    val cv = ColumnVector.timestampMicroSecondsFromBoxedLongs(
      maxSecond * TimeUnit.SECONDS.toMicros(1) + TimeUnit.HOURS.toMicros(8))
    val ret = cv.asStrings(""%Y-%m-%d"")
    val host = ret.copyToHost()
    println(host.getJavaString(0))
    println(""my debug: end"")
  }
```

Output:
```
my debug: begin: 
0000-01-01
my debug: end
```

I guess cuDF expect %Y prints 4 digits, so it truncates 10000 to 0000.

Spark output is:
```
+10000-01-01
```",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869221726/reactions,0,0,0,0,0,0,0,0,0,10083
1103,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869222370,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869222370,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869222370,IC_kwDOD7z77c5vahHi,2023-12-26T03:17:56Z,2023-12-26T03:25:35Z,COLLABORATOR,"Depending on cuDF to handle years bigger than 9999:
```
cv.asStrings(""%Y-%m-%d"")
```
The long value in cv is: 253402329599000000L
253402329599000000L = micros of 9999-12-31T23:59:59 + micors of 8 hours.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869222370/reactions,0,0,0,0,0,0,0,0,0,10083
1104,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869228104,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869228104,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869228104,IC_kwDOD7z77c5vaihI,2023-12-26T03:36:09Z,2023-12-26T03:36:09Z,COLLABORATOR,"This is not trivially supported in cudf since it requires fixed width input strings for each field. For example, `%Y` requires 4 numbers like `0001`.

Ref: https://github.com/rapidsai/cudf/blob/branch-24.02/cpp/src/strings/convert/convert_datetime.cu#L114",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869228104/reactions,1,1,0,0,0,0,0,0,0,10083
1105,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869235291,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869235291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869235291,IC_kwDOD7z77c5vakRb,2023-12-26T04:00:04Z,2023-12-26T04:00:04Z,COLLABORATOR,"Thanks @ttnghia 

Related to the following issues:
https://github.com/NVIDIA/spark-rapids-jni/issues/1655
https://github.com/NVIDIA/spark-rapids/issues/10032
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869235291/reactions,0,0,0,0,0,0,0,0,0,10083
1106,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869291050,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869291050,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869291050,IC_kwDOD7z77c5vax4q,2023-12-26T06:20:10Z,2023-12-26T06:20:10Z,COLLABORATOR,"Java API returns `+10000-01-01`
```
    val p = DateTimeFormatter.ofPattern(""yyyy-MM-dd"")
    val s = p.format(Instant.ofEpochSecond(253402329599L).atZone(ZoneId.of(""Asia/Shanghai"")).toLocalDate)
    println(s)
```
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869291050/reactions,0,0,0,0,0,0,0,0,0,10083
1107,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869865189,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1869865189,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1869865189,IC_kwDOD7z77c5vc-Dl,2023-12-27T01:36:47Z,2023-12-27T09:22:32Z,COLLABORATOR,"Temporarily fix: https://github.com/NVIDIA/spark-rapids/pull/10095
Final fix depends on: https://github.com/NVIDIA/spark-rapids/issues/10032
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1869865189/reactions,0,0,0,0,0,0,0,0,0,10083
1108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871454804,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1871454804,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1871454804,IC_kwDOD7z77c5vjCJU,2023-12-28T20:00:56Z,2023-12-28T20:00:56Z,COLLABORATOR,"@res-life I think `from_unixtime` (`GpuFromUnixTime`) will also have the same overflow issue for non-UTC timezones, since it also relies on `asStrings(strfFormat)` like `GpuDateFormatClass`",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871454804/reactions,0,0,0,0,0,0,0,0,0,10083
1109,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871789307,https://github.com/NVIDIA/spark-rapids/issues/10083#issuecomment-1871789307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10083,1871789307,IC_kwDOD7z77c5vkTz7,2023-12-29T06:43:59Z,2023-12-29T06:43:59Z,COLLABORATOR,"Thanks @NVnavkumar 
```
@pytest.mark.parametrize('data_gen', [LongGen(min_val=int(datetime(1, 2, 1).timestamp()), max_val=int(datetime(9999, 12, 30).timestamp()))], ids=idfn)
test_from_unixtime
```
Currently `from_unixtime`  does not use full ragne, it will not fail.

I added a sub-task in this issue to track this.

",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871789307/reactions,0,0,0,0,0,0,0,0,0,10083
1110,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865132319,https://github.com/NVIDIA/spark-rapids/issues/10086#issuecomment-1865132319,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10086,1865132319,IC_kwDOD7z77c5vK6kf,2023-12-20T21:01:08Z,2023-12-20T21:01:08Z,COLLABORATOR,Note one of these is documented in FAQ: https://docs.nvidia.com/spark-rapids/user-guide/latest/faq.html#how-can-i-check-if-the-rapids-accelerator-is-installed-and-which-version-is-running,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865132319/reactions,0,0,0,0,0,0,0,0,0,10086
1111,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865367926,https://github.com/NVIDIA/spark-rapids/issues/10086#issuecomment-1865367926,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10086,1865367926,IC_kwDOD7z77c5vL0F2,2023-12-21T01:55:30Z,2023-12-21T01:55:30Z,COLLABORATOR,"The other half is documented here https://github.com/NVIDIA/spark-rapids-jni/blob/98dc423dfbacb68e0d5d8d15069455aaffad618f/CONTRIBUTING.md?plain=1#L166-L191 

with so many repos we should have a convention for the shared doc location ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1865367926/reactions,0,0,0,0,0,0,0,0,0,10086
1112,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871333798,https://github.com/NVIDIA/spark-rapids/issues/10107#issuecomment-1871333798,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10107,1871333798,IC_kwDOD7z77c5vikmm,2023-12-28T16:43:43Z,2023-12-28T16:43:43Z,COLLABORATOR,We don't support `raise_error` or `RaiseError` yet. We have not seen any customers using it so there has been no push to implement it. Even then it would probably just be a copy of the CPU version as there is nothing that we can do on the GPU with it.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1871333798/reactions,0,0,0,0,0,0,0,0,0,10107
1113,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875452898,https://github.com/NVIDIA/spark-rapids/issues/10107#issuecomment-1875452898,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10107,1875452898,IC_kwDOD7z77c5vySPi,2024-01-03T14:23:39Z,2024-01-03T14:23:39Z,COLLABORATOR,"I was wrong and this is in fact supported by us, so we need to make corresponding changes. We might need to make some changes to the UDF transpiler because this was added to support that.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875452898/reactions,0,0,0,0,0,0,0,0,0,10107
1114,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874571945,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1874571945,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1874571945,IC_kwDOD7z77c5vu7Kp,2024-01-02T21:09:27Z,2024-01-02T21:09:27Z,COLLABORATOR,@gerashegalov  -- would removing `-T1C` fix this issue as well (similar to #10105)?,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1874571945/reactions,0,0,0,0,0,0,0,0,0,10115
1115,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875505520,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1875505520,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1875505520,IC_kwDOD7z77c5vyfFw,2024-01-03T14:57:36Z,2024-01-03T14:57:36Z,COLLABORATOR,"> @gerashegalov -- would removing `-T1C` fix this issue as well (similar to #10105)?

Same concern, if removing `-T1C` can fix the issue, we're good to remove it from pre-merge/nightly CI.",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875505520/reactions,0,0,0,0,0,0,0,0,0,10115
1116,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875601523,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1875601523,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1875601523,IC_kwDOD7z77c5vy2hz,2024-01-03T16:02:58Z,2024-01-03T16:02:58Z,COLLABORATOR,"@NvTimLiu @mattahrens  yes this is the simplest workaround

for this issue #10115 remove `--threads 1C` here https://github.com/NVIDIA/spark-rapids/blob/f28f25884b6b243236594c73b8c989adf40b7813/.github/workflows/mvn-verify-check.yml#L32

for #10105 
https://github.com/NVIDIA/spark-rapids/blob/f28f25884b6b243236594c73b8c989adf40b7813/jenkins/spark-nightly-build.sh#L106-L109",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1875601523/reactions,1,1,0,0,0,0,0,0,0,10115
1117,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1878362922,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1878362922,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1878362922,IC_kwDOD7z77c5v9Ysq,2024-01-05T09:22:28Z,2024-01-05T09:22:28Z,COLLABORATOR,Can we close the issue ass https://github.com/NVIDIA/spark-rapids/pull/10153 got merged?,,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1878362922/reactions,0,0,0,0,0,0,0,0,0,10115
1118,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879261422,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1879261422,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1879261422,IC_kwDOD7z77c5wA0Du,2024-01-05T21:11:48Z,2024-01-05T21:11:48Z,COLLABORATOR,"#10153  is a quick bandaid fix for this PR

I would like to keep it open to do a real fix, eventually 

The PR check workflows were actually designed to deal with the problem, so either:
-  a nuanced fix is to disable threads only for JDK9+ 
- or better fix make sure that JDK11, 17-based  compiler bridge artifacts already are guaranteed to exist via cache-dependencies step.  ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879261422/reactions,1,1,0,0,0,0,0,0,0,10115
1119,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1886053185,https://github.com/NVIDIA/spark-rapids/issues/10115#issuecomment-1886053185,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10115,1886053185,IC_kwDOD7z77c5wauNB,2024-01-11T01:32:20Z,2024-01-11T01:32:20Z,COLLABORATOR,"Thanks @gerashegalov, hand over it issue to you for tracking.",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1886053185/reactions,0,0,0,0,0,0,0,0,0,10115
1120,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879194737,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1879194737,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1879194737,IC_kwDOD7z77c5wAjxx,2024-01-05T20:02:02Z,2024-01-05T20:02:02Z,COLLABORATOR,"From CI, this test failure occurred in Spark 3.5.0 ",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879194737/reactions,0,0,0,0,0,0,0,0,0,10147
1121,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879363955,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1879363955,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1879363955,IC_kwDOD7z77c5wBNFz,2024-01-05T23:07:46Z,2024-01-05T23:07:46Z,COLLABORATOR,"I was able to replicate both failures on Spark 3.2.4, 3.3.3, 3.4.0, and 3.5.0 (all versions of Spark that support AQE + DPP)",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879363955/reactions,0,0,0,0,0,0,0,0,0,10147
1122,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879365172,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1879365172,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1879365172,IC_kwDOD7z77c5wBNY0,2024-01-05T23:09:39Z,2024-01-05T23:09:39Z,COLLABORATOR,"Basically by the plan output here, it looks like this is an AQE optimization that is turning the entire plan into a `LocalTableScan`

```
E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains.
E                   : java.lang.AssertionError: assertion failed: Could not find DynamicPruningExpression in the Spark plan
E                   AdaptiveSparkPlan isFinalPlan=true
E                   +- == Final Plan ==
E                      LocalTableScan <empty>, [key#2006, max(value)#2017L]
E                   +- == Initial Plan ==
E                   +- == Initial Plan ==
E                      Sort [key#2006 ASC NULLS FIRST, max(value)#2017L ASC NULLS FIRST], true, 0
E                      +- Exchange rangepartitioning(key#2006 ASC NULLS FIRST, max(value)#2017L ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=5302]
E                         +- HashAggregate(keys=[key#2006], functions=[max(value#2007L)], output=[key#2006, max(value)#2017L])
E                            +- Exchange hashpartitioning(key#2006, 4), ENSURE_REQUIREMENTS, [plan_id=5299]
E                               +- HashAggregate(keys=[key#2006], functions=[partial_max(value#2007L)], output=[key#2006, max#2023L])
E                                  +- Union
...
```",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879365172/reactions,0,0,0,0,0,0,0,0,0,10147
1123,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879483404,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1879483404,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1879483404,IC_kwDOD7z77c5wBqQM,2024-01-06T01:30:26Z,2024-01-06T01:30:26Z,COLLABORATOR,"> Basically by the plan output here, it looks like this is an AQE optimization that is turning the entire plan into a `LocalTableScan`
> 
> 
> 
> ```
> 
> E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.rapids.ExecutionPlanCaptureCallback.assertContains.
> 
> E                   : java.lang.AssertionError: assertion failed: Could not find DynamicPruningExpression in the Spark plan
> 
> E                   AdaptiveSparkPlan isFinalPlan=true
> 
> E                   +- == Final Plan ==
> 
> E                      LocalTableScan <empty>, [key#2006, max(value)#2017L]
> 
> E                   +- == Initial Plan ==
> 
> E                   +- == Initial Plan ==
> 
> E                      Sort [key#2006 ASC NULLS FIRST, max(value)#2017L ASC NULLS FIRST], true, 0
> 
> E                      +- Exchange rangepartitioning(key#2006 ASC NULLS FIRST, max(value)#2017L ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=5302]
> 
> E                         +- HashAggregate(keys=[key#2006], functions=[max(value#2007L)], output=[key#2006, max(value)#2017L])
> 
> E                            +- Exchange hashpartitioning(key#2006, 4), ENSURE_REQUIREMENTS, [plan_id=5299]
> 
> E                               +- HashAggregate(keys=[key#2006], functions=[partial_max(value#2007L)], output=[key#2006, max#2023L])
> 
> E                                  +- Union
> 
> ...
> 
> ```



I guess it determined via the join that this would return empty",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879483404/reactions,0,0,0,0,0,0,0,0,0,10147
1124,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879561695,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1879561695,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1879561695,IC_kwDOD7z77c5wB9Xf,2024-01-06T05:38:05Z,2024-01-06T05:38:05Z,COLLABORATOR,"So basically after some debugging, I think one the subqueries returned an empty result, so that was short-circuited by AQE to return a `LocalTableScan <empty>`. This happens on both the CPU and GPU, but of course this means that the result did not contain a DynamicPruningExpression, so it looks like the solution here is that we need update the test logic to be something like an either/or capture. Either there is a single LocalTableScanExec or the GPU plan needs to contain DynamicPruningExpression. ",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1879561695/reactions,0,0,0,0,0,0,0,0,0,10147
1125,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881139729,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1881139729,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1881139729,IC_kwDOD7z77c5wH-oR,2024-01-08T14:40:47Z,2024-01-08T14:40:47Z,MEMBER,"> it looks like the solution here is that we need update the test logic to be something like an either/or capture

I'm not sure that's the best fix.  The point of this test is to check handling of DPP, and the problem here is that the datagen happened to produce inputs that failed to produce a plan requiring DPP.  IMHO a better fix is to update the input data generation to ensure there isn't a degenerate join.  If we want to test handling of degenerate joins as well, that should be a separate test that explicitly sets up inputs to produce a degenerate join.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881139729/reactions,0,0,0,0,0,0,0,0,0,10147
1126,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881654755,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1881654755,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1881654755,IC_kwDOD7z77c5wJ8Xj,2024-01-08T18:59:09Z,2024-01-08T18:59:09Z,COLLABORATOR,"> > it looks like the solution here is that we need update the test logic to be something like an either/or capture
> 
> I'm not sure that's the best fix. The point of this test is to check handling of DPP, and the problem here is that the datagen happened to produce inputs that failed to produce a plan requiring DPP. IMHO a better fix is to update the input data generation to ensure there isn't a degenerate join. If we want to test handling of degenerate joins as well, that should be a separate test that explicitly sets up inputs to produce a degenerate join.

Makes sense. Will investigate what is producing the empty join",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1881654755/reactions,0,0,0,0,0,0,0,0,0,10147
1127,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1883424646,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1883424646,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1883424646,IC_kwDOD7z77c5wQseG,2024-01-09T16:55:16Z,2024-01-09T16:55:16Z,COLLABORATOR,"`test_dpp_empty_relation` already exists, so I think we just need to prevent the degenerate join in this test",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1883424646/reactions,0,0,0,0,0,0,0,0,0,10147
1128,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942249326,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1942249326,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1942249326,IC_kwDOD7z77c5zxF9u,2024-02-13T19:32:41Z,2024-02-13T19:32:41Z,COLLABORATOR,"Test is now failing again:

```
FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-parquet][DATAGEN_SEED=1707665221, INJECT_OOM, IGNORE_ORDER]
```",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942249326/reactions,0,0,0,0,0,0,0,0,0,10147
1129,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960288857,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1960288857,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1960288857,IC_kwDOD7z77c5016JZ,2024-02-22T20:47:59Z,2024-02-22T20:47:59Z,MEMBER,"Saw this fail again on Dataproc nightly run.
```
[2024-02-22T15:41:19.602Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-parquet][DATAGEN_SEED=1708615902, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.s...
[2024-02-22T15:41:19.602Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-orc][DATAGEN_SEED=1708615902, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.s...
[2024-02-22T15:41:19.602Z] = 2 failed, 116 passed, 11 skipped, 26232 deselected, 9 warnings in 557.14s (0:09:17) =
```",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960288857/reactions,0,0,0,0,0,0,0,0,0,10147
1130,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970985734,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-1970985734,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,1970985734,IC_kwDOD7z77c51etsG,2024-02-29T11:58:33Z,2024-02-29T11:58:33Z,COLLABORATOR,"Another failure
```
[2024-02-29T10:10:57.500Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-parquet][DATAGEN_SEED=1709192431, INJECT_OOM, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.s...

[2024-02-29T10:10:57.500Z] FAILED ../../src/main/python/dpp_test.py::test_dpp_reuse_broadcast_exchange[true-5-orc][DATAGEN_SEED=1709192431, IGNORE_ORDER] - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.s...
```",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1970985734/reactions,0,0,0,0,0,0,0,0,0,10147
1131,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110950906,https://github.com/NVIDIA/spark-rapids/issues/10147#issuecomment-2110950906,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10147,2110950906,IC_kwDOD7z77c590o36,2024-05-14T19:04:13Z,2024-05-14T19:04:13Z,COLLABORATOR,"Considering this is actually a test issue (the test not being able to avoid an empty LocalTableScan) and not an issue with the plugin, lowering the priority",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110950906/reactions,0,0,0,0,0,0,0,0,0,10147
1132,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894574620,https://github.com/NVIDIA/spark-rapids/issues/10200#issuecomment-1894574620,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10200,1894574620,IC_kwDOD7z77c5w7Ooc,2024-01-16T21:53:10Z,2024-01-16T21:53:10Z,COLLABORATOR,We should fix this in all versions of our shims.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894574620/reactions,0,0,0,0,0,0,0,0,0,10200
1133,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894597595,https://github.com/NVIDIA/spark-rapids/issues/10200#issuecomment-1894597595,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10200,1894597595,IC_kwDOD7z77c5w7UPb,2024-01-16T22:07:25Z,2024-01-16T22:07:25Z,COLLABORATOR,"To be clear it would be nice to fix this in all versions of our shim, but it is not a requirement at all.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894597595/reactions,0,0,0,0,0,0,0,0,0,10200
1134,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894317722,https://github.com/NVIDIA/spark-rapids/issues/10203#issuecomment-1894317722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10203,1894317722,IC_kwDOD7z77c5w6P6a,2024-01-16T18:43:29Z,2024-01-16T18:49:05Z,COLLABORATOR,"I was able to replicate this locally trying to build against Spark 3.5.0 using JDK 8 + Scala **2.12.17** by editing `pom.xml` and setting the `scala.version` to `2.12.17`.

```diff
diff --git a/pom.xml b/pom.xml
index fd1c821cf..0d6ed32d9 100644
--- a/pom.xml
+++ b/pom.xml
@@ -639,7 +639,7 @@
             <id>scala-2.12</id>
             <properties>
                 <scala.binary.version>2.12</scala.binary.version>
-                <scala.version>2.12.15</scala.version>
+                <scala.version>2.12.17</scala.version>
             </properties>
         </profile>
         <profile>
@@ -727,7 +727,7 @@
         <scala.binary.version>2.12</scala.binary.version>
         <alluxio.client.version>2.8.0</alluxio.client.version>
         <scala.recompileMode>incremental</scala.recompileMode>
-        <scala.version>2.12.15</scala.version>
+        <scala.version>2.12.17</scala.version>
         <!--
         -processing
         to suppress unactionable ""No processor claimed any of these annotations""
diff --git a/scala2.13/pom.xml b/scala2.13/pom.xml
index 5ccc17c4b..3a4806fe5 100644
--- a/scala2.13/pom.xml
+++ b/scala2.13/pom.xml
@@ -639,7 +639,7 @@
             <id>scala-2.12</id>
             <properties>
                 <scala.binary.version>2.12</scala.binary.version>
-                <scala.version>2.12.15</scala.version>
+                <scala.version>2.12.17</scala.version>
             </properties>
         </profile>
         <profile>
```


 Interesting in that this issue doesn't occur in Scala 2.13 using JDK8 + Scala **2.13.8** ",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1894317722/reactions,0,0,0,0,0,0,0,0,0,10203
1135,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998767163,https://github.com/NVIDIA/spark-rapids/issues/10221#issuecomment-1998767163,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10221,1998767163,IC_kwDOD7z77c53IsQ7,2024-03-15T01:46:33Z,2024-03-15T01:46:33Z,COLLABORATOR,"Not planing work on this for release 24.04, unassign it from me.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998767163/reactions,0,0,0,0,0,0,0,0,0,10221
1136,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904460394,https://github.com/NVIDIA/spark-rapids/issues/10233#issuecomment-1904460394,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10233,1904460394,IC_kwDOD7z77c5xg8Jq,2024-01-22T17:19:41Z,2024-01-22T17:19:41Z,COLLABORATOR,"This is 100% repeatable, and it calculates different results for 0.5 (median value) every time. I think this is a bug in Spark that I found a while ago.

https://issues.apache.org/jira/browse/SPARK-45599

Not sure if we want to avoid -0.0 in our test cases until this is fixed or what. (This one had 42 out of 2048 that were -0.0 and 42 that were 0.0, which is what is needed to make the error happen with Spark)
",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904460394/reactions,0,0,0,0,0,0,0,0,0,10233
1137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906941602,https://github.com/NVIDIA/spark-rapids/issues/10233#issuecomment-1906941602,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10233,1906941602,IC_kwDOD7z77c5xqZ6i,2024-01-23T21:24:23Z,2024-01-23T21:24:23Z,COLLABORATOR,"I think the solution here is to update FloatGen and DoubleGen so that we can replace -0.0 with 0.0. We would enable it for these tests, but keep other tests still using -0.0s. We also should have a follow on issue so when SPARK-45599 is fixed that we can come back and turn on -0.0 testing for versions of Spark that get the right answer.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906941602/reactions,0,0,0,0,0,0,0,0,0,10233
1138,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2075441665,https://github.com/NVIDIA/spark-rapids/issues/10233#issuecomment-2075441665,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10233,2075441665,IC_kwDOD7z77c57tLoB,2024-04-24T17:09:29Z,2024-04-24T17:09:29Z,COLLABORATOR,"The underlying issue [SPARK-45599](https://issues.apache.org/jira/browse/SPARK-45599) has been resolved, so we should follow up to turn on -0.0 testing for Spark [4.0.0+](https://issues.apache.org/jira/issues/?jql=project+%3D+SPARK+AND+fixVersion+%3D+4.0.0), [3.5.2+](https://issues.apache.org/jira/issues/?jql=project+%3D+SPARK+AND+fixVersion+%3D+3.5.2)",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2075441665/reactions,0,0,0,0,0,0,0,0,0,10233
1139,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904308806,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1904308806,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1904308806,IC_kwDOD7z77c5xgXJG,2024-01-22T15:59:11Z,2024-01-22T15:59:11Z,COLLABORATOR,"I ran this locally and was not able to reproduce it. 

I think it is the same problem as https://github.com/NVIDIA/spark-rapids/issues/9822 and https://github.com/NVIDIA/spark-rapids/issues/10026

because average really is a `SUM(X)/COUNT(x)` and if `SUM(x)` can fail, then dividing it can also fail, but it likely to fail by less. We should look at what is the best way to mitigate these issues around floats/doubles.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1904308806/reactions,0,0,0,0,0,0,0,0,0,10234
1140,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1973538130,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1973538130,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1973538130,IC_kwDOD7z77c51oc1S,2024-03-01T16:54:19Z,2024-03-01T17:03:08Z,COLLABORATOR,"I'm not certain this is the same issue.  I was unable to reproduce this with spark 3.3.3-scala-2.12, but it repros consistently for me with spark-3.3.3-scala-2.13 using DATAGEN_SEED= 1705756525.  I'm not sure how the scala version could affect this.  @revans2 did you test with 2.12 or 2.13?",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1973538130/reactions,0,0,0,0,0,0,0,0,0,10234
1141,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1973601319,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1973601319,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1973601319,IC_kwDOD7z77c51osQn,2024-03-01T17:32:31Z,2024-03-01T17:32:31Z,COLLABORATOR,I think I only tried 2.12,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1973601319/reactions,0,0,0,0,0,0,0,0,0,10234
1142,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977116581,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1977116581,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1977116581,IC_kwDOD7z77c512Gel,2024-03-04T17:34:34Z,2024-03-04T17:34:34Z,COLLABORATOR,"With debug logging, I have verified that the arithmetic results I am getting from the pluing with scala2.12 and 2.13 are the same.  I believe the problem here is that for some reason when I run with scala-2.13, the pytest.ini file is not being loaded, so our marks are not being honored.   There are a lot of warnings about marks when I run with scala-2.13, for example:
```
../../../../integration_tests/src/main/python/marks.py:20
  /home/jimb/git/spark-rapids/integration_tests/src/main/python/marks.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.approximate_float - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/mark.html
    approximate_float = pytest.mark.approximate_float
```
I suspect that is why this particular test is failing in 2.13 and not in 2.12.  From the logs, I can see a difference here:
```
scala 2.12:
============================= test session starts ==============================
platform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /home/jimb/git/spark-rapids/integration_tests, configfile: pytest.ini
plugins: xdist-2.4.0, forked-1.3.0
collecting ... collected 26674 items / 26673 deselected / 1 selected
```
```
scala 2.13:
============================= test session starts ==============================
platform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /home/jimb/git/spark-rapids/integration_tests
plugins: xdist-2.4.0, forked-1.3.0
collecting ... collected 26674 items / 26673 deselected / 1 selected
```
I haven't quite nailed down why this is happening yet.  The root-dirs are the same, so I'm not sure why it's not finding the pytest.ini file.
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977116581/reactions,0,0,0,0,0,0,0,0,0,10234
1143,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977329152,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1977329152,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1977329152,IC_kwDOD7z77c5126YA,2024-03-04T19:41:32Z,2024-03-04T19:41:32Z,COLLABORATOR,"Actually, I didn't notice this before, because I wasn't printing the test output data in the successful run.  But it looks like in this case it is the CPU that is producing different results between 2.12 and 2.13:
```
2.12:

--- CPU OUTPUT
+++ GPU OUTPUT
-Row(avg(DISTINCT a)=-9.961353300130207e+25, avg(DISTINCT b)=nan, avg(DISTINCT c)=-6.749297777543448e+17)
+Row(avg(DISTINCT a)=-9.961353300130207e+25, avg(DISTINCT b)=nan, avg(DISTINCT c)=-6.74929777754345e+17)

2.13
--- CPU OUTPUT
+++ GPU OUTPUT
-Row(avg(DISTINCT a)=-9.961254917487832e+25, avg(DISTINCT b)=nan, avg(DISTINCT c)=-6.749297777543448e+17)
+Row(avg(DISTINCT a)=-9.961353300130207e+25, avg(DISTINCT b)=nan, avg(DISTINCT c)=-6.74929777754345e+17)
```
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977329152/reactions,0,0,0,0,0,0,0,0,0,10234
1144,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1978990543,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1978990543,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1978990543,IC_kwDOD7z77c519P_P,2024-03-05T15:08:21Z,2024-03-05T15:08:21Z,COLLABORATOR,"[hash-debug-input.json](https://github.com/NVIDIA/spark-rapids/files/14497237/hash-debug-input.json)

Uploading the input data I captured from the integration test.
So far I haven't quite been able to reproduce this in spark shell.
",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1978990543/reactions,0,0,0,0,0,0,0,0,0,10234
1145,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1978998855,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-1978998855,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,1978998855,IC_kwDOD7z77c519SBH,2024-03-05T15:12:09Z,2024-03-05T15:12:09Z,COLLABORATOR,"This is what I got running in spark shell.  I did it with just `avg(distinct a)` and also with all of them.
```
spark.conf.set(""spark.rapids.sql.enabled"", false)
val df=spark.read.json(""/opt/data/jimb/hash-debug-input.json"")
val adf=df.select(""a"")
adf.createOrReplaceTempView(""input"")
val sql = ""SELECT avg(distinct a) from input""
val avg = spark.sql(sql).collect
```

```
spark shell scala 2.12:
CPU: avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917495146E25])           
GPU: avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917498747E25]) 

CPU: avg: Array[org.apache.spark.sql.Row] = Array([-9.961353333333334E25,NaN,-6.7492977775434509E17])
GPU: avg: Array[org.apache.spark.sql.Row] = Array([-9.961353333333334E25,NaN,-6.7492977775434496E17])
          

spark shell 2.13
CPU: val avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917495146E25])
GPU: val avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917498747E25])

CPU: val avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917487832E25,NaN,-6.749297777543447E17])
GPU: val avg: Array[org.apache.spark.sql.Row] = Array([-9.961254917487832E25,NaN,-6.7492977775434496E17])

```",,jbrennan333,69316431,MDQ6VXNlcjY5MzE2NDMx,https://avatars.githubusercontent.com/u/69316431?v=4,,https://api.github.com/users/jbrennan333,https://github.com/jbrennan333,https://api.github.com/users/jbrennan333/followers,https://api.github.com/users/jbrennan333/following{/other_user},https://api.github.com/users/jbrennan333/gists{/gist_id},https://api.github.com/users/jbrennan333/starred{/owner}{/repo},https://api.github.com/users/jbrennan333/subscriptions,https://api.github.com/users/jbrennan333/orgs,https://api.github.com/users/jbrennan333/repos,https://api.github.com/users/jbrennan333/events{/privacy},https://api.github.com/users/jbrennan333/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1978998855/reactions,0,0,0,0,0,0,0,0,0,10234
1146,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008101778,https://github.com/NVIDIA/spark-rapids/issues/10234#issuecomment-2008101778,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10234,2008101778,IC_kwDOD7z77c53sTOS,2024-03-19T20:48:03Z,2024-03-19T20:48:03Z,COLLABORATOR,saw this again in last night integration tests: DATAGEN_SEED=1710866199,,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008101778/reactions,0,0,0,0,0,0,0,0,0,10234
1147,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998763008,https://github.com/NVIDIA/spark-rapids/issues/10235#issuecomment-1998763008,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10235,1998763008,IC_kwDOD7z77c53IrQA,2024-03-15T01:45:04Z,2024-03-15T01:46:26Z,COLLABORATOR,"Not planing work on this for release 24.04, unassign it from me.",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998763008/reactions,0,0,0,0,0,0,0,0,0,10235
1148,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906606616,https://github.com/NVIDIA/spark-rapids/issues/10241#issuecomment-1906606616,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10241,1906606616,IC_kwDOD7z77c5xpIIY,2024-01-23T17:50:36Z,2024-01-23T17:50:36Z,CONTRIBUTOR,Depends on https://github.com/rapidsai/cudf/issues/14830,,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906606616/reactions,0,0,0,0,0,0,0,0,0,10241
1149,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906887850,https://github.com/NVIDIA/spark-rapids/issues/10252#issuecomment-1906887850,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10252,1906887850,IC_kwDOD7z77c5xqMyq,2024-01-23T20:44:24Z,2024-01-23T20:44:24Z,COLLABORATOR,"This is likely going to need some help from CUDF in the general case.

@viadea do we need this for all types of window operations ROW/RANGE RUNNING, UNBOUNDED to UNBOUNDED, BOUNDED, ETC or are there very specific windows and operators that we need to worry about?",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906887850/reactions,0,0,0,0,0,0,0,0,0,10252
1150,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906907235,https://github.com/NVIDIA/spark-rapids/issues/10252#issuecomment-1906907235,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10252,1906907235,IC_kwDOD7z77c5xqRhj,2024-01-23T20:58:07Z,2024-01-23T20:58:07Z,COLLABORATOR,It is very specific for this user. I can ping you offline about the exact use case @revans2 ,,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906907235/reactions,1,1,0,0,0,0,0,0,0,10252
1151,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906912016,https://github.com/NVIDIA/spark-rapids/issues/10252#issuecomment-1906912016,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10252,1906912016,IC_kwDOD7z77c5xqSsQ,2024-01-23T21:01:43Z,2024-01-23T21:06:48Z,COLLABORATOR,"~The only problem here is that the partition-spec isn't specified, so it amounts to:~
```sql
RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
```
~This could well be reinterpreted as:~
```sql
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
```
~I wonder if we could address that directly in the plugin `GpuWindowExec`.~

Nope. My mistake.  This amounts to `UNBOUNDED PRECEDING AND CURRENT ROW`.  That's going to be tricky.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906912016/reactions,0,0,0,0,0,0,0,0,0,10252
1152,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906972212,https://github.com/NVIDIA/spark-rapids/issues/10252#issuecomment-1906972212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10252,1906972212,IC_kwDOD7z77c5xqhY0,2024-01-23T21:46:07Z,2024-01-23T21:47:18Z,COLLABORATOR,"This is a sort of corner-case.  Explicit range values aren't specified, but we would still need equality comparisons for the order-by columns (plural).

@revans2 saw this right away: This will need some handling in `libcudf`, whose APIs assume a single order-by column.

I'll think on it.  Perhaps we need a special path for cases where `RANGE` values cannot be specified (like this case).",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1906972212/reactions,0,0,0,0,0,0,0,0,0,10252
1153,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008547736,https://github.com/NVIDIA/spark-rapids/issues/10254#issuecomment-2008547736,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10254,2008547736,IC_kwDOD7z77c53uAGY,2024-03-20T01:41:28Z,2024-03-20T01:41:28Z,COLLABORATOR,"Verified the following issues successfully against unmerged [PR](https://github.com/NVIDIA/spark-rapids-jni/pull/1868):   
https://github.com/NVIDIA/spark-rapids/issues/10537
https://github.com/NVIDIA/spark-rapids/issues/10217
https://github.com/NVIDIA/spark-rapids/issues/10216
https://github.com/NVIDIA/spark-rapids/issues/10196
https://github.com/NVIDIA/spark-rapids/issues/10194
https://github.com/NVIDIA/spark-rapids/issues/9033

https://github.com/NVIDIA/spark-rapids/issues/10218:  3/4 pass 1/4 fail, haoyang will fix
https://github.com/NVIDIA/spark-rapids/issues/10212:  Haoyang will fix in [Plugin PR](https://github.com/NVIDIA/spark-rapids/pull/10581)

",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008547736/reactions,0,0,0,0,0,0,0,0,0,10254
1154,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2017102929,https://github.com/NVIDIA/spark-rapids/issues/10254#issuecomment-2017102929,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10254,2017102929,IC_kwDOD7z77c54OoxR,2024-03-25T02:31:22Z,2024-03-25T02:31:22Z,COLLABORATOR,"1. **#10194, #10196, #10537, #10217, #10216, #9033** 

     Will be fixed by PRs:
- JNI PR: https://github.com/NVIDIA/spark-rapids-jni/pull/1893
- https://github.com/NVIDIA/spark-rapids/pull/10581

2. #10212 
will be fixed, @thirtiseven please double confirm.

3. And for #10218, 3/4 sub-items are done, and sub-item number normalization is postphoned.

4. For testing #10226 
    did not plan in Release 24.04.

",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2017102929/reactions,0,0,0,0,0,0,0,0,0,10254
1155,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1939201860,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1939201860,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1939201860,IC_kwDOD7z77c5zld9E,2024-02-12T17:29:07Z,2024-02-12T17:29:07Z,COLLABORATOR,"Using `grep`, extracted out the list of failing tests.

```
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus[Decimal(36,5)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus[Decimal(38,10)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus[Decimal(38,0)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus[Decimal(36,-5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus[Decimal(38,-10)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus_ansi_no_overflow[Decimal(36,5)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus_ansi_no_overflow[Decimal(38,10)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus_ansi_no_overflow[Decimal(38,0)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus_ansi_no_overflow[Decimal(36,-5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_unary_minus_ansi_no_overflow[Decimal(38,-10)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs[Decimal(36,5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs[Decimal(38,10)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs[Decimal(38,0)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs[Decimal(36,-5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs[Decimal(38,-10)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs_ansi_no_overflow[Decimal(36,5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs_ansi_no_overflow[Decimal(38,10)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs_ansi_no_overflow[Decimal(38,0)]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs_ansi_no_overflow[Decimal(36,-5)][INJECT_OOM]
arithmetic-ops-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/arithmetic_ops_test.py::test_abs_ansi_no_overflow[Decimal(38,-10)][INJECT_OOM]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_eq[(String, False)]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_ne[(String, False)]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_lt[(String, False)][INJECT_OOM]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_lte[(String, False)]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_gt[(String, False)]
ast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/ast_test.py::test_gte[(String, False)]
cast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/cast_test.py::test_cast_decimal_to[to:ByteType()-from:Decimal(7,-3)][APPROXIMATE_FLOAT]
cast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/cast_test.py::test_cast_decimal_to[to:ShortType()-from:Decimal(7,-3)][APPROXIMATE_FLOAT]
cast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/cast_test.py::test_cast_decimal_to[to:IntegerType()-from:Decimal(7,-3)][INJECT_OOM, APPROXIMATE_FLOAT]
cast-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/cast_test.py::test_cast_decimal_to[to:LongType()-from:Decimal(7,-3)][INJECT_OOM, APPROXIMATE_FLOAT]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[single_partition_int_value-orc]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[single_partition_single_value_without_nulls-orc]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[single_partition_empty_value_with_nulls-orc][INJECT_OOM]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[single_partition_single_empty_value_with_nulls-orc][INJECT_OOM]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[single_partition_multiple_value-orc]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[multiple_partition_single_value-orc]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[multiple_partition_int_value-orc][INJECT_OOM]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[multiple_partition_multiple_value_wider_first_col-orc][INJECT_OOM]
col-size-exceeding-cudf-limit-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/col_size_exceeding_cudf_limit_test.py::test_col_size_exceeding_cudf_limit[multiple_partition_multiple_value_narrow_first_col-orc]
datasourcev2-read-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_read_test.py::test_read_int
datasourcev2-read-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_read_test.py::test_read_strings
datasourcev2-read-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_read_test.py::test_read_all_types[INJECT_OOM]
datasourcev2-read-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_read_test.py::test_read_all_types_count
datasourcev2-read-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_read_test.py::test_read_arrow_off[INJECT_OOM]
datasourcev2-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_write_test.py::test_write_hive_bucketed_table_fallback[parquet][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
datasourcev2-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/datasourcev2_write_test.py::test_write_hive_bucketed_table_fallback[orc][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
explain-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/explain_test.py::test_explain_bucketd_scan[ALLOW_NON_GPU(ANY)]
explain-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/explain_test.py::test_explain_bucket_column_not_read[INJECT_OOM, ALLOW_NON_GPU(ANY)]
explain-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/explain_test.py::test_explain_bucket_disabled_by_conf[INJECT_OOM, ALLOW_NON_GPU(ANY)]
explain-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/explain_test.py::test_explain_bucket_disabled_by_query_planner[INJECT_OOM, ALLOW_NON_GPU(ANY)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Byte(not_null)][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Byte][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Short(not_null)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Short][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Integer(not_null)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Integer]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Long(not_null)][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Long][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Float(not_null)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Float][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Double(not_null)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Double]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Decimal(not_null)(18,0)][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_written_with_fastparquet[Decimal(18,0)]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_rewritten_with_fastparquet[Date(not_null)-int961][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_rewritten_with_fastparquet[Date-int96]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_rewritten_with_fastparquet[Timestamp(not_null)-int960][INJECT_OOM]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_rewritten_with_fastparquet[Timestamp-int96]
fastparquet-compatibility-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/fastparquet_compatibility_test.py::test_reading_file_rewritten_with_fastparquet[Struct(not_null)(('first', Integer(not_null)))-int96]
hive-delimited-text-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_delimited_text_test.py::test_read_compressed_hive_text[BZip2Codec]
hive-delimited-text-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_delimited_text_test.py::test_read_compressed_hive_text[DefaultCodec]
hive-delimited-text-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_delimited_text_test.py::test_read_compressed_hive_text[GzipCodec][INJECT_OOM]
hive-delimited-text-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_delimited_text_test.py::test_partitioned_hive_text_write[PartitionWriteMode.Static][IGNORE_ORDER({'local': True})]
hive-delimited-text-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_delimited_text_test.py::test_partitioned_hive_text_write[PartitionWriteMode.Dynamic][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[PARQUET-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[PARQUET-[Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)]), Struct(['child0', Byte],['child1', Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)])]), Struct(['child0', Array(Short)],['child1', Double])]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[PARQUET-[Array(Byte), Array(Short), Array(Integer), Array(Long), Array(Float), Array(Double), Array(String), Array(Boolean), Array(Date), Array(Timestamp), Array(Decimal(7,3)), Array(Decimal(12,2)), Array(Decimal(20,2)), Array(Array(Short)), Array(Array(String)), Array(Struct(['child0', Byte],['child1', String],['child2', Float]))]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[PARQUET-[Map(String(not_null),String), Map(Boolean(not_null),Boolean), Map(Byte(not_null),Byte), Map(Short(not_null),Short), Map(Integer(not_null),Integer), Map(Long(not_null),Long), Map(Float(not_null),Float), Map(Double(not_null),Double), Map(Timestamp(not_null),Timestamp), Map(Date(not_null),Date), Map(Decimal(not_null)(15,1),Decimal(15,1)), Map(Decimal(not_null)(36,5),Decimal(36,5))]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[nativeorc-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[nativeorc-[Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)]), Struct(['child0', Byte],['child1', Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)])]), Struct(['child0', Array(Short)],['child1', Double])]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[nativeorc-[Array(Byte), Array(Short), Array(Integer), Array(Long), Array(Float), Array(Double), Array(String), Array(Boolean), Array(Date), Array(Timestamp), Array(Decimal(7,3)), Array(Decimal(12,2)), Array(Decimal(20,2)), Array(Array(Short)), Array(Array(String)), Array(Struct(['child0', Byte],['child1', String],['child2', Float]))]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[nativeorc-[Map(String(not_null),String), Map(Boolean(not_null),Boolean), Map(Byte(not_null),Byte), Map(Short(not_null),Short), Map(Integer(not_null),Integer), Map(Long(not_null),Long), Map(Float(not_null),Float), Map(Double(not_null),Double), Map(Timestamp(not_null),Timestamp), Map(Date(not_null),Date), Map(Decimal(not_null)(15,1),Decimal(15,1)), Map(Decimal(not_null)(36,5),Decimal(36,5))]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[hiveorc-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[hiveorc-[Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)]), Struct(['child0', Byte],['child1', Struct(['child0', Byte],['child1', Short],['child2', Integer],['child3', Long],['child4', Float],['child5', Double],['child6', String],['child7', Boolean],['child8', Date],['child9', Timestamp],['child10', Decimal(7,3)],['child11', Decimal(12,2)],['child12', Decimal(20,2)])]), Struct(['child0', Array(Short)],['child1', Double])]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[hiveorc-[Array(Byte), Array(Short), Array(Integer), Array(Long), Array(Float), Array(Double), Array(String), Array(Boolean), Array(Date), Array(Timestamp), Array(Decimal(7,3)), Array(Decimal(12,2)), Array(Decimal(20,2)), Array(Array(Short)), Array(Array(String)), Array(Struct(['child0', Byte],['child1', String],['child2', Float]))]][INJECT_OOM, IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_basic[hiveorc-[Map(String(not_null),String), Map(Boolean(not_null),Boolean), Map(Byte(not_null),Byte), Map(Short(not_null),Short), Map(Integer(not_null),Integer), Map(Long(not_null),Long), Map(Float(not_null),Float), Map(Double(not_null),Double), Map(Timestamp(not_null),Timestamp), Map(Date(not_null),Date), Map(Decimal(not_null)(15,1),Decimal(15,1)), Map(Decimal(not_null)(36,5),Decimal(36,5))]][IGNORE_ORDER({'local': True})]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_configs_fallback[('PARQUET', {'spark.sql.legacy.parquet.datetimeRebaseModeInWrite': 'LEGACY', 'spark.sql.legacy.parquet.int96RebaseModeInWrite': 'LEGACY'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_configs_fallback[('PARQUET', {'parquet.encryption.footer.key': 'k1', 'parquet.encryption.column.keys': 'k2:a'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_configs_fallback[('PARQUET', {'spark.sql.parquet.compression.codec': 'gzip'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_configs_fallback[('PARQUET', {'spark.sql.parquet.writeLegacyFormat': 'true'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_configs_fallback[('ORC', {'spark.sql.orc.compression.codec': 'zlib'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_options_fallback[('PARQUET', {'parquet.encryption.footer.key': 'k1', 'parquet.encryption.column.keys': 'k2:a'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_ctas_options_fallback[('ORC', {'orc.compress': 'zlib'})-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_bucketed_fallback_33X[PARQUET-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_optimized_hive_bucketed_fallback_33X[ORC-[Byte, Short, Integer, Long, Float, Double, String, Boolean, Date, Timestamp, Decimal(7,3), Decimal(12,2), Decimal(20,2)]][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_hive_copy_ints_to_long
hive-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/hive_write_test.py::test_hive_copy_longs_to_float[INJECT_OOM]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_broadcast_join_with_condition_ast_type_fallback[Left-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(BroadcastExchangeExec,BroadcastHashJoinExec,Cast,GreaterThan,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_broadcast_join_with_condition_ast_type_fallback[Right-String][INJECT_OOM, IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(BroadcastExchangeExec,BroadcastHashJoinExec,Cast,GreaterThan,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_broadcast_join_with_condition_ast_type_fallback[FullOuter-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(BroadcastExchangeExec,BroadcastHashJoinExec,Cast,GreaterThan,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_broadcast_join_with_condition_ast_type_fallback[LeftSemi-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(BroadcastExchangeExec,BroadcastHashJoinExec,Cast,GreaterThan,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_broadcast_join_with_condition_ast_type_fallback[LeftAnti-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(BroadcastExchangeExec,BroadcastHashJoinExec,Cast,GreaterThan,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_sortmerge_join_with_condition_ast_type_fallback[Left-String][INJECT_OOM, IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(GreaterThan,ShuffleExchangeExec,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_sortmerge_join_with_condition_ast_type_fallback[Right-String][INJECT_OOM, IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(GreaterThan,ShuffleExchangeExec,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_sortmerge_join_with_condition_ast_type_fallback[FullOuter-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(GreaterThan,ShuffleExchangeExec,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_sortmerge_join_with_condition_ast_type_fallback[LeftSemi-String][IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(GreaterThan,ShuffleExchangeExec,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_sortmerge_join_with_condition_ast_type_fallback[LeftAnti-String][INJECT_OOM, IGNORE_ORDER({'local': True}), ALLOW_NON_GPU(GreaterThan,ShuffleExchangeExec,SortMergeJoinExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_join_bucketed_table[true][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
join-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/join_test.py::test_join_bucketed_table[false][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs0-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs0-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs0-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs1-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs1-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs1-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs2-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs2-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs2-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs3-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs3-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs3-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs4-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs4-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs4-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs5-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs5-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs5-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs6-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs6-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs6-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs7-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs7-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs7-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs8-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs8-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs8-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs9-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs9-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs9-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs10-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs10-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[-reader_confs10-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs0-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs0-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs0-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs1-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs1-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs1-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs2-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs2-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs2-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs3-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs3-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs3-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs4-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs4-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs4-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs5-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs5-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs5-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs6-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs6-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs6-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs7-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs7-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs7-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs8-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs8-LEGACY-TIMESTAMP_MICROS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs8-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs9-LEGACY-INT96-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs9-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs9-LEGACY-TIMESTAMP_MILLIS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs10-LEGACY-INT96-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs10-LEGACY-TIMESTAMP_MICROS-Timestamp]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_ts_read_fails_datetime_legacy[parquet-reader_confs10-LEGACY-TIMESTAMP_MILLIS-Timestamp][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs0][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs1][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs2][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs3][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs4][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs5][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs6][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs7][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs8][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs9][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[-reader_confs10][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs0][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs1][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs2][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs3][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs4][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs5][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs6][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs7][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs8][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs9][IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_buckets[parquet-reader_confs10][INJECT_OOM, IGNORE_ORDER, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'PERFILE', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE'}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '0', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE'}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.coalescing.reader.numFilterParallel': '2', 'spark.rapids.sql.reader.chunked': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE', 'spark.rapids.sql.reader.chunked': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'PERFILE'}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '0', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING'}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.coalescing.reader.numFilterParallel': '2', 'spark.rapids.sql.reader.chunked': False}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '64m', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.format.parquet.multithreaded.combine.sizeBytes': '64m', 'spark.rapids.sql.format.parquet.multithreaded.read.keepOrder': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'PERFILE', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE'}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '0', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE'}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.coalescing.reader.numFilterParallel': '2', 'spark.rapids.sql.reader.chunked': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.format.parquet.reader.footer.type': 'NATIVE', 'spark.rapids.sql.reader.chunked': True}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'PERFILE'}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '0', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING'}]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'COALESCING', 'spark.rapids.sql.coalescing.reader.numFilterParallel': '2', 'spark.rapids.sql.reader.chunked': False}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.reader.multithreaded.combine.sizeBytes': '64m', 'spark.rapids.sql.reader.multithreaded.read.keepOrder': True}][INJECT_OOM]
parquet-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_test.py::test_disorder_read_schema[parquet-{'spark.rapids.sql.format.parquet.reader.type': 'MULTITHREADED', 'spark.rapids.sql.format.parquet.multithreaded.combine.sizeBytes': '64m', 'spark.rapids.sql.format.parquet.multithreaded.read.keepOrder': True}]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_parquet_write_legacy_fallback[LEGACY-INT96][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_parquet_write_legacy_fallback[LEGACY-TIMESTAMP_MICROS][ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_parquet_write_legacy_fallback[LEGACY-TIMESTAMP_MILLIS][INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,ExecutedCommandExec,WriteFilesExec)]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_partitioned_sql_parquet_write[PartitionWriteMode.Static][IGNORE_ORDER({'local': True})]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_partitioned_sql_parquet_write[PartitionWriteMode.Dynamic][INJECT_OOM, IGNORE_ORDER({'local': True})]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_dynamic_partitioned_parquet_write[INJECT_OOM, IGNORE_ORDER({'local': True})]
parquet-write-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/parquet_write_test.py::test_hive_timestamp_value_fallback[INJECT_OOM, ALLOW_NON_GPU(DataWritingCommandExec,WriteFilesExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_project[orc-False][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_project[orc-True]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_project[orc-False][INJECT_OOM, ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_project[orc-True][ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[a-orc-False]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[a-orc-True][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[b-orc-False][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[b-orc-True]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[c-orc-False]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_project[c-orc-True][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[a-orc-False][ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[a-orc-True][INJECT_OOM, ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[b-orc-False][ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[b-orc-True][INJECT_OOM, ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[c-orc-False][ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_and_project[c-orc-True][INJECT_OOM, ALLOW_NON_GPU(ProjectExec,FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[a-orc-False][INJECT_OOM, ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[a-orc-True][ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[b-orc-False][ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[b-orc-True][ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[c-orc-False][INJECT_OOM, ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_fallback_filter_project[c-orc-True][ALLOW_NON_GPU(FilterExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[a-orc-False][INJECT_OOM, ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[a-orc-True][INJECT_OOM, ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[b-orc-False][ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[b-orc-True][ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[c-orc-False][ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_prune_partition_column_when_filter_fallback_project[c-orc-True][ALLOW_NON_GPU(ProjectExec)]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_select_complex_field[orc-True-select friends.middle, friends from {} where p=1-struct<friends:array<struct<first:string,middle:string,last:string>>>]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_select_complex_field[orc-True-select name.first from {} where name.first = 'Jane'-struct<name:struct<first:string>>][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_select_complex_field[orc-False-select friends.middle, friends from {} where p=1-struct<friends:array<struct<first:string,middle:string,last:string>>>]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_select_complex_field[orc-False-select name.first from {} where name.first = 'Jane'-struct<name:struct<first:string>>][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_nested_column_prune_on_generator_output[orc-True-friend.First-struct<friends:array<struct<first:string>>>]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_nested_column_prune_on_generator_output[orc-True-friend.MIDDLE-struct<friends:array<struct<middle:string>>>]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_nested_column_prune_on_generator_output[orc-False-friend.First-struct<friends:array<struct<first:string>>>][INJECT_OOM]
prune-partition-column-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/prune_partition_column_test.py::test_nested_column_prune_on_generator_output[orc-False-friend.MIDDLE-struct<friends:array<struct<middle:string>>>][INJECT_OOM]
row-based-udf-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/row-based_udf_test.py::test_hive_empty_simple_udf
row-based-udf-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/row-based_udf_test.py::test_hive_empty_generic_udf
schema-evolution-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/schema_evolution_test.py::test_column_add_after_partition[parquet][IGNORE_ORDER({'local': True})]
schema-evolution-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/schema_evolution_test.py::test_column_add_after_partition[orc][IGNORE_ORDER({'local': True})]
udf-test_output.txt:FAILED ../../../../home/spark-rapids/integration_tests/src/main/python/udf_test.py::test_cogroup_apply_udf[Short(not_null)][INJECT_OOM, IGNORE_ORDER]

```",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1939201860/reactions,0,0,0,0,0,0,0,0,0,10347
1156,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1958716903,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1958716903,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1958716903,IC_kwDOD7z77c50v6Xn,2024-02-22T05:14:49Z,2024-02-22T05:14:49Z,COLLABORATOR,"It looks like the test container (which contains the test files) being used was out of date. I tried just the arithmetic_ops_test.py tests with an updated container, and all the tests in that file pass. I think we should try this with an updated container and updated JAR and close this issue if all the tests pass.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1958716903/reactions,0,0,0,0,0,0,0,0,0,10347
1157,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1958719582,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1958719582,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1958719582,IC_kwDOD7z77c50v7Be,2024-02-22T05:18:11Z,2024-02-22T05:18:11Z,COLLABORATOR,"> It looks like the test container (which contains the test files) being used was out of date. I tried just the arithmetic_ops_test.py tests with an updated container, and all the tests in that file pass. I think we should try this with an updated container and updated JAR and close this issue if all the tests pass.

To elaborate with more context, there are several integration tests that were updated when the Scala 2.13 build was enabled, and certain tests in arithmetic_ops_test (for certain) were skipped if the runtime detected Scala 2.13 (mostly because Spark built with Scala 2.13 had inconsistent behavior with high precision Decimal values)",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1958719582/reactions,0,0,0,0,0,0,0,0,0,10347
1158,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1986936934,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1986936934,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1986936934,IC_kwDOD7z77c52bkBm,2024-03-09T18:08:19Z,2024-03-09T18:08:19Z,COLLABORATOR,Decimal cast tests to integer are failing on CPU Spark here. Will defer those until later.,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1986936934/reactions,0,0,0,0,0,0,0,0,0,10347
1159,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989735149,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1989735149,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1989735149,IC_kwDOD7z77c52mPLt,2024-03-12T01:21:27Z,2024-03-12T01:21:27Z,COLLABORATOR,"So in general, the challenge is that GCP Dataproc Serverless is currently using 23.12.1 of the plugin (makes sense, it's the latest release), so we need to use integration tests from that version of plugin. Re-running them with an updated container that only takes the tests from that version of plugin.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1989735149/reactions,0,0,0,0,0,0,0,0,0,10347
1160,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998579475,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-1998579475,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,1998579475,IC_kwDOD7z77c53H-cT,2024-03-14T22:23:07Z,2024-03-14T22:23:07Z,COLLABORATOR,"So a couple of fixes that have worked for several groups of tests:

* Deploying a [Dataproc Metastore](https://cloud.google.com/dataproc-metastore/docs) that we configure with our integration test batches. This enables access to `saveAsTable()` and other Hive-related functionality.  This moves it further in line with how our nightly integration test runs are deployed. In particular, this helped enable all of the tests in parquet_test.py and hive tests to pass.  There are more tests spread out that have also moved from FAILED to PASSED.

* Incorporating the JARs that are created for integration_tests. There are a few tests that rely on some classes defined in this part  of the codebase that aren't normally shipped with the release JAR.  So we add these to the custom container 

* Many tests are failing due to insufficient resources when using the test script, so we need to figure out how to detect that failure and re-run the set of tests.

Current blockers:
* ORC support is impacted by a class path issue. The relevant error is:
```
E                   Caused by: java.lang.NoSuchMethodError: 'com.google.protobuf.CodedInputStream org.apache.orc.impl.InStream.createCodedInputStream(org.apache.orc.impl.InStream)'
E                   	at com.nvidia.spark.rapids.shims.OrcShims320untilAllBase.$anonfun$parseFooterFromBuffer$1(OrcShims320untilAllBase.scala:153)
E                   	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)
E                   	at com.nvidia.spark.rapids.shims.OrcShims320untilAllBase.parseFooterFromBuffer(OrcShims320untilAllBase.scala:148)
E                   	at com.nvidia.spark.rapids.shims.OrcShims320untilAllBase.parseFooterFromBuffer$(OrcShims320untilAllBase.scala:140)
E                   	at com.nvidia.spark.rapids.shims.OrcShims$.parseFooterFromBuffer(OrcShims.scala:34)
E                   	at com.nvidia.spark.rapids.GpuOrcFileFilterHandler$.loadOrcTailFromBuffer(GpuOrcScan.scala:1795)
E                   	at com.nvidia.spark.rapids.GpuOrcFileFilterHandler$.com$nvidia$spark$rapids$GpuOrcFileFilterHandler$$getOrcTail(GpuOrcScan.scala:1780)
E                   	at com.nvidia.spark.rapids.GpuOrcFileFilterHandler.filterStripes(GpuOrcScan.scala:1328)
E                   	at com.nvidia.spark.rapids.MultiFileCloudOrcPartitionReader$ReadBatchRunner.doRead(GpuOrcScan.scala:2051)
E                   	at com.nvidia.spark.rapids.MultiFileCloudOrcPartitionReader$ReadBatchRunner.call(GpuOrcScan.scala:2029)
E                   	at com.nvidia.spark.rapids.MultiFileCloudOrcPartitionReader$ReadBatchRunner.call(GpuOrcScan.scala:2018)
E                   	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
```

This particular issue is currently responsible for the majority of current test failures on Dataproc Serverless.

* Decimal support in cast_test.py.  This is actually a CPU failure where Dataproc Serverless seemingly runs into a codepath in Apache Spark that doesn't handle negative scale well. This also only happens in integration tests and not in the notebook environment.
",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1998579475/reactions,0,0,0,0,0,0,0,0,0,10347
1161,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2115873769,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-2115873769,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,2115873769,IC_kwDOD7z77c5-Havp,2024-05-16T17:58:06Z,2024-05-16T17:58:06Z,COLLABORATOR,"Update on ORC support:

Google uses an ORC jar with a shaded protobuf:

 `orc-core-1.8.6-shaded-protobuf.jar`, so I'm guessing that `org.apache.orc.impl.InStream.createCodedInputStream(org.apache.orc.impl.InStream)` is probably returning a version of CodedInputStream in a shaded package path.
",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2115873769/reactions,0,0,0,0,0,0,0,0,0,10347
1162,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116305068,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-2116305068,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,2116305068,IC_kwDOD7z77c5-JECs,2024-05-16T22:26:10Z,2024-05-16T22:26:10Z,COLLABORATOR,"> Update on ORC support:
> 
> Google uses an ORC jar with a shaded protobuf:
> 
> `orc-core-1.8.6-shaded-protobuf.jar`, so I'm guessing that `org.apache.orc.impl.InStream.createCodedInputStream(org.apache.orc.impl.InStream)` is probably returning a version of CodedInputStream in a shaded package path.

I was actually able to simulate the GCP serverless environment issue this by replacing the following jars in $SPARK_HOME:

orc-core-1.7.10.jar ---> orc-core-1.8.6-shaded-protobuf.jar
orc-mapreduce-1.7.10.jar ---> orc-mapreduce-1.8.6-shaded-protobuf.jar
orc-shims-1.7.10.jar ---> orc-shims-1.8.6.jar
protobuf-java-2.5.0.jar ---> protobuf-java-3.21.12.jar
 ",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116305068/reactions,0,0,0,0,0,0,0,0,0,10347
1163,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137606901,https://github.com/NVIDIA/spark-rapids/issues/10347#issuecomment-2137606901,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10347,2137606901,IC_kwDOD7z77c5_aUr1,2024-05-29T14:49:18Z,2024-05-29T14:49:18Z,COLLABORATOR,"The ORC tests in `orc_test.py` actually pass with Serverless Runtime 2.1, which uses Spark 3.4.1 and our shim already handles the orc with shaded protobuf JARs.  However, there are other failures to investigate, which could be test-related.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137606901/reactions,0,0,0,0,0,0,0,0,0,10347
1164,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1919964515,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1919964515,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1919964515,IC_kwDOD7z77c5ycFVj,2024-01-31T21:10:05Z,2024-01-31T21:10:05Z,MEMBER,"<details>
<summary>Row comparison failure details</summary>

```
[2024-01-31T20:47:58.663Z] --- CPU OUTPUT

[2024-01-31T20:47:58.663Z] +++ GPU OUTPUT

[2024-01-31T20:47:58.663Z] @@ -82,48 +82,48 @@

[2024-01-31T20:47:58.663Z]  Row(a=None, from_json(a)=None)

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""iz""} }', from_json(a)=Row(a='{""b"":""iz""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""md""} }', from_json(a)=Row(a='{""b"":""md""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{""b"":""mh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""od""} }', from_json(a)=Row(a='{""b"":""od""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lq""} }', from_json(a)=Row(a='{""b"":""lq""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ss""} }', from_json(a)=Row(a='{""b"":""ss""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""yw""} }', from_json(a)=Row(a='{""b"":""yw""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""bc""} }', from_json(a)=Row(a='{""b"":""bc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ud""} }', from_json(a)=Row(a='{""b"":""ud""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""rj""} }', from_json(a)=Row(a='{""b"":""rj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qe""} }', from_json(a)=Row(a='{""b"":""qe""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{""b"":""fr""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wz""} }', from_json(a)=Row(a='{""b"":""wz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ry""} }', from_json(a)=Row(a='{""b"":""ry""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hh""} }', from_json(a)=Row(a='{""b"":""hh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{""b"":""ag""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""tt""} }', from_json(a)=Row(a='{""b"":""tt""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jc""} }', from_json(a)=Row(a='{""b"":""jc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{""b"":""jg""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ay""} }', from_json(a)=Row(a='{""b"":""ay""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lp""} }', from_json(a)=Row(a='{""b"":""lp""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""sp""} }', from_json(a)=Row(a='{""b"":""sp""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hg""} }', from_json(a)=Row(a='{""b"":""hg""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hf""} }', from_json(a)=Row(a='{""b"":""hf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fd""} }', from_json(a)=Row(a='{""b"":""fd""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""en""} }', from_json(a)=Row(a='{""b"":""en""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{""b"":""zk""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wp""} }', from_json(a)=Row(a='{""b"":""wp""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""og""} }', from_json(a)=Row(a='{""b"":""og""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lb""} }', from_json(a)=Row(a='{""b"":""lb""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hj""} }', from_json(a)=Row(a='{""b"":""hj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mc""} }', from_json(a)=Row(a='{""b"":""mc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qg""} }', from_json(a)=Row(a='{""b"":""qg""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ip""} }', from_json(a)=Row(a='{""b"":""ip""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{""b"":""hc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ee""} }', from_json(a)=Row(a='{""b"":""ee""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""dr""} }', from_json(a)=Row(a='{""b"":""dr""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{""b"":""uy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{""b"":""hc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{""b"":""km""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ka""} }', from_json(a)=Row(a='{""b"":""ka""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fz""} }', from_json(a)=Row(a='{""b"":""fz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""rw""} }', from_json(a)=Row(a='{""b"":""rw""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""pv""} }', from_json(a)=Row(a='{""b"":""pv""}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{mh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""od""} }', from_json(a)=Row(a='{od}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lq""} }', from_json(a)=Row(a='{lq}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ss""} }', from_json(a)=Row(a='{ss}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""yw""} }', from_json(a)=Row(a='{yw}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""bc""} }', from_json(a)=Row(a='{bc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ud""} }', from_json(a)=Row(a='{ud}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""rj""} }', from_json(a)=Row(a='{rj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qe""} }', from_json(a)=Row(a='{qe}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{fr}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""wz""} }', from_json(a)=Row(a='{wz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ry""} }', from_json(a)=Row(a='{ry}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hh""} }', from_json(a)=Row(a='{hh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{ag}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""tt""} }', from_json(a)=Row(a='{tt}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""jc""} }', from_json(a)=Row(a='{jc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{jg}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ay""} }', from_json(a)=Row(a='{ay}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lp""} }', from_json(a)=Row(a='{lp}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""sp""} }', from_json(a)=Row(a='{sp}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hg""} }', from_json(a)=Row(a='{hg}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hf""} }', from_json(a)=Row(a='{hf}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fd""} }', from_json(a)=Row(a='{fd}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""en""} }', from_json(a)=Row(a='{en}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{zk}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""wp""} }', from_json(a)=Row(a='{wp}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""og""} }', from_json(a)=Row(a='{og}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lb""} }', from_json(a)=Row(a='{lb}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hj""} }', from_json(a)=Row(a='{hj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mc""} }', from_json(a)=Row(a='{mc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qg""} }', from_json(a)=Row(a='{qg}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ip""} }', from_json(a)=Row(a='{ip}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{hc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ee""} }', from_json(a)=Row(a='{ee}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""dr""} }', from_json(a)=Row(a='{dr}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{uy}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{hc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{km}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ka""} }', from_json(a)=Row(a='{ka}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fz""} }', from_json(a)=Row(a='{fz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""rw""} }', from_json(a)=Row(a='{rw}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""pv""} }', from_json(a)=Row(a='{pv}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""tu""} }', from_json(a)=Row(a='{""b"":""tu""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""zz""} }', from_json(a)=Row(a='{""b"":""zz""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""il""} }', from_json(a)=Row(a='{""b"":""il""}'))

[2024-01-31T20:47:58.663Z] @@ -544,48 +544,48 @@

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""tq""} }', from_json(a)=Row(a='{""b"":""tq""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""xh""} }', from_json(a)=Row(a='{""b"":""xh""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""mw""} }', from_json(a)=Row(a='{""b"":""mw""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""og""} }', from_json(a)=Row(a='{""b"":""og""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""oq""} }', from_json(a)=Row(a='{""b"":""oq""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""nl""} }', from_json(a)=Row(a='{""b"":""nl""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lg""} }', from_json(a)=Row(a='{""b"":""lg""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""cx""} }', from_json(a)=Row(a='{""b"":""cx""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{""b"":""uy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{""b"":""zk""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ie""} }', from_json(a)=Row(a='{""b"":""ie""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{""b"":""fr""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""go""} }', from_json(a)=Row(a='{""b"":""go""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{""b"":""bj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{""b"":""sn""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{""b"":""le""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""od""} }', from_json(a)=Row(a='{""b"":""od""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""vq""} }', from_json(a)=Row(a='{""b"":""vq""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hb""} }', from_json(a)=Row(a='{""b"":""hb""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""cx""} }', from_json(a)=Row(a='{""b"":""cx""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ri""} }', from_json(a)=Row(a='{""b"":""ri""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ux""} }', from_json(a)=Row(a='{""b"":""ux""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qy""} }', from_json(a)=Row(a='{""b"":""qy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{""b"":""uy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mo""} }', from_json(a)=Row(a='{""b"":""mo""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zn""} }', from_json(a)=Row(a='{""b"":""zn""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""vu""} }', from_json(a)=Row(a='{""b"":""vu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wp""} }', from_json(a)=Row(a='{""b"":""wp""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""rs""} }', from_json(a)=Row(a='{""b"":""rs""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hm""} }', from_json(a)=Row(a='{""b"":""hm""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ys""} }', from_json(a)=Row(a='{""b"":""ys""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""de""} }', from_json(a)=Row(a='{""b"":""de""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lh""} }', from_json(a)=Row(a='{""b"":""lh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{""b"":""ag""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fj""} }', from_json(a)=Row(a='{""b"":""fj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gu""} }', from_json(a)=Row(a='{""b"":""gu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ff""} }', from_json(a)=Row(a='{""b"":""ff""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ga""} }', from_json(a)=Row(a='{""b"":""ga""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""lo""} }', from_json(a)=Row(a='{""b"":""lo""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""yf""} }', from_json(a)=Row(a='{""b"":""yf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{""b"":""gv""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""dy""} }', from_json(a)=Row(a='{""b"":""dy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jj""} }', from_json(a)=Row(a='{""b"":""jj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zn""} }', from_json(a)=Row(a='{""b"":""zn""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uj""} }', from_json(a)=Row(a='{""b"":""uj""}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""og""} }', from_json(a)=Row(a='{og}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""oq""} }', from_json(a)=Row(a='{oq}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""nl""} }', from_json(a)=Row(a='{nl}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lg""} }', from_json(a)=Row(a='{lg}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""cx""} }', from_json(a)=Row(a='{cx}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{uy}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{zk}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ie""} }', from_json(a)=Row(a='{ie}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{fr}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""go""} }', from_json(a)=Row(a='{go}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{bj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{sn}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{le}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""od""} }', from_json(a)=Row(a='{od}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""vq""} }', from_json(a)=Row(a='{vq}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hb""} }', from_json(a)=Row(a='{hb}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""cx""} }', from_json(a)=Row(a='{cx}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ri""} }', from_json(a)=Row(a='{ri}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ux""} }', from_json(a)=Row(a='{ux}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qy""} }', from_json(a)=Row(a='{qy}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""uy""} }', from_json(a)=Row(a='{uy}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mo""} }', from_json(a)=Row(a='{mo}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zn""} }', from_json(a)=Row(a='{zn}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""vu""} }', from_json(a)=Row(a='{vu}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""wp""} }', from_json(a)=Row(a='{wp}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""rs""} }', from_json(a)=Row(a='{rs}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hm""} }', from_json(a)=Row(a='{hm}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ys""} }', from_json(a)=Row(a='{ys}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""de""} }', from_json(a)=Row(a='{de}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lh""} }', from_json(a)=Row(a='{lh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{ag}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fj""} }', from_json(a)=Row(a='{fj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""gu""} }', from_json(a)=Row(a='{gu}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ff""} }', from_json(a)=Row(a='{ff}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ga""} }', from_json(a)=Row(a='{ga}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""lo""} }', from_json(a)=Row(a='{lo}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""yf""} }', from_json(a)=Row(a='{yf}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{gv}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""dy""} }', from_json(a)=Row(a='{dy}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""jj""} }', from_json(a)=Row(a='{jj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zn""} }', from_json(a)=Row(a='{zn}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""uj""} }', from_json(a)=Row(a='{uj}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""mz""} }', from_json(a)=Row(a='{""b"":""mz""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""pt""} }', from_json(a)=Row(a='{""b"":""pt""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""fn""} }', from_json(a)=Row(a='{""b"":""fn""}'))

[2024-01-31T20:47:58.663Z] @@ -712,48 +712,48 @@

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""ov""} }', from_json(a)=Row(a='{""b"":""ov""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""nk""} }', from_json(a)=Row(a='{""b"":""nk""}'))

[2024-01-31T20:47:58.663Z]  Row(a=None, from_json(a)=None)

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""sb""} }', from_json(a)=Row(a='{""b"":""sb""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""in""} }', from_json(a)=Row(a='{""b"":""in""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""sf""} }', from_json(a)=Row(a='{""b"":""sf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""oi""} }', from_json(a)=Row(a='{""b"":""oi""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{""b"":""au""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""cq""} }', from_json(a)=Row(a='{""b"":""cq""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uz""} }', from_json(a)=Row(a='{""b"":""uz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hk""} }', from_json(a)=Row(a='{""b"":""hk""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""iz""} }', from_json(a)=Row(a='{""b"":""iz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""tj""} }', from_json(a)=Row(a='{""b"":""tj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""sx""} }', from_json(a)=Row(a='{""b"":""sx""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{""b"":""nc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{""b"":""gv""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gd""} }', from_json(a)=Row(a='{""b"":""gd""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mc""} }', from_json(a)=Row(a='{""b"":""mc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""iu""} }', from_json(a)=Row(a='{""b"":""iu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wd""} }', from_json(a)=Row(a='{""b"":""wd""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""um""} }', from_json(a)=Row(a='{""b"":""um""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jh""} }', from_json(a)=Row(a='{""b"":""jh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jk""} }', from_json(a)=Row(a='{""b"":""jk""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""nu""} }', from_json(a)=Row(a='{""b"":""nu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""am""} }', from_json(a)=Row(a='{""b"":""am""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""at""} }', from_json(a)=Row(a='{""b"":""at""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qx""} }', from_json(a)=Row(a='{""b"":""qx""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ce""} }', from_json(a)=Row(a='{""b"":""ce""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zu""} }', from_json(a)=Row(a='{""b"":""zu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mz""} }', from_json(a)=Row(a='{""b"":""mz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fn""} }', from_json(a)=Row(a='{""b"":""fn""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""iz""} }', from_json(a)=Row(a='{""b"":""iz""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""us""} }', from_json(a)=Row(a='{""b"":""us""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""zl""} }', from_json(a)=Row(a='{""b"":""zl""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ko""} }', from_json(a)=Row(a='{""b"":""ko""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{""b"":""mh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qs""} }', from_json(a)=Row(a='{""b"":""qs""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""pd""} }', from_json(a)=Row(a='{""b"":""pd""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mr""} }', from_json(a)=Row(a='{""b"":""mr""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""vl""} }', from_json(a)=Row(a='{""b"":""vl""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""kh""} }', from_json(a)=Row(a='{""b"":""kh""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mo""} }', from_json(a)=Row(a='{""b"":""mo""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qe""} }', from_json(a)=Row(a='{""b"":""qe""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""la""} }', from_json(a)=Row(a='{""b"":""la""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wl""} }', from_json(a)=Row(a='{""b"":""wl""}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""sb""} }', from_json(a)=Row(a='{sb}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""in""} }', from_json(a)=Row(a='{in}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""sf""} }', from_json(a)=Row(a='{sf}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""oi""} }', from_json(a)=Row(a='{oi}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{au}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""cq""} }', from_json(a)=Row(a='{cq}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""uz""} }', from_json(a)=Row(a='{uz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""hk""} }', from_json(a)=Row(a='{hk}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""iz""} }', from_json(a)=Row(a='{iz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""tj""} }', from_json(a)=Row(a='{tj}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""sx""} }', from_json(a)=Row(a='{sx}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{nc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{gv}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""gd""} }', from_json(a)=Row(a='{gd}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mc""} }', from_json(a)=Row(a='{mc}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""iu""} }', from_json(a)=Row(a='{iu}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""wd""} }', from_json(a)=Row(a='{wd}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""um""} }', from_json(a)=Row(a='{um}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""jh""} }', from_json(a)=Row(a='{jh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""jk""} }', from_json(a)=Row(a='{jk}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""nu""} }', from_json(a)=Row(a='{nu}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""am""} }', from_json(a)=Row(a='{am}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""at""} }', from_json(a)=Row(a='{at}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qx""} }', from_json(a)=Row(a='{qx}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ce""} }', from_json(a)=Row(a='{ce}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zu""} }', from_json(a)=Row(a='{zu}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mz""} }', from_json(a)=Row(a='{mz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""fn""} }', from_json(a)=Row(a='{fn}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""iz""} }', from_json(a)=Row(a='{iz}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""us""} }', from_json(a)=Row(a='{us}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""zl""} }', from_json(a)=Row(a='{zl}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""ko""} }', from_json(a)=Row(a='{ko}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{mh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qs""} }', from_json(a)=Row(a='{qs}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""pd""} }', from_json(a)=Row(a='{pd}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mr""} }', from_json(a)=Row(a='{mr}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""vl""} }', from_json(a)=Row(a='{vl}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""kh""} }', from_json(a)=Row(a='{kh}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mo""} }', from_json(a)=Row(a='{mo}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""qe""} }', from_json(a)=Row(a='{qe}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""la""} }', from_json(a)=Row(a='{la}'))

[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""wl""} }', from_json(a)=Row(a='{wl}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""gl""} }', from_json(a)=Row(a='{""b"":""gl""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""ht""} }', from_json(a)=Row(a='{""b"":""ht""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""sq""} }', from_json(a)=Row(a='{""b"":""sq""}'))

[2024-01-31T20:47:58.663Z] @@ -922,48 +922,48 @@

[2024-01-31T20:47:58.663Z]  Row(a=None, from_json(a)=None)

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""ai""} }', from_json(a)=Row(a='{""b"":""ai""}'))

[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""ak""} }', from_json(a)=Row(a='{""b"":""ak""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jf""} }', from_json(a)=Row(a='{""b"":""jf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jy""} }', from_json(a)=Row(a='{""b"":""jy""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wb""} }', from_json(a)=Row(a='{""b"":""wb""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mf""} }', from_json(a)=Row(a='{""b"":""mf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{""b"":""nc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ie""} }', from_json(a)=Row(a='{""b"":""ie""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ka""} }', from_json(a)=Row(a='{""b"":""ka""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""jc""} }', from_json(a)=Row(a='{""b"":""jc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{""b"":""ag""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""xt""} }', from_json(a)=Row(a='{""b"":""xt""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gi""} }', from_json(a)=Row(a='{""b"":""gi""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ia""} }', from_json(a)=Row(a='{""b"":""ia""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ac""} }', from_json(a)=Row(a='{""b"":""ac""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""rf""} }', from_json(a)=Row(a='{""b"":""rf""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""uq""} }', from_json(a)=Row(a='{""b"":""uq""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{""b"":""nc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""yp""} }', from_json(a)=Row(a='{""b"":""yp""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""xm""} }', from_json(a)=Row(a='{""b"":""xm""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gu""} }', from_json(a)=Row(a='{""b"":""gu""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""do""} }', from_json(a)=Row(a='{""b"":""do""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ob""} }', from_json(a)=Row(a='{""b"":""ob""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""bc""} }', from_json(a)=Row(a='{""b"":""bc""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""hi""} }', from_json(a)=Row(a='{""b"":""hi""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""rb""} }', from_json(a)=Row(a='{""b"":""rb""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""yo""} }', from_json(a)=Row(a='{""b"":""yo""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""ay""} }', from_json(a)=Row(a='{""b"":""ay""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""gs""} }', from_json(a)=Row(a='{""b"":""gs""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""qa""} }', from_json(a)=Row(a='{""b"":""qa""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""iv""} }', from_json(a)=Row(a='{""b"":""iv""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""fj""} }', from_json(a)=Row(a='{""b"":""fj""}'))

[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""wq""} }', from_json(a)=Row(a='{""b"":""wq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""an""} }', from_json(a)=Row(a='{""b"":""an""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{""b"":""sl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bp""} }', from_json(a)=Row(a='{""b"":""bp""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""qp""} }', from_json(a)=Row(a='{""b"":""qp""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{""b"":""le""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vk""} }', from_json(a)=Row(a='{""b"":""vk""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{""b"":""sn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""rn""} }', from_json(a)=Row(a='{""b"":""rn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""iw""} }', from_json(a)=Row(a='{""b"":""iw""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""qh""} }', from_json(a)=Row(a='{""b"":""qh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bp""} }', from_json(a)=Row(a='{""b"":""bp""}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jf""} }', from_json(a)=Row(a='{jf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jy""} }', from_json(a)=Row(a='{jy}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""wb""} }', from_json(a)=Row(a='{wb}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mf""} }', from_json(a)=Row(a='{mf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{nc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ie""} }', from_json(a)=Row(a='{ie}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ka""} }', from_json(a)=Row(a='{ka}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jc""} }', from_json(a)=Row(a='{jc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ag""} }', from_json(a)=Row(a='{ag}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xt""} }', from_json(a)=Row(a='{xt}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gi""} }', from_json(a)=Row(a='{gi}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ia""} }', from_json(a)=Row(a='{ia}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ac""} }', from_json(a)=Row(a='{ac}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rf""} }', from_json(a)=Row(a='{rf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""uq""} }', from_json(a)=Row(a='{uq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nc""} }', from_json(a)=Row(a='{nc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""yp""} }', from_json(a)=Row(a='{yp}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xm""} }', from_json(a)=Row(a='{xm}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gu""} }', from_json(a)=Row(a='{gu}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""do""} }', from_json(a)=Row(a='{do}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ob""} }', from_json(a)=Row(a='{ob}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bc""} }', from_json(a)=Row(a='{bc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hi""} }', from_json(a)=Row(a='{hi}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rb""} }', from_json(a)=Row(a='{rb}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""yo""} }', from_json(a)=Row(a='{yo}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ay""} }', from_json(a)=Row(a='{ay}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gs""} }', from_json(a)=Row(a='{gs}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""qa""} }', from_json(a)=Row(a='{qa}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""iv""} }', from_json(a)=Row(a='{iv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fj""} }', from_json(a)=Row(a='{fj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""wq""} }', from_json(a)=Row(a='{wq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""an""} }', from_json(a)=Row(a='{an}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{sl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bp""} }', from_json(a)=Row(a='{bp}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""qp""} }', from_json(a)=Row(a='{qp}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{le}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vk""} }', from_json(a)=Row(a='{vk}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{sn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rn""} }', from_json(a)=Row(a='{rn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""iw""} }', from_json(a)=Row(a='{iw}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""qh""} }', from_json(a)=Row(a='{qh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bp""} }', from_json(a)=Row(a='{bp}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""kt""} }', from_json(a)=Row(a='{""b"":""kt""}'))

[2024-01-31T20:47:58.664Z]  Row(a=None, from_json(a)=None)

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""lo""} }', from_json(a)=Row(a='{""b"":""lo""}'))

[2024-01-31T20:47:58.664Z] @@ -1090,48 +1090,48 @@

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ja""} }', from_json(a)=Row(a='{""b"":""ja""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ak""} }', from_json(a)=Row(a='{""b"":""ak""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""sr""} }', from_json(a)=Row(a='{""b"":""sr""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""er""} }', from_json(a)=Row(a='{""b"":""er""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""po""} }', from_json(a)=Row(a='{""b"":""po""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vb""} }', from_json(a)=Row(a='{""b"":""vb""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hs""} }', from_json(a)=Row(a='{""b"":""hs""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vf""} }', from_json(a)=Row(a='{""b"":""vf""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ap""} }', from_json(a)=Row(a='{""b"":""ap""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""df""} }', from_json(a)=Row(a='{""b"":""df""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sz""} }', from_json(a)=Row(a='{""b"":""sz""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vn""} }', from_json(a)=Row(a='{""b"":""vn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""xh""} }', from_json(a)=Row(a='{""b"":""xh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dx""} }', from_json(a)=Row(a='{""b"":""dx""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{""b"":""au""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mb""} }', from_json(a)=Row(a='{""b"":""mb""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dx""} }', from_json(a)=Row(a='{""b"":""dx""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ei""} }', from_json(a)=Row(a='{""b"":""ei""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mv""} }', from_json(a)=Row(a='{""b"":""mv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""xv""} }', from_json(a)=Row(a='{""b"":""xv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dq""} }', from_json(a)=Row(a='{""b"":""dq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ba""} }', from_json(a)=Row(a='{""b"":""ba""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{""b"":""bj""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""os""} }', from_json(a)=Row(a='{""b"":""os""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""pf""} }', from_json(a)=Row(a='{""b"":""pf""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""rn""} }', from_json(a)=Row(a='{""b"":""rn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sh""} }', from_json(a)=Row(a='{""b"":""sh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""kn""} }', from_json(a)=Row(a='{""b"":""kn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ui""} }', from_json(a)=Row(a='{""b"":""ui""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mu""} }', from_json(a)=Row(a='{""b"":""mu""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""gi""} }', from_json(a)=Row(a='{""b"":""gi""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{""b"":""jg""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""oq""} }', from_json(a)=Row(a='{""b"":""oq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vx""} }', from_json(a)=Row(a='{""b"":""vx""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sa""} }', from_json(a)=Row(a='{""b"":""sa""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hv""} }', from_json(a)=Row(a='{""b"":""hv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""nl""} }', from_json(a)=Row(a='{""b"":""nl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""se""} }', from_json(a)=Row(a='{""b"":""se""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""uc""} }', from_json(a)=Row(a='{""b"":""uc""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ut""} }', from_json(a)=Row(a='{""b"":""ut""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bk""} }', from_json(a)=Row(a='{""b"":""bk""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hv""} }', from_json(a)=Row(a='{""b"":""hv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vx""} }', from_json(a)=Row(a='{""b"":""vx""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mn""} }', from_json(a)=Row(a='{""b"":""mn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mj""} }', from_json(a)=Row(a='{""b"":""mj""}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""er""} }', from_json(a)=Row(a='{er}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""po""} }', from_json(a)=Row(a='{po}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vb""} }', from_json(a)=Row(a='{vb}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hs""} }', from_json(a)=Row(a='{hs}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vf""} }', from_json(a)=Row(a='{vf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ap""} }', from_json(a)=Row(a='{ap}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""df""} }', from_json(a)=Row(a='{df}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sz""} }', from_json(a)=Row(a='{sz}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vn""} }', from_json(a)=Row(a='{vn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xh""} }', from_json(a)=Row(a='{xh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dx""} }', from_json(a)=Row(a='{dx}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{au}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mb""} }', from_json(a)=Row(a='{mb}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dx""} }', from_json(a)=Row(a='{dx}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ei""} }', from_json(a)=Row(a='{ei}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mv""} }', from_json(a)=Row(a='{mv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xv""} }', from_json(a)=Row(a='{xv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dq""} }', from_json(a)=Row(a='{dq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ba""} }', from_json(a)=Row(a='{ba}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{bj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""os""} }', from_json(a)=Row(a='{os}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""pf""} }', from_json(a)=Row(a='{pf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rn""} }', from_json(a)=Row(a='{rn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sh""} }', from_json(a)=Row(a='{sh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""kn""} }', from_json(a)=Row(a='{kn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ui""} }', from_json(a)=Row(a='{ui}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mu""} }', from_json(a)=Row(a='{mu}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gi""} }', from_json(a)=Row(a='{gi}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{jg}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""oq""} }', from_json(a)=Row(a='{oq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vx""} }', from_json(a)=Row(a='{vx}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sa""} }', from_json(a)=Row(a='{sa}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hv""} }', from_json(a)=Row(a='{hv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nl""} }', from_json(a)=Row(a='{nl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""se""} }', from_json(a)=Row(a='{se}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""uc""} }', from_json(a)=Row(a='{uc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ut""} }', from_json(a)=Row(a='{ut}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bk""} }', from_json(a)=Row(a='{bk}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hv""} }', from_json(a)=Row(a='{hv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vx""} }', from_json(a)=Row(a='{vx}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mn""} }', from_json(a)=Row(a='{mn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mj""} }', from_json(a)=Row(a='{mj}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ki""} }', from_json(a)=Row(a='{""b"":""ki""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""cd""} }', from_json(a)=Row(a='{""b"":""cd""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""qs""} }', from_json(a)=Row(a='{""b"":""qs""}'))

[2024-01-31T20:47:58.664Z] @@ -1594,48 +1594,48 @@

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{""b"":""km""}'))

[2024-01-31T20:47:58.664Z]  Row(a=None, from_json(a)=None)

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""xt""} }', from_json(a)=Row(a='{""b"":""xt""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""wq""} }', from_json(a)=Row(a='{""b"":""wq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fa""} }', from_json(a)=Row(a='{""b"":""fa""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vh""} }', from_json(a)=Row(a='{""b"":""vh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bs""} }', from_json(a)=Row(a='{""b"":""bs""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""nr""} }', from_json(a)=Row(a='{""b"":""nr""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""my""} }', from_json(a)=Row(a='{""b"":""my""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mf""} }', from_json(a)=Row(a='{""b"":""mf""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ai""} }', from_json(a)=Row(a='{""b"":""ai""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""is""} }', from_json(a)=Row(a='{""b"":""is""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ow""} }', from_json(a)=Row(a='{""b"":""ow""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ih""} }', from_json(a)=Row(a='{""b"":""ih""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ts""} }', from_json(a)=Row(a='{""b"":""ts""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ns""} }', from_json(a)=Row(a='{""b"":""ns""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""kj""} }', from_json(a)=Row(a='{""b"":""kj""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hb""} }', from_json(a)=Row(a='{""b"":""hb""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""wz""} }', from_json(a)=Row(a='{""b"":""wz""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""yz""} }', from_json(a)=Row(a='{""b"":""yz""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""tf""} }', from_json(a)=Row(a='{""b"":""tf""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""rj""} }', from_json(a)=Row(a='{""b"":""rj""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""qv""} }', from_json(a)=Row(a='{""b"":""qv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{""b"":""fr""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{""b"":""le""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ex""} }', from_json(a)=Row(a='{""b"":""ex""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{""b"":""km""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""nq""} }', from_json(a)=Row(a='{""b"":""nq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ki""} }', from_json(a)=Row(a='{""b"":""ki""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ky""} }', from_json(a)=Row(a='{""b"":""ky""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ui""} }', from_json(a)=Row(a='{""b"":""ui""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""oo""} }', from_json(a)=Row(a='{""b"":""oo""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""kd""} }', from_json(a)=Row(a='{""b"":""kd""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ud""} }', from_json(a)=Row(a='{""b"":""ud""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""vg""} }', from_json(a)=Row(a='{""b"":""vg""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""im""} }', from_json(a)=Row(a='{""b"":""im""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dh""} }', from_json(a)=Row(a='{""b"":""dh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""xe""} }', from_json(a)=Row(a='{""b"":""xe""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ky""} }', from_json(a)=Row(a='{""b"":""ky""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""nm""} }', from_json(a)=Row(a='{""b"":""nm""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{""b"":""km""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fh""} }', from_json(a)=Row(a='{""b"":""fh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ca""} }', from_json(a)=Row(a='{""b"":""ca""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hu""} }', from_json(a)=Row(a='{""b"":""hu""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ql""} }', from_json(a)=Row(a='{""b"":""ql""}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""wq""} }', from_json(a)=Row(a='{wq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fa""} }', from_json(a)=Row(a='{fa}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vh""} }', from_json(a)=Row(a='{vh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bs""} }', from_json(a)=Row(a='{bs}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nr""} }', from_json(a)=Row(a='{nr}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""my""} }', from_json(a)=Row(a='{my}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mf""} }', from_json(a)=Row(a='{mf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ai""} }', from_json(a)=Row(a='{ai}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""is""} }', from_json(a)=Row(a='{is}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ow""} }', from_json(a)=Row(a='{ow}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ih""} }', from_json(a)=Row(a='{ih}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ts""} }', from_json(a)=Row(a='{ts}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ns""} }', from_json(a)=Row(a='{ns}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""kj""} }', from_json(a)=Row(a='{kj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hb""} }', from_json(a)=Row(a='{hb}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""wz""} }', from_json(a)=Row(a='{wz}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""yz""} }', from_json(a)=Row(a='{yz}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""tf""} }', from_json(a)=Row(a='{tf}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rj""} }', from_json(a)=Row(a='{rj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""qv""} }', from_json(a)=Row(a='{qv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fr""} }', from_json(a)=Row(a='{fr}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""le""} }', from_json(a)=Row(a='{le}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ex""} }', from_json(a)=Row(a='{ex}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{km}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nq""} }', from_json(a)=Row(a='{nq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ki""} }', from_json(a)=Row(a='{ki}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ky""} }', from_json(a)=Row(a='{ky}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ui""} }', from_json(a)=Row(a='{ui}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""oo""} }', from_json(a)=Row(a='{oo}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""kd""} }', from_json(a)=Row(a='{kd}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ud""} }', from_json(a)=Row(a='{ud}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""vg""} }', from_json(a)=Row(a='{vg}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""im""} }', from_json(a)=Row(a='{im}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dh""} }', from_json(a)=Row(a='{dh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xe""} }', from_json(a)=Row(a='{xe}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ky""} }', from_json(a)=Row(a='{ky}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nm""} }', from_json(a)=Row(a='{nm}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""km""} }', from_json(a)=Row(a='{km}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fh""} }', from_json(a)=Row(a='{fh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ca""} }', from_json(a)=Row(a='{ca}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hu""} }', from_json(a)=Row(a='{hu}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ql""} }', from_json(a)=Row(a='{ql}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""vr""} }', from_json(a)=Row(a='{""b"":""vr""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ox""} }', from_json(a)=Row(a='{""b"":""ox""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ro""} }', from_json(a)=Row(a='{""b"":""ro""}'))

[2024-01-31T20:47:58.664Z] @@ -1804,48 +1804,48 @@

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""mg""} }', from_json(a)=Row(a='{""b"":""mg""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ul""} }', from_json(a)=Row(a='{""b"":""ul""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""pp""} }', from_json(a)=Row(a='{""b"":""pp""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""rv""} }', from_json(a)=Row(a='{""b"":""rv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ua""} }', from_json(a)=Row(a='{""b"":""ua""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fg""} }', from_json(a)=Row(a='{""b"":""fg""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""xs""} }', from_json(a)=Row(a='{""b"":""xs""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bm""} }', from_json(a)=Row(a='{""b"":""bm""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{""b"":""hc""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""iy""} }', from_json(a)=Row(a='{""b"":""iy""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ly""} }', from_json(a)=Row(a='{""b"":""ly""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{""b"":""sl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""tn""} }', from_json(a)=Row(a='{""b"":""tn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{""b"":""bj""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fu""} }', from_json(a)=Row(a='{""b"":""fu""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{""b"":""sl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{""b"":""gv""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""pk""} }', from_json(a)=Row(a='{""b"":""pk""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{""b"":""au""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""mp""} }', from_json(a)=Row(a='{""b"":""mp""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sc""} }', from_json(a)=Row(a='{""b"":""sc""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ob""} }', from_json(a)=Row(a='{""b"":""ob""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""ys""} }', from_json(a)=Row(a='{""b"":""ys""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""yt""} }', from_json(a)=Row(a='{""b"":""yt""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""aa""} }', from_json(a)=Row(a='{""b"":""aa""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{""b"":""jg""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dl""} }', from_json(a)=Row(a='{""b"":""dl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""qa""} }', from_json(a)=Row(a='{""b"":""qa""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""zr""} }', from_json(a)=Row(a='{""b"":""zr""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""gp""} }', from_json(a)=Row(a='{""b"":""gp""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""xj""} }', from_json(a)=Row(a='{""b"":""xj""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{""b"":""sn""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{""b"":""zk""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""sc""} }', from_json(a)=Row(a='{""b"":""sc""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""fu""} }', from_json(a)=Row(a='{""b"":""fu""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hg""} }', from_json(a)=Row(a='{""b"":""hg""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""un""} }', from_json(a)=Row(a='{""b"":""un""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dw""} }', from_json(a)=Row(a='{""b"":""dw""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""hq""} }', from_json(a)=Row(a='{""b"":""hq""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""yl""} }', from_json(a)=Row(a='{""b"":""yl""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""lz""} }', from_json(a)=Row(a='{""b"":""lz""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""oe""} }', from_json(a)=Row(a='{""b"":""oe""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""nw""} }', from_json(a)=Row(a='{""b"":""nw""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""dh""} }', from_json(a)=Row(a='{""b"":""dh""}'))

[2024-01-31T20:47:58.664Z] -Row(a='{""a"": {""b"":""jp""} }', from_json(a)=Row(a='{""b"":""jp""}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""rv""} }', from_json(a)=Row(a='{rv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ua""} }', from_json(a)=Row(a='{ua}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fg""} }', from_json(a)=Row(a='{fg}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xs""} }', from_json(a)=Row(a='{xs}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bm""} }', from_json(a)=Row(a='{bm}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hc""} }', from_json(a)=Row(a='{hc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""iy""} }', from_json(a)=Row(a='{iy}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ly""} }', from_json(a)=Row(a='{ly}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{sl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""tn""} }', from_json(a)=Row(a='{tn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""bj""} }', from_json(a)=Row(a='{bj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fu""} }', from_json(a)=Row(a='{fu}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sl""} }', from_json(a)=Row(a='{sl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gv""} }', from_json(a)=Row(a='{gv}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""pk""} }', from_json(a)=Row(a='{pk}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""au""} }', from_json(a)=Row(a='{au}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""mp""} }', from_json(a)=Row(a='{mp}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sc""} }', from_json(a)=Row(a='{sc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ob""} }', from_json(a)=Row(a='{ob}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""ys""} }', from_json(a)=Row(a='{ys}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""yt""} }', from_json(a)=Row(a='{yt}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""aa""} }', from_json(a)=Row(a='{aa}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jg""} }', from_json(a)=Row(a='{jg}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dl""} }', from_json(a)=Row(a='{dl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""qa""} }', from_json(a)=Row(a='{qa}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""zr""} }', from_json(a)=Row(a='{zr}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""gp""} }', from_json(a)=Row(a='{gp}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""xj""} }', from_json(a)=Row(a='{xj}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sn""} }', from_json(a)=Row(a='{sn}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""zk""} }', from_json(a)=Row(a='{zk}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""sc""} }', from_json(a)=Row(a='{sc}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""fu""} }', from_json(a)=Row(a='{fu}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hg""} }', from_json(a)=Row(a='{hg}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""un""} }', from_json(a)=Row(a='{un}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dw""} }', from_json(a)=Row(a='{dw}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""hq""} }', from_json(a)=Row(a='{hq}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""yl""} }', from_json(a)=Row(a='{yl}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""lz""} }', from_json(a)=Row(a='{lz}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""oe""} }', from_json(a)=Row(a='{oe}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""nw""} }', from_json(a)=Row(a='{nw}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""dh""} }', from_json(a)=Row(a='{dh}'))

[2024-01-31T20:47:58.664Z] +Row(a='{""a"": {""b"":""jp""} }', from_json(a)=Row(a='{jp}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""dt""} }', from_json(a)=Row(a='{""b"":""dt""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""ql""} }', from_json(a)=Row(a='{""b"":""ql""}'))

[2024-01-31T20:47:58.664Z]  Row(a='{""a"": {""b"":""gw""} }', from_json(a)=Row(a='{""b"":""gw""}'))

```
</details>",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1919964515/reactions,0,0,0,0,0,0,0,0,0,10351
1165,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920128622,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1920128622,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1920128622,IC_kwDOD7z77c5yctZu,2024-01-31T22:56:20Z,2024-01-31T22:56:20Z,CONTRIBUTOR,"The output shows that an input of `{""a"": {""b"":""md""} }` produces the same results between CPU and GPU:

```
[2024-01-31T20:47:58.663Z]  Row(a='{""a"": {""b"":""md""} }', from_json(a)=Row(a='{""b"":""md""}'))
```

But an almost identical input of `{""a"": {""b"":""mh""} }` produces different results between CPU and GPU:

```
[2024-01-31T20:47:58.663Z] -Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{""b"":""mh""}'))
[2024-01-31T20:47:58.663Z] +Row(a='{""a"": {""b"":""mh""} }', from_json(a)=Row(a='{mh}'))
```

I have been unable to reproduce this so far with Spark 3.3.0, even using the same datagen seed.

However, a manual test does show differences between CPU and GPU, but does not match the results from the failed CI run exactly.

```
scala> val df = Seq(""""""{""a"": {""b"":""md""} }"""""", """"""{""a"": {""b"":""mh""} }"""""").toDF(""json"").repartition(2)
scala> spark.conf.set(""spark.rapids.sql.expression.JsonToStructs"", true)
scala> spark.conf.set(""spark.rapids.sql.json.read.mixedTypesAsString.enabled"", true)
scala> import org.apache.spark.sql.types._
scala> val schema = StructType(Seq(StructField(""a"", DataTypes.StringType, true)))
scala> df.select(col(""json""), from_json(col(""json""), schema)).show
```

## GPU output

```
+------------------+---------------+
|              json|from_json(json)|
+------------------+---------------+
|{""a"": {""b"":""md""} }|         {{md}}|
|{""a"": {""b"":""mh""} }|         {{mh}}|
+------------------+---------------+
```

## CPU Output

```
+------------------+---------------+
|              json|from_json(json)|
+------------------+---------------+
|{""a"": {""b"":""md""} }|   {{""b"":""md""}}|
|{""a"": {""b"":""mh""} }|   {{""b"":""mh""}}|
+------------------+---------------+
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920128622/reactions,0,0,0,0,0,0,0,0,0,10351
1166,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920133876,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1920133876,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1920133876,IC_kwDOD7z77c5ycur0,2024-01-31T22:58:47Z,2024-01-31T22:58:47Z,MEMBER,"Note that this failure was from a distributed cluster setup, so the nature of the failure may have something to do with how the input data is partitioned across tasks.  That particular distribution is probably not replicated in the default local run environment.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920133876/reactions,1,1,0,0,0,0,0,0,0,10351
1167,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920137137,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1920137137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1920137137,IC_kwDOD7z77c5ycvex,2024-01-31T23:01:44Z,2024-01-31T23:01:44Z,CONTRIBUTOR,"Also, my manual test is using `show` ... if I run `collect` then I do see the same results. I think the `show` issue is already known under issue https://github.com/NVIDIA/spark-rapids/issues/8558",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920137137/reactions,0,0,0,0,0,0,0,0,0,10351
1168,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920140867,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1920140867,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1920140867,IC_kwDOD7z77c5ycwZD,2024-01-31T23:04:50Z,2024-01-31T23:04:50Z,CONTRIBUTOR,"> Note that this failure was from a distributed cluster setup, so the nature of the failure may have something to do with how the input data is partitioned across tasks. That particular distribution is probably not replicated in the default local run environment.

So if some partitions contain mixed types and some don't ... I will try and repro that in an integration test.

I will create a PR to xfail this test while I investigate.

",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1920140867/reactions,0,0,0,0,0,0,0,0,0,10351
1169,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1927680202,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1927680202,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,1927680202,IC_kwDOD7z77c5y5hDK,2024-02-05T18:11:59Z,2024-02-05T18:11:59Z,COLLABORATOR,Depends on https://github.com/rapidsai/cudf/issues/14830,,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1927680202/reactions,0,0,0,0,0,0,0,0,0,10351
1170,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2048495615,https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-2048495615,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10351,2048495615,IC_kwDOD7z77c56GY__,2024-04-10T21:51:19Z,2024-04-10T21:51:19Z,COLLABORATOR,"The test code from the comment in https://github.com/NVIDIA/spark-rapids/issues/10351#issuecomment-1920128622 now works, but the test itself still fails because it needs support for LISTs not just STRUCTs.

https://github.com/rapidsai/cudf/issues/15278

is the issue we want/need fixed for this to start passing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2048495615/reactions,0,0,0,0,0,0,0,0,0,10351
1171,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1923269108,https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1923269108,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366,1923269108,IC_kwDOD7z77c5yosH0,2024-02-02T07:55:23Z,2024-02-02T08:06:51Z,COLLABORATOR,"Spark supports [two algorithms generating the bucket id](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala#L123). We'd better support both of them.
```
      if (options.getOrElse(BucketingUtils.optionForHiveCompatibleBucketWrite, ""false"") ==
        ""true"") {
        // Hive bucketed table: use `HiveHash` and bitwise-and as bucket id expression.
        // Without the extra bitwise-and operation, we can get wrong bucket id when hash value of
        // columns is negative. See Hive implementation in
        // `org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#getBucketNumber()`.
        val hashId = BitwiseAnd(HiveHash(bucketColumns), Literal(Int.MaxValue))
        val bucketIdExpression = Pmod(hashId, Literal(spec.numBuckets))

        // The bucket file name prefix is following Hive, Presto and Trino conversion, so this
        // makes sure Hive bucketed table written by Spark, can be read by other SQL engines.
        //
        // Hive: `org.apache.hadoop.hive.ql.exec.Utilities#getBucketIdFromFile()`.
        // Trino: `io.trino.plugin.hive.BackgroundHiveSplitLoader#BUCKET_PATTERNS`.
        val fileNamePrefix = (bucketId: Int) => f""$bucketId%05d_0_""
        WriterBucketSpec(bucketIdExpression, fileNamePrefix)
      } else {
        // Spark bucketed table: use `HashPartitioning.partitionIdExpression` as bucket id
        // expression, so that we can guarantee the data distribution is same between shuffle and
        // bucketed data source, which enables us to only shuffle one side when join a bucketed
        // table and a normal one.
        val bucketIdExpression = HashPartitioning(bucketColumns, spec.numBuckets)
          .partitionIdExpression
        WriterBucketSpec(bucketIdExpression, (_: Int) => """")
      }
```",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1923269108/reactions,0,0,0,0,0,0,0,0,0,10366
1172,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924063095,https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1924063095,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366,1924063095,IC_kwDOD7z77c5yrt93,2024-02-02T15:05:20Z,2024-02-02T15:05:20Z,COLLABORATOR,"Hash partitioning is simple because we already support it. To support the other one we would need to implement the HiveHash expression. That would be a new kernel.

https://github.com/apache/spark/blob/25d96f7bacb43a7d5a835454ecc075e40d4f3c93/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/hash.scala#L686-L1035

Is the Spark code for this. The core hash function itself is very simple.

ints are unchanged
longs are computed as `(int) ((input >>> 32) ^ input);`
a byte array uses
```
    int result = 0;
    for (int i = 0; i < lengthInBytes; i++) {
      result = (result * 31) + (int) Platform.getByte(base, offset + i);
    }
    return result;
```
for timestamps it is
```
    val timestampInSeconds = MICROSECONDS.toSeconds(timestamp)
    val nanoSecondsPortion = (timestamp % MICROS_PER_SECOND) * NANOS_PER_MICROS

    var result = timestampInSeconds
    result <<= 30 // the nanosecond part fits in 30 bits
    result |= nanoSecondsPortion
    ((result >>> 32) ^ result).toInt
```

But for decimal numbers they convert it to a BigDecimal, normalize the value, and then call `hashCode` on it. I think in the short term we will not be able to support decimal values as the BigDecimal implementation is GPL licensed so we cannot copy it. 

The hard parts are with how some data is normalized before it is hashed.

We are also going to have problems with Maps. The way maps are hashed it depends on the order that they are stored. Maps are not ordered so we are not likely to get the same result and it might be best to just not say that we support them.

We also don't really support CalendarInterval very well, so we probably want to say at the beginning that don't support it.

Or we could just fall back to the CPU if it is a hive compatible bucketing and file a follow on for the rest.


",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924063095/reactions,1,1,0,0,0,0,0,0,0,10366
1173,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924068613,https://github.com/NVIDIA/spark-rapids/issues/10366#issuecomment-1924068613,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10366,1924068613,IC_kwDOD7z77c5yrvUF,2024-02-02T15:08:30Z,2024-02-02T15:08:30Z,MEMBER,"Note that the normal spark hash bucketing feature request is already tracked by #22.  We should either morph this into ""I want Hive-style bucketing"" and leave #22 to track normal bucketing, or make this a bucket write epic with the Hive and Spark hashing implementations as subtasks.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1924068613/reactions,0,0,0,0,0,0,0,0,0,10366
1174,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1934769753,https://github.com/NVIDIA/spark-rapids/issues/10368#issuecomment-1934769753,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10368,1934769753,IC_kwDOD7z77c5zUj5Z,2024-02-08T19:08:57Z,2024-02-08T19:08:57Z,COLLABORATOR,"Depends on https://github.com/NVIDIA/spark-rapids/issues/10374.
",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1934769753/reactions,0,0,0,0,0,0,0,0,0,10368
1175,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1930813229,https://github.com/NVIDIA/spark-rapids/issues/10388#issuecomment-1930813229,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10388,1930813229,IC_kwDOD7z77c5zFd8t,2024-02-06T21:53:13Z,2024-02-06T21:53:13Z,COLLABORATOR,Next step should be to hard-code the seed since float values will not be consistent.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1930813229/reactions,0,0,0,0,0,0,0,0,0,10388
1176,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1930924649,https://github.com/NVIDIA/spark-rapids/issues/10388#issuecomment-1930924649,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10388,1930924649,IC_kwDOD7z77c5zF5Jp,2024-02-06T23:11:30Z,2024-02-07T01:38:11Z,COLLABORATOR,"This is interesting.  Over the weekend, the same tests failed, alongside `test_range_running_window_float_decimal_sum_runs_batched`. The window function test seems to be passing now.

Edit: Link to the window-test failure: https://github.com/NVIDIA/spark-rapids/issues/10378",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1930924649/reactions,0,0,0,0,0,0,0,0,0,10388
1177,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942675586,https://github.com/NVIDIA/spark-rapids/issues/10405#issuecomment-1942675586,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10405,1942675586,IC_kwDOD7z77c5zyuCC,2024-02-13T21:45:50Z,2024-02-13T21:45:50Z,COLLABORATOR,"1. Turn off `json_tuple` support by default given potential data quality issues.
2. Consider re-implementing `json_tuple` using `from_json` and then re-enable by default.",,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942675586/reactions,0,0,0,0,0,0,0,0,0,10405
1178,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942769722,https://github.com/NVIDIA/spark-rapids/issues/10405#issuecomment-1942769722,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10405,1942769722,IC_kwDOD7z77c5zzFA6,2024-02-13T22:52:34Z,2024-02-13T22:52:34Z,COLLABORATOR,"https://github.com/NVIDIA/spark-rapids/pull/10420 disables json_tuple by default.

Ideally we should look at pulling in Spark unit tests for json_tuple to make sure that we can match what they are doing. At a minimum we need to look at adding tests around single quotes, white space normalization, number normalization, mixed types, etc. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942769722/reactions,0,0,0,0,0,0,0,0,0,10405
1179,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942761532,https://github.com/NVIDIA/spark-rapids/issues/10417#issuecomment-1942761532,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10417,1942761532,IC_kwDOD7z77c5zzDA8,2024-02-13T22:44:32Z,2024-02-13T22:44:32Z,MEMBER,Note that this also should remove the repartition by partition key for partitioned tables when writing a MERGE because we're going to turn around and repartition for the optimized write anyway.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1942761532/reactions,0,0,0,0,0,0,0,0,0,10417
1180,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2033197534,https://github.com/NVIDIA/spark-rapids/issues/10417#issuecomment-2033197534,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10417,2033197534,IC_kwDOD7z77c55MCHe,2024-04-02T22:18:57Z,2024-04-02T22:18:57Z,MEMBER,"Note that for MERGE the user can specify `spark.databricks.delta.merge.repartitionBeforeWrite.enabled=false` to avoid repartitioning by the partition key when doing a merge into a few number of partitions to avoid sending all the write data to just a small number of tasks.  Not exactly semantically equivalent to optimize write and auto compact, but it can avoid the terrible write performance for that partitioned write case.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2033197534/reactions,0,0,0,0,0,0,0,0,0,10417
1181,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2063099273,https://github.com/NVIDIA/spark-rapids/issues/10417#issuecomment-2063099273,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10417,2063099273,IC_kwDOD7z77c56-GWJ,2024-04-18T06:32:47Z,2024-04-18T06:32:47Z,COLLABORATOR,"Hi, @jlowe delta oss have added support for optimized write: https://github.com/delta-io/delta/pull/2145 I think we can always enable optimized write after porting this?",,liurenjie1024,2771941,MDQ6VXNlcjI3NzE5NDE=,https://avatars.githubusercontent.com/u/2771941?v=4,,https://api.github.com/users/liurenjie1024,https://github.com/liurenjie1024,https://api.github.com/users/liurenjie1024/followers,https://api.github.com/users/liurenjie1024/following{/other_user},https://api.github.com/users/liurenjie1024/gists{/gist_id},https://api.github.com/users/liurenjie1024/starred{/owner}{/repo},https://api.github.com/users/liurenjie1024/subscriptions,https://api.github.com/users/liurenjie1024/orgs,https://api.github.com/users/liurenjie1024/repos,https://api.github.com/users/liurenjie1024/events{/privacy},https://api.github.com/users/liurenjie1024/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2063099273/reactions,0,0,0,0,0,0,0,0,0,10417
1182,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2063883831,https://github.com/NVIDIA/spark-rapids/issues/10417#issuecomment-2063883831,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10417,2063883831,IC_kwDOD7z77c57BF43,2024-04-18T13:34:37Z,2024-04-18T13:35:02Z,MEMBER,"This is a Databricks-specific behavior per the doc linked above, not a behavior in OSS Delta Lake, at least for the versions of OSS Delta Lake that we support.  There's already a separate issue for tracking the OSS versions of optimized write and auto compact, see #10397 and #10398, respectively, but I do not see it as being relevant for this issue.  We already support optimized write and auto compact on Databricks.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2063883831/reactions,1,1,0,0,0,0,0,0,0,10417
1183,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2066734357,https://github.com/NVIDIA/spark-rapids/issues/10417#issuecomment-2066734357,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10417,2066734357,IC_kwDOD7z77c57L90V,2024-04-19T14:44:44Z,2024-04-19T14:44:44Z,COLLABORATOR,I'll take this.,,liurenjie1024,2771941,MDQ6VXNlcjI3NzE5NDE=,https://avatars.githubusercontent.com/u/2771941?v=4,,https://api.github.com/users/liurenjie1024,https://github.com/liurenjie1024,https://api.github.com/users/liurenjie1024/followers,https://api.github.com/users/liurenjie1024/following{/other_user},https://api.github.com/users/liurenjie1024/gists{/gist_id},https://api.github.com/users/liurenjie1024/starred{/owner}{/repo},https://api.github.com/users/liurenjie1024/subscriptions,https://api.github.com/users/liurenjie1024/orgs,https://api.github.com/users/liurenjie1024/repos,https://api.github.com/users/liurenjie1024/events{/privacy},https://api.github.com/users/liurenjie1024/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2066734357/reactions,0,0,0,0,0,0,0,0,0,10417
1184,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960291792,https://github.com/NVIDIA/spark-rapids/issues/10458#issuecomment-1960291792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10458,1960291792,IC_kwDOD7z77c50163Q,2024-02-22T20:50:30Z,2024-02-22T20:55:55Z,COLLABORATOR,"Another odd example of this is +INF and -INF.  Even if allowNonNumericNumbers is disabled +INF and -INF are valid floats and are normalized to ""Infinity"" and ""-Infinity"" respectively. And the quotes come out in the string itself.  This is also true for unquoted Infinity, -Infinity, and NaN",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960291792/reactions,0,0,0,0,0,0,0,0,0,10458
1185,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960520250,https://github.com/NVIDIA/spark-rapids/issues/10477#issuecomment-1960520250,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10477,1960520250,IC_kwDOD7z77c502yo6,2024-02-22T23:42:58Z,2024-02-22T23:42:58Z,CONTRIBUTOR,"The same issue exists for `Map`.  `StructsToJson` supports Structs, Arrays, and Map, but our implementation currently assumes Structs.

GpuStructsToJson:

```scala
GpuCast.castStructToJsonString(input.getBase, child.dataType.asInstanceOf[StructType].fields, ...
```",,andygrove,934084,MDQ6VXNlcjkzNDA4NA==,https://avatars.githubusercontent.com/u/934084?v=4,,https://api.github.com/users/andygrove,https://github.com/andygrove,https://api.github.com/users/andygrove/followers,https://api.github.com/users/andygrove/following{/other_user},https://api.github.com/users/andygrove/gists{/gist_id},https://api.github.com/users/andygrove/starred{/owner}{/repo},https://api.github.com/users/andygrove/subscriptions,https://api.github.com/users/andygrove/orgs,https://api.github.com/users/andygrove/repos,https://api.github.com/users/andygrove/events{/privacy},https://api.github.com/users/andygrove/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1960520250/reactions,0,0,0,0,0,0,0,0,0,10477
1186,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997526906,https://github.com/NVIDIA/spark-rapids/issues/10479#issuecomment-1997526906,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10479,1997526906,IC_kwDOD7z77c53D9d6,2024-03-14T13:59:13Z,2024-03-14T13:59:13Z,COLLABORATOR,"This is mostly fixed, but if we try to read the data as a string, then it is not validated, it is just returned as a string.

I see this as a subset of https://github.com/rapidsai/cudf/issues/15222

We can probably still fix it in our code, for non-nested data but it means we will have to run a regular expression over all of the returned string output, and ultimately we really should have CUDF do the validation everywhere if we want it to be right.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997526906/reactions,0,0,0,0,0,0,0,0,0,10479
1187,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997843324,https://github.com/NVIDIA/spark-rapids/issues/10483#issuecomment-1997843324,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10483,1997843324,IC_kwDOD7z77c53FKt8,2024-03-14T16:23:21Z,2024-03-14T16:23:21Z,COLLABORATOR,Currently this is throwing a NullPointerException in CUDF on the java side. I think we can probably fix it without too much trouble.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997843324/reactions,0,0,0,0,0,0,0,0,0,10483
1188,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1961696441,https://github.com/NVIDIA/spark-rapids/issues/10488#issuecomment-1961696441,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10488,1961696441,IC_kwDOD7z77c507Ry5,2024-02-23T17:11:13Z,2024-02-23T17:11:13Z,COLLABORATOR,I just failed for me when I ran it with all of the tests together instead of running it single threaded by itself.  This might be a test related issue.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1961696441/reactions,0,0,0,0,0,0,0,0,0,10488
1189,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997533529,https://github.com/NVIDIA/spark-rapids/issues/10489#issuecomment-1997533529,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10489,1997533529,IC_kwDOD7z77c53D_FZ,2024-03-14T14:02:24Z,2024-03-14T14:02:24Z,COLLABORATOR,"Actually after looking at more real world data it is clear that this does happen, and we need to support it. I filed https://github.com/rapidsai/cudf/issues/15277 in CUDF to help us address this.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997533529/reactions,0,0,0,0,0,0,0,0,0,10489
1190,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1964340954,https://github.com/NVIDIA/spark-rapids/issues/10498#issuecomment-1964340954,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10498,1964340954,IC_kwDOD7z77c51FXba,2024-02-26T14:56:02Z,2024-02-26T14:56:02Z,COLLABORATOR,What version of Spark was this running against? It looks like something in Spark itself changed and not our code.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1964340954/reactions,0,0,0,0,0,0,0,0,0,10498
1191,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1971354646,https://github.com/NVIDIA/spark-rapids/issues/10515#issuecomment-1971354646,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10515,1971354646,IC_kwDOD7z77c51gHwW,2024-02-29T15:13:46Z,2024-02-29T15:13:46Z,COLLABORATOR,This is a big change and at a minimum we are going to have to update our operators. We might even have to update some of our kernels.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1971354646/reactions,0,0,0,0,0,0,0,0,0,10515
1192,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1979655761,https://github.com/NVIDIA/spark-rapids/issues/10529#issuecomment-1979655761,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10529,1979655761,IC_kwDOD7z77c51_yZR,2024-03-05T21:21:16Z,2024-03-05T21:21:16Z,COLLABORATOR,"After talking to @jlowe this is by design. I don't know if we want to document this better and include the semaphore wait time, or if we just drop the scan time and make sure the other times that really represent the operation without the semaphore wait time are always visible.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1979655761/reactions,0,0,0,0,0,0,0,0,0,10529
1193,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977121894,https://github.com/NVIDIA/spark-rapids/issues/10534#issuecomment-1977121894,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10534,1977121894,IC_kwDOD7z77c512Hxm,2024-03-04T17:37:38Z,2024-03-04T17:37:38Z,COLLABORATOR,I filed https://github.com/rapidsai/cudf/issues/15222 with CUDF for this.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1977121894/reactions,0,0,0,0,0,0,0,0,0,10534
1194,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021378110,https://github.com/NVIDIA/spark-rapids/issues/10545#issuecomment-2021378110,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10545,2021378110,IC_kwDOD7z77c54e8g-,2024-03-26T20:12:49Z,2024-03-26T20:12:49Z,COLLABORATOR,Scope: validate fallback happens properly.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021378110/reactions,0,0,0,0,0,0,0,0,0,10545
1195,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1985897777,https://github.com/NVIDIA/spark-rapids/issues/10566#issuecomment-1985897777,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10566,1985897777,IC_kwDOD7z77c52XmUx,2024-03-08T15:29:32Z,2024-03-08T15:29:32Z,COLLABORATOR,"while this error looks different, it might be related to https://github.com/NVIDIA/spark-rapids/issues/10318",,tgravescs,4563792,MDQ6VXNlcjQ1NjM3OTI=,https://avatars.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1985897777/reactions,1,1,0,0,0,0,0,0,0,10566
1196,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992545585,https://github.com/NVIDIA/spark-rapids/issues/10566#issuecomment-1992545585,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10566,1992545585,IC_kwDOD7z77c52w9Ux,2024-03-12T20:45:00Z,2024-03-12T20:45:23Z,COLLABORATOR,@captify-sivakhno Could you please provide the details of the table? You can retrieve the information by executing either `spark.sql(f'DESCRIBE FORMATTED unity_catalog_name.schema_name.table_name').show(truncate=False)` in Spark or `DESCRIBE FORMATTED unity_catalog_name.schema_name.table_name;` in SQL Editor. Can you also share the complete stack trace of the error?,,SurajAralihalli,24796335,MDQ6VXNlcjI0Nzk2MzM1,https://avatars.githubusercontent.com/u/24796335?v=4,,https://api.github.com/users/SurajAralihalli,https://github.com/SurajAralihalli,https://api.github.com/users/SurajAralihalli/followers,https://api.github.com/users/SurajAralihalli/following{/other_user},https://api.github.com/users/SurajAralihalli/gists{/gist_id},https://api.github.com/users/SurajAralihalli/starred{/owner}{/repo},https://api.github.com/users/SurajAralihalli/subscriptions,https://api.github.com/users/SurajAralihalli/orgs,https://api.github.com/users/SurajAralihalli/repos,https://api.github.com/users/SurajAralihalli/events{/privacy},https://api.github.com/users/SurajAralihalli/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1992545585/reactions,0,0,0,0,0,0,0,0,0,10566
1197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995665044,https://github.com/NVIDIA/spark-rapids/issues/10566#issuecomment-1995665044,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10566,1995665044,IC_kwDOD7z77c52826U,2024-03-13T20:20:27Z,2024-03-13T20:20:27Z,COLLABORATOR,"@captify-sivakhno What is the logic of this use case?
I saw you firstly read a Spark dataframe and then convert it to a pandas dataframe? 
Does it mean you plan to do some transformation on pandas dataframe?",,viadea,9665750,MDQ6VXNlcjk2NjU3NTA=,https://avatars.githubusercontent.com/u/9665750?v=4,,https://api.github.com/users/viadea,https://github.com/viadea,https://api.github.com/users/viadea/followers,https://api.github.com/users/viadea/following{/other_user},https://api.github.com/users/viadea/gists{/gist_id},https://api.github.com/users/viadea/starred{/owner}{/repo},https://api.github.com/users/viadea/subscriptions,https://api.github.com/users/viadea/orgs,https://api.github.com/users/viadea/repos,https://api.github.com/users/viadea/events{/privacy},https://api.github.com/users/viadea/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1995665044/reactions,0,0,0,0,0,0,0,0,0,10566
1198,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997600771,https://github.com/NVIDIA/spark-rapids/issues/10595#issuecomment-1997600771,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10595,1997600771,IC_kwDOD7z77c53EPgD,2024-03-14T14:34:00Z,2024-03-14T14:34:00Z,COLLABORATOR,"I should add that an empty struct results in a different error.

```
Seq(""""""{""a"":1,""b"":"""",""c"":{}}"""""").toDF(""json"").repartition(1).selectExpr(""from_json(json, 'a int, b string, c struct<a string>')"").show()
```

```
Caused by: java.lang.NullPointerException
  at ai.rapids.cudf.Table.gatherJSONColumns(Table.java:1105)
  at ai.rapids.cudf.Table.gatherJSONColumns(Table.java:1225)
  at ai.rapids.cudf.Table.readJSON(Table.java:1391)
  at org.apache.spark.sql.rapids.GpuJsonToStructs.$anonfun$doColumnar$2(GpuJsonToStructs.scala:180)
  at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
  at org.apache.spark.sql.rapids.GpuJsonToStructs.$anonfun$doColumnar$1(GpuJsonToStructs.scala:178)
  at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
  at org.apache.spark.sql.rapids.GpuJsonToStructs.doColumnar(GpuJsonToStructs.scala:176)
```

This looks almost identical to reading an list with only empty top level structs.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997600771/reactions,0,0,0,0,0,0,0,0,0,10595
1199,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997796992,https://github.com/NVIDIA/spark-rapids/issues/10596#issuecomment-1997796992,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10596,1997796992,IC_kwDOD7z77c53E_aA,2024-03-14T16:01:12Z,2024-03-14T16:01:12Z,COLLABORATOR,This depends on https://github.com/rapidsai/cudf/issues/15303,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997796992/reactions,0,0,0,0,0,0,0,0,0,10596
1200,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997797766,https://github.com/NVIDIA/spark-rapids/issues/10596#issuecomment-1997797766,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10596,1997797766,IC_kwDOD7z77c53E_mG,2024-03-14T16:01:34Z,2024-03-14T16:01:34Z,COLLABORATOR,This depends on https://github.com/rapidsai/cudf/issues/15303,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/1997797766/reactions,0,0,0,0,0,0,0,0,0,10596
1201,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077984136,https://github.com/NVIDIA/spark-rapids/issues/10600#issuecomment-2077984136,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10600,2077984136,IC_kwDOD7z77c5724WI,2024-04-25T19:01:30Z,2024-04-25T19:01:30Z,COLLABORATOR,Related to: https://github.com/NVIDIA/spark-rapids/issues/10741,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077984136/reactions,0,0,0,0,0,0,0,0,0,10600
1202,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000024197,https://github.com/NVIDIA/spark-rapids/issues/10601#issuecomment-2000024197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10601,2000024197,IC_kwDOD7z77c53NfKF,2024-03-15T16:29:47Z,2024-03-15T16:29:47Z,COLLABORATOR,Could we optionally include a lit of metrics so that it matches `NvtxWithMetrics` and then we just totally delete `NvtxWithMetrics`.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000024197/reactions,0,0,0,0,0,0,0,0,0,10601
1203,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000422019,https://github.com/NVIDIA/spark-rapids/issues/10607#issuecomment-2000422019,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10607,2000422019,IC_kwDOD7z77c53PASD,2024-03-15T20:49:37Z,2024-03-15T20:49:37Z,COLLABORATOR,"I am not sure what is happening, it works for me some of the time, but not all of the time, and I don't know why. I will keep digging.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2000422019/reactions,0,0,0,0,0,0,0,0,0,10607
1204,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008615758,https://github.com/NVIDIA/spark-rapids/issues/10612#issuecomment-2008615758,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10612,2008615758,IC_kwDOD7z77c53uQtO,2024-03-20T03:39:29Z,2024-03-20T03:39:29Z,MEMBER,The GPU coalesce on the read side is tracked by #10402.,,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2008615758/reactions,1,1,0,0,0,0,0,0,0,10612
1205,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2009625670,https://github.com/NVIDIA/spark-rapids/issues/10613#issuecomment-2009625670,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10613,2009625670,IC_kwDOD7z77c53yHRG,2024-03-20T13:55:05Z,2024-03-20T13:55:05Z,COLLABORATOR,"On the GPU the problems typically show up around thread divergence and non-coalesed memory access patterns.

I am not 100% sure about this so we should run some experiments and see, but I don't think it is a clear win every time. If the string is long, then it will always have a bad memory access pattern, and the length of time that the kernel takes to run is likely the amount of time it takes for the longest string to be processed by a warp. If the string is short then the memory access pattern is likely good, and even though we might still have thread divergence it is decently fast.

Replacing string values with nulls requires a memory copy, and copy_if_else for strings is not known to be that great. So we might need some kind of a heuristic to see what matters. I think in really bad cases today this could be a big win, especially if we can free up some warps early to process more data, and there are enough input strings to make that be a win.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2009625670/reactions,0,0,0,0,0,0,0,0,0,10613
1206,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021546882,https://github.com/NVIDIA/spark-rapids/issues/10618#issuecomment-2021546882,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10618,2021546882,IC_kwDOD7z77c54fluC,2024-03-26T22:01:59Z,2024-03-26T22:01:59Z,MEMBER,"Thanks @raven-biscocho-HH for filing the issue.  Unfortunately performing RLIKE directly in the join kernel is not something we have plans to do.  Regular expressions are very complex operations, and combining them with the join kernel would be a very large footprint kernel that is unlikely to scale well due to such high occupancy.

Given this is an inner join, normally we try to support those still on the GPU by keeping the hash join portion as a join but the non-hash join condition (i.e.: the complex case when) as a post-filter.  However we currently do not support LIKE where the pattern to search for is not a scalar.  RAPIDS libcudf does support this, so it seems like something we could add.  If the join is highly explosive without the case when being evaluated, this may still not be performant.

RLIKE is a different matter.  RAPIDS does not support regular expression searching where the search pattern is not a literal string (i.e.: is instead different for each row being searched).  Like the join kernel discussion above, it's highly unlikely RLIKE with a column instead of a literal as the search pattern, would be supported anytime soon.

The headline says RLIKE but I only see LIKE statements in the screenshots above.  If the query is only using LIKE and not RLIKE, there's a chance we could do some changes to make this join stay on the GPU by performing the complex case when. statement as a post-filter (with the assumption the inner join without the case when is not extremely explosive).",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021546882/reactions,0,0,0,0,0,0,0,0,0,10618
1207,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021823252,https://github.com/NVIDIA/spark-rapids/issues/10618#issuecomment-2021823252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10618,2021823252,IC_kwDOD7z77c54gpMU,2024-03-27T02:52:55Z,2024-03-27T02:52:55Z,NONE,IF RLIKE is not possible. Then I think it's okay with the LIKE statements. can you guide me what would be the process to make it enable and be able to RUN on GPU.,,raven-biscocho-HH,164294455,U_kgDOCcrvNw,https://avatars.githubusercontent.com/u/164294455?v=4,,https://api.github.com/users/raven-biscocho-HH,https://github.com/raven-biscocho-HH,https://api.github.com/users/raven-biscocho-HH/followers,https://api.github.com/users/raven-biscocho-HH/following{/other_user},https://api.github.com/users/raven-biscocho-HH/gists{/gist_id},https://api.github.com/users/raven-biscocho-HH/starred{/owner}{/repo},https://api.github.com/users/raven-biscocho-HH/subscriptions,https://api.github.com/users/raven-biscocho-HH/orgs,https://api.github.com/users/raven-biscocho-HH/repos,https://api.github.com/users/raven-biscocho-HH/events{/privacy},https://api.github.com/users/raven-biscocho-HH/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2021823252/reactions,0,0,0,0,0,0,0,0,0,10618
1208,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2022898713,https://github.com/NVIDIA/spark-rapids/issues/10618#issuecomment-2022898713,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10618,2022898713,IC_kwDOD7z77c54kvwZ,2024-03-27T14:22:43Z,2024-03-27T14:22:57Z,MEMBER,"> can you guide me what would be the process to make it enable and be able to RUN on GPU.

This isn't something a user could enable today, as there needs to be code changes in the RAPIDS Accelerator to implement this.  It's essentially finishing the work from #65.  Need to add JNI bindings to libcudf's `like` functionality that takes a column of patterns instead of a scalar pattern, and then update the plugin's `GpuLike` expression to support a column as well as a literal for the pattern.

This would not support the like conditions directly in the join, but rather perform the join just on the column keys without the condition then post-filter the join result with the columnar-pattern LIKE expressions.  I'm assuming the join is not a nested loop join on this condition but a sort-merge-join where this is an additional condition beyond the normal equality check between other join key columns.  I cannot verify that from the truncated screenshot above, would be great to see the warning message from the driver log for that join exec to verify.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2022898713/reactions,0,0,0,0,0,0,0,0,0,10618
1209,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2018746880,https://github.com/NVIDIA/spark-rapids/issues/10632#issuecomment-2018746880,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10632,2018746880,IC_kwDOD7z77c54U6IA,2024-03-25T19:24:53Z,2024-03-25T19:24:53Z,MEMBER,"After chatting with the Nsight Systems team, we can probably accomplish most of the tracing needs we want by leveraging the [cupti](https://developer.nvidia.com/cupti) toolkit.  This won't generate a qdrep file, but it might be easy to post-process the cupti trace data such that we could generate the qdrep file from it.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2018746880/reactions,0,0,0,0,0,0,0,0,0,10632
1210,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146904123,https://github.com/NVIDIA/spark-rapids/issues/10638#issuecomment-2146904123,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10638,2146904123,IC_kwDOD7z77c5_9yg7,2024-06-04T08:20:21Z,2024-06-04T08:20:21Z,COLLABORATOR,"Changed `Update spark-rapids-jni dependency version` to 2024/06/07

![image](https://github.com/NVIDIA/spark-rapids/assets/50287591/34eceb40-2b5a-4285-a39c-0da855dd9304)
",,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146904123/reactions,0,0,0,0,0,0,0,0,0,10638
1211,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2033039004,https://github.com/NVIDIA/spark-rapids/issues/10639#issuecomment-2033039004,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10639,2033039004,IC_kwDOD7z77c55Lbac,2024-04-02T20:31:07Z,2024-04-02T20:31:07Z,COLLABORATOR,Can you give more details on the test failures that necessitate this change?,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2033039004/reactions,0,0,0,0,0,0,0,0,0,10639
1212,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2034936595,https://github.com/NVIDIA/spark-rapids/issues/10639#issuecomment-2034936595,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10639,2034936595,IC_kwDOD7z77c55SqsT,2024-04-03T15:32:11Z,2024-04-03T15:32:11Z,COLLABORATOR,We are not testing for this. I saw this while working on adding tests for #10518. I can create a test for this and add to this issue. ,,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2034936595/reactions,0,0,0,0,0,0,0,0,0,10639
1213,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2034940800,https://github.com/NVIDIA/spark-rapids/issues/10639#issuecomment-2034940800,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10639,2034940800,IC_kwDOD7z77c55SruA,2024-04-03T15:34:02Z,2024-04-03T15:34:02Z,COLLABORATOR,"[Here](https://github.com/NVIDIA/spark-rapids/blob/526663c8596c73fd426387343e29b3e7b51f2404/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuInMemoryTableScanExec.scala#L72) is the code I am referring to, we can see that we are passing the parameters without converting them to the GPU versions. 

I think this is not OK but I have to write a test for it before I can prove it. ",,razajafri,8813002,MDQ6VXNlcjg4MTMwMDI=,https://avatars.githubusercontent.com/u/8813002?v=4,,https://api.github.com/users/razajafri,https://github.com/razajafri,https://api.github.com/users/razajafri/followers,https://api.github.com/users/razajafri/following{/other_user},https://api.github.com/users/razajafri/gists{/gist_id},https://api.github.com/users/razajafri/starred{/owner}{/repo},https://api.github.com/users/razajafri/subscriptions,https://api.github.com/users/razajafri/orgs,https://api.github.com/users/razajafri/repos,https://api.github.com/users/razajafri/events{/privacy},https://api.github.com/users/razajafri/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2034940800/reactions,0,0,0,0,0,0,0,0,0,10639
1214,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2075446307,https://github.com/NVIDIA/spark-rapids/issues/10641#issuecomment-2075446307,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10641,2075446307,IC_kwDOD7z77c57tMwj,2024-04-24T17:12:13Z,2024-04-24T17:12:13Z,COLLABORATOR,"Not sure if we should set a fixed seed for this issue, seems like it needs to be investigated and resolved.  ",,sameerz,7036315,MDQ6VXNlcjcwMzYzMTU=,https://avatars.githubusercontent.com/u/7036315?v=4,,https://api.github.com/users/sameerz,https://github.com/sameerz,https://api.github.com/users/sameerz/followers,https://api.github.com/users/sameerz/following{/other_user},https://api.github.com/users/sameerz/gists{/gist_id},https://api.github.com/users/sameerz/starred{/owner}{/repo},https://api.github.com/users/sameerz/subscriptions,https://api.github.com/users/sameerz/orgs,https://api.github.com/users/sameerz/repos,https://api.github.com/users/sameerz/events{/privacy},https://api.github.com/users/sameerz/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2075446307/reactions,0,0,0,0,0,0,0,0,0,10641
1215,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077891593,https://github.com/NVIDIA/spark-rapids/issues/10641#issuecomment-2077891593,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10641,2077891593,IC_kwDOD7z77c572hwJ,2024-04-25T18:20:14Z,2024-04-25T18:20:14Z,COLLABORATOR,We should resolve all of the failures. Setting a fixed seed is a way for the build to stop failing while we do it.,,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077891593/reactions,1,1,0,0,0,0,0,0,0,10641
1216,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111263941,https://github.com/NVIDIA/spark-rapids/issues/10641#issuecomment-2111263941,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10641,2111263941,IC_kwDOD7z77c5911TF,2024-05-14T22:45:12Z,2024-05-14T22:45:12Z,COLLABORATOR,"The particular failure here:
```
CPU: ... regexp_extract(a, (abc1a$|ab2ab$), 1)='ab2ab'....
GPU: ...regexp_extract(a, (abc1a$|ab2ab$), 1)='ab2ab\r'...
```

The input string is `aab2ab\r\n`",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111263941/reactions,0,0,0,0,0,0,0,0,0,10641
1217,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2096821085,https://github.com/NVIDIA/spark-rapids/issues/10687#issuecomment-2096821085,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10687,2096821085,IC_kwDOD7z77c58-vNd,2024-05-06T20:09:46Z,2024-05-06T20:13:54Z,COLLABORATOR,"See https://github.com/apache/spark/commit/6ee662c28ff

The default collation can be changed with `spark.sql.session.collation.default`. It's possible that we might have to support explicitly UTF8_BINARY (the original default) as the only collation type for strings on the GPU.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2096821085/reactions,0,0,0,0,0,0,0,0,0,10687
1218,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059880276,https://github.com/NVIDIA/spark-rapids/issues/10702#issuecomment-2059880276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10702,2059880276,IC_kwDOD7z77c56x0dU,2024-04-16T20:34:56Z,2024-04-16T20:34:56Z,COLLABORATOR,Can you give more information on what we might have to change?  It doesn't look like our code is affected by this.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059880276/reactions,0,0,0,0,0,0,0,0,0,10702
1219,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059950535,https://github.com/NVIDIA/spark-rapids/issues/10702#issuecomment-2059950535,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10702,2059950535,IC_kwDOD7z77c56yFnH,2024-04-16T21:27:47Z,2024-04-16T21:27:47Z,COLLABORATOR,I thought this changed behaviour from V1 to V2 datasources for certain type. We should ensure we are consistent.,,parthosa,13639815,MDQ6VXNlcjEzNjM5ODE1,https://avatars.githubusercontent.com/u/13639815?v=4,,https://api.github.com/users/parthosa,https://github.com/parthosa,https://api.github.com/users/parthosa/followers,https://api.github.com/users/parthosa/following{/other_user},https://api.github.com/users/parthosa/gists{/gist_id},https://api.github.com/users/parthosa/starred{/owner}{/repo},https://api.github.com/users/parthosa/subscriptions,https://api.github.com/users/parthosa/orgs,https://api.github.com/users/parthosa/repos,https://api.github.com/users/parthosa/events{/privacy},https://api.github.com/users/parthosa/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2059950535/reactions,0,0,0,0,0,0,0,0,0,10702
1220,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077255336,https://github.com/NVIDIA/spark-rapids/issues/10741#issuecomment-2077255336,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10741,2077255336,IC_kwDOD7z77c570Gao,2024-04-25T13:55:01Z,2024-04-25T13:55:01Z,COLLABORATOR,"Actually `pattern[0-9]{3,4}` is a string followed by 3 or 4 digits. I don't know if we want to write a custom kernel for that just yet. I don't know how common something like that is, and I don't want to write a custom kernel for a single operation in a single customer query.

I would rather see us write a custom kernel for `[A-B]{X,Y}` where `A` and `B` are any character value and `X` and `Y` are any integer value, including `null` which would indicate unbounded. I see this as much more common/generic. That would let us match any Chinese character `[\u4e00-\u9fa5]{1,}` (`{1,}` is the same as `+`, one or more). It would also let us do things like 0 or more digits `[0-9]{0,}` (`{0,}` is the same as `*`, zero or more). Or any printable ASCII character. Or really any possible range of values.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2077255336/reactions,1,1,0,0,0,0,0,0,0,10741
1221,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2079151764,https://github.com/NVIDIA/spark-rapids/issues/10741#issuecomment-2079151764,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10741,2079151764,IC_kwDOD7z77c577VaU,2024-04-26T10:51:20Z,2024-04-26T10:51:20Z,COLLABORATOR,"> Actually `pattern[0-9]{3,4}` is a string followed by 3 or 4 digits. I don't know if we want to write a custom kernel for that just yet. I don't know how common something like that is, and I don't want to write a custom kernel for a single operation in a single customer query.

`pattern[0-9]{3,4}` is a misleading example, I am working on a kernel that will match `pattern[0-9]{X, Y}` (so it will also match `[0-9]{X, Y}` by setting pattern = """").

> I would rather see us write a custom kernel for `[A-B]{X,Y}` where `A` and `B` are any character value and `X` and `Y` are any integer value, including `null` which would indicate unbounded. I see this as much more common/generic. That would let us match any Chinese character `[\u4e00-\u9fa5]{1,}` (`{1,}` is the same as `+`, one or more). It would also let us do things like 0 or more digits `[0-9]{0,}` (`{0,}` is the same as `*`, zero or more). Or any printable ASCII character. Or really any possible range of values.

Yes, this kernel can be more general so these two cases can be combined. Thanks for catching this!",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2079151764/reactions,0,0,0,0,0,0,0,0,0,10741
1222,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2079663234,https://github.com/NVIDIA/spark-rapids/issues/10741#issuecomment-2079663234,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10741,2079663234,IC_kwDOD7z77c579SSC,2024-04-26T15:55:23Z,2024-04-26T15:55:23Z,COLLABORATOR,"I still don't see a lot of generality for `STATIC_STRING[RANGE]{X, Y}`.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2079663234/reactions,0,0,0,0,0,0,0,0,0,10741
1223,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099213736,https://github.com/NVIDIA/spark-rapids/issues/10762#issuecomment-2099213736,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10762,2099213736,IC_kwDOD7z77c59H3Wo,2024-05-07T20:07:32Z,2024-05-07T20:07:32Z,COLLABORATOR,Priority is to investigate that we are not falling back with this new default state (true).,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099213736/reactions,0,0,0,0,0,0,0,0,0,10762
1224,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2094235147,https://github.com/NVIDIA/spark-rapids/issues/10764#issuecomment-2094235147,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10764,2094235147,IC_kwDOD7z77c58034L,2024-05-04T14:55:54Z,2024-05-04T14:55:54Z,COLLABORATOR,"For some reason the number of capturing groups in `selectExpr`  `regexp_extract(a, '(\\:|$)', 1)` and the GpuOverrides output `regexp_extract(a#28, B=(.*?)(:|$), 1)` are different",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2094235147/reactions,0,0,0,0,0,0,0,0,0,10764
1225,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2094253946,https://github.com/NVIDIA/spark-rapids/issues/10764#issuecomment-2094253946,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10764,2094253946,IC_kwDOD7z77c5808d6,2024-05-04T15:13:32Z,2024-05-04T15:13:32Z,COLLABORATOR,"> For some reason the number of capturing groups in `selectExpr`  `regexp_extract(a, '(\\:|$)', 1)` and the GpuOverrides output `regexp_extract(a#28, B=(.*?)(:|$), 1)` are different

Good catch, fixed. I had copy/pasted the wrong line.",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2094253946/reactions,0,0,0,0,0,0,0,0,0,10764
1226,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2096971952,https://github.com/NVIDIA/spark-rapids/issues/10764#issuecomment-2096971952,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10764,2096971952,IC_kwDOD7z77c58_UCw,2024-05-06T21:40:56Z,2024-05-06T21:40:56Z,COLLABORATOR,This will involve incorporating the existing $ transpilation into the choice expression. ,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2096971952/reactions,0,0,0,0,0,0,0,0,0,10764
1227,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110977424,https://github.com/NVIDIA/spark-rapids/issues/10764#issuecomment-2110977424,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10764,2110977424,IC_kwDOD7z77c590vWQ,2024-05-14T19:18:02Z,2024-05-14T19:18:02Z,COLLABORATOR,Might require the implementation of https://github.com/rapidsai/cudf/issues/15746,,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110977424/reactions,0,0,0,0,0,0,0,0,0,10764
1228,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099586657,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2099586657,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2099586657,IC_kwDOD7z77c59JSZh,2024-05-08T01:50:35Z,2024-05-08T01:50:35Z,COLLABORATOR,"could u first try to increase the value of ""concurrentGpuTask"" to see if we can get any better perf ?",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099586657/reactions,0,0,0,0,0,0,0,0,0,10770
1229,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2101598895,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2101598895,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2101598895,IC_kwDOD7z77c59Q9qv,2024-05-08T22:22:51Z,2024-05-08T22:22:51Z,CONTRIBUTOR,"The computation gets pretty much stuck with essentially no progress.   I don't think that will make a difference.   Partial stack trace after reaching this point (might be from similar but not identical example to this repro):
<details><summary>Details</summary>
<p>

```
```
	at sun.misc.Unsafe.copyMemory(Native Method)
	at sun.misc.Unsafe.copyMemory(Unsafe.java:560)
	at java.nio.DirectByteBuffer.put(DirectByteBuffer.java:331)
	at org.apache.spark.util.DirectByteBufferOutputStream.grow(DirectByteBufferOutputStream.scala:63)
	at org.apache.spark.util.DirectByteBufferOutputStream.ensureCapacity(DirectByteBufferOutputStream.scala:49)
	at org.apache.spark.util.DirectByteBufferOutputStream.write(DirectByteBufferOutputStream.scala:44)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	- locked <0x0000000768a99630> (a java.io.DataOutputStream)
	at org.apache.spark.sql.rapids.execution.python.BufferToStreamWriter.$anonfun$handleBuffer$1(GpuArrowWriter.scala:48)
	at org.apache.spark.sql.rapids.execution.python.BufferToStreamWriter.$anonfun$handleBuffer$1$adapted(GpuArrowWriter.scala:42)
	at org.apache.spark.sql.rapids.execution.python.BufferToStreamWriter$$Lambda$3498/1244767780.apply(Unknown Source)
	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
	at org.apache.spark.sql.rapids.execution.python.BufferToStreamWriter.handleBuffer(GpuArrowWriter.scala:42)
	at ai.rapids.cudf.Table.writeArrowIPCArrowChunk(Native Method)
	at ai.rapids.cudf.Table.access$2000(Table.java:41)
	at ai.rapids.cudf.Table$ArrowIPCTableWriter.write(Table.java:1739)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.$anonfun$write$1(GpuArrowWriter.scala:99)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.$anonfun$write$1$adapted(GpuArrowWriter.scala:97)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter$$Lambda$3493/108528776.apply(Unknown Source)
	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.write(GpuArrowWriter.scala:97)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.write$(GpuArrowWriter.scala:96)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowPythonWriter.write(GpuArrowWriter.scala:144)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.$anonfun$writeAndClose$1(GpuArrowWriter.scala:93)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.$anonfun$writeAndClose$1$adapted(GpuArrowWriter.scala:92)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter$$Lambda$3492/1674125626.apply(Unknown Source)
	at com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:30)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.writeAndClose(GpuArrowWriter.scala:92)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowWriter.writeAndClose$(GpuArrowWriter.scala:92)
	at org.apache.spark.sql.rapids.execution.python.GpuArrowPythonWriter.writeAndClose(GpuArrowWriter.scala:144)
	at org.apache.spark.sql.rapids.execution.python.shims.GpuArrowPythonRunner$$anon$1.writeNextInputToStream(GpuArrowPythonRunner.scala:74)
	at org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:931)
	at org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:851)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	- locked <0x0000000765602c48> (a java.io.BufferedInputStream)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.rapids.execution.python.shims.GpuArrowPythonOutput$$anon$1.read(GpuArrowPythonOutput.scala:71)
	at org.apache.spark.sql.rapids.execution.python.shims.GpuArrowPythonOutput$$anon$1.read(GpuArrowPythonOutput.scala:48)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:635)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at com.nvidia.spark.rapids.AbstractProjectSplitIterator.hasNext(basicPhysicalOperators.scala:233)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at com.nvidia.spark.rapids.GpuMergeAggregateIterator.$anonfun$next$2(GpuAggregateExec.scala:751)
	at com.nvidia.spark.rapids.GpuMergeAggregateIterator$$Lambda$3706/165795055.apply(Unknown Source)```

</p>
</details> 
",,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2101598895/reactions,0,0,0,0,0,0,0,0,0,10770
1230,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151289800,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2151289800,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2151289800,IC_kwDOD7z77c6AOhPI,2024-06-06T02:13:01Z,2024-06-06T02:13:14Z,COLLABORATOR,"Hi @eordentlich, where can i get the file `s3a://spark-rapids-ml-bm-datasets-public/pca/1m_3k_singlecol_float32_50_files.parquet ?`",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151289800/reactions,0,0,0,0,0,0,0,0,0,10770
1231,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151347584,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2151347584,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2151347584,IC_kwDOD7z77c6AOvWA,2024-06-06T03:32:06Z,2024-06-06T03:32:06Z,COLLABORATOR,And another try is to set the `spark.rapids.sql.python.gpu.enabled` to `false` and remove this line `spark.python.daemon.module rapids.daemon_databricks` if no GPU is required in the UDFs.,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151347584/reactions,0,0,0,0,0,0,0,0,0,10770
1232,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151496647,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2151496647,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2151496647,IC_kwDOD7z77c6APTvH,2024-06-06T06:21:59Z,2024-06-06T06:21:59Z,CONTRIBUTOR,"> Hi @eordentlich, where can i get the file `s3a://spark-rapids-ml-bm-datasets-public/pca/1m_3k_singlecol_float32_50_files.parquet ?`

It's a public s3 bucket/file.   Can you access via spark parquet reader or s3 cli?",,eordentlich,36281329,MDQ6VXNlcjM2MjgxMzI5,https://avatars.githubusercontent.com/u/36281329?v=4,,https://api.github.com/users/eordentlich,https://github.com/eordentlich,https://api.github.com/users/eordentlich/followers,https://api.github.com/users/eordentlich/following{/other_user},https://api.github.com/users/eordentlich/gists{/gist_id},https://api.github.com/users/eordentlich/starred{/owner}{/repo},https://api.github.com/users/eordentlich/subscriptions,https://api.github.com/users/eordentlich/orgs,https://api.github.com/users/eordentlich/repos,https://api.github.com/users/eordentlich/events{/privacy},https://api.github.com/users/eordentlich/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151496647/reactions,0,0,0,0,0,0,0,0,0,10770
1233,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2154278336,https://github.com/NVIDIA/spark-rapids/issues/10770#issuecomment-2154278336,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10770,2154278336,IC_kwDOD7z77c6AZ63A,2024-06-07T07:43:40Z,2024-06-07T08:09:09Z,COLLABORATOR,"> It's a public s3 bucket/file. Can you access via spark parquet reader or s3 cli?

I tried to reproduce this locally, but always getting the error as below, seems there is something I missed.
```
Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))
	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)
	at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6431)
	at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)
	... 22 more
Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))
	at com.amazonaws.auth.EnvironmentVariableCredentialsProvider.getCredentials(EnvironmentVariableCredentialsProvider.java:50)
	at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)
	... 43 more

```",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2154278336/reactions,0,0,0,0,0,0,0,0,0,10770
1234,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099265343,https://github.com/NVIDIA/spark-rapids/issues/10771#issuecomment-2099265343,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10771,2099265343,IC_kwDOD7z77c59ID8_,2024-05-07T20:36:11Z,2024-05-07T20:36:11Z,COLLABORATOR,Initial scope is triaging unit test failures to determine priorities of individual issues.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099265343/reactions,0,0,0,0,0,0,0,0,0,10771
1235,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099261088,https://github.com/NVIDIA/spark-rapids/issues/10772#issuecomment-2099261088,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10772,2099261088,IC_kwDOD7z77c59IC6g,2024-05-07T20:33:12Z,2024-05-07T20:33:12Z,COLLABORATOR,Initial scope is triaging unit test failures to determine priorities of individual issues.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099261088/reactions,0,0,0,0,0,0,0,0,0,10772
1236,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127883137,https://github.com/NVIDIA/spark-rapids/issues/10772#issuecomment-2127883137,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10772,2127883137,IC_kwDOD7z77c5-1OuB,2024-05-23T19:31:08Z,2024-05-23T19:31:08Z,COLLABORATOR,"I actually see 5 test failures, not 6. I even tried the @NVnavkumar recommended, so one of the tests marked as `except` is passing fine:

```
- SPARK-24788: RelationalGroupedDataset.toString with unresolved exprs should not fail
```

The other 5 are failing with what looks like  array sort differences:

```
collect functions *** FAILED ***
!struct<>                     struct<collect_set(a):array<int>,collect_set(b):array<int>>
![List(1, 2, 3),List(2, 4)]   [ArrayBuffer(2, 1, 3),ArrayBuffer(4, 2)] (RapidsSQLTestsTrait.scala:99)
``` 

```
collect functions structs *** FAILED ***
!== Correct Answer - 1 ==             == RAPIDS Answer - 1 ==
!struct<>                             struct<collect_set(a):array<int>,sort_array(collect_set(b), true):array<struct<x:int,y:int>>>
![List(1, 2, 3),List([2,2], [4,1])]   [ArrayBuffer(2, 1, 3),ArrayBuffer([2,2], [4,1])] (RapidsSQLTestsTrait.scala:99)
```

```
SPARK-17641: collect functions should not collect null values *** FAILED ***
!== Correct Answer - 1 ==   == RAPIDS Answer - 1 ==
!struct<>                   struct<collect_set(a):array<string>,collect_set(b):array<int>>
![List(1),List(2, 4)]       [ArrayBuffer(1),ArrayBuffer(4, 2)] (RapidsSQLTestsTrait.scala:99)
```

```
collect functions should be able to cast to array type with no null values *** FAILED ***
!== Correct Answer - 1 ==   == RAPIDS Answer - 1 ==
!struct<>                   struct<collect_set(a):array<float>>
![List(1.0, 2.0)]           [ArrayBuffer(2.0, 1.0)] (RapidsSQLTestsTrait.scala:99)
```

and this last one seems to be something that we probably don't do the same on the GPU and I don't know if it matters yet.

```
SPARK-19471: AggregationIterator does not initialize the generated result projection before using it *** FAILED ***
DataFrameAggregateSuite.this.find(hashAggPlan)(((x0$1: org.apache.spark.sql.execution.SparkPlan) => x0$1 match {
  case (child: org.apache.spark.sql.execution.SparkPlan)(codegenStageId: Int) org.apache.spark.sql.execution.WholeStageCodegenExec((_: org.apache.spark.sql.execution.aggregate.HashAggregateExec)) => true
case _ => false
})).isDefined was false (DataFrameAggregateSuite.scala:691)
```
",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127883137/reactions,0,0,0,0,0,0,0,0,0,10772
1237,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129866825,https://github.com/NVIDIA/spark-rapids/issues/10772#issuecomment-2129866825,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10772,2129866825,IC_kwDOD7z77c5-8zBJ,2024-05-24T15:47:36Z,2024-05-24T15:47:36Z,COLLABORATOR,"Ok the last test is invalid. It's looking for `HashAggregateExec` specifically and we replace this, so we need to skip this test.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129866825/reactions,0,0,0,0,0,0,0,0,0,10772
1238,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129888336,https://github.com/NVIDIA/spark-rapids/issues/10772#issuecomment-2129888336,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10772,2129888336,IC_kwDOD7z77c5-84RQ,2024-05-24T16:00:17Z,2024-05-24T16:08:58Z,COLLABORATOR,"From the documentation of `collect_set`:

> The function is non-deterministic because the order of collected results depends on the order of the rows which may be non-deterministic after a shuffle.

`collect functions structs` uses `sort_array` for the second column `b` but not for the first column. The test fails because `a` is not matching, and that's not sorted.

I think the problem is that we need to have a way to sort the arrays. That said, I don't see consistent use or lack of use of `sort_array` from spar's side, so I don't think we can enable these tests.. unless we just ported them over. @binmahone.",,abellina,1901059,MDQ6VXNlcjE5MDEwNTk=,https://avatars.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129888336/reactions,0,0,0,0,0,0,0,0,0,10772
1239,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2132495549,https://github.com/NVIDIA/spark-rapids/issues/10772#issuecomment-2132495549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10772,2132495549,IC_kwDOD7z77c5_G0y9,2024-05-27T01:36:14Z,2024-05-27T01:36:14Z,COLLABORATOR,well received. This happens a lot. For some test cases we may need to adjust to make them work. This is one of the acceptable ways out.,,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2132495549/reactions,0,0,0,0,0,0,0,0,0,10772
1240,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099265544,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2099265544,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2099265544,IC_kwDOD7z77c59IEAI,2024-05-07T20:36:21Z,2024-05-07T20:36:21Z,COLLABORATOR,Initial scope is triaging unit test failures to determine priorities of individual issues.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2099265544/reactions,0,0,0,0,0,0,0,0,0,10775
1241,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2108874924,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2108874924,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2108874924,IC_kwDOD7z77c59suCs,2024-05-13T22:00:07Z,2024-05-13T22:00:07Z,COLLABORATOR,"All but one of the reported failures disappear when whole-stage codegen is disabled.

There does seem to be a genuine failure in the tests for `ParseUrl`. I will raise a separate bug to track that one.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2108874924/reactions,0,0,0,0,0,0,0,0,0,10775
1242,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2108942046,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2108942046,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2108942046,IC_kwDOD7z77c59s-be,2024-05-13T22:57:58Z,2024-05-13T22:57:58Z,COLLABORATOR,"I've filed https://github.com/NVIDIA/spark-rapids/pull/10810, for which a fix is available at https://github.com/NVIDIA/spark-rapids/pull/10811.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2108942046/reactions,0,0,0,0,0,0,0,0,0,10775
1243,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110813679,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2110813679,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2110813679,IC_kwDOD7z77c590HXv,2024-05-14T18:00:12Z,2024-05-14T18:00:12Z,COLLABORATOR,"@binmahone, @NVnavkumar, what are your thoughts on disabling whole-stage codegen for these tests?  It should resolve almost all the failures listed here (save for the last one, which will be fixed with #10811).",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2110813679/reactions,0,0,0,0,0,0,0,0,0,10775
1244,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111292197,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2111292197,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2111292197,IC_kwDOD7z77c5918Ml,2024-05-14T22:58:15Z,2024-05-14T22:58:15Z,COLLABORATOR,"I think it's the right move. We do need further investigation to disable any of the codegen specific-tests here. I know that in the regexp suite I saw a handful of places that were testing the java output of the codegen step.

> @binmahone, @NVnavkumar, what are your thoughts on disabling whole-stage codegen for these tests? It should resolve almost all the failures listed here (save for the last one, which will be fixed with #10811).

",,NVnavkumar,97137715,U_kgDOBco0Mw,https://avatars.githubusercontent.com/u/97137715?v=4,,https://api.github.com/users/NVnavkumar,https://github.com/NVnavkumar,https://api.github.com/users/NVnavkumar/followers,https://api.github.com/users/NVnavkumar/following{/other_user},https://api.github.com/users/NVnavkumar/gists{/gist_id},https://api.github.com/users/NVnavkumar/starred{/owner}{/repo},https://api.github.com/users/NVnavkumar/subscriptions,https://api.github.com/users/NVnavkumar/orgs,https://api.github.com/users/NVnavkumar/repos,https://api.github.com/users/NVnavkumar/events{/privacy},https://api.github.com/users/NVnavkumar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111292197/reactions,1,1,0,0,0,0,0,0,0,10775
1245,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2122120967,https://github.com/NVIDIA/spark-rapids/issues/10775#issuecomment-2122120967,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10775,2122120967,IC_kwDOD7z77c5-fP8H,2024-05-21T08:57:16Z,2024-05-21T08:57:16Z,COLLABORATOR,"after discussion with Gary and Ferdinand, we have refined the Spark UT Framework by https://github.com/NVIDIA/spark-rapids/pull/10851 , which eliminates the necessity of disabling wholeStage. Now the list of the failed tests is refreshed. After PR 10851 is merged, we can move forward to investigate the remaining failed test cases.",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2122120967/reactions,0,0,0,0,0,0,0,0,0,10775
1246,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2106770982,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2106770982,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2106770982,IC_kwDOD7z77c59ksYm,2024-05-13T06:43:16Z,2024-05-13T06:43:16Z,NONE,Great!,,zhanglistar,985418,MDQ6VXNlcjk4NTQxOA==,https://avatars.githubusercontent.com/u/985418?v=4,,https://api.github.com/users/zhanglistar,https://github.com/zhanglistar,https://api.github.com/users/zhanglistar/followers,https://api.github.com/users/zhanglistar/following{/other_user},https://api.github.com/users/zhanglistar/gists{/gist_id},https://api.github.com/users/zhanglistar/starred{/owner}{/repo},https://api.github.com/users/zhanglistar/subscriptions,https://api.github.com/users/zhanglistar/orgs,https://api.github.com/users/zhanglistar/repos,https://api.github.com/users/zhanglistar/events{/privacy},https://api.github.com/users/zhanglistar/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2106770982/reactions,0,0,0,0,0,0,0,0,0,10790
1247,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116563175,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2116563175,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2116563175,IC_kwDOD7z77c5-KDDn,2024-05-17T03:32:44Z,2024-05-17T03:33:48Z,COLLABORATOR,"Celeborn works as a normal Spark shuffle manager, so Plugin always works well with it.
This issue is just to track if enabling GPU slicing and compression can get better perf when working with Celeborn. Or at least get better perf from some customer queries.",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116563175/reactions,0,0,0,0,0,0,0,0,0,10790
1248,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116579685,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2116579685,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2116579685,IC_kwDOD7z77c5-KHFl,2024-05-17T03:57:39Z,2024-05-17T04:00:31Z,NONE,"Hi, as a committer from Celeborn community, I'd like to help if any features are required from Celeborn, and you're always welcome to contribute to Celeborn :)",,waitinfuture,948245,MDQ6VXNlcjk0ODI0NQ==,https://avatars.githubusercontent.com/u/948245?v=4,,https://api.github.com/users/waitinfuture,https://github.com/waitinfuture,https://api.github.com/users/waitinfuture/followers,https://api.github.com/users/waitinfuture/following{/other_user},https://api.github.com/users/waitinfuture/gists{/gist_id},https://api.github.com/users/waitinfuture/starred{/owner}{/repo},https://api.github.com/users/waitinfuture/subscriptions,https://api.github.com/users/waitinfuture/orgs,https://api.github.com/users/waitinfuture/repos,https://api.github.com/users/waitinfuture/events{/privacy},https://api.github.com/users/waitinfuture/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116579685/reactions,0,0,0,0,0,0,0,0,0,10790
1249,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116739549,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2116739549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2116739549,IC_kwDOD7z77c5-KuHd,2024-05-17T05:47:44Z,2024-05-17T05:47:44Z,COLLABORATOR,Really appreciate that @waitinfuture. Will let you know if any action is required from Celeborn side.,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116739549/reactions,1,0,0,1,0,0,0,0,0,10790
1250,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2122732707,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2122732707,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2122732707,IC_kwDOD7z77c5-hlSj,2024-05-21T14:11:40Z,2024-05-21T14:11:40Z,COLLABORATOR,"I updated the name of this issue to make it clear. Our shuffle works with Celeborn, but the goal here is to improve the performance of that shuffle.

@firestarman and @winningsix If we have patches that improve the performance could you please explain how GPU compression and slicing improves the performance? In the past we tried to do compression on the GPU as a part of shuffle and the performance was generally worse because of the opportunity cost.  Generally the CPU was mostly idle waiting for the GPU to finish, and by offloading the shuffle data to the CPU for compression improved the performance, especially if we could do the compression using multiple CPU threads. I really would like to understand how this improves performance, what tests have been run so we can know which situations should enable this and which should not. ",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2122732707/reactions,0,0,0,0,0,0,0,0,0,10790
1251,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134433463,https://github.com/NVIDIA/spark-rapids/issues/10790#issuecomment-2134433463,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10790,2134433463,IC_kwDOD7z77c5_ON63,2024-05-28T06:24:58Z,2024-05-28T06:24:58Z,COLLABORATOR,"> I updated the name of this issue to make it clear. Our shuffle works with Celeborn, but the goal here is to improve the performance of that shuffle.
> 
> @firestarman and @winningsix If we have patches that improve the performance could you please explain how GPU compression and slicing improves the performance? In the past we tried to do compression on the GPU as a part of shuffle and the performance was generally worse because of the opportunity cost. Generally the CPU was mostly idle waiting for the GPU to finish, and by offloading the shuffle data to the CPU for compression improved the performance, especially if we could do the compression using multiple CPU threads. I really would like to understand how this improves performance, what tests have been run so we can know which situations should enable this and which should not.

Thanks for the title update. It looks more suitable. The benefiting point comes from a notable compression ratio (saying 3 ~ 10) in customer queries. If higher CR, less time spent in device-to-host. We should introduce a heuristic approach to determine GPU shuffle based on the data pattern. For current case, I would suggest to start with compression ratio per batch as the determining point. For example, if the compression ratio > 3 for 1st batch handled by current executor, it will sit on GPU shuffle, otherwise CPU shuffle compression.",,winningsix,2278268,MDQ6VXNlcjIyNzgyNjg=,https://avatars.githubusercontent.com/u/2278268?v=4,,https://api.github.com/users/winningsix,https://github.com/winningsix,https://api.github.com/users/winningsix/followers,https://api.github.com/users/winningsix/following{/other_user},https://api.github.com/users/winningsix/gists{/gist_id},https://api.github.com/users/winningsix/starred{/owner}{/repo},https://api.github.com/users/winningsix/subscriptions,https://api.github.com/users/winningsix/orgs,https://api.github.com/users/winningsix/repos,https://api.github.com/users/winningsix/events{/privacy},https://api.github.com/users/winningsix/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134433463/reactions,0,0,0,0,0,0,0,0,0,10790
1252,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111938318,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2111938318,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2111938318,IC_kwDOD7z77c594Z8O,2024-05-15T08:53:13Z,2024-05-15T09:32:26Z,COLLABORATOR,"The stacktrace pasted by Tim is expected in `Cast from string to int using hand-picked values` , it's not actually failing.

The real failed test case  is ` test(""SPARK-24788: RelationalGroupedDataset.toString with unresolved exprs should not fail"") ` in RapidsDataFrameAggregateSuite

The root cause of this failed test case is that, when Spark-Rapids is built by JDK 17 and Run on JDK 17, the expression `RelationalGroupedDataset.GroupByType.getClass.getSimpleName` will return """" instead of correct value ""GroupByType$"", RelationalGroupedDataset is a object in Apache Spark.

I have tried to reproduce this behavior in a simple project (https://github.com/binmahone/test_jdk17_java, you can download it, build & run it with JDK17), the output is always ""GroupByType$"". Even if I have tried to move many pof Spark-Rapids's pom setttings to the simple project (such as scalatest configurations, etc.), it's still not reproducing the wrong value """".

So I have no clue what settings in Spark-Rapids leads to this wrong behaviour. Since this issue looks not very urgent, I'll first ignore the failed test case and unblock Nightly build
",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2111938318/reactions,0,0,0,0,0,0,0,0,0,10801
1253,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113495507,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2113495507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2113495507,IC_kwDOD7z77c59-WHT,2024-05-15T21:36:32Z,2024-05-15T21:36:32Z,COLLABORATOR,It looks related to https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8198818 but I cannot reproduce it outside Spark and Plugin either.,,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113495507/reactions,0,0,0,0,0,0,0,0,0,10801
1254,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113741966,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2113741966,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2113741966,IC_kwDOD7z77c59_SSO,2024-05-16T01:34:01Z,2024-05-16T01:34:45Z,COLLABORATOR,"hi @gerashegalov, dead link on my side, can you check it please? Also, just to clarify, the behavior is correct in vanilla spark, but wrong in spark-rapids",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113741966/reactions,0,0,0,0,0,0,0,0,0,10801
1255,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113925590,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2113925590,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2113925590,IC_kwDOD7z77c59__HW,2024-05-16T03:01:53Z,2024-05-16T03:01:53Z,COLLABORATOR,"@binmahone I can still click through it, maybe it was temporarily unavailable? 

Can you access and search https://bugs.java.com/bugdatabase/  for ""JDK-8198818 : Class.simpleName different for anonymous classes""? 

It came up in https://youtrack.jetbrains.com/issue/KT-23072/Class.simpleName-of-anonymous-object-is-not-an-empty-string-in-JDK8-but-on-JDK9

I am not sure that Spark test code is actually robust. Calling getSimpleName on weird Scala object might be sub-optimal. 


",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2113925590/reactions,0,0,0,0,0,0,0,0,0,10801
1256,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116369165,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2116369165,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2116369165,IC_kwDOD7z77c5-JTsN,2024-05-16T23:26:57Z,2024-05-16T23:26:57Z,COLLABORATOR,"It turns out, there is a history of getSimpleName being broken with various combinations of Scala version and JDK  
https://issues.apache.org/jira/browse/SPARK-34596?jql=text%20~%20getSimpleName%20AND%20project%20%3D%20Spark%20ORDER%20BY%20created%20DESC

Yet it is still in use in various places.

We just got unlucky that we are testing Spark 3.3 that bundles 2.12.15 . This combo yields an empty string for GroupByType

```bash
$ echo 'Seq(1,2,3).toDF.groupBy($""value"")' | JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 /home/gshegalov/dist/spark-3.3.4-bin-hadoop2/bin/spark-shell |& grep 'Scala\|Relational'
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 17.0.10)
res0: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [value], value: [value: int], type: ]
```

And it is fixed in 3.4+ simply because it upgraded to Scala 2.12.17
```bash
$ echo 'Seq(1,2,3).toDF.groupBy($""value"")' | JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 /home/gshegalov/dist/spark-3.4.0-bin-hadoop3/bin/spark-shell |& grep 'Scala\|Relational'
Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 17.0.10)
res0: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [value], value: [value: int], type: GroupBy]
```

  

",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116369165/reactions,0,0,0,0,0,0,0,0,0,10801
1257,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116570102,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2116570102,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2116570102,IC_kwDOD7z77c5-KEv2,2024-05-17T03:43:36Z,2024-05-17T03:44:05Z,COLLABORATOR,"@gerashegalov great findings!

It's interesting that you can reproduce the problem with Spark 3.3 + scala 2.12.15 (I tried your way, it reproduces, too), because :

1. Spark 3.3 hard coded scala 2.12.15 in its pom , but when it runs `test(""SPARK-24788: RelationalGroupedDataset.toString with unresolved exprs should not fail"")` everything is okay, i.e. it does not have the empty simple name problem in UT
2. my toy project https://github.com/binmahone/test_jdk17_java also hard coded scala 2.12.15 in its pom, but it cannot reproduce the issue either:

```shell
/usr/lib/jvm/java-1.17.0-openjdk-amd64/bin/java -javaagent:/home/hongbin/.local/share/JetBrains/Toolbox/apps/intellij-idea-ultimate/lib/idea_rt.jar=46767:/home/hongbin/.local/share/JetBrains/Toolbox/apps/intellij-idea-ultimate/bin -Dfile.encoding=UTF-8 -classpath /home/hongbin/.local/share/JetBrains/IntelliJIdea2024.1/Scala/lib/runners.jar:/home/hongbin/code/test_jdk17_java/target/spark311/test-classes:/home/hongbin/code/test_jdk17_java/target/spark311/classes:/home/hongbin/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest_2.12/3.2.16/scalatest_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-core_2.12/3.2.16/scalatest-core_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-compatible/3.2.16/scalatest-compatible-3.2.16.jar:/home/hongbin/.m2/repository/org/scalactic/scalactic_2.12/3.2.16/scalactic_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scala-lang/modules/scala-xml_2.12/2.1.0/scala-xml_2.12-2.1.0.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-featurespec_2.12/3.2.16/scalatest-featurespec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-flatspec_2.12/3.2.16/scalatest-flatspec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-freespec_2.12/3.2.16/scalatest-freespec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-funsuite_2.12/3.2.16/scalatest-funsuite_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-funspec_2.12/3.2.16/scalatest-funspec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-propspec_2.12/3.2.16/scalatest-propspec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-refspec_2.12/3.2.16/scalatest-refspec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-wordspec_2.12/3.2.16/scalatest-wordspec_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-diagrams_2.12/3.2.16/scalatest-diagrams_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-matchers-core_2.12/3.2.16/scalatest-matchers-core_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-shouldmatchers_2.12/3.2.16/scalatest-shouldmatchers_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scalatest/scalatest-mustmatchers_2.12/3.2.16/scalatest-mustmatchers_2.12-3.2.16.jar:/home/hongbin/.m2/repository/org/scala-lang/scala-reflect/2.12.17/scala-reflect-2.12.17.jar org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s org.example.SimpleSuite -showProgressMessages true
Testing started at 11:26 am ...




Hello world!
GroupByType$


``` 

Even if I manually replace `/home/hongbin/.m2/repository/org/scala-lang/scala-reflect/2.12.17/scala-reflect-2.12.17.jar ` with `/home/hongbin/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar ` (this is introduced by scalatest, as shown in below snapshot), it's still printing `GroupByType$` instread of empty string 

![image](https://github.com/NVIDIA/spark-rapids/assets/6416599/8e9ae8e1-d6bc-4dcf-ba32-c664f8abc869)
",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116570102/reactions,0,0,0,0,0,0,0,0,0,10801
1258,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116572869,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2116572869,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2116572869,IC_kwDOD7z77c5-KFbF,2024-05-17T03:47:34Z,2024-05-17T03:47:34Z,COLLABORATOR,So what would be you advice next ? I suggest holding upgrading 2.12.17 until we have more stronger reasons to do so. For the meanwhile we'll still exclude this test case as I did in https://github.com/NVIDIA/spark-rapids/pull/10820,,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2116572869/reactions,0,0,0,0,0,0,0,0,0,10801
1259,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2117878531,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2117878531,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2117878531,IC_kwDOD7z77c5-PEMD,2024-05-17T15:44:30Z,2024-05-17T15:44:30Z,COLLABORATOR,"> So what would be you advice next ? 

Keep enabling the UT for all Spark 3.3+ version. Make sure we can make exclude/xfaill tests conditionally just like we do with pytests and ""xfail"" this particular test only for 3.3.x while keeping it running for 3.4+

Regarding not being able to repro with the toy project, it may have some mismatch with bytecode generation in Spark build:
-  same compile flags as in Spark build?
- or not exactly the same javac/scalac version combo ,
- or some scala-maven-plugin issue with Zinc
- Spark uses the shade plugin for all artifacts https://github.com/apache/spark/blob/3edd6c7e1d504860fefc9921208ec47ab562ae41/pom.xml#L3451-L3454 , which may massage the bytecode causing this issue.

 ",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2117878531/reactions,1,1,0,0,0,0,0,0,0,10801
1260,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2119518286,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2119518286,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2119518286,IC_kwDOD7z77c5-VUhO,2024-05-20T01:38:05Z,2024-05-20T01:38:05Z,COLLABORATOR,"hi @NvTimLiu , with above discussion I think we can close this issue or move this to backlog, what you think?",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2119518286/reactions,0,0,0,0,0,0,0,0,0,10801
1261,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2119681311,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2119681311,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2119681311,IC_kwDOD7z77c5-V8Uf,2024-05-20T05:12:37Z,2024-05-20T05:12:37Z,COLLABORATOR,Sounds good to me,,NvTimLiu,50287591,MDQ6VXNlcjUwMjg3NTkx,https://avatars.githubusercontent.com/u/50287591?v=4,,https://api.github.com/users/NvTimLiu,https://github.com/NvTimLiu,https://api.github.com/users/NvTimLiu/followers,https://api.github.com/users/NvTimLiu/following{/other_user},https://api.github.com/users/NvTimLiu/gists{/gist_id},https://api.github.com/users/NvTimLiu/starred{/owner}{/repo},https://api.github.com/users/NvTimLiu/subscriptions,https://api.github.com/users/NvTimLiu/orgs,https://api.github.com/users/NvTimLiu/repos,https://api.github.com/users/NvTimLiu/events{/privacy},https://api.github.com/users/NvTimLiu/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2119681311/reactions,0,0,0,0,0,0,0,0,0,10801
1262,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2120568158,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2120568158,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2120568158,IC_kwDOD7z77c5-ZU1e,2024-05-20T14:23:49Z,2024-05-20T14:23:49Z,MEMBER,"I'm confused how this was resolved.  @gerashegalov proposed keeping the test and xfailing it only on Spark 3.3 but run it on Spark 3.4+, so we're at least running it somewhere.  As it is now, this test was turned off for all Spark versions in #10820 which is not the state I thought we wanted to leave it in.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2120568158/reactions,0,0,0,0,0,0,0,0,0,10801
1263,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123710286,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2123710286,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2123710286,IC_kwDOD7z77c5-lT9O,2024-05-22T01:30:47Z,2024-05-22T01:30:47Z,COLLABORATOR,"Hi @jlowe , the Spark UT is only enabled in spark 3.3.0 for now(as stated in https://github.com/NVIDIA/spark-rapids/issues/10745), so we don't actually have anywhere else to run the test case. https://github.com/NVIDIA/spark-rapids/pull/10820 is a workaround and it leaves a ""KNOWN_ISSUE"" in there, so at least this issue will not be forgot. What would be your prefered way out here?",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123710286/reactions,0,0,0,0,0,0,0,0,0,10801
1264,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123744096,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2123744096,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2123744096,IC_kwDOD7z77c5-lcNg,2024-05-22T02:13:36Z,2024-05-22T02:13:36Z,MEMBER,"If there's followup work to do, there needs to be a way to track that work.  Github issues is how we prefer to track.  There's stuff to do here, but no issue to track it.  Therefore it is very likely it will never be done, because we'll forget that we were supposed to do it.  The ""KNOWN_ISSUE"" points to a closed issue, so either:
- don't close it, it remains the tracking issue (probably the best since code refers to it)
- file a new issue to track what remains to be done and reference it here.",,jlowe,1360766,MDQ6VXNlcjEzNjA3NjY=,https://avatars.githubusercontent.com/u/1360766?v=4,,https://api.github.com/users/jlowe,https://github.com/jlowe,https://api.github.com/users/jlowe/followers,https://api.github.com/users/jlowe/following{/other_user},https://api.github.com/users/jlowe/gists{/gist_id},https://api.github.com/users/jlowe/starred{/owner}{/repo},https://api.github.com/users/jlowe/subscriptions,https://api.github.com/users/jlowe/orgs,https://api.github.com/users/jlowe/repos,https://api.github.com/users/jlowe/events{/privacy},https://api.github.com/users/jlowe/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123744096/reactions,0,0,0,0,0,0,0,0,0,10801
1265,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123744419,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2123744419,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2123744419,IC_kwDOD7z77c5-lcSj,2024-05-22T02:14:02Z,2024-05-22T02:14:02Z,COLLABORATOR,"As advised by @jlowe , I reopened this issue to keep this issue still in track",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123744419/reactions,0,0,0,0,0,0,0,0,0,10801
1266,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2128670477,https://github.com/NVIDIA/spark-rapids/issues/10801#issuecomment-2128670477,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10801,2128670477,IC_kwDOD7z77c5-4O8N,2024-05-24T06:33:30Z,2024-05-24T06:44:15Z,COLLABORATOR,"Low priority it since it's a conflict between scala 2.12.15 and JDK17. Change it target in 24.08.
Two follow up things we may need here:
1. Upgrade test framework to support excluding based on JDK version. So the case is only excluded on JDK17 + Scala 2.12.15, but still run on JDK8. (Let's target it in 24.08.) https://github.com/NVIDIA/spark-rapids/issues/10889
2. Don't exclude the case when JDK17 + Scala 2.12.17+ when we support new Spark versions in the future.
",,GaryShen2008,22128535,MDQ6VXNlcjIyMTI4NTM1,https://avatars.githubusercontent.com/u/22128535?v=4,,https://api.github.com/users/GaryShen2008,https://github.com/GaryShen2008,https://api.github.com/users/GaryShen2008/followers,https://api.github.com/users/GaryShen2008/following{/other_user},https://api.github.com/users/GaryShen2008/gists{/gist_id},https://api.github.com/users/GaryShen2008/starred{/owner}{/repo},https://api.github.com/users/GaryShen2008/subscriptions,https://api.github.com/users/GaryShen2008/orgs,https://api.github.com/users/GaryShen2008/repos,https://api.github.com/users/GaryShen2008/events{/privacy},https://api.github.com/users/GaryShen2008/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2128670477/reactions,0,0,0,0,0,0,0,0,0,10801
1267,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123379936,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2123379936,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2123379936,IC_kwDOD7z77c5-kDTg,2024-05-21T20:22:26Z,2024-05-21T20:30:03Z,COLLABORATOR,Scope: make casting of floats to decimals feature flag off by default and update documentation accordingly with this example.  Ref: [spark.rapids.sql.castFloatToDecimal.enabled](https://github.com/NVIDIA/spark-rapids/blob/branch-24.06/docs/additional-functionality/advanced_configs.md#sql.castFloatToDecimal.enabled).  Check if supported operators in tools needs to be updated.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123379936/reactions,0,0,0,0,0,0,0,0,0,10809
1268,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130346217,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2130346217,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2130346217,IC_kwDOD7z77c5--oDp,2024-05-24T20:58:49Z,2024-05-24T20:58:49Z,COLLABORATOR,Filed a cudf issue: https://github.com/rapidsai/cudf/issues/15862,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130346217/reactions,0,0,0,0,0,0,0,0,0,10809
1269,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130397925,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2130397925,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2130397925,IC_kwDOD7z77c5--0rl,2024-05-24T21:36:14Z,2024-05-25T18:08:25Z,COLLABORATOR,"Also take a look at the discussion around https://github.com/NVIDIA/spark-rapids/issues/9682#issuecomment-2126616418 and above. 9.95 is not representable as double

```java
$ jshell 
|  Welcome to JShell -- Version 21.0.2
|  For an introduction type: /help intro

jshell> new BigDecimal(9.95)
$8 ==> 9.949999999999999289457264239899814128875732421875

jshell> new BigDecimal(9.949999999999999289457264239899814128875732421875)
$9 ==> 9.949999999999999289457264239899814128875732421875

jshell> new BigDecimal(9.95).setScale(1, BigDecimal.ROUND_HALF_UP)
$11 ==> 9.9

jshell> new BigDecimal(String.valueOf(9.95)).setScale(1, BigDecimal.ROUND_HALF_UP)
$1 ==> 10.0

jshell> new BigDecimal(""9.95"").setScale(1, BigDecimal.ROUND_HALF_UP)
$12 ==> 10.0
```

 

",,gerashegalov,3187938,MDQ6VXNlcjMxODc5Mzg=,https://avatars.githubusercontent.com/u/3187938?v=4,,https://api.github.com/users/gerashegalov,https://github.com/gerashegalov,https://api.github.com/users/gerashegalov/followers,https://api.github.com/users/gerashegalov/following{/other_user},https://api.github.com/users/gerashegalov/gists{/gist_id},https://api.github.com/users/gerashegalov/starred{/owner}{/repo},https://api.github.com/users/gerashegalov/subscriptions,https://api.github.com/users/gerashegalov/orgs,https://api.github.com/users/gerashegalov/repos,https://api.github.com/users/gerashegalov/events{/privacy},https://api.github.com/users/gerashegalov/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130397925/reactions,0,0,0,0,0,0,0,0,0,10809
1270,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130484968,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2130484968,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2130484968,IC_kwDOD7z77c5-_J7o,2024-05-24T22:54:40Z,2024-05-24T22:54:40Z,COLLABORATOR,Okay after reading through the issue https://github.com/NVIDIA/spark-rapids/issues/9682 then I realize that this issue is also one instance of it.,,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130484968/reactions,0,0,0,0,0,0,0,0,0,10809
1271,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130763674,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2130763674,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2130763674,IC_kwDOD7z77c5_AN-a,2024-05-25T04:54:26Z,2024-05-25T05:02:27Z,COLLABORATOR,"I tried the float => string => decimal path (it is very easy to implement in plugin), it can pass the Spark UT, but still some differences from the known limits of ryu float to string and a very edge cases in string to decimal https://github.com/NVIDIA/spark-rapids/issues/10890. I will post a pr and share some results for review next week, but not sure if the diffs are acceptable or original way can match it better.",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130763674/reactions,0,0,0,0,0,0,0,0,0,10809
1272,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130808543,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2130808543,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2130808543,IC_kwDOD7z77c5_AY7f,2024-05-25T05:43:34Z,2024-05-25T05:43:34Z,COLLABORATOR,"That sounds good. I've also found a way to implement in C++ which is very efficient but not sure if it will pass integration test. I'll verify that and will post a PR too.

If having a chance, please list the related tests that I can run to verify.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2130808543/reactions,1,1,0,0,0,0,0,0,0,10809
1273,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134389235,https://github.com/NVIDIA/spark-rapids/issues/10809#issuecomment-2134389235,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10809,2134389235,IC_kwDOD7z77c5_ODHz,2024-05-28T05:45:25Z,2024-05-28T05:45:25Z,COLLABORATOR,"@thirtiseven Please test https://github.com/NVIDIA/spark-rapids/pull/10917 to see if you have any test fails. On my local env, all the unit tests and integration tests passed but I'm not sure if I missed anything.",,ttnghia,7416935,MDQ6VXNlcjc0MTY5MzU=,https://avatars.githubusercontent.com/u/7416935?v=4,,https://api.github.com/users/ttnghia,https://github.com/ttnghia,https://api.github.com/users/ttnghia/followers,https://api.github.com/users/ttnghia/following{/other_user},https://api.github.com/users/ttnghia/gists{/gist_id},https://api.github.com/users/ttnghia/starred{/owner}{/repo},https://api.github.com/users/ttnghia/subscriptions,https://api.github.com/users/ttnghia/orgs,https://api.github.com/users/ttnghia/repos,https://api.github.com/users/ttnghia/events{/privacy},https://api.github.com/users/ttnghia/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134389235/reactions,1,0,0,0,0,0,0,0,1,10809
1274,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123404784,https://github.com/NVIDIA/spark-rapids/issues/10823#issuecomment-2123404784,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10823,2123404784,IC_kwDOD7z77c5-kJXw,2024-05-21T20:39:37Z,2024-05-21T20:39:37Z,COLLABORATOR,We need to understand more details for why these are failing.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123404784/reactions,0,0,0,0,0,0,0,0,0,10823
1275,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123737396,https://github.com/NVIDIA/spark-rapids/issues/10823#issuecomment-2123737396,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10823,2123737396,IC_kwDOD7z77c5-lak0,2024-05-22T02:04:57Z,2024-05-22T02:05:44Z,COLLABORATOR,">We need to understand more details for why these are failing.

Added what I have found so far in the description. 
>""Looked further into this and found all the failures are due to the plan after AQE changed when GPU Serde is on, different from the expectation. For example""",,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2123737396/reactions,0,0,0,0,0,0,0,0,0,10823
1276,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2140685291,https://github.com/NVIDIA/spark-rapids/issues/10835#issuecomment-2140685291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10835,2140685291,IC_kwDOD7z77c5_mEPr,2024-05-30T18:57:25Z,2024-05-30T18:57:25Z,COLLABORATOR,Scope is to add additional tests for creating V2 parquet files and testing reads/writes.,,mattahrens,5303895,MDQ6VXNlcjUzMDM4OTU=,https://avatars.githubusercontent.com/u/5303895?v=4,,https://api.github.com/users/mattahrens,https://github.com/mattahrens,https://api.github.com/users/mattahrens/followers,https://api.github.com/users/mattahrens/following{/other_user},https://api.github.com/users/mattahrens/gists{/gist_id},https://api.github.com/users/mattahrens/starred{/owner}{/repo},https://api.github.com/users/mattahrens/subscriptions,https://api.github.com/users/mattahrens/orgs,https://api.github.com/users/mattahrens/repos,https://api.github.com/users/mattahrens/events{/privacy},https://api.github.com/users/mattahrens/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2140685291/reactions,0,0,0,0,0,0,0,0,0,10835
1277,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148746975,https://github.com/NVIDIA/spark-rapids/issues/10862#issuecomment-2148746975,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10862,2148746975,IC_kwDOD7z77c6AE0bf,2024-06-05T02:34:38Z,2024-06-05T02:34:38Z,COLLABORATOR,"PR: https://github.com/NVIDIA/spark-rapids/pull/10825
",,res-life,8166419,MDQ6VXNlcjgxNjY0MTk=,https://avatars.githubusercontent.com/u/8166419?v=4,,https://api.github.com/users/res-life,https://github.com/res-life,https://api.github.com/users/res-life/followers,https://api.github.com/users/res-life/following{/other_user},https://api.github.com/users/res-life/gists{/gist_id},https://api.github.com/users/res-life/starred{/owner}{/repo},https://api.github.com/users/res-life/subscriptions,https://api.github.com/users/res-life/orgs,https://api.github.com/users/res-life/repos,https://api.github.com/users/res-life/events{/privacy},https://api.github.com/users/res-life/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148746975/reactions,0,0,0,0,0,0,0,0,0,10862
1278,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127060306,https://github.com/NVIDIA/spark-rapids/issues/10876#issuecomment-2127060306,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10876,2127060306,IC_kwDOD7z77c5-yF1S,2024-05-23T13:05:54Z,2024-05-23T13:05:54Z,COLLABORATOR,"The plan is explicitly to not support collated strings and to fallback to the CPU if we see them. This is a very large amount of work to try and support it, and I don't think we want to try and tackle it would a clear customer need.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2127060306/reactions,0,0,0,0,0,0,0,0,0,10876
1279,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2128320387,https://github.com/NVIDIA/spark-rapids/issues/10877#issuecomment-2128320387,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10877,2128320387,IC_kwDOD7z77c5-25eD,2024-05-24T01:31:58Z,2024-05-24T01:36:08Z,COLLABORATOR,"Hi @NvTimLiu , can we try [this](https://ntotten.com/2015/08/04/using-jenkins-to-detect-broken-links-on-your-site/) to monitor the status of released jar links on the download webpage?(Maybe you have a better way.) Set the desired notification preferences such as email or Slack when the build fails is good enough, we don't want to block the PR checks.",,nvliyuan,84758614,MDQ6VXNlcjg0NzU4NjE0,https://avatars.githubusercontent.com/u/84758614?v=4,,https://api.github.com/users/nvliyuan,https://github.com/nvliyuan,https://api.github.com/users/nvliyuan/followers,https://api.github.com/users/nvliyuan/following{/other_user},https://api.github.com/users/nvliyuan/gists{/gist_id},https://api.github.com/users/nvliyuan/starred{/owner}{/repo},https://api.github.com/users/nvliyuan/subscriptions,https://api.github.com/users/nvliyuan/orgs,https://api.github.com/users/nvliyuan/repos,https://api.github.com/users/nvliyuan/events{/privacy},https://api.github.com/users/nvliyuan/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2128320387/reactions,0,0,0,0,0,0,0,0,0,10877
1280,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129545233,https://github.com/NVIDIA/spark-rapids/issues/10887#issuecomment-2129545233,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10887,2129545233,IC_kwDOD7z77c5-7kgR,2024-05-24T13:31:03Z,2024-05-24T13:31:03Z,COLLABORATOR,"To be clear in order to remove `isFoldableNonLitAllowed`. We need to remove the check for the foldable config entirely. We fixed the issue related to it a long time ago, but we never updated the code to ignore it. It would be good to disable folding for some of the integration tests, like some binary operations, and add in a few tests that would pass in all literal parameters to validate that those expressions are doing the right thing.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129545233/reactions,0,0,0,0,0,0,0,0,0,10887
1281,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129524034,https://github.com/NVIDIA/spark-rapids/issues/10891#issuecomment-2129524034,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10891,2129524034,IC_kwDOD7z77c5-7fVC,2024-05-24T13:20:13Z,2024-05-24T13:20:13Z,COLLABORATOR,"@Feng-Jiang28 Thanks for submitting this. I put in some fixes into CUDF, but it still requires a small change to the plugin to help address it. I thought I had put up a PR for it. But I guess I forgot to. If you want to give it a try you just need to change

https://github.com/NVIDIA/spark-rapids/blob/c5da29d4576d91dd1cf24d00b38ea318c93fcec1/sql-plugin/src/main/scala/org/apache/spark/sql/rapids/GpuJsonToStructs.scala#L179

So it passes `numRows` as the last argument after `ds`.  This will let the CUDF code know that if it sees a situation where there were no columns could be returned (a limitation in the current CUDF JSON parser) that it can create null columns of for the schema passed in. If you do want to do it let me know, otherwise I'll try to get to it next week.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2129524034/reactions,1,1,0,0,0,0,0,0,0,10891
1282,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134232439,https://github.com/NVIDIA/spark-rapids/issues/10891#issuecomment-2134232439,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10891,2134232439,IC_kwDOD7z77c5_Nc13,2024-05-28T02:05:38Z,2024-05-28T02:05:38Z,COLLABORATOR,"No problem Bobby, I will I would put a PR for it",,Feng-Jiang28,106386742,U_kgDOBldVNg,https://avatars.githubusercontent.com/u/106386742?v=4,,https://api.github.com/users/Feng-Jiang28,https://github.com/Feng-Jiang28,https://api.github.com/users/Feng-Jiang28/followers,https://api.github.com/users/Feng-Jiang28/following{/other_user},https://api.github.com/users/Feng-Jiang28/gists{/gist_id},https://api.github.com/users/Feng-Jiang28/starred{/owner}{/repo},https://api.github.com/users/Feng-Jiang28/subscriptions,https://api.github.com/users/Feng-Jiang28/orgs,https://api.github.com/users/Feng-Jiang28/repos,https://api.github.com/users/Feng-Jiang28/events{/privacy},https://api.github.com/users/Feng-Jiang28/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2134232439/reactions,0,0,0,0,0,0,0,0,0,10891
1283,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138862792,https://github.com/NVIDIA/spark-rapids/issues/10898#issuecomment-2138862792,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10898,2138862792,IC_kwDOD7z77c5_fHTI,2024-05-30T07:30:30Z,2024-05-30T07:30:30Z,COLLABORATOR,"It is a bug from cast string to decimal:
```
--- CPU OUTPUT
+++ GPU OUTPUT
@@ -453,7 +453,7 @@
 Row(a='{ ""a"": ""-57303.62e+66"" }', from_json(a)=Row(a=None))
 Row(a='{ ""a"": ""-85630.6E48"" }', from_json(a)=Row(a=None))
 Row(a='', from_json(a)=None)
-Row(a='{ ""a"": ""00000.0E57"" }', from_json(a)=Row(a=Decimal('0')))
+Row(a='{ ""a"": ""00000.0E57"" }', from_json(a)=Row(a=None))
 Row(a='{ ""a"": ""-96009.9e+08"" }', from_json(a)=Row(a=None))
 Row(a='null', from_json(a)=Row(a=None))
 Row(a='{ ""a"": ""+36638.56e+0"" }', from_json(a)=Row(a=Decimal('36639')))
FAILED
```
Some strings be converted to 0 on cpu but None on gpu. A easier repro it:
```
@allow_non_gpu(*non_utc_allow)
def test_from_json_struct_decimal():
    json_string_gen = StringGen(r'{ ""a"": ""[+-]?0(\.0{0,2})?([eE][+-]?[0-9]{1,2})?"" }') \
        .with_special_pattern('', weight=50) \
        .with_special_pattern('null', weight=50)
    assert_gpu_and_cpu_are_equal_collect(
        lambda spark : unary_op_df(spark, json_string_gen, length=100) \
            .select(f.col('a'), f.from_json('a', 'struct<a:decimal>')),
        conf=_enable_all_types_conf)
```

loacl reproduce In spark-shell:

```
scala> val data = Seq(""-0.E+85"", ""0.0E+24"", ""0.e+19"").toDF
data: org.apache.spark.sql.DataFrame = [value: string]

scala> data.write.mode(""OVERWRITE"").parquet(""TEMP"")

scala> val df = spark.read.parquet(""TEMP"")
df: org.apache.spark.sql.DataFrame = [value: string]

scala> df.select(col(""value""), col(""value"").cast(DecimalType(10, 0))).show()
24/05/30 06:30:26 WARN GpuOverrides:
!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it
  @Partitioning <SinglePartition$> could run on GPU
  *Exec <ProjectExec> will run on GPU
    *Expression <Alias> cast(cast(value#75 as decimal(10,0)) as string) AS value#98 will run on GPU
      *Expression <Cast> cast(cast(value#75 as decimal(10,0)) as string) will run on GPU
        *Expression <Cast> cast(value#75 as decimal(10,0)) will run on GPU
    *Exec <FileSourceScanExec> will run on GPU

+-------+-----+
|  value|value|
+-------+-----+
|-0.E+85| null|
|0.0E+24| null|
| 0.e+19| null|
+-------+-----+


scala> spark.conf.set(""spark.rapids.sql.enabled"", ""false"")

scala> df.select(col(""value""), col(""value"").cast(DecimalType(10, 0))).show()
+-------+-----+
|  value|value|
+-------+-----+
|-0.E+85| null|
|0.0E+24|    0|
| 0.e+19|    0|
+-------+-----+
```
seems -0.E+85 failed on test_from_json_struct_decimal but not in spark-shell somehow 
",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2138862792/reactions,0,0,0,0,0,0,0,0,0,10898
1284,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2135888037,https://github.com/NVIDIA/spark-rapids/issues/10930#issuecomment-2135888037,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10930,2135888037,IC_kwDOD7z77c5_TxCl,2024-05-28T18:38:44Z,2024-05-28T18:38:44Z,COLLABORATOR,Not sure if this is a udf or not?,,kuhushukla,20541681,MDQ6VXNlcjIwNTQxNjgx,https://avatars.githubusercontent.com/u/20541681?v=4,,https://api.github.com/users/kuhushukla,https://github.com/kuhushukla,https://api.github.com/users/kuhushukla/followers,https://api.github.com/users/kuhushukla/following{/other_user},https://api.github.com/users/kuhushukla/gists{/gist_id},https://api.github.com/users/kuhushukla/starred{/owner}{/repo},https://api.github.com/users/kuhushukla/subscriptions,https://api.github.com/users/kuhushukla/orgs,https://api.github.com/users/kuhushukla/repos,https://api.github.com/users/kuhushukla/events{/privacy},https://api.github.com/users/kuhushukla/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2135888037/reactions,0,0,0,0,0,0,0,0,0,10930
1285,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2136490297,https://github.com/NVIDIA/spark-rapids/issues/10930#issuecomment-2136490297,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10930,2136490297,IC_kwDOD7z77c5_WEE5,2024-05-29T04:28:22Z,2024-05-29T04:28:22Z,COLLABORATOR,"@kuhushukla brought this to my attention this morning. Sorry I didn't respond here sooner.

> > `collect_limited_list(true, 0, 0, 2)`...
>
> Not sure if this is a udf or not?

This isn't a standard SQL window aggregation, as far as I know.  My guess is that this is a UDAF from [Apache Datafu](https://github.com/apache/datafu), per https://github.com/apache/datafu/pull/24.

From poking around, it appears that `collect_limited_list(col, N)` functions like `collect_list(col)`, but limits the output list-row to a maximum of `N` elements.

On the face of it, I don't foresee this being difficult to implement. (It won't be deterministic.)  I would, however, need to peruse the detailed definition of the function's behaviour.",,mythrocks,5607330,MDQ6VXNlcjU2MDczMzA=,https://avatars.githubusercontent.com/u/5607330?v=4,,https://api.github.com/users/mythrocks,https://github.com/mythrocks,https://api.github.com/users/mythrocks/followers,https://api.github.com/users/mythrocks/following{/other_user},https://api.github.com/users/mythrocks/gists{/gist_id},https://api.github.com/users/mythrocks/starred{/owner}{/repo},https://api.github.com/users/mythrocks/subscriptions,https://api.github.com/users/mythrocks/orgs,https://api.github.com/users/mythrocks/repos,https://api.github.com/users/mythrocks/events{/privacy},https://api.github.com/users/mythrocks/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2136490297/reactions,0,0,0,0,0,0,0,0,0,10930
1286,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148535999,https://github.com/NVIDIA/spark-rapids/issues/10930#issuecomment-2148535999,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10930,2148535999,IC_kwDOD7z77c6AEA6_,2024-06-04T22:49:17Z,2024-06-04T22:49:17Z,COLLABORATOR,Thank you Mithun! ,,kuhushukla,20541681,MDQ6VXNlcjIwNTQxNjgx,https://avatars.githubusercontent.com/u/20541681?v=4,,https://api.github.com/users/kuhushukla,https://github.com/kuhushukla,https://api.github.com/users/kuhushukla/followers,https://api.github.com/users/kuhushukla/following{/other_user},https://api.github.com/users/kuhushukla/gists{/gist_id},https://api.github.com/users/kuhushukla/starred{/owner}{/repo},https://api.github.com/users/kuhushukla/subscriptions,https://api.github.com/users/kuhushukla/orgs,https://api.github.com/users/kuhushukla/repos,https://api.github.com/users/kuhushukla/events{/privacy},https://api.github.com/users/kuhushukla/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148535999/reactions,0,0,0,0,0,0,0,0,0,10930
1287,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137412549,https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2137412549,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942,2137412549,IC_kwDOD7z77c5_ZlPF,2024-05-29T13:25:45Z,2024-05-29T13:25:45Z,COLLABORATOR,"Do you have any plans on how to support this? Project is fairly simple because it is a one to one relationship with input and output batches. Technically it could be a one to many relationship if we run out of memory and split an input batch to make it work.  How would this work for hash aggregate that can have a many to many relationship?  Even trying to trigger it on how long it took to run feels problematic because we might not see any slowness until after the first batch, which then would require us to save around all input batches or save them out on the chance that they might be needed to reproduce the problem.",,revans2,3441321,MDQ6VXNlcjM0NDEzMjE=,https://avatars.githubusercontent.com/u/3441321?v=4,,https://api.github.com/users/revans2,https://github.com/revans2,https://api.github.com/users/revans2/followers,https://api.github.com/users/revans2/following{/other_user},https://api.github.com/users/revans2/gists{/gist_id},https://api.github.com/users/revans2/starred{/owner}{/repo},https://api.github.com/users/revans2/subscriptions,https://api.github.com/users/revans2/orgs,https://api.github.com/users/revans2/repos,https://api.github.com/users/revans2/events{/privacy},https://api.github.com/users/revans2/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2137412549/reactions,0,0,0,0,0,0,0,0,0,10942
1288,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148761507,https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2148761507,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942,2148761507,IC_kwDOD7z77c6AE3-j,2024-06-05T02:54:56Z,2024-06-05T02:54:56Z,COLLABORATOR,"@revans2 Sorry for my late reply. Internally we had a discussion around this with @binmahone @res-life @liurenjie1024 @GaryShen2008. Just as what we offline discussed, we will change the granularity from batch to task level. Thus, it should work for stateful operator like aggregation. Also, regards to the dump timing, we're considering introducing other two modes: i. exact id matching via task id or split id; ii. dumping first few tasks. Later one can help non-tailing case. For this part, let's explore option whether we can be consistent with @jlowe 's profiler tool.   @liurenjie1024 will help on that later and @binmahone is helping explore options regards id matching approach.",,winningsix,2278268,MDQ6VXNlcjIyNzgyNjg=,https://avatars.githubusercontent.com/u/2278268?v=4,,https://api.github.com/users/winningsix,https://github.com/winningsix,https://api.github.com/users/winningsix/followers,https://api.github.com/users/winningsix/following{/other_user},https://api.github.com/users/winningsix/gists{/gist_id},https://api.github.com/users/winningsix/starred{/owner}{/repo},https://api.github.com/users/winningsix/subscriptions,https://api.github.com/users/winningsix/orgs,https://api.github.com/users/winningsix/repos,https://api.github.com/users/winningsix/events{/privacy},https://api.github.com/users/winningsix/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148761507/reactions,0,0,0,0,0,0,0,0,0,10942
1289,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151541838,https://github.com/NVIDIA/spark-rapids/issues/10942#issuecomment-2151541838,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10942,2151541838,IC_kwDOD7z77c6APexO,2024-06-06T06:56:31Z,2024-06-06T07:02:27Z,COLLABORATOR,"hi @revans2 

In the new LORE implementation we'll use two IDs to uniquely identify the lifespan of a specific operator in a specific task:

1. LoreID, to identify an operator in the SQL. We can't use SparkPlan.id as it is not stable. We will use sth like a DFS traversal to number each operator as LoreID,  and print the LoreID in spark UI for each operator. AQE case will also be covered.
2. ParittionID, since there will be N concurrent task for a specific operator, we need to identify which one. We can't use taskID as it is unstable. We'll instead use RDD's parititon index.

Consider a case where we have skew in e.g. JoinExec, the skew task will exhibit consistent LoreID+ParittionID among different runs of the same SQL (Even in the same spark session). With this design, users can dump data only related to the problematic operator in a specific task, and we can replay the specific operator at local in a single thread.

The LoreID+PartitionID design can also be extend to enable self-contained profiling (https://github.com/NVIDIA/spark-rapids/pull/10870). Currently, #10870 can be enabled based on time range/job range/stage range. However job range and stage range should be considered unstable and may result in unexpected traces dumped. With LoreID+PartitionID, we are more specific and acurate to express what traces we need. (LoreID+PartitionID can uniquely identify which task on which executor)

This is how we see it, what you think  @revans2 @jlowe @GaryShen2008 ?  @winningsix @liurenjie1024 please feel free to add your inputs.",,binmahone,6416599,MDQ6VXNlcjY0MTY1OTk=,https://avatars.githubusercontent.com/u/6416599?v=4,,https://api.github.com/users/binmahone,https://github.com/binmahone,https://api.github.com/users/binmahone/followers,https://api.github.com/users/binmahone/following{/other_user},https://api.github.com/users/binmahone/gists{/gist_id},https://api.github.com/users/binmahone/starred{/owner}{/repo},https://api.github.com/users/binmahone/subscriptions,https://api.github.com/users/binmahone/orgs,https://api.github.com/users/binmahone/repos,https://api.github.com/users/binmahone/events{/privacy},https://api.github.com/users/binmahone/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2151541838/reactions,0,0,0,0,0,0,0,0,0,10942
1290,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2144142683,https://github.com/NVIDIA/spark-rapids/issues/10948#issuecomment-2144142683,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10948,2144142683,IC_kwDOD7z77c5_zQVb,2024-06-03T01:54:58Z,2024-06-03T01:54:58Z,COLLABORATOR,This also happens on Spark 351. See https://github.com/NVIDIA/spark-rapids/issues/10956,,firestarman,7280411,MDQ6VXNlcjcyODA0MTE=,https://avatars.githubusercontent.com/u/7280411?v=4,,https://api.github.com/users/firestarman,https://github.com/firestarman,https://api.github.com/users/firestarman/followers,https://api.github.com/users/firestarman/following{/other_user},https://api.github.com/users/firestarman/gists{/gist_id},https://api.github.com/users/firestarman/starred{/owner}{/repo},https://api.github.com/users/firestarman/subscriptions,https://api.github.com/users/firestarman/orgs,https://api.github.com/users/firestarman/repos,https://api.github.com/users/firestarman/events{/privacy},https://api.github.com/users/firestarman/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2144142683/reactions,0,0,0,0,0,0,0,0,0,10948
1291,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148622305,https://github.com/NVIDIA/spark-rapids/issues/10955#issuecomment-2148622305,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10955,2148622305,IC_kwDOD7z77c6AEV_h,2024-06-05T00:26:49Z,2024-06-05T06:09:20Z,COLLABORATOR,"From CICD perspective,
1. Pick a new default - Done (confirmed as 3.2.0)
2. changes to CICD setup based on the new default (still building 3.1.x shims) - in progress (6/14)
3. developer help switch the default shim to a 3.2.x version (stop building 3.1.x shims)
4. remove all the 3.1.x code

In these steps it should not block any nightly pipelines.",,pxLi,8086184,MDQ6VXNlcjgwODYxODQ=,https://avatars.githubusercontent.com/u/8086184?v=4,,https://api.github.com/users/pxLi,https://github.com/pxLi,https://api.github.com/users/pxLi/followers,https://api.github.com/users/pxLi/following{/other_user},https://api.github.com/users/pxLi/gists{/gist_id},https://api.github.com/users/pxLi/starred{/owner}{/repo},https://api.github.com/users/pxLi/subscriptions,https://api.github.com/users/pxLi/orgs,https://api.github.com/users/pxLi/repos,https://api.github.com/users/pxLi/events{/privacy},https://api.github.com/users/pxLi/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2148622305/reactions,0,0,0,0,0,0,0,0,0,10955
1292,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146851128,https://github.com/NVIDIA/spark-rapids/issues/10973#issuecomment-2146851128,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10973,2146851128,IC_kwDOD7z77c5_9lk4,2024-06-04T07:53:40Z,2024-06-04T07:53:40Z,COLLABORATOR,I think this is related to a bug in this rule: https://github.com/NVIDIA/spark-rapids/blob/64cfae3790f8e050028c1ff92cbd5544281b80f2/sql-plugin/src/main/scala/com/nvidia/spark/rapids/InputFileBlockRule.scala#L35,,liurenjie1024,2771941,MDQ6VXNlcjI3NzE5NDE=,https://avatars.githubusercontent.com/u/2771941?v=4,,https://api.github.com/users/liurenjie1024,https://github.com/liurenjie1024,https://api.github.com/users/liurenjie1024/followers,https://api.github.com/users/liurenjie1024/following{/other_user},https://api.github.com/users/liurenjie1024/gists{/gist_id},https://api.github.com/users/liurenjie1024/starred{/owner}{/repo},https://api.github.com/users/liurenjie1024/subscriptions,https://api.github.com/users/liurenjie1024/orgs,https://api.github.com/users/liurenjie1024/repos,https://api.github.com/users/liurenjie1024/events{/privacy},https://api.github.com/users/liurenjie1024/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146851128/reactions,0,0,0,0,0,0,0,0,0,10973
1293,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146851560,https://github.com/NVIDIA/spark-rapids/issues/10973#issuecomment-2146851560,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10973,2146851560,IC_kwDOD7z77c5_9lro,2024-06-04T07:53:53Z,2024-06-04T07:53:53Z,COLLABORATOR,cc @firestarman ,,liurenjie1024,2771941,MDQ6VXNlcjI3NzE5NDE=,https://avatars.githubusercontent.com/u/2771941?v=4,,https://api.github.com/users/liurenjie1024,https://github.com/liurenjie1024,https://api.github.com/users/liurenjie1024/followers,https://api.github.com/users/liurenjie1024/following{/other_user},https://api.github.com/users/liurenjie1024/gists{/gist_id},https://api.github.com/users/liurenjie1024/starred{/owner}{/repo},https://api.github.com/users/liurenjie1024/subscriptions,https://api.github.com/users/liurenjie1024/orgs,https://api.github.com/users/liurenjie1024/repos,https://api.github.com/users/liurenjie1024/events{/privacy},https://api.github.com/users/liurenjie1024/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2146851560/reactions,0,0,0,0,0,0,0,0,0,10973
1294,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2149002948,https://github.com/NVIDIA/spark-rapids/issues/10980#issuecomment-2149002948,https://api.github.com/repos/NVIDIA/spark-rapids/issues/10980,2149002948,IC_kwDOD7z77c6AFy7E,2024-06-05T06:43:49Z,2024-06-05T06:43:49Z,COLLABORATOR,"Specific cases in Spark UT that fails (commented):
```scala
 test(""SPARK-35112: Cast string to day-time interval2"") {
//    checkEvaluation(cast(Literal.create(""0""), DayTimeIntervalType()), 0L)
//    checkEvaluation(cast(Literal.create(""0 0:0:0""), DayTimeIntervalType()), 0L)
//    checkEvaluation(cast(Literal.create("" interval '0 0:0:0' Day TO second   ""),
//      DayTimeIntervalType()), 0L)
    checkEvaluation(cast(Literal.create(""INTERVAL '1 2:03:04' DAY TO SECOND""),
      DayTimeIntervalType()), 93784000000L)
    checkEvaluation(cast(Literal.create(""INTERVAL '1 03:04:00' DAY TO SECOND""),
      DayTimeIntervalType()), 97440000000L)
    checkEvaluation(cast(Literal.create(""INTERVAL '1 03:04:00.0000' DAY TO SECOND""),
      DayTimeIntervalType()), 97440000000L)
//    checkEvaluation(cast(Literal.create(""1 2:03:04""), DayTimeIntervalType()), 93784000000L)
    checkEvaluation(cast(Literal.create(""INTERVAL '-10 2:03:04' DAY TO SECOND""),
      DayTimeIntervalType()), -871384000000L)
//    checkEvaluation(cast(Literal.create(""-10 2:03:04""), DayTimeIntervalType()), -871384000000L)
//    checkEvaluation(cast(Literal.create(""-106751991 04:00:54.775808""), DayTimeIntervalType()),
//      Long.MinValue)
//    checkEvaluation(cast(Literal.create(""106751991 04:00:54.775807""), DayTimeIntervalType()),
//      Long.MaxValue)
//
//    Seq(""-106751991 04:00:54.775808"", ""106751991 04:00:54.775807"").foreach { interval =>
//      val ansiInterval = s""INTERVAL '$interval' DAY TO SECOND""
//      checkEvaluation(
//        cast(cast(Literal.create(interval), DayTimeIntervalType()), StringType), ansiInterval)
//      checkEvaluation(cast(cast(Literal.create(ansiInterval),
//        DayTimeIntervalType()), StringType), ansiInterval)
//    }
//
    if (!isTryCast) {
      Seq(""INTERVAL '-106751991 04:00:54.775809' DAY TO SECOND"",
        ""INTERVAL '106751991 04:00:54.775808' DAY TO SECOND"").foreach { interval =>
        val e = intercept[ArithmeticException] {
          cast(Literal.create(interval), DayTimeIntervalType()).eval()
        }.getMessage
        assert(e.contains(""long overflow""))
      }
    }

    Seq(Byte.MaxValue, Short.MaxValue, Int.MaxValue, Long.MaxValue, Long.MinValue + 1,
      Long.MinValue).foreach { duration =>
      val interval = Literal.create(
        Duration.of(duration, ChronoUnit.MICROS),
        DayTimeIntervalType())
      checkEvaluation(cast(cast(interval, StringType), DayTimeIntervalType()), duration)
    }
  }

  test(""SPARK-35735: Take into account day-time interval fields in cast2"") {
    def typeName(dataType: DayTimeIntervalType): String = {
      if (dataType.startField == dataType.endField) {
        DayTimeIntervalType.fieldToString(dataType.startField).toUpperCase(Locale.ROOT)
      } else {
        s""${DayTimeIntervalType.fieldToString(dataType.startField)} TO "" +
          s""${DayTimeIntervalType.fieldToString(dataType.endField)}"".toUpperCase(Locale.ROOT)
      }
    }

    Seq((""1"", DayTimeIntervalType(DAY, DAY), (86400) * MICROS_PER_SECOND),
      (""-1"", DayTimeIntervalType(DAY, DAY), -(86400) * MICROS_PER_SECOND),
      (""1 01"", DayTimeIntervalType(DAY, HOUR), (86400 + 3600) * MICROS_PER_SECOND),
      (""-1 01"", DayTimeIntervalType(DAY, HOUR), -(86400 + 3600) * MICROS_PER_SECOND),
      (""1 01:01"", DayTimeIntervalType(DAY, MINUTE), (86400 + 3600 + 60) * MICROS_PER_SECOND),
      (""-1 01:01"", DayTimeIntervalType(DAY, MINUTE), -(86400 + 3600 + 60) * MICROS_PER_SECOND),
      (""1 01:01:01.12345"", DayTimeIntervalType(DAY, SECOND),
        ((86400 + 3600 + 60 + 1.12345) * MICROS_PER_SECOND).toLong),
      (""-1 01:01:01.12345"", DayTimeIntervalType(DAY, SECOND),
        (-(86400 + 3600 + 60 + 1.12345) * MICROS_PER_SECOND).toLong),

      (""01"", DayTimeIntervalType(HOUR, HOUR), (3600) * MICROS_PER_SECOND),
      (""-01"", DayTimeIntervalType(HOUR, HOUR), -(3600) * MICROS_PER_SECOND),
      (""01:01"", DayTimeIntervalType(HOUR, MINUTE), (3600 + 60) * MICROS_PER_SECOND),
      (""-01:01"", DayTimeIntervalType(HOUR, MINUTE), -(3600 + 60) * MICROS_PER_SECOND),
      (""01:01:01.12345"", DayTimeIntervalType(HOUR, SECOND),
        ((3600 + 60 + 1.12345) * MICROS_PER_SECOND).toLong),
      (""-01:01:01.12345"", DayTimeIntervalType(HOUR, SECOND),
        (-(3600 + 60 + 1.12345) * MICROS_PER_SECOND).toLong),

      (""01"", DayTimeIntervalType(MINUTE, MINUTE), (60) * MICROS_PER_SECOND),
      (""-01"", DayTimeIntervalType(MINUTE, MINUTE), -(60) * MICROS_PER_SECOND),
      (""01:01"", DayTimeIntervalType(MINUTE, SECOND), ((60 + 1) * MICROS_PER_SECOND)),
      (""01:01.12345"", DayTimeIntervalType(MINUTE, SECOND),
        ((60 + 1.12345) * MICROS_PER_SECOND).toLong),
      (""-01:01.12345"", DayTimeIntervalType(MINUTE, SECOND),
        (-(60 + 1.12345) * MICROS_PER_SECOND).toLong),

      (""01.12345"", DayTimeIntervalType(SECOND, SECOND), ((1.12345) * MICROS_PER_SECOND).toLong),
      (""-01.12345"", DayTimeIntervalType(SECOND, SECOND), (-(1.12345) * MICROS_PER_SECOND).toLong))
      .foreach { case (str, dataType, dt) =>
//        checkEvaluation(cast(Literal.create(str), dataType), dt)
        checkEvaluation(
          cast(Literal.create(s""INTERVAL '$str' ${typeName(dataType)}""), dataType), dt)
        checkEvaluation(
          cast(Literal.create(s""INTERVAL -'$str' ${typeName(dataType)}""), dataType), -dt)
      }

    // Check max value
    Seq((""INTERVAL '106751991' DAY"", DayTimeIntervalType(DAY), 106751991L * MICROS_PER_DAY),
      (""INTERVAL '106751991 04' DAY TO HOUR"", DayTimeIntervalType(DAY, HOUR), 9223372036800000000L),
      (""INTERVAL '106751991 04:00' DAY TO MINUTE"",
        DayTimeIntervalType(DAY, MINUTE), 9223372036800000000L),
      (""INTERVAL '106751991 04:00:54.775807' DAY TO SECOND"", DayTimeIntervalType(), Long.MaxValue),
      (""INTERVAL '2562047788' HOUR"", DayTimeIntervalType(HOUR), 9223372036800000000L),
      (""INTERVAL '2562047788:00' HOUR TO MINUTE"",
        DayTimeIntervalType(HOUR, MINUTE), 9223372036800000000L),
      (""INTERVAL '2562047788:00:54.775807' HOUR TO SECOND"",
        DayTimeIntervalType(HOUR, SECOND), Long.MaxValue),
      (""INTERVAL '153722867280' MINUTE"", DayTimeIntervalType(MINUTE), 9223372036800000000L),
      (""INTERVAL '153722867280:54.775807' MINUTE TO SECOND"",
        DayTimeIntervalType(MINUTE, SECOND), Long.MaxValue),
      (""INTERVAL '9223372036854.775807' SECOND"", DayTimeIntervalType(SECOND), Long.MaxValue))
      .foreach { case (interval, dataType, dt) =>
        checkEvaluation(cast(Literal.create(interval), dataType), dt)
        checkEvaluation(cast(Literal.create(interval.toLowerCase(Locale.ROOT)), dataType), dt)
      }

    Seq((""INTERVAL '-106751991' DAY"", DayTimeIntervalType(DAY), -106751991L * MICROS_PER_DAY),
      (""INTERVAL '-106751991 04' DAY TO HOUR"",
        DayTimeIntervalType(DAY, HOUR), -9223372036800000000L),
      (""INTERVAL '-106751991 04:00' DAY TO MINUTE"",
        DayTimeIntervalType(DAY, MINUTE), -9223372036800000000L),
      (""INTERVAL '-106751991 04:00:54.775808' DAY TO SECOND"", DayTimeIntervalType(), Long.MinValue),
      (""INTERVAL '-2562047788' HOUR"", DayTimeIntervalType(HOUR), -9223372036800000000L),
      (""INTERVAL '-2562047788:00' HOUR TO MINUTE"",
        DayTimeIntervalType(HOUR, MINUTE), -9223372036800000000L),
      (""INTERVAL '-2562047788:00:54.775808' HOUR TO SECOND"",
        DayTimeIntervalType(HOUR, SECOND), Long.MinValue),
      (""INTERVAL '-153722867280' MINUTE"", DayTimeIntervalType(MINUTE), -9223372036800000000L),
      (""INTERVAL '-153722867280:54.775808' MINUTE TO SECOND"",
        DayTimeIntervalType(MINUTE, SECOND), Long.MinValue),
      (""INTERVAL '-9223372036854.775808' SECOND"", DayTimeIntervalType(SECOND), Long.MinValue))
      .foreach { case (interval, dataType, dt) =>
        checkEvaluation(cast(Literal.create(interval), dataType), dt)
      }

    if (!isTryCast) {
      Seq(
        (""INTERVAL '1 01:01:01.12345' DAY TO SECOND"", DayTimeIntervalType(DAY, HOUR)),
        (""INTERVAL '1 01:01:01.12345' DAY TO HOUR"", DayTimeIntervalType(DAY, SECOND)),
        (""INTERVAL '1 01:01:01.12345' DAY TO MINUTE"", DayTimeIntervalType(DAY, MINUTE)),
        (""1 01:01:01.12345"", DayTimeIntervalType(DAY, DAY)),
        (""1 01:01:01.12345"", DayTimeIntervalType(DAY, HOUR)),
        (""1 01:01:01.12345"", DayTimeIntervalType(DAY, MINUTE)),

        (""INTERVAL '01:01:01.12345' HOUR TO SECOND"", DayTimeIntervalType(DAY, HOUR)),
        (""INTERVAL '01:01:01.12345' HOUR TO HOUR"", DayTimeIntervalType(DAY, SECOND)),
        (""INTERVAL '01:01:01.12345' HOUR TO MINUTE"", DayTimeIntervalType(DAY, MINUTE)),
        (""01:01:01.12345"", DayTimeIntervalType(DAY, DAY)),
        (""01:01:01.12345"", DayTimeIntervalType(HOUR, HOUR)),
        (""01:01:01.12345"", DayTimeIntervalType(DAY, MINUTE)),
        (""INTERVAL '1.23' DAY"", DayTimeIntervalType(DAY)),
        (""INTERVAL '1.23' HOUR"", DayTimeIntervalType(HOUR)),
        (""INTERVAL '1.23' MINUTE"", DayTimeIntervalType(MINUTE)),
        (""INTERVAL '1.23' SECOND"", DayTimeIntervalType(MINUTE)),
        (""1.23"", DayTimeIntervalType(DAY)),
        (""1.23"", DayTimeIntervalType(HOUR)),
        (""1.23"", DayTimeIntervalType(MINUTE)),
        (""1.23"", DayTimeIntervalType(MINUTE)))
        .foreach { case (interval, dataType) =>
          val e = intercept[IllegalArgumentException] {
            cast(Literal.create(interval), dataType).eval()
          }.getMessage
          assert(e.contains(s""Interval string does not match day-time format of "" +
            s""${IntervalUtils.supportedFormat((dataType.startField, dataType.endField))
              .map(format => s""`$format`"").mkString("", "")} "" +
            s""when cast to ${dataType.typeName}: $interval, "" +
            s""set ${SQLConf.LEGACY_FROM_DAYTIME_STRING.key} to true "" +
            ""to restore the behavior before Spark 3.0.""))
        }

      // Check first field outof bound
      Seq((""INTERVAL '1067519911' DAY"", DayTimeIntervalType(DAY)),
        (""INTERVAL '10675199111 04' DAY TO HOUR"", DayTimeIntervalType(DAY, HOUR)),
        (""INTERVAL '1067519911 04:00' DAY TO MINUTE"", DayTimeIntervalType(DAY, MINUTE)),
        (""INTERVAL '1067519911 04:00:54.775807' DAY TO SECOND"", DayTimeIntervalType()),
        (""INTERVAL '25620477881' HOUR"", DayTimeIntervalType(HOUR)),
        (""INTERVAL '25620477881:00' HOUR TO MINUTE"", DayTimeIntervalType(HOUR, MINUTE)),
        (""INTERVAL '25620477881:00:54.775807' HOUR TO SECOND"", DayTimeIntervalType(HOUR, SECOND)),
        (""INTERVAL '1537228672801' MINUTE"", DayTimeIntervalType(MINUTE)),
        (""INTERVAL '1537228672801:54.7757' MINUTE TO SECOND"", DayTimeIntervalType(MINUTE, SECOND)),
        (""INTERVAL '92233720368541.775807' SECOND"", DayTimeIntervalType(SECOND)))
        .foreach { case (interval, dataType) =>
          val e = intercept[IllegalArgumentException] {
            cast(Literal.create(interval), dataType).eval()
          }.getMessage
          assert(e.contains(s""Interval string does not match day-time format of "" +
            s""${IntervalUtils.supportedFormat((dataType.startField, dataType.endField))
              .map(format => s""`$format`"").mkString("", "")} "" +
            s""when cast to ${dataType.typeName}: $interval, "" +
            s""set ${SQLConf.LEGACY_FROM_DAYTIME_STRING.key} to true "" +
            ""to restore the behavior before Spark 3.0.""))
        }
    }
  }
```",,thirtiseven,7326403,MDQ6VXNlcjczMjY0MDM=,https://avatars.githubusercontent.com/u/7326403?v=4,,https://api.github.com/users/thirtiseven,https://github.com/thirtiseven,https://api.github.com/users/thirtiseven/followers,https://api.github.com/users/thirtiseven/following{/other_user},https://api.github.com/users/thirtiseven/gists{/gist_id},https://api.github.com/users/thirtiseven/starred{/owner}{/repo},https://api.github.com/users/thirtiseven/subscriptions,https://api.github.com/users/thirtiseven/orgs,https://api.github.com/users/thirtiseven/repos,https://api.github.com/users/thirtiseven/events{/privacy},https://api.github.com/users/thirtiseven/received_events,User,False,https://api.github.com/repos/NVIDIA/spark-rapids/issues/comments/2149002948/reactions,0,0,0,0,0,0,0,0,0,10980
